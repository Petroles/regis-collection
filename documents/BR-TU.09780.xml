<?xml version="1.0" encoding="utf-8"?>
<add>
	<doc>
		<field name="docid">BR-TU.09780</field>
		<field name="filename">14774_000710038.pdf</field>
		<field name="filetype">PDF</field>
		<field name="text">
UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL 

INSTITUTO DE INFORMÁTICA 

PROGRAMA DE PÓS-GRADUAÇÃO EM COMPUTAÇÃO 

 

 

 

 

 

SANDRO RAMA FIORINI 

 

 

 

S-Chart: Um Arcabouço para 
Interpretação Visual de Gráficos 

 

 

 

 

Dissertação apresentada como requisito 
parcial para a obtenção do grau de Mestre 
em Ciência da Computação 

 

 

Prof. Drª. Mara Abel 

Orientadora 

 

 

Prof. Dr. Claiton M. dos Santos Scherer 

Co-orientador 

 

Porto Alegre, março de 2009.



CIP – CATALOGAÇÃO NA PUBLICAÇÃO 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL 
Reitor: Prof. Carlos Alexandre Netto 
Vice-Reitor: Prof. Rui Vicente Oppermann 
Pró-Reitor de Pós-Graduação: Prof. Aldo Bolten Lucion 
Diretor do Instituto de Informática: Prof. Flávio Rech Wagner 
Coordenador do PPGC: Prof. Álvaro Freitas Moreira 
Bibliotecária-Chefe do Instituto de Informática: Beatriz Regina B. Haro 

Fiorini, Sandro Rama 

S-Chart: Um Arcabouço para Interpretação Visual de Grá-
ficos / Sandro Rama Fiorini. Porto Alegre: Programa de Pós-
Graduação em Computação, 2009. 

122 p.:il. 

Dissertação (mestrado) – Universidade Federal do Rio 
Grande do Sul. Programa de Pós-Graduação em Computa-
ção. Porto Alegre, BR – RS, 2009. Orientador: Prof. Drª. Mara 
Abel; Co-orientador: Prof. Dr. Claiton M. dos Santos Scherer. 

1.Visão Computacional. 2.Interpretação Semântica de 
Gráficos. 3.Representação de Conhecimento Visual. I. Abel, 
Mara.  II. Scherer, Claiton. III. Título. 



AGRADECIMENTOS 

Vejo os agradecimentos que faço aqui como uma continuação dos agradeci-
mentos que fiz no meu trabalho de conclusão. No entanto, agora tenho um pro-
blema mais complicado, pois agradecer a todos que contribuem em um trabalho 
deste tamanho é quase impossível.  

Inicialmente devo agradecer a UFRGS e o Instituto de Informática por mais 
dois anos onde me propiciaram o melhor dos ambientes para o desenvolvimen-
to do trabalho apresentado nesta dissertação. Agora tenho ainda mais orgulho 
de estar ligado a essas instituições. Agradeço também ao CNPq, por ter financi-
ado o meu trabalho por esses dois. 

Sou grato a todos os colegas e amigos que estiveram comigo ao longo desses 
dois anos no grupo de Bancos de Dados Inteligentes: Alexandre Lorenzatti, Joel 
Carbonera, Felipe Victoreti, Carlos Santin, Marília Mello, Laura Mastella, Willi-
an Gonçalves e Bruno Nunes! Todos foram e são muito importantes. Sem toda 
essa turma certamente este trabalho seria muito mais difícil de ser completado.  

Gostaria de agradecer ao meu co-orientador, o Professor Claiton Scherer, do 
Instituto de Geociências, pelo tempo despendido tentando ensinar um pouco de 
estratigrafia para um simples mestrando de computação. Sem ele, este trabalho 
seria impossível. 

Fechando os agradecimentos acadêmicos, devo fazer um agradecimento es-
pecial a minha orientadora Professora Mara Abel. Já passados mais de seis anos 
de orientação, devo admitir que os seus conselhos e desafios formaram grande 
– se não a maior – parte do meu perfil profissional.  

Amigos novamente! Posso recortar e colar essa parte dos agradecimentos 
que escrevi no meu trabalho de conclusão. Ter amizades duradouras é a chave 
para a manutenção da sanidade em um trabalho de pós-graduação. Enilton, o 
mais antigo e parceiro para idéias infantis e sem a menor possibilidade de reali-
zação ?. Eduardo Castro, que desde os primórdios do colégio é o meu eterno 
“colega”: no colégio, na faculdade, do laboratório, de trabalho. Grande parceiro 
para ouvir as minhas idéias caóticas que conduziram finalmente a este trabalho. 
Ao Grupo dos Cinco, composto pelos meus amigos Bill, Francisco, Gustavo e 
Leandro. Agora já mais diluído pelas responsabilidades, ainda é a minha prin-
cipal fonte de risadas e relaxamento intelectual. Obrigado de novo pela compa-
nhia e apoio de vocês. 



 4

Agradeço também a minha família, especialmente à minha tia Maria e ao 
meu primo Fabian pelo apoio e pela companhia que não consigo encontrar em 
outro lugar. 

Agradeço aos meus pais: o seu Olir e a dona Marlene. Não vou repetir o que 
já agradeci antes. Seria “chover no molhado”. Sou imensamente grato pelo seu 
apoio nesses últimos anos, mesmo quando o filho não entendia isso direito.  

Por fim, apenas para deixar registrado para o futuro: meu agnosticismo ain-
da continua.  



 

 

SUMÁRIO

LISTA DE ABREVIATURAS E SIGLAS ............................................................ 7 
LISTA DE FIGURAS .......................................................................................... 8 
LISTA DE TABELAS ....................................................................................... 10 
RESUMO.......................................................................................................... 11 
ABSTRACT ...................................................................................................... 12 
1  INTRODUÇÃO .................................................................................... 13 
1.1  Modelos de conhecimento visual em três níveis .......................................... 14 
1.2  Interpretação semântica de gráficos ............................................................ 16 
1.3  Objetivo ........................................................................................................... 19 
1.4  Domínio de aplicação: sistema InteliStrata ................................................. 19 
1.5  Organização dos capítulos ............................................................................. 20 

2  INTERPRETAÇÃO SEMÂNTICA DE IMAGENS................................ 21 
2.1  Representação de Conhecimento Visual ...................................................... 24 
2.2  Abordagens de interpretação semântica ...................................................... 25 
2.2.1  Abordagem 1: Agregados ............................................................................ 25 
2.2.2  Abordagem 2: Abdução ............................................................................... 27 
2.2.3  Abordagem 3: Espaços Conceituais ........................................................... 29 
2.2.4  Abordagem 4: Abordagem Orion ............................................................... 31 
2.3  Discussão e Comparação entre Abordagens de Interpretação Semântica 
de Imagens ..................................................................................................................... 34 

3  S-Chart: UM MODELO PARA INTERPRETAÇÃO SEMÂNTICA DE 
GRÁFICOS ...................................................................................................... 37 
3.1  Linguagem de Representação ....................................................................... 38 
3.2  Arquitetura do Framework S-Chart ............................................................ 39 
3.2.1  Nível semântico ............................................................................................. 40 
3.2.2  Nível visual .................................................................................................... 41 
3.2.3  Nível analógico .............................................................................................. 46 
3.2.4  Mapeamento e ancoramento simbólico ..................................................... 49 
3.2.5  Componente de Interpretação ..................................................................... 54 

4  ESTRATIGRAFIA DE SEQUÊNCIAS ................................................. 61 
4.1  Apresentação do domínio .............................................................................. 61 
4.2  Interpretação de sequências em perfis de raios gama ................................ 63 



 

 

4.3  Análise de perfis por wavelets ....................................................................... 64 
4.3.1  Detecção de sequências: wavelet gaussiana .............................................. 66 
4.3.2  Detecção de parassequências: wavelet smoothtooth ............................... 67 

5  SISTEMA INTELISTRATA ................................................................. 70 
5.1  Objetivos do sistema ...................................................................................... 70 
5.2  Arquitetura do sistema .................................................................................. 71 
5.2.1  Modelos de representação e extensões ...................................................... 72 
5.2.2  Componentes de infra-estrutura ................................................................. 78 
5.2.3  Componentes de interpretação ................................................................... 81 
5.3  Validação do sistema InteliStrata ................................................................. 84 
5.3.1  Preparação dos perfis ................................................................................... 84 
5.3.2  Avaliação dos resultados e validação ........................................................ 85 

6  CONCLUSÃO ..................................................................................... 89 
6.1  Framework S-Chart e modelos de conhecimento visual ............................ 89 
6.2  Sugestão para trabalhos futuros ................................................................... 90 

REFERÊNCIAS ................................................................................................ 92 
ANEXO  METAMODELO DA LINUGAGEM OWL .......................................... 96 
APÊNDICE  DESCRIÇÃO RDF/XML DOS MODELOS OWL DE 
CONHECIMENTO VISUAL DO SISTEMA INTELISTRATA ............................ 97 
B.1  Modelo do nível analógico ............................................................................. 97 
B.1.1  Modelo OWL do nível analógico ................................................................ 97 
B.1.2  Modelo OWL para processamento de sinais .......................................... 100 
B.2  Modelo do nível visual ................................................................................. 101 
B.3  Modelo do nível semântico .......................................................................... 110 
B.3.1  Modelo OWL do nível semântico ............................................................. 110 
B.3.2  Modelo OWL do domínio .......................................................................... 110 
B.4  Modelo de ancoramento simbólico ............................................................. 113 
B.5  Modelos Adicionais ...................................................................................... 115 
B.5.1  Modelo OWL do sistema InteliStrata ....................................................... 115 
B.5.2  Modelo OWL de relações de ordem (desigualdades) ........................... 118 
B.5.3  Detectores simbólicos ................................................................................. 119 



 

 

LISTA DE ABREVIATURAS E SIGLAS 

OWL Ontology Web Language 

SWRL Semantic Web Rule Language 

DL Description Logics 

MFS Maximum Flooding Durface 

MIS Máquina de Interpretação Semântica 

MIV  Máquina de Interpretação Visual 

TWC Transformada Wavelet Contínua 

 

 



 

 

LISTA DE FIGURAS

Figura 1.1: Exemplo de representação em três níveis semânticos. ....................... 15 

Figura 1.2: Representação gráfica de dados numéricos. ........................................ 16 

Figura 1.3: Diferenças entre interpretação de imagens naturais e gráficos. ........ 18 

Figura 2.1: Exemplo de interpretação por ancoramento simbólico. ..................... 23 

Figura 2.2: Exemplo de hierarquia de composição. ................................................ 27 

Figura 2.3: Passos para extração das linhas como primitivas de baixo nível de 
uma imagem. ............................................................................................ 28 

Figura 2.4: Representação de um martelo em um espaço superquadrático e 
conceitual. ................................................................................................. 29 

Figura 2.5: Fragmento do componente terminológico da abordagem, utilizando 
a representação gráfica de KL-ONE. ..................................................... 30 

Figura 2.6: Exemplo de representação de uma laranja em uma imagem 
utilizando três níveis semânticos. .......................................................... 32 

Figura 2.7. Exemplo de mapeamento do conceito Superfície Elíptica do nível 
visual para o nível analógico utilizando conjuntos fuzzy. ................. 34 

Figura 3.1: Componentes da arquitetura do framework S-Chart. .......................... 40 

Figura 3.2: Taxonomia de entidades visuais. ........................................................... 42 

Figura 3.3: Taxonomia de propriedade visuais com definição de contradomínio 
de valores. A especialização do domínio é omitida. ........................... 43 

Figura 3.4: Exemplo de modelagem de contradomínios de propriedades visuais.
 .................................................................................................................... 44 

Figura 3.5: Extensão da modelagem de contradomínios de propriedades visuais.
 .................................................................................................................... 44 

Figura 3.6: Taxonomia de relações espaciais. .......................................................... 45 

Figura 3.7: Taxonomia de entidades analógicas e relações de composição. ....... 46 

Figura 3.8: Taxonomia básica de funcionalidades de processamento de sinal... 49 

Figura 3.9: Arquitetura do componente de interpretação. .................................... 55 

Figura 4.1: Perfil típico em diferentes escalas. Raios gama em destaque. ........... 61 



 

 

Figura 4.2: Expressões genéricas das feições geológicas no perfil. ....................... 64 

Figura 4.3: Wavelet gussiana 2. ................................................................................. 65 

Figura 4.4: Transformada wavelet gaussiana 2 em um perfil de raios gama. ...... 66 

Figura 4.5: Transformada wavelet sawtooth em um perfil de raios gama. ........ 68 

Figura 5.1: Arquitetura de componentes do sistema InteliStrata. ........................ 71 

Figura 5.2: Dependências entre ontologias OWL do sistema InteliStrata. .......... 72 

Figura 5.3: Ontologia da Estratigrafia. ...................................................................... 73 

Figura 5.4: Modelo para representação de dados brutos. ...................................... 75 

Figura 5.5: Componentes de infra-estrutura do sistema InteliStrata. .................. 79 

Figura 5.6: Componentes de interpretação do sistema InteliStrata. ..................... 81 

Figura 5.7: Interpretação de parte do perfil Tenneco Rattlesnake State 2-12. ......... 83 

Figura 5.8: Comparação das interpretações do especialista e do sistema 
InteliStrata para Seção 1. ......................................................................... 87 

Figura 5.9: Comparação das interpretações do especialista e do sistema 
InteliStrata para Seção 2. ......................................................................... 88 



 

 

LISTA DE TABELAS

Tabela 2.1: Tabela de comparação entre as quatro abordagens estudadas. ........ 36 

Tabela 3.1: Primitivas OWL necessárias para representação de primitivas 
visuais do framework S-Chart. .............................................................. 39 

Tabela 3.2: Avaliação da função fuzzy:associa. .................................................. 54 

Tabela 3.3: Variáveis de contexto para interpretação semântica .......................... 56 



 

 

 

RESUMO 

Interpretação semântica de imagens tem se mostrado uma das fronteiras mais 
promissoras da área de Visão Computacional, especificamente aplicada a inter-
pretação imagens. Nas abordagens que estão sendo propostas atualmente, co-
nhecimento visual explicitamente modelado é utilizado com algoritmos de ra-
ciocínio simbólico combinados a algoritmos de processamento de imagem a fim 
de se extrair o conteúdo de imagens e associá-lo a modelos semanticamente ri-
cos. 

Este trabalho apresenta uma abordagem de interpretação semântica de ima-
gens especificamente voltada para interpretação de gráficos de linhas, chamada 
S-Chart. Ela consiste um conjunto de modelos de conhecimento e algoritmos 
que podem ser instanciados para interpretação de gráficos em diversos domí-
nios. Os modelos são representados em três níveis semânticos e aplicam o con-
ceito de ancoramento simbólico (symbol grounding) para mapear as primitivas 
entre os níveis. Os algoritmos de interpretação propostos fazem a interação en-
tre o raciocínio simbólico de alto nível e os algoritmos de processamento de si-
nal para os dados brutos dos gráficos analisados. 

Para demonstrar a aplicabilidade do framework S-Chart, foi desenvolvido o 
sistema InteliStrata, uma aplicação no domínio da Geologia, voltada para inter-
pretação semântica de gráficos de perfis de poço.  Utilizando a aplicação, foram 
interpretados dois perfis de raios gama capturados em poços de exploração, de 
modo que o sistema identificasse a presença de Sequências Estratigráficas e su-
perfícies de inundação máximas. Os resultados foram comparados com a inter-
pretação de um geólogo especialista sobre os mesmos dados. O sistema aponta 
as mesmas sequências já identificadas e oferece outras opções de interpretação 
compatíveis com as do geólogo utilizando os mesmos dados.  

O framework S-Chart tem seus pontos fortes nos seus modelos representa-
ção de conhecimento visual independentes de domínio, que permitem a utiliza-
ção do mesmo arcabouço em diferentes aplicações e, em especial, no seu mode-
lo de ancoramento simbólico entre primitivas de representação. 

Palavras-Chave: Interpretação Semântica de Gráficos, Conhecimento Visual, 
Modelos de Conhecimento, Ancoramento Simbólico, Visão Computacional 



 

 

 

S-Chart: A Framework for Visual Interpretation of Line Charts  

ABSTRACT 

Semantic image interpretation is one of the most promising frontiers in the 
Computer Vision area, specifically when applied to Image Interpretation. To 
reach semantic interpretation, visual knowledge explicitly represented is ap-
plied by symbolic reasoning algorithms combined with image processing algo-
rithms in order to extract the content of the images and associate it with seman-
tically rich models. 

This work describes the S-Chart approach, a semantic image interpretation 
approach designed for interpretation of line charts. It is structured as a set of 
knowledge models and algorithms that can be instantiated to accomplish chart 
interpretation in other domains. The models are represented in three semantic 
levels and apply the concept of symbol grounding in order to map the primi-
tives between the levels. The interpretation algorithms carry out the interaction 
between the symbolic reasoning in the high level, and the signal processing al-
gorithms in the low level data. 

In order to demonstrate the applicability of the S-Chart framework, we de-
veloped the InteliStrata system, an application in Geology for the semantic in-
terpretation of well log profiles.  Using the application, we have interpreted the 
graphs of two gamma-ray profiles captured in exploration wells, to indicate the 
position of Stratigraphic Sequences and the maximum flooding surfaces. The 
results were compared with the interpretation of an experienced geologist using 
the same data input. The system was able to point the same identified se-
quences and offered alternative interpretation that were compatible with the 
geologist interpretation over the data.   

The S-Chart framework demonstrates its effectiveness on interpretation of 
pictorial information in knowledge intensive domains. The stronger points of 
the approach are its domain independent models for visual knowledge repre-
sentation and, specially, the application of a symbol grounding model to pro-
vide a correlation between representation primitives. 

 

Keywords: Semantic Chart Interpretation, Visual Knowledge, Knowledge 
Models, Symbol Grounding, Computer Vision. 



 

 

 

1 INTRODUÇÃO 

Este trabalho descreve um estudo na área de visão computacional. Mais es-
pecificamente, sobre representação de conhecimento visual e interpretação se-
mântica de informações pictoriais.  

Por muitos anos, um dos principais desafios de pesquisa na computação foi 
a busca por meios eficientes para armazenagem de grandes quantidade de da-
dos, textuais e não textuais. Tendo atingido um patamar razoável de desenvol-
vimento nesta área, o foco passou a ser maneiras eficientes de extrair informa-
ções úteis dessa imensa quantidade de dados, armazenadas durante anos em 
bases distribuídas pelo mundo. Em se tratando de dados textuais, é significante 
o grau de desenvolvimento de ferramentas para extração de conteúdo, como 
ferramentas de busca e descoberta de conhecimento. Da mesma forma, com o 
avanço nas capacidades de armazenamento, as mesmas bases de dados estão 
cada vez mais repletas de dados não-textuais, como imagem, som e vídeo, cujo 
conteúdo também deve ser extraído e interpretado. No entanto, no caso de da-
dos pictoriais, foco deste trabalho, ainda são relativamente raros os resultados 
de pesquisas na extração e interpretação do seu conteúdo.  

Grande parte dos trabalhos na área de extração e interpretação de conteúdo 
de imagens que focam em métodos puramente numéricos e estocásticos, como 
em Gomez &amp;amp; Dactu (2007) e Fan et al. (2005), sem dar a devida atenção aos as-
pectos relacionado a modelagem semântica do conteúdo. Enquanto tais méto-
dos apresentam resultados positivos em domínios visualmente menos comple-
xos, elas falham em domínios onde o acesso ao conteúdo das imagens requer 
uma grande quantidade de conhecimento visual. Estes são denominados domí-
nios imagísticos (YIP; ZHAO, 1996), como é o caso da medicina (OGIELA; 
TADEUSIEWICZ, 2003), geologia (SANTIN, 2008), vigilância (WITHAGEN, 
2006) e etc. 

Perceber porque os métodos de interpretação citados falham em domínios 
imagísticos é relativamente simples. Embora o exato mecanismo de funciona-
mento da visão humana (e de outros processos sensórios) ainda seja disputado 
pela ciência (BARSALOU, 1999), é possível entendê-la como um processo que 
envolve dois componentes: um componente sensório e um cognitivo. O compo-
nente sensório se encarrega do processamento básico da visão (como detecção 
de regiões, bordas, objetos em movimento e etc.). O componente cognitivo apli-
ca conhecimento visual a respeito do domínio onde a cena visualizada se encon-
tra e, com base nele, interpreta as informações sensórias em símbolos que repre-



14 

 

 

sentam o seu conteúdo. O fluxo de informações entre esses componentes é cir-
cular. Enquanto o componente sensório processa e repassa informações visuais 
básicas para o componente cognitivo, este guia o processamento do componen-
te sensório influindo na seleção das informações que serão processadas. Usual-
mente, estes dois componentes não existem nas abordagens clássicas. As técni-
cas puramente baseadas em processamento de imagens podem ser comparadas 
ao componente sensório. Elas produzem interpretações baseadas somente nos 
dados brutos das imagens, sem utilizar – ao menos explicitamente – conheci-
mento visual sobre a imagem, como contexto, relações espaciais e etc. Esse pro-
cesso é especialmente importante em domínios imagísticos, onde a interpreta-
ção de dados visuais (imagens, cenas e etc.) é fortemente baseada em conheci-
mento visual sobre o domínio. Por exemplo, devido ao treinamento e experiên-
cias, médicos conseguem visualizar estruturas em imagens de raios-x que são 
inacessíveis a um leigo no assunto. Isso demonstra o quão necessário é haver 
uma integração entre o processamento de baixo nível e mecanismos de simbóli-
cos de alto nível em sistemas de interpretação de imagens. No entanto, os tipos 
de sistemas citados anteriormente (numéricos e estocásticos) falham em atingir 
este objetivo, pois não implementam completamente componentes mínimos da 
visão.  

Evidentemente, as abordagens clássicas ainda continuam sendo aprimora-
das. Porém, atualmente é possível encontrar sistemas de visão computacional 
que buscam incorporar os dois componentes da visão. Estes sistemas são refe-
renciados como sistemas de interpretação semântica de imagens, pois, além do 
processamento numérico, aplicam conhecimento visual para extrair o conteúdo 
de imagens e representá-lo em modelos semanticamente ricos. Eles buscam in-
tegrar modelos de conhecimento visual com algoritmos de processamento de 
imagens, a fim de implementar um mecanismo de interpretação automático 
inspirado na visão humana. Estes modelos de conhecimento visual descrevem 
as entidades existentes no domínio das imagens, bem como as suas feições vi-
suais e inter-relações espaciais. Como será apresentado nos próximos capítulos, 
sistemas de interpretação semântica são mais eficientes para interpretação em 
domínios imagísticos. 

1.1 Modelos de conhecimento visual em três níveis 
Existem diversas formas de se representar conhecimento visual. Contudo 

grande parte das abordagens de interpretação semântica utiliza modelos de repre-
sentação em três níveis semânticos. Cada nível semântico representa em um grau 
diferente de abstração as entidades contidas em uma imagem.  

O nível semântico mais alto é o nível semântico propriamente dito. Ele repre-
senta as entidades do domínio da imagem. Usualmente, os modelos nesse nível 
modelam o conhecimento de domínio independe de aspectos visuais.  

O nível mais baixo de abstração é representado pelo nível analógico. Ele cap-
tura informações de baixo nível de uma imagem, geralmente informações es-
sencialmente quantitativas. O conjunto de primitivas desse nível é formado por 



15 

 

 

pixel, conjuntos de pixels, códigos de cores e outros índices numéricos. Em ge-
ral, esse nível faz a integração entre o domínio dos algoritmos de interpretação 
de imagem e o domínio simbólico. 

É fácil perceber a distância semântica entre esses dois níveis opostos. Há 
uma grande diferença de propósito entre informações sobre, por exemplo, pixel 
e entidades do domínio. Para diminuir este “degrau” semântico, existe o nível 
visual intermediário. Ele expressa o conteúdo da imagem em termos de primiti-
vas visuais genéricas e compartilhadas, como formas geométricas, propriedades 
visuais e relações espaciais. Por exemplo, na Figura 1.1 a mesma entidade é ex-
pressa como um morango no nível semântico, uma região de pixels no nível a-
nalógico e como uma forma de coração com cor vermelha e textura granulada 
no nível visual. Assim, em um processo de interpretação simbólico dessa ima-
gem, o algoritmo não interpreta entidades do domínio diretamente dos dados 
analógicos da imagem, ele antes interpreta os dados analógicos em instâncias 
de primitivas visuais e, a partir destas, infere as entidades de domínio. 

 
Figura 1.1: Exemplo de representação em três níveis semânticos. 

Outro conceito relacionado aos modelos de conhecimento visual utilizado 
neste trabalho é o problema do ancoramento simbólico (tradução livre do inglês 
symbol grounding problem). Ele está relacionado ao problema da interpretação de 
um sistema de símbolos, que de acordo com Harnad (1990, 1994), está pura-
mente a cargo do interpretador. Ou seja, o símbolo por si só não reflete a sua in-
terpretação. Para que essa interpretação seja inerente ao sistema, os símbolos 
devem ser “ancorados” em um substrato sensório-motor. Como, no exemplo da 



16 

 

 

figura Figura 1.1, o símbolo “morango” só tem real significado se associado com 
as informações sensórias básicas (pixels, regiões de pixels e etc.) representadas 
pela imagem no nível analógico. Essa questão está diretamente relacionada à in-
terpretação semântica de imagens. 

Este trabalho propõe um framework para interpretação semântica, que inte-
gra os conceitos de modelos em três níveis semânticos e ancoramento simbólico. 
Este framework é voltado para um tipo específico de dados pictoriais, apresen-
tado a seguir. 

1.2 Interpretação semântica de gráficos 
As tarefas realizadas por especialistas humanos freqüentemente incluem a 

organização e interpretação de todo tipo de dados colhidos do ambiente. Esses 
dados podem variar consideravelmente na sua forma e apresentação. Em al-
guns casos, o especialista que realiza a interpretação busca maneiras de projetar 
esses dados complexos de alguma forma que evidencie feições importantes. Por 
exemplo, dado um economista que busca identificar tendências de variação de 

 
Figura 1.2: Representação gráfica de dados numéricos. 



17 

 

 

preços de um dado ativo financeiro. Analisando somente os dados brutos da sé-
rie temporal desse ativo (ex.: uma tabela simples de preços indexada pelo tem-
po), o economista dificilmente perceberá que tipo de tendência ou padrão de 
comportamento que eles apresentam (Figura 1.2a). A projeção dos dados em 
um gráfico (ex.: tempo × preço) converte a sua variação numérica em um padrão 
visual, mais fácil de ser reconhecido e interpretado. Além disso, em domínios 
complexos, o processo de interpretação visual desse tipo de dado é fortemente 
calcado em conhecimento de domínio. Ou seja, padrões visuais apresentados 
nos gráficos são reconhecidos e relacionados com entidades do domínio onde se 
dá a interpretação. Este trabalho propõe uma forma de capturar este compor-
tamento de interpretação visual realizado por um especialista ao analisar um 
gráfico como o exposto na Figura 1.2b. Mais especificamente, será demonstrado 
como algoritmos de processamento numérico destas séries de dados podem ser 
combinados com algoritmos e modelos de raciocínio simbólico. A proposta a-
presentada utiliza os princípios de interpretação semântica, de forma a combi-
nar mecanismos de extração de padrões com conhecimento de domínio, através 
de um modelo de representação em três níveis semânticos. 

A necessidade de uma abordagem de interpretação semântica específica pa-
ra gráficos como o da Figura 1.2b pode não parecer tão importante em um pri-
meiro momento. Entretanto, ela fica evidente quando se compara a natureza 
dos dados que compõem um gráfico. As abordagens de interpretação semântica 
de imagens usuais focam especificamente na interpretação de imagens natu-
rais1, digitalizadas por câmeras e outros dispositivos de captura. Nesse caso, os 
dados gerados são representados em uma matriz de duas dimensões, com a in-
formação de cor em cada posição da matriz (Figura 1.3a). Os algoritmos de pro-
cessamento de baixo nível (segmentação, agrupamento e etc.) geralmente anali-
sam esses dados como se este fosse um sinal de duas dimensões espaciais. Ou 
seja, uns conjuntos de valores indexados por duas dimensões contínuas. Já no 
caso dos gráficos considerados aqui, a série de dados que carrega a informação 
relevante tem essencialmente uma dimensão contínua (Figura 1.3b). Essa série 
pode ser analisada como um sinal de uma dimensão. Dessa forma, existe uma 
incompatibilidade essencial entre a natureza dos dados processados pelas a-
bordagens clássicas de interpretação semântica de imagens e os dados que são 
interpretados como gráficos. A implicação disso é que novas primitivas devem 
ser criadas a fim de que seja possível extrair e representar informações de baixo 
nível de sinais de uma dimensão contínua. De forma semelhante, a interpreta-
ção visual de gráficos requer primitivas visuais específicas, que capturem fei-
ções como tipos de curvas e suas propriedades. Este trabalho propõe novas 
primitivas analógicas e visuais para dar suporte a interpretação semântica de 

                                                 

 

1 O termo imagem natural se refere aqui a representações que capturam cenas de uma reali-
dade física. Fotos ou mesmo pinturas são exemplos de imagens naturais. 



18 

 

 

gráficos, em conjunto com alguns algoritmos para processamento e interpreta-
ção desse tipo de dados. 

Deste ponto em diante, o termo “gráfico” se refere a projeções gráficas com 
linhas como a da Figura 1.2b e o termo “sinal” ao conjunto de dados brutos que 
as geram. 

0 0 0 00

0 0 10 52

0 9 1 23

00 34 14 85

00 56 65 21

00

21

42

98

12

(i,j)

0 12 34 67 76

(i)

Interpretação 
Humana

Interpretação
Semântica

(a) Imagem Natural

(b) Gráfico

SinalVisualização

Visualização Sinal

(i,j)

(i,j)

(i) (i)

 
Figura 1.3: Diferenças entre interpretação de imagens naturais e gráficos. 



19 

 

 

1.3 Objetivo 
O objetivo principal deste trabalho é propor um framework para interpreta-

ção semântica de gráficos, chamado S-Chart,. Ele apresenta os seguintes com-
ponentes: 

• Um modelo de representação de conhecimento visual para suporte a in-
terpretação de gráficos. Este modelo é composto por três modelos repre-
sentando os níveis semânticos e um modelo de mapeamento entre estes 
níveis. Cada nível apresenta primitivas específicas para garantir a repre-
sentação de entidades da imagem em diferentes níveis de abstração. O 
modelo de mapeamento apresenta primitivas para modelagem da ligação 
entre os níveis; 

• Algoritmos de interpretação simbólicos para operacionalizar a interpre-
tação semântica com base nos modelos. Eles coordenam os algoritmos de 
processamento de imagem e os integram no processo de raciocínio sim-
bólico. 

1.4 Domínio de aplicação: sistema InteliStrata 
Este trabalho apresenta também o sistema InteliStrata, uma implementação 

do framework S-Chart voltada para interpretação de gráficos no domínio da 
Geologia, mas especificamente na Estratigrafia de Sequências. Este sistema de-
monstra como os conceitos apresentados no framework S-Chart podem ser im-
plementados em um sistema real de interpretação.  

O sistema InteliStrata representa um dos componentes de um projeto em de-
senvolvimento de um sistema de conhecimento para interpretação estratigráfica 
de bacias sedimentares a partir de dados de poço e afloramento. O sistema se 
estrutura a partir de uma ontologia de domínio de estruturas sedimentares e 
um conjunto de ferramentas de visualização e integração de dados. 

A tarefa realizada pelo sistema InteliStrata é a interpretação estratigráfica no 
contexto da Estratigrafia de Sequencias. A Estratigrafia de Sequencias é uma á-
rea de Geologia que estuda a relação entre as variações históricas do nível do 
mar na Terra e a deposição de sedimentos. A tarefa de interpretação consiste 
em analisar os dados sobre os estratos de sedimentos que compõem uma bacia 
de deposição em uma dada região e, a partir das suas características e padrões 
de empilhamento, inferir os ciclos (“sequências”) de variações do nível no mar 
na região. Essa informação é bastante importante para a atividade prospecção 
de reservatórios de petróleo.  

A análise dos estratos é feita de duas formas. Diretamente, com base em a-
mostras de rocha extraídas de poços de prospecção. Ou indiretamente, com ba-
se em medidas feitas em poços de onde não se extraem testemunhos (visto que 
este é um processo caro). Uma dessas medidas é o perfil de raios gama, uma 
medida de radioatividade da rocha feita ao longo do poço. O resultado é um 
gráfico semelhante a um eletro-encefalograma. Com base na análise visual des-



20 

 

 

se gráfico, o geólogo tenta inferir feições geológicas relativas às variações do ní-
vel do mar, como sequências e parassequências. A interpretação considera que 
progressão (avanço) ou regressão (retrocesso) do nível do mar gera registros 
com desenhos característicos nos perfis de raios gama. Esse padrão visual é re-
conhecido pelo geólogo quando realiza a interpretação.  

A tarefa de interpretação do sistema InteliStrata é, com base em um perfil de 
raios gama, sugerir interpretações de sequências e parassequências. Uma se-
quência sedimentar é uma sucessão de estratos geneticamente relacionados 
(formados na mesma época pelos mesmos processos sedimentares) cujos limites 
são definidos em resposta a uma queda relativa do nível do mar. Enquanto pa-
rassequências são os blocos de construção das sequências, limitados por oscila-
ções menores do nível do mar (DELLA FÁVERA, 2001; VAN WAGONER et al., 
1990). 

Um objetivo secundário do sistema InteliStrata é demonstrar a implementa-
ção do framework S-Chart com a utilização de formalismos e ferramentas off the 
shelf. Por exemplo, para representação dos modelos de conhecimento visual que 
compões os níveis semânticos são utilizadas linguagens padrão da Web Semân-
tica. Isso permite que ferramentas padronizadas sejam utilizadas na implemen-
tação do sistema. 

1.5 Organização dos capítulos 
O capítulo 2 aprofunda a discussão dos conceitos relacionados à interpreta-

ção semântica de imagens, já descritos anteriormente. Ela também faz uma aná-
lise de alguns frameworks para interpretação já existentes na literatura e os 
compara. 

O capítulo 3 apresenta e detalha o framework S-Chart. A sua arquitetura de 
modelos de conhecimento visual é apresentada, bem como os modelos de ma-
peamento entre níveis semânticos. Ao fim do capítulo são apresentados e deta-
lhados os algoritmos de interpretação simbólica que operacionalizam a inter-
pretação semântica. 

O capítulo 4 apresenta uma breve discussão sobre a Estratigrafia de Sequên-
cias, domínio de aplicação deste trabalho. 

O capítulo 5 descreve o sistema  InteliStrata. É demonstrado como o frame-
work S-Chart pode ser implementado em um sistema real para interpretação de 
expressões gráficas de perfis de poço na estratigrafia de sequências. Aspectos de 
sistema não abordados pelo framework são desenvolvidos nesse capítulo. Ao 
final do capítulo são analisados dois casos de interpretação de perfil utilizando 
o sistema e confrontando seus resultados com a interpretação dos mesmos per-
fis pelo especialista no domínio.  

No capítulo 6 são apresentadas as conclusões e possíveis trabalhos futuros 
para extensão dos conceitos apresentados. 



 

 

 

2 INTERPRETAÇÃO SEMÂNTICA DE IMAGENS 

O principal desafio dos sistemas de visão computacional é interpretar corre-
tamente o conteúdo de imagens. Um sistema desse tipo deve ter como saída 
uma descrição, formal ou não, dos objetos presentes em uma imagem, bem co-
mo a sua classificação e suas propriedades visuais. Por exemplo, um sistema de 
visão computacional para interpretação de imagens de satélite pode ter como 
saída quais artefatos de uma dada imagem são casas, quais são carros e quais 
são estradas, além da maneira como esses objetos estão relacionados topologi-
camente.  

Até hoje foram pesquisadas diversas formas de solução para o problema de 
interpretação de imagens. A primeira geração de sistemas de interpretação bus-
cava solucionar o problema de interpretação através da aplicação simples de al-
goritmos processamento de imagens para reconhecimento de formas geométri-
cas. No entanto, essas abordagens se mostraram ineficazes ao tentar replicar o 
comportamento humano na interpretação em domínios com tarefas de grande 
complexidade visual, como geologia e medicina. Nesses domínios, especialistas 
humanos utilizam uma grande quantidade de conhecimento de domínio – ge-
ralmente implícito – para realizar a interpretação. Para uma pessoa leiga, os ar-
tefatos presentes em uma imagem de Raios-X podem não fazer sentido algum, 
enquanto que para um médico, esses artefatos representam órgãos internos e 
possíveis doenças. O médico sabe quais as feições visuais que um órgão ou uma 
doença apresentam em uma imagem de Raio-X. Ele utiliza esse conhecimento 
visual para interpretar os artefatos da imagem em informações significativas. 
Abordagens puramente baseadas em processamento de imagens não conse-
guem capturar esse comportamento de interpretação, pois carecem de uma re-
presentação explícita do conhecimento visual do domínio onde estão sendo a-
plicados, que possa guiar a solução do problema (MAILLOT, 2005).  

A partir da década de 80, começou o surgimento dos primeiros trabalhos u-
nindo conceitos de representação de conhecimento visual com técnicas de pro-
cessamento de imagens (CREVIER; LEPAGE, 1997), dando origem aos primei-
ros sistemas de interpretação semântica de imagens, como os sistemas SIGMA e 
ACRONYM (MATSUYAMA, 1987). Esses sistemas buscavam imitar o compor-
tamento de interpretação de imagens de um especialista em domínios visual-
mente complexos. 

Um sistema de interpretação semântica integra algoritmos de processamento 
de imagem com mecanismos de raciocínio simbólico, com o intuito de capturar 



22 

 

 

o conteúdo de uma imagem através um modelo simbólico semanticamente rico. 
Enquanto os algoritmos de processamento extraem as feições visuais básicas da 
imagem (bordas, regiões, cores, texturas e etc.), os mecanismos de raciocínio 
simbólico combinam e interpretam tais feições de acordo com um modelo que 
represente conhecimento visual do domínio onde a imagem se insere. Esse mo-
delo define as entidades que podem estar presentes na imagem e os seus inter-
relacionamentos. O resultado da interpretação é um modelo simbólico, seman-
ticamente rico, do conteúdo da imagem. Uma vez capturado, esse conteúdo po-
de ser disponibilizado para consulta ou outros tipos de inferências de mais alto 
nível. 

O principal componente do processo de interpretação semântica é o modelo 
de conhecimento visual que lhe dá suporte. Esse modelo explicita as entidades 
do domínio de discurso das imagens, bem como as suas inter-relações espaciais, 
mereológicas, taxonômicas e outras relações pertinentes ao domínio, visuais ou 
não. Por exemplo, um modelo de conhecimento visual para interpretação de 
imagens de satélite certamente conterá os conceitos como Casa, Estrada e Au?
tomóvel, em conjunto com relações espaciais como aoLadoDe, sobre e próxi?
moDe. Modelos de conhecimento visual como este dão algumas vantagens aos 
sistemas de interpretação. Em primeiro lugar, eles explicitam o conhecimento 
visual utilizado no sistema, facilitando a sua depuração e manutenção. Eles 
também ajudam a melhorar o resultado do processo de interpretação como um 
todo, uma vez que podem guiar os algoritmos de processamento de baixo nível. 
No mesmo exemplo, se uma região da imagem foi identificada como uma ins-
tância de Estrada, e se o modelo especifica que casas estão sempre próximas de 
estradas, então o mecanismo de interpretação pode buscar por feições visuais 
de casas próximas a região classificada como uma estrada. 

Em geral, os modelos de conhecimento tentam proporcionar alguma forma 
de ancoramento simbólico (HARNAD, 1990, 1994) das entidades do domínio. Sis-
temas que utilizam esse conceito partem do princípio que cada símbolo do do-
mínio é reconhecido no ambiente dado algum padrão básico de impulsos nas 
suas entradas analógicas. Ou seja, as entidades de alto nível semântico são “an-
coradas” nas feições de baixo nível que indicam a sua detecção. Assim, uma vez 
que essas feições são reconhecidas nas entradas, o sistema pode assinalar a de-
tecção da entidade representada pelo símbolo. No caso de sistemas de interpre-
tação de imagens, as entidades que devem ser reconhecidas das imagens são 
mapeadas nas feições visuais que possibilitam a sua detecção, como uma região 
de pixels com alguma característica específica.  

O ancoramento simbólico é importante para a interpretação semântica, pois 
permite a interpretação seja feita em dois sentidos. No sentido top-down, as en-
tidades de alto nível indicam aos algoritmos de processamento quais são as fei-
ções visuais específicas que estão esperando. Os algoritmos buscam especifica-
mente por essas feições e indicam a sua presença ou não. Caso positivo, a enti-
dade visual pode ser indicada como reconhecida. No sentido bottom-up, em 
primeiro lugar os algoritmos de processamento extraem todas as feições possí-



23 

 

 

veis da imagem. Após isso, as feições extraídas são analisadas em busca das fei-
ções esperadas por cada uma das entidades do domínio. Nos casos onde há cor-
respondência, as entidades são marcadas como reconhecidas. No entanto, em 
muitas das abordagens de interpretação semântica são utilizados os dois senti-
dos de interpretação, iterativamente. Por exemplo, uma dada imagem de satéli-
te é analisada em um programa de processamento de imagens (Figura 2.1a), re-
sultando na extração de duas regiões de pixels. A aplicação inicial de uma in-
terpretação bottom-up interpreta os dois segmentados como os conceitos Estra?
da e Rio em um modelo simbólico (Figura 2.1b). Esse mesmo modelo define 
que na superposição de entidades do tipo Estrada e Rio pode haver uma enti-
dade do tipo Ponte. Então o mecanismo de interpretação, em uma interpretação 
no sentido top-down, ajusta o foco dos algoritmos de processamento de imagem 
para buscar uma entidade com as características visuais da entidade Ponte no 
cruzamento das duas regiões segmentadas anteriormente (Figura 2.1c). Se estas 
características visuais forem encontradas, então a interpretação de Ponte é ob-
tida. 

 
Figura 2.1: Exemplo de interpretação por ancoramento simbólico.  

(a) 

(b) 
(c) 



24 

 

 

Geralmente, uma vez que o símbolo é reconhecido em uma imagem, o algo-
ritmo realiza um mapeamento explícito entre ele e as feições visuais que lhe de-
ram origem. Isso provê uma maneira do mecanismo de interpretação – ou o seu 
usuário – revisitarem as informações que deram origem a uma dada interpreta-
ção. 

Por fim, a interpretação semântica de imagens permite associar significado 
aos dados visuais analógicos, através de modelos conceituais que representam o 
conhecimento contido nas imagens analisadas. Esses modelos conceituais dão 
suporte aos algoritmos de processamento de imagens e possibilitam a utilização 
do conhecimento visual capturado na solução de problemas e em mecanismos 
de inferência para suporte a tomada de decisão. 

2.1 Representação de Conhecimento Visual 
Uma vez que o conhecimento visual tem um papel fundamental na interpre-

tação semântica de imagens, diversos trabalhos focam em encontrar maneiras 
de incorporá-lo em sistemas de visão computacional de forma efetiva. Prova-
velmente, o maior desafio, do ponto de vista de sistema, é representá-lo de for-
ma explícita em algum tipo de estrutura com alto nível semântico e que dê su-
porte a todo o processo de raciocínio visual.  

Como apontado por Hudelot e colegas (2005), a análise de sistemas de visão 
computacional baseados em conhecimento visual (VISIONS2, SIGMA3, 
PROGAL (OSSOLA et al., 1996), MESSIE4 e etc.) induz a conclusão que os seus 
modelos de representação são estruturados em pelo menos três níveis de abs-
tração (Figura 1.1): um baixo nível analógico, um alto nível semântico e um nível 
visual intermediário. Cada nível busca tratar os aspectos de representação de 
um tipo específico de informação.  

O nível semântico, mais alto na escala de abstração, representa as entidades 
do domínio que podem ser reconhecidas na imagem. O nível analógico engloba 
as informações ligadas diretamente aos pixels que compõem a imagem, como 
bordas, valores de cor e regiões. Em geral, informações no nível analógico são 
obtidas diretamente por algoritmos de processamento de imagem. O mapea-
mento intermediário entre os dois é representado pelo nível visual. Ele é com-
posto por primitivas visuais genéricas, como formas geométricas e relações to-
pológicas  

As entidades de cada nível são mapeadas para uma ou mais entidades do 
nível inferior. Primitivas do nível semântico são definidas em termos de primi-

                                                 

 
2 (HANSON; RISEMAN, 1978). 
3 (MATSUYAMA, 1987). 
4 (SANDAKLY; GIRAUDON, 1994). 



25 

 

 

tivas no nível visual e essas, por sua vez, são definidas em termos de primitivas 
no nível analógico. De fato, a existência de um modelo intermediário especifi-
camente construído para definição de aspectos visuais de entidades do domínio 
traz diversas vantagens a sistemas de interpretação. A primeira é a diminuição 
do grande lacuna semântica (semantic gap) entre nível analógico e o nível visual, 
a fim de flexibilizar o processo de interpretação e torná-lo mais imune a ruídos 
advindos do processamento inicial nos dados analógicos (CHELLA et al., 1997). 
Ele também melhora a comunicação do sistema com o usuário, pois o ser hu-
mano utiliza informações no nível visual para representar feições visuais e se 
comunicar. Em especial, “especialistas em diferentes domínios freqüentemente 
usam e trocam um vocabulário visual genérico para descrever conceitos semân-
ticos nos seus domínios” (HUDELOT et al., 2005). Por fim, o nível visual permi-
te uma maior independência entre a representação e a aplicação, o que propor-
ciona maior reusabilidade dos modelos gerados. 

A seguir são apresentados alguns trabalhos encontrados na literatura que u-
tilizam os conceitos apresentados até aqui. 

2.2 Abordagens de interpretação semântica 
Em geral, os projetos de sistemas de interpretação semântica de imagens a-

presentam as suas propostas de representação de conhecimento visual dentro 
de um framework completo, incluindo mecanismos de inferência visual (anco-
ramento semântico, raciocínio simbólico e etc.) e algoritmos de processamento 
de imagens. 

Nas seções seguintes, serão descritas as quatro abordagens de representação 
de conhecimento visual que melhor se encaixam na visão de representação em 
três níveis semânticos. Em cada uma será dada uma visão geral das característi-
cas da abordagem e uma descrição mais detalhada de cada nível de representa-
ção. Ao fim, as abordagens são confrontadas. 

2.2.1 Abordagem 1: Agregados 
A abordagem apresentada em  Neumann &amp;amp; Möller (2008) foca na utilização 

de lógica de descrição (DL) para representação de conhecimento e sistema de 
raciocínio pra interpretação de cenas. Inicialmente criada para monitoração de 
tráfego, a abordagem também inclui primitivas para representação de objetos 
ao longo do tempo. 

Os construtos básicos de modelagem são os agregados (do inglês aggregates), 
que representam conceitos presentes nas imagens através da agregação das suas 
partes, satisfazendo restrições espaço-temporais. Os agregados são mapeados 
para a representação de baixo nível (pixels) através da GSD (Geometrical Scene 
Description). GSDs são “descrições quantitativas de propriedades visuais de ce-
nas que variam no tempo” (JARKE et al., 1989) no nível visual. Os autores ainda 
apontam que os serviços de inferência comuns de lógica descritiva (verificação 



26 

 

 

de consistências, classificação e etc.) podem ser utilizados em modelos semânti-
cos de imagens representados com esse paradigma. 

2.2.1.1 Nível Semântico 

As principais entidades dessa abordagem são os agregados, utilizados para 
representar situações e objetos no modelo conceitual. Um agregado é basica-
mente um conceito definido pela descrição das partes que o compõe. Ele ainda 
contém um conjunto de restrições espaço-temporais que definem as condições e 
o contexto de ocorrência do agregado (e as suas partes) na imagem. Uma parte 
também pode ser um agregado, o que induz uma estrutura mereológica dos 
conceitos. Além disso, os agregados podem constituir uma hierarquia taxonô-
mica.  

Nessa representação, os objetos mais básicos de uma cena são agregados es-
peciais, compostos por: um objeto físico ou um “corpo” no mundo 3D; uma “vi-
são” que constitui a evidência visual do objeto no campo de visão da câmera. 
Através dessas duas propriedades, as entidades de alto nível são mapeadas pa-
ra uma representação no nível visual.  

Os autores defendem que o nível semântico pode ser representado com uma 
lógica de descrição ALCF(D), pois ela é o suficiente para atender os requisitos de 
representação dos agregados. Além disso, a representação em DL permite que 
se utilizem ferramentas de inferência da DL (como RACER5 ou FaCT++6) para 
checagem de consistência e inferência sobre os modelos visuais.  

2.2.1.2 Nível Visual 

A abordagem não especifica claramente como é constituído o nível visual de 
representação. Segundo apresentado Neumann e seus colegas, a representação 
intermediária se dá pela GSD (Geometrical Scene Description). Já segundo 
(HERZOG, 1995), a GSD tem o objetivo de representar a cena original sem ne-
nhuma perda de informação. Ela descreve, para cada frame de uma cena, dados 
de posição e orientação dos objetos, bem como seu modelo tridimensional. 

A construção de uma descrição GSD pode ser automática ou manual. Por 
exemplo, o sistema VITRA descrito por Herzog, consegue reconstruir automati-
camente modelos tridimensionais de carros, mas o cenário de fundo deve ser 
construído manualmente.  

2.2.1.3 Nível analógico 

Embora citado, nenhum dos trabalhos estudados descrevem exatamente 
como se dá o mapeamento do GSD para o pixel.  Sabe-se apenas que os dados 

                                                 

 
5 http://www.racer-systems.com/ 
6 http://owl.man.ac.uk/factplusplus/ 



27 

 

 

brutos são mapeados por algoritmos de processamento de imagens que resul-
tam em descrições GSD. 

2.2.2 Abordagem 2: Abdução 
Inicialmente proposta por Murray Shanahan e colegas em (2004; 1996) para 

aplicação em robótica, a abordagem abdutiva consiste em um modelo lógico de 
percepção visual. Ele é caracterizado da seguinte forma: seja ? uma teoria que 
define as relações entre os objetos de uma cena e as informações de baixo nível 
que lhes dão origem. Então, dada uma conjunção ? de fórmulas representando 
uma coleção de informações imagísticas de baixo nível, o papel da percepção 
visual é encontrar uma conjunção de fórmulas ? tal que, 

???  ? ?. 

Em outras palavras, a abordagem define, dentro de um contexto lógico, o 
ancoramento semântico onde o modelo conceitual ? é mapeado para os dados 
sensoriais ? através do mapeamento definido em ?. A tarefa de um sistema de 
percepção visual é encontrar o conjunto ? que melhor explica os dados sensori-
ais em ?. 

O modelo de inferência descrita por Shanahan e colegas utiliza as aborda-
gens top-down e bottom-up. O algoritmo infere diversos possíveis conjuntos de 
hipóteses de explicação ? e escolhe, com base em um cálculo de probabilidade, 
qual a hipótese que melhor explica a imagem. 

2.2.2.1 Nível Semântico 

Embora Shanahan não defina explicitamente um nível semântico, Cohn e co-
legas (2003) sugerem, dentro do mesmo paradigma, que o modelo conceitual ? 
deve estar definido como uma hierarquia mereológica, como apresentado no 
exemplo da Figura 2.2. 

 
Figura 2.2: Exemplo de hierarquia de composição (adaptado de (COHN, 

ANTHONY G. et al., 2003)). 

Sala

Mesa
Chão

Cadeira

Estante

Doc1 Doc2 ... Tampo

Encosto

Assento
Rodas

Estrutura Livros

Livro VerdeLivro Vermelho

Região Azul

Região Preta

Região Cinza

pixel(a,b) pixel(c,h) pixel(f,e)

(a)

(b)



28 

 

 

2.2.2.2 Nível Visual 

O nível Visual é definido como um conjunto de axiomas lógicos de primeira 
ordem contidos no conjunto ?. Esses axiomas relacionam primitivas de baixo 
nível, como retas e pontos, com primitivas do nível visual, como regiões e vo-
lumes. Além disso, uma peculiaridade da abordagem é a possibilidade de ma-
pear ruído identificado pelos algoritmos de baixo nível.  

Por exemplo, dada uma primitiva de baixo nível Line(w,p1,p2), que identifica 
uma reta w entre os pontos p1 e p2. Um conjunto de axiomas válidos no nível vi-
sual seria (copiado do original): 

( )[ ] ( ) ( )[ ]
( ) ( )[ ] ( ) ( )[ ]

( )[ ] ( )

1 2 1 2

1 2 1 2 1 2 1 2

1 2 1 2

Re p , p  Line w, p , p  r gion r SideOf w, r
SideOf w , r SideOf w ,r w w Parallel w , w   Joins w , w
 p , p  Line w, p , p   Noise w

? ? ? ?
? ? ? ? ?

? ?
 

O primeiro axioma explica a presença de uma linha visível pela presença de 
uma região visível, enquanto a segunda linha define uma região com quatro la-
dos. A abordagem abdutiva também permite a modelagem de ruído no nível 
intermediário. Assim, instâncias de primitivas do baixo nível, como Line, que 
não são mapeadas por nenhum axioma no nível visual podem ser interpretadas 
como ruído. 

Cohn e colegas também descrevem o mapeamento para o nível semântico 
como uma Hierarquia Observacional, como no mapeamento do objeto Encosto 
na figura Figura 2.2a. O mapeamento para o nível analógico é descrito por uma 
Hierarquia Sensorial, como no exemplo da Figura 2.2b, onde uma entidade vi-
sual e descrita em termos de pixels por pixels em determinadas coordenadas do 
espaço. 

2.2.2.3 Nível analógico 

O nível analógico da abordagem abdutiva é composto por primitivas sim-
ples como ponto ou linha, extraídos da imagem por algoritmos de processa-
mento de imagem. O nível analógico raramente conterá o dado bruto em si.  

 
Figura 2.3: Passos para extração das linhas como primitivas de baixo nível de 

uma imagem (SHANAHAN, MURRAY; RANDELL, DAVID, 2004). 

Por exemplo, em Shanahan e colegas (2004), imagens capturadas por um ro-
bô são processadas por um algoritmo de detecção de bordas baseado no opera-
dor Sobel (Figura 2.3). Então um algoritmo seguidor de bordas é utilizado para 
extrair as instâncias de primitivas de baixo nível, como: 



29 

 

 

[ ] [ ]( )
[ ] [ ]( )

1, 238 157 241 147
2 240 159 247 157

Line , , ,
Line , , , ,

 

A partir dessas primitivas é possível inferir regiões e conceitos de maior ní-
vel semântico nos níveis superiores, como descrito na seção 2.2.2.2. 

2.2.3 Abordagem 3: Espaços Conceituais 
A Teoria dos Espaços Conceituais, apresentadas por Peter Gärdenfors (2004), 

apresenta o conceito de espaço conceitual, um espaço métrico onde entidades são 
caracterizadas por um número de dimensões qualitativas (como cor, coordena-
das espaciais, tamanho, etc.). Essa teoria pode ser aplicada em várias áreas de 
Inteligência Artificial, sendo que Chella e colegas (1997) propuseram a sua utili-
zação na área de visão computacional, dentro do contexto da robótica. Nessa 
abordagem, o espaço conceitual faz o papel de ponte entre a representação de 
baixo nível e a representação simbólica.  

Chella e colegas introduzem o conceito de knoxel, um ponto genérico no es-
paço conceitual, correspondendo a uma entidade primitiva no nível visual (co-
mo uma região ou volume). Na abordagem, os knoxels são volumes obtidos da 
imagem por algoritmos de processamento e mapeados para o nível semântico 
(CHELLA et al., 2001). Por exemplo, um martelo “genérico” pode ser represen-
tado por dois knoxels no nível visual, como na Figura 2.4a. O conjunto de knoxels 
{k1, k2} é então mapeado para o conceito Hammer no nível semântico como re-
presentado na Figura 2.4b. 

A correlação entre os knoxels e as entidades do nível semântico é realizada 
através de um algoritmo de atenção, utilizando geração e teste de hipóteses. 

 
Figura 2.4: Representação de um martelo em um espaço superquadrático e con-

ceitual (CHELLA et al., 2001). 



30 

 

 

2.2.3.1 Nível Semântico 

O nível semântico descreve a cena percebida em termo de uma linguagem 
lógica de alto nível. É adotado um formalismo de representação híbrido, consti-
tuído por um componente terminológico e um componente assertivo. O primei-
ro contém o modelo conceitual propriamente dito, enquanto o segundo guarda 
a descrição de uma cena individual (instâncias). 

O componente terminológico é baseado em uma taxonomia de objetos, além 
de induzir relações de composição entre os objetos. Por exemplo, a Figura 2.5 
demonstra a especialização da entidade genérica Object - e sua relação has-part - 
na a entidade Hammer. Nesse caso, conforme a Figura 2.4b, as entidades primi-
tivas Box-shaped e Cylinder-chaped são representações diretas de knoxels do nível 
visual. 

Já o componente assertivo representa as instâncias de uma cena utilizando 
lógica de primeira ordem. Por exemplo, a declaração da instância Hammer#1 do 
conceito Hammer é dada pela fórmula 

#1  . 

Para expressar que a relação has-handle entre Hammer#1 e o knoxel Cylinder-
shaped#1, declara-se a fórmula 

? #1, #1 . 

Além disso, o componente de terminológico contém representações para re-
lações espaciais e topológicas entre objetos. 

 
Figura 2.5: Fragmento do componente terminológico da abordagem, utilizando 

a representação gráfica de KL-ONE (CHELLA et al., 1997). 

2.2.3.2 Nível Visual 

Como dito, a primitiva de representação do nível visual é uma forma 3D 
primitiva chamada knoxel. Nos trabalhos de Chella e colegas, as primitivas para 
representação do knoxel são as superquadráticas.  



31 

 

 

Superquadráticas são formas geométricas derivadas de uma equação qua-
drática paramétrica com as funções trigonométricas elevadas a dois expoentes 
reais. Em resumo, um knoxel superquadrático é representado por um vetor no 
espaço , ou seja, um knoxel k é dado por 

                          

onde           definem a forma da superquadrática e            
definem a posição especial e inclinação. Além disso, os knoxels podem ser agru-
pados em clusters de percepção (PC). Por exemplo, a Figura 2.4a sugere um PC 
dado por k1 e k2. 

A relação entre os knoxels e as entidades no nível superior é representada a-
través da função de interpretação 

?C:C PC 

que mapeia o conjunto C de instâncias no componente assertivo para o con-
junto de todos os clusters de percepção no nível visual. Por exemplo, no exem-
plo da figura Figura 2.4: 

? #1   ,   
ou ainda, 

? Cylinder?shaped#1   , 
? Cylinder?shaped#1 ? #1 .  

A função ? é inferida por um algoritmo baseado em atenção que utiliza o 
conhecimento contido no nível semântico em um esquema de inferência top-
down. O algoritmo busca por possíveis objetos através dos knoxels já identifica-
dos. Ainda no exemplo da Figura 2.4, se o knoxel k2 já foi identificado, o algo-
ritmo irá supor a existencial potencial de um martelo e irá realizar uma buscar 
no nível visual pelo knoxel k1. Se k2 sugerir a existência de outros objetos, será 
feita uma busca por evidências que comprovem a existência de todos eles. 

2.2.3.3 Nível analógico 

O nível analógico, na abordagem dos espaços conceituais, é constituído so-
mente dos dados brutos da imagem, sem nenhum tipo de abstração inicial. 
(CHELLA et al., 2001) listam diversos trabalhos que tratam da extração de for-
mas superquadráticas em imagens reais, mesmo com a presença de oclusões.  

2.2.4 Abordagem 4: Abordagem Orion 
A proposta apresentada pelo Grupo Orion (HUDELOT, 2005; HUDELOT et 

al., 2005; MAILLOT, 2005) define uma abordagem completa de visão cognitiva 
que se encaixa com precisão na representação em três níveis semânticos Figura 
2.6.  

Uma das principais preocupações da abordagem é a modelagem do anco-
ramento simbólico entre o nível semântico e o nível analógico. Por isso, duas 
ontologias são propostas para mapeamento entre os três níveis.  



32 

 

 

 
Figura 2.6: Exemplo de representação de uma laranja em uma imagem utilizan-

do três níveis semânticos (HUDELOT et al., 2005). 

A Ontologia de Conceitos Visuais faz o papel de um modelo intermediário en-
tre o nível semântico e o nível analógico, representando as entidades do modelo 
conceitual através de “um vocabulário comum utilizado por humanos para re-
presentar visualmente objetos e cenas” (HUDELOT et al., 2005).  

Já a Ontologia de Conceitos Imagísticos mapeia os resultados brutos dos algo-
ritmos de processamento de imagens para o nível visual. Ela descreve as ima-
gens e os resultados dos algoritmos do ponto de vista do especialista em pro-
cessamento de imagens. Além disso, a ontologia inclui conceitos para represen-
tar as funcionalidades dos algoritmos de processamento de imagem, o que 
permite atrelar as entidades do nível intermediário aos processos que são utili-
zados para identificá-las e extraí-las das imagens. 

A inferência do ancoramento semântico utiliza as duas formas de interpreta-
ção semântica de imagens (top-down e bottom-up). 

Céline Hudelot (2005) propõe uma plataforma completa para interpretação 
semântica de imagens utilizando a abordagem Orion. A implementação do sis-
tema foi realizada com a plataforma LAMA (CRUBÉZY et al., 1998) para desen-
volvimento de sistemas baseados em conhecimento. A aplicação foi validada 
dentro do domínio da Biologia, para diagnóstico de doenças em roseiras. 

2.2.4.1 Nível Semântico 

O nível semântico representa o conhecimento visual eliciado de um especia-
lista no domínio da aplicação. A abordagem define a forma com que o conhe-
cimento deve ser eliciado, definindo os seguintes passos para a tarefa: 

• Primeiramente, o especialista no domínio da aplicação provê uma taxo-
nomia de conceitos do domínio. Esse modelo também inclui relações co-
mo parte-de; 



33 

 

 

• Então, os especialistas utilizam a Ontologia de Conceitos Visuais para 
descrever os conceitos do domínio com base na sua aparência visual e 
suas relações espaciais; 

• Opcionalmente, o processo pode incluir a anotação de amostras de ima-
gens utilizando o modelo semântico. 

2.2.4.2 Nível Visual 

O nível visual é representado pela já citada Ontologia de Conceitos Visuais. Ela 
é uma ontologia independente de domínio que permite modelar explicitamente 
os aspectos visuais de objetos do mundo, abstraídos de qualquer dado numéri-
co. Ela é composta por: 

• Conceitos Espaciais que descrevem objetos geometricamente. São conceitos 
como forma, tamanho e localização. São também representadas relações es-
paciais como topologia, distância e orientação. 

• Conceitos para Cor, com termos relacionados a matiz, luminosidade e sa-
turação.  

• Conceitos para Textura, obtidos por experimentos da comunidade de ciên-
cia cognitiva. São conceitos como textura granulada e orientada ou uniforme.  

Como dito na seção anterior, essa ontologia é diretamente mapeada para a 
ontologia de domínio no nível semântico. 

2.2.4.3 Nível analógico 

Como apresentado na seção 2.2.4.2, no nível analógico os dados brutos da 
imagem são interpretados por algoritmos de processamento de imagem e o re-
sultado é representado pela Ontologia de Conceitos Imagísticos, formada por pri-
mitivas de representação ligadas ao domínio de processamento de imagens na 
computação. Ela é formada pelas seguintes partes: 

• Um conjunto de 11 conceitos representando diferentes tipos de estruturas 
de dados que podem ser extraídos de imagens, como regiões e arestas, ou 
estruturas mais complexas, como grafos de regiões e de vizinhança.  

• Um conjunto de 167 conceitos para representar características que podem 
ser medidas em imagens, como área, posição e propriedades de formas, 
cor e texturas. 

A ontologia também contém conceitos para representar o conhecimento li-
gado aos algoritmos de processamento de imagem. 

Todos esses conceitos permitem a construção de uma requisição de proces-
samento de imagem com base no conhecimento representado no nível visual e 
que o resultado do processamento seja mapeado para ele. 

O mapeamento pode acontecer de duas formas. Na forma automática, para 
um dado conceito visual (azul claro, por exemplo), são escolhidas as característi-
cas imagísticas que melhor descrevem o conceito. A escolha é feita utilizando a 



34 

 

 

técnica de Analise Discriminante Linear (ADL), com base em imagens manual-
mente anotadas por especialistas no domínio. As características escolhidas for-
mam a entrada do detector do conceito visual. Ele é constituído por uma Má-
quina de Vetor de Suporte (SVM, do inglês Support Vector Machine), treinada 
com base em imagens anotadas manualmente e específicas para o treinamento. 

No mapeamento manual, a ligação entre os conceitos do nível visual e as ca-
racterísticas da imagem é explicitamente construída. A correspondência entre as 
características imagísticas numéricas e os conceitos no nível visual é represen-
tada por conjuntos fuzzy. Por exemplo, na Figura 2.7, o conceito Superfície Elíp-
tica no nível visual é mapeado para o nível analógico por cinco conjuntos fuzzy 
definidos sobre a primitiva analógica excentricidade.  

 
Figura 2.7. Exemplo de mapeamento do conceito Superfície Elíptica do nível vi-

sual para o nível analógico utilizando conjuntos fuzzy (HUDELOT, 2005). 

2.3 Discussão e Comparação entre Abordagens de Interpreta-
ção Semântica de Imagens 

No capítulo anterior foram descritas quatro abordagens de interpretação de 
conhecimento visual baseados em modelos de representação em três níveis co-
nhecimento.  

A primeira abordagem estudada, dos agregados, foca principalmente na re-
presentação no nível semântico com uma lógica descritiva formal. Ela trata de 
questões importantes no domínio visual, como representação taxonômica e me-
reológica, com a introdução de uma entidade conceitual básica chamada agre-
gado. No campo da inferência, os autores justificam a utilização de uma lógica 
descritiva mínima, pois isso permite que os já bem estudados serviços de infe-



35 

 

 

rência da lógica descritiva sejam utilizados para validar os modelos visuais e 
derivar novas interpretações. 

No entanto, o ponto fraco da abordagem dos agregados está nos níveis infe-
riores. Pouco se fala sobre as formas de mapeamento do nível semântico para os 
níveis inferiores. As informações a respeito do modelo GSD são demasiadamen-
te esparsas na literatura.  Ainda sim, pelo estudo das informações disponíveis, o 
GSD não provê uma forma de representação clara e a forma de mapeamento en-
tre ele e os outros níveis não está totalmente clara. 

Já na abordagem abdutiva, o foco está na representação nos níveis visual e 
imagístico e no mapeamento entre os dois. A modelagem é baseada em lógica 
de primeira ordem, onde as entidades do nível visual são definidas como axio-
mas sobre primitivas do nível analógico. Isso permite a verificação formal do 
conhecimento modelado. Além disso, a abordagem inclui noção de ruído no ní-
vel visual, tornando a proposta menos suscetível a erros dos algoritmos de pro-
cessamento de imagens. Como as primitivas do nível analógico são visualmente 
simples (linhas e pontos), a extração delas também se torna simples e robusta. 

O ponto fraco da abordagem abdutiva está na sua simplicidade. Nenhum 
dos níveis inferiores busca representar características visuais como cor e textu-
ra. Isso limita a interpretação no nível conceitual a somente à forma e contorno 
dos objetos. Além disso, a modelagem no nível conceitual ainda foi pouco ex-
plorada.  

A proposta dos espaços conceituais se encaixa melhor que as duas anteriores 
na abordagem em três níveis conceituais. Ela define um ótimo arcabouço teóri-
co para representação de conhecimento visual em todos os níveis. O nível se-
mântico define uma ontologia de representação genérica, que pode ser estendi-
da, e uma representação para as suas instâncias. O nível visual é mapeado para 
o nível semântico através de funções de mapeamento formais e bem definidas, 
interpretadas através de um mecanismo de raciocínio imagístico baseado em 
atenção. 

A abordagem, embora bastante sólida, sofre o mesmo problema de simplici-
dade da abordagem abdutiva. O nível visual leva em conta somente a forma de 
objetos visuais, deixando de fora atributos como cor, textura e etc. Isso inviabi-
liza a utilização da abordagem em áreas onde essas são as características que 
definem os objetos. 

Finalmente, ao contrário das outras propostas, a abordagem Orion foca re-
almente na representação e interpretação em domínios intensivos em conheci-
mento visual. Ela define uma abordagem completa de visão cognitiva, definin-
do modelos de representação em todos os três níveis e seus respectivos mape-
amentos. O problema do ancoramento semântico é abordado diretamente, com 
propostas de ancoramento manual e com técnicas de aprendizado de máquina.  

A proposta Orion falha em não apresentar uma possibilidade de expansão 
para interpretação de vídeos, integrando noções formais de modelagem de e-
ventos. Além disso, embora completa, a proposta pode se tornar muito comple-



36 

 

 

xa para aplicação em dispositivos com pouco poder computacional embarcado 
(como robótica).  

Por fim, a Tabela 2.1 faz uma comparação direta entre as abordagens estu-
dadas, levando em conta as diversas dimensões pertinentes à interpretação se-
mântica de imagens. Ela evidencia uma dicotomia interessante entre a aborda-
gem Orion e as demais. Nota-se que a abordagem Orion se baseia em um for-
malismo de representação menos estruturado (Frames, em contraponto com o 
formalismo lógico). Isso está em conformidade com a aplicação básica em do-
mínios intensivos em conhecimento, que normalmente requerem modelos de 
representação flexíveis e menos estruturados, sem grande preocupação com 
performance. Já nas outras abordagens, em especial a abdução e de espaços 
conceituais, a formalização lógica e matemática (como a superquadráticas) se 
adaptam aos requisitos de performance e robustez exigidos por sistemas robóti-
cos e de tempo real.  

Tabela 2.1: Tabela de comparação entre as quatro abordagens estudadas. 

 Abordagem 1: Agregados 
Abordagem 2: 

Abdução 

Abordagem 3: 
Espaços  

Conceituais 

Abordagem 4: 
Orion 

Formalismo de 
Representação Lógica descritiva 

Lógica de pri-
meira ordem 

Lógica, Frames e 
superquadráticas 

Frames 

Aplicação  
Básica 

Interpretação de 
cenas simples 

Robótica e inter-
pretação de ce-
nas (trânsito) 

Robótica 

Recuperação de 
imagens e inter-
pretação de ce-

nas em domínios 
intensivos em 
conhecimento 

Ancoramento 
Semântico 

Não definido ex-
plicitamente; 

possivelmente 
manual. 

Abordagem 
híbrida (top-down 

e bottom-up) 

Abordagem 
híbrida (top-down 

e bottom-up) 

Abordagem hí-
brida (top-down e 

bottom-up) ou 
manualmente 

Processamento 
de imagem 

Qualquer um 
que extraia mo-
delos GSD de 

imagens 

Algoritmo de de-
tecção e extração 

de bordas 

Qualquer algo-
ritmo que gere 
volumes super-
quadráticos de 

imagens 

Qualquer algo-
ritmo de proces-
samento de ima-
gens ou manu-

almente (via ano-
tação) 

 



 

 

 

3 S-Chart: UM MODELO PARA INTERPRETAÇÃO 
SEMÂNTICA DE GRÁFICOS 

Este capítulo descreve o framework S-Chart, a principal contribuição deste  
deste trabalho. O framework S-Chart busca capturar em modelos e algoritmos o 
comportamento de interpretação visual realizado por um especialista ao anali-
sar um gráfico como o exposto na Figura 1.2b. Mais especificamente, será de-
monstrado como algoritmos de processamento de sinais podem ser combinados 
com algoritmos e modelos de raciocínio simbólico a fim de chegar a uma inter-
pretação simbólica do gráfico. A proposta apresentada utiliza os princípios de 
interpretação semântica, de forma a combinar mecanismos de extração de pa-
drões com conhecimento de domínio. 

Para chegar a interpretação semântica da projeção gráfica de um sinal, o 
framework S-Chart propõe modelos e algoritmos em três níveis semânticos. O 
estudo de outras abordagens de interpretação semântica apresentadas no capí-
tulo anterior e do requisito de aplicabilidade em domínios intensivos em conhe-
cimento visual, sugerem a incorporação dos seguintes conceitos ao framework 
S-Chart:  

• Incorporação de ontologias de domínio. O modelo que representa os ob-
jetos do domínio é incorporado no nível semântico. Embora ele possa ser 
mais orientado à aplicação, é interessante que o nível semântico possa in-
corporar modelos de conhecimento propriamente dito, como ontologias 
de domínio. Ontologias de domínio capturam conhecimento comparti-
lhado sobre um determinado domínio de forma não ambígua. Uma onto-
logia de domínio aplicada ao nível semântico permite que as informações 
extraídas dos gráficos sejam compartilhadas mais eficientemente com ou-
tros agentes e sistemas. Em especial, estas informações podem ser dispo-
nibilizadas a outros mecanismos de raciocínio simbólico para realização 
de inferências adicionais ao sistema de interpretação visual. 

• Primitivas de modelagem independentes de domínio. Como explicitado 
anteriormente, novas primitivas se fazem necessárias para suporte a in-
terpretação de gráficos e dos dados onde eles se baseiam. Elas se locali-
zam no nível analógico e no nível visual. Elas também devem ser genéri-
cas o suficiente para permitir que o framework seja reutilizável em diver-
sos domínios de aplicação. 



38 

 

 

• Ancoramento simbólico explícito. O mapeamento entre os símbolos dos 
três níveis deve ser explicitamente representado. Em técnicas de mapea-
mento conexionistas, um número suficiente de casos é necessário para 
treinamento e teste das redes que operacionalizam os mapeamentos. No 
entanto, em domínios restritos, mas intensivos em conhecimento visual, 
nem sempre se tem acesso ao número de casos necessários para treina-
mento. Uma vez que este trabalho foca em domínios como este, se faz ne-
cessária uma forma de mapeamento direta e que possa ser explicitamente 
representada. 

• Modelo processável por computador: o formalismo escolhido para re-
presentação dos modelos de conhecimento deve ser formal e estruturado 
o suficiente para ser diretamente processável por computador. Ou seja, os 
modelos de conhecimento devem ser processáveis diretamente pelos al-
goritmos de interpretação, sem a necessidade de mapeá-los manualmente 
para formalismos de baixo nível. 

As seções descrevem seguintes os modelos e algoritmos que compões o fra-
mework S-Chart incorporando os conceitos apresentados. Na seção seguinte, 
são feitas algumas definições sobre a linguagem de representação utilizada ao 
longo do capítulo. A seguir, são apresentados os modelos de conhecimento vi-
sual em três níveis, seguidos de algoritmos para interpretação semântica sobre 
estes modelos. 

3.1 Linguagem de Representação 
O formalismo escolhido para representação dos modelos de conhecimento 

visual e ancoramento simbólico é a linguagem OWL 1.0 (MCGUINNESS; 
HARMELEN, 2004). OWL foi proposto pela W3C como uma linguagem para 
representação de conhecimento na Web. No entanto, a sua padronização tem 
levado a uma aceitação maior para a formalização de conhecimento e informa-
ção nos mais diferentes tipos de sistemas.  

A escolha de OWL como linguagem de representação neste trabalho se dá 
por fatores de natureza conceitual a prática. Do ponto de vista conceitual, a lin-
guagem combina uma semântica bem definida com uma expressividade sufici-
ente para representar os construtos do framework S-Chart. Todos os modelos 
apresentados a seguir (com exceção dos modelos de mapeamento) podem ser 
modelados com a sublinguagem OWL DL, que equivale à lógica de descrição 

. A Tabela 3.1 mostra quais primitivas OWL DL são utilizadas pra 
representar as principais primitivas S-Chart detalhadas nas próximas sessões. 
Adicionalmente, modelos descritos em OWL DL herdam as propriedades for-
mais dessa , o que possibilita uma modelagem mais precisa. De fato, Neu-
mann e colegas (2008) suportam idéia da lógica descritiva como uma ferramen-
ta para formalização de conhecimento visual. Uma segunda característica im-
portante da linguagem é a possibilidade de metamodelagem, ou seja, é possível 
relações entre elementos dos modelos criados na linguagem com elementos do 



39 

 

 

metamodelo da própria linguagem (como os elementos classe e propriedade). 
Dessa forma é possível definir extensões da própria linguagem de representa-
ção. Essa característica é importante para criação das regras de mapeamentos 
entre os níveis de conhecimento visual, como apresentado na seção 3.2.4 a se-
guir.  

Tabela 3.1: Primitivas OWL necessárias para representação de primitivas visu-
ais do framework S-Chart. 

Primitiva S-Chart Primitiva OWL DL 
Entidades Visuais Class 

Propriedades Visuais ObjectProperty e Class 

Relações Espaciais ObjectProperty 

Entidades Analógicas Class 

Propriedades Analógicas DatatypeProperty 

Do ponto de vista prático – e não menos importante –, a linguagem OWL 
apresenta uma série de características que facilitam a sua utilização em sistemas 
de conhecimento, como uma sintaxe bem defina (XML) e uma grande comuni-
dade de usuários (com o desenvolvimento de ferramentas de suporte a mode-
lagem e desenvolvimento). Na opinião do autor deste trabalho, a utilização de 
OWL diretamente como linguagem de representação no framework S-Chart 
pode facilitar a instanciação do framework nas diversas áreas de aplicação. No 
Capítulo 5, será demonstrado como a linguagem pode utilizada para imple-
mentação de um sistema de interpretação visual. 

Nas seções seguintes, é utilizada a sintaxe OWL Manchester (HORRIDGE et 
al., 2006) para representação de expressões OWL, a fim de facilitar a leitura. Ao 
contrário da sintaxe padrão XML/OWL, a sintaxe Manchester é menos prolixa e 
mais simples de ser compreendida por seres humanos. 

3.2 Arquitetura do Framework S-Chart 
A arquitetura do framework S-Chart é composta por três elementos funda-

mentais (Figura 3.1): um componente de representação de conhecimento visual, 
um componente de ancoramento simbólico (ou “mapeamento”) e um compo-
nente de interpretação e inferência de informações visuais.  

O componente de representação é composto por um modelo de dividido em 
três níveis de representação (semântico, visual e analógico). Como em outras 
abordagens de interpretação simbólica, esses três níveis representam as infor-
mações extraídas do sinal em graus crescentes de abstração. O nível analógico 
captura informações numéricas extraídas diretamente do sinal, o nível visual 
captura feições visuais da projeção gráfica desse sinal, e o nível semântico cap-
tura as entidades do domínio reconhecidas visualmente no gráfico. 



40 

 

 

 
Figura 3.1: Componentes da arquitetura do framework S-Chart. 

A informação utilizada para mapeamento entre entidades de níveis adjacen-
tes é representada pelo componente de ancoramento simbólico. Ele é que indica 
explicitamente como o processo de interpretação deve abstrair entidades de um 
nível para o outro. Esse mapeamento indica, por exemplo, como em qual confi-
guração certas feições analógicas devem aparecer no sinal para indicar a pre-
sença de uma feição visual.  

O componente de interpretação processa o sinal em conjunto com os mode-
los de representação e ancoramento simbólico até inferir algum objeto do domí-
nio presente no sinal. Esse mecanismo se baseia em geração e teste de hipóteses 
e se divide em duas partes. A máquina de interpretação visual infere feições visu-
ais das informações contidas no nível analógico. Instâncias de entidades visuais 
são criadas (geração) e classificadas conforme os dados analógicos extraídos do 
sinal (teste). De forma análoga, a máquina de interpretação semântica, infere enti-
dades do nível semântico conforme as feições visuais extraídas pela máquina de 
interpretação visual. 

Nas subseções seguintes, cada um dos componentes da arquitetura do fra-
mework S-Chart é descrito em detalhes.  

3.2.1 Nível semântico 
O nível semântico comporta o modelo de conhecimento que define as entida-

des do domínio que podem ser identificadas no sinal interpretado. Todas essas 
entidades devem estar organizadas em uma única estrutura taxonômica e/ou 
mereológica. Ou seja, todos os conceitos do domínio devem ser estar relaciona-
dos por alguma relação de sub-classe ou de sub-parte.  

Como apontado nos objetivos, ontologias de domínio podem ser utilizadas 
como modelo para o nível semântico. Ontologias são construídas com o intuito 
de capturar o conhecimento consensual de determinado domínio. Quando mo-



41 

 

 

deladas corretamente prevendo integração de modelos, podem ser utilizadas 
como suporte para comunicação de diversos agentes (sistemas, usuário e etc). 
Assim, um sistema de conhecimento baseado em uma ontologia de domínio 
poderá integrar um sistema S-Chart se essa ontologia atender a estrutura básica 
apresentada anteriormente. No Capítulo 5, é apresentada uma aplicação onde é 
modelada uma ontologia no domínio da Estratigrafia de Sequências utilizada 
como modelo de conhecimento de domínio no nível semântico. 

Para facilitar a importação desses modelos de conhecimento, o componente 
de representação define uma primitiva auxiliar no nível semântico na forma da 
classe EntidadeSemântica. Essa entidade representa qualquer objeto do domí-
nio de discurso que foi extraído pelo sistema de interpretação. Ou seja, qualquer 
objeto interpretado será instância de um conceito do domínio e da classe Enti?
dadeSemântica, em uma relação de herança múltipla. O seu papel no processo 
de interpretação ficará mais claro nas seções seguintes. 

3.2.2 Nível visual 
Ao contrário do nível semântico, o nível visual é independente do domínio de 

aplicação. Ele apresenta exclusivamente as primitivas que representam feições 
visuais genéricas, identificáveis por qualquer ser humano em gráficos como o 
da Figura 1.2b. Por causa disso, o modelo de conhecimento que captura as pri-
mitivas visuais pode ser classificado como uma ontologia visual, uma vez que ela 
representa simbolicamente os elementos visuais de uma conceitualização com-
partilhada. 

As feições visuais de um gráfico são capturadas por três tipos de construtos 
básicos: (i) as entidades visuais, que representam os objetos visuais concretos; (ii) 
as propriedades visuais, que caracterizam as entidades; (iii) relações espaciais, defi-
nidas entre as entidades visuais. Todas essas primitivas modeladas em uma on-
tologia, organizadas como uma taxonomia de conceitos e de relações.  

3.2.2.1 Entidades Visuais 

Uma entidade visual corresponde a uma feição visual concreta, que foi reco-
nhecida no gráfico. Esse conceito é capturado pela classe EntidadeVisual no 
modelo ontológico. Somente entidades visuais têm extensão na realidade, logo, 
qualquer objeto visual inferido pela interpretação visual será instância da classe 
EntidadeVisual ou de alguma de suas subclasses. 

A classe EntidadeVisual é especializada nos diversos tipos de entidades 
visuais que podem aparecer em um gráfico (Figura 3.2). Como apresentado no 
Capítulo 2, diversos trabalhos propuseram ontologias para representação de 
conhecimento no nível visual. Em geral, essas ontologias compartilham as 
mesmas primitivas elementares. As primitivas mais escuras na Figura 3.2 foram 
propostas no modelo de conhecimento visual proposto pela abordagem Orion 
(seção 2.2.4) e reutilizadas aqui. As novas primitivas definidas neste trabalho 
descrevem feições visuais específicas de gráficos, como tipos de curvas e pontos 



42 

 

 

máximos e mínimos. Essa listagem não pretende ser extensiva e pode ser esten-
dida com outras primitivas (ex.: outros tipos de curva) conforme a necessidade. 

3.2.2.2 Propriedades Visuais 

Algumas feições visuais são modeladas como propriedades de entidades vi-
suais. Ou seja, são feições visuais que tem a sua existência dependente da exis-
tência de uma entidade visual. Dessa forma, todas as propriedades visuais têm 
o seu domínio definido em EntidadeVisual ou em alguma de suas subclasses. 
As propriedades visuais também são estruturadas como uma taxonomia de 
propriedades, segundo a relação de subpropriedade de OWL (Figura 3.3). No 
entanto, a definição do contradomínio de propriedades visuais necessita de 
uma discussão adicional. 

O nível visual descreve simbolicamente aspectos visuais genéricos de uma 
imagem. Aspectos que possam ser descritos por uma linguagem comum a 
qualquer imagem, livre de detalhes de baixo nível. Essa opção exclui a possibi-

 
Figura 3.2: Taxonomia de entidades visuais. 



43 

 

 

lidade de definição de propriedades visuais através de contradomínios contí-
nuos, numéricos. Ou seja, no framework S-Chart, as propriedades visuais ma-
peiam instâncias de entidades visuais em conjuntos de valores expressos na 
forma de símbolos lingüísticos ou valores simbólicos. Por exemplo, proprieda-
des como comprimento podem assumir somente valores lingüísticos como 
longo, muito longo, curto e etc. Neste trabalho, estes valores são vistos como 
instâncias de uma categoria de valores possíveis (ex.: ValoresDeComprimento). 
Essa alternativa de modelagem é baseada na proposta de Alan Rector 
(RECTOR, ALAN, 2005), fundamentada na caracterização formal de espaços de 
qualidades (quality spaces) (GANGEMI et al., 2003). Assim, o contradomínio de 
cada propriedade visual é modelado como um conceito específico, cujas instân-
cias caracterizam os possíveis valores que a propriedade pode assumir. A Figu-
ra 3.4 apresenta um exemplo disso. 

 
Figura 3.3: Taxonomia de propriedade visuais com definição de contradomínio 

de valores. A especialização do domínio é omitida. 

Contudo, existem alguns aspectos de propriedade visuais que devem ser le-
vados em conta. É sabido que a interpretação humana de propriedades visuais 
como comprimento é bastante subjetiva. Ela varia conforme o contexto que se es-
tá considerando ou a entidade a que se refere. Por exemplo, do ponto de vista 
visual, a representação de um avião com comprimento longo certamente é dife-
rente da interpretação do comprimento longo para a representação de um carro. 
Ambos adquirem um significado diferente, pois se referem a entidades visuais 
em escalas diferentes. Se o mesmo conjunto de valores simbólicos para com-
primento é utilizado para caracterizar entidades em escalas diferentes, será im-
possível interpretá-las de forma correta. Uma propriedade visual não pode ter 
um contradomínio com um único conjunto de valores. Pelo menos não obriga-
toriamente. 



44 

 

 

 
Figura 3.4: Exemplo de modelagem de contradomínios de propriedades visuais. 

Para resolver este problema – ao mesmo tempo em que se mantém a restri-
ção de contradomínio com valores simbólicos – escolheu-se estender o modelo 
apresentado anteriormente permitindo a representação dos contradomínios de 
propriedades visuais como uma partição de classes (Figura 3.5). Nesse modelo 
as instâncias que a compõem são os possíveis valores para a propriedade. As-
sim, propriedades visuais que tem mais de uma interpretação em um dado do-
mínio de discurso podem ser especializadas para suportar mais de um tipo de 
contradomínio7. 

Em alguns casos é preciso que se mantenha uma relação de ordem explícita 
entre os símbolos que constituem o contradomínio de propriedades visuais, a 
fim de se simplificar o processo de interpretação. No caso da propriedade com?
primento, por exemplo, existe uma relação de “tamanho” entre os elementos 
do seu contradomínio (longo é maior que médio que, por sua vez, é maior que 
curto). Para representar esse tipo de relação, o nível visual define a proprieda-
de maiorQue e a sua inversa menorQue, ambas transitivas, anti-simétricas e irre-
flexivas. 

EntidadeVisual ValorDeComprimento
temComprimento

longomédiocurto

ComprimentoDeAvião ComprimentoDeCarroComprimentoGenérico

longoAviãomédioAviãocurtoAvião longoCarrocurtoCarro

 
Figura 3.5: Extensão da modelagem de contradomínios de propriedades visuais. 

                                                 

 
7 A relação de subpropriedade permite também a especialização das propriedades visuais, o 

que também colabora para solução do problema de múltiplos domínios. 



45 

 

 

3.2.2.3 Relações Espaciais 

Relações espaciais capturam as formas de relacionamento entre entidades do 
domínio visual. Elas podem ser relações de orientação ou relações topológicas. 
A sua função é permitir a combinação de entidades visuais em entidades mais 
complexas. Da mesma forma como acontece com as propriedades visuais, as re-
lações espaciais são representadas em OWL como um taxonomia de proprieda-
des (Figura 3.6). A propriedade OWL temRelaçãoEspacial  constitui a raiz 
dessa taxonomia. Se domínio e contradomínio são definidos no conceito Enti?
dadeVisual, bem como todas as suas subpropriedades. 

 
Figura 3.6: Taxonomia de relações espaciais. 

Representação de conhecimento e raciocínio sobre relações topológicas são 
tópicos bastante recorrentes na literatura. Em um dos trabalhos mais importan-
te da área, Cohn e colegas (1997; 1992) propuseram um conjunto de primitivas 



46 

 

 

simples e robusto para representação de relações topológicas, chamado RCC-8. 
O seu objetivo é capturar todas as possíveis relações topológicas entre duas en-
tidades físicas. No framework S-Chart as primitivas do RCC-8 são modeladas 
na forma de uma taxonomia, como em Santin (2008), conforme a Figura 3.6. 

Relações de orientação também são importantes para modelar como dois ob-
jetos estão posicionados um em relação ao outro. O nível visual disponibiliza as 
propriedades acimaDe, abaixoDe, aDireitaDe e aEsquerdaDe. Essas quatro 
relações induzem um espaço de duas dimensões espaciais no nível visual. Co-
mo dito anteriormente, embora o processamento do sinal se dê em um espaço 
de uma dimensão, o raciocínio no nível visual é baseado em um espaço duas 
dimensões. 

3.2.3 Nível analógico 
As primitivas no nível analógico representam informações extraídas direta-

mente pelos algoritmos de processamento de sinal, como pontos, intervalos e 
padrões. Analogamente ao nível visual, as primitivas analógicas se dividem em 
entidades analógicas e suas propriedades (Figura 3.7).  

 
Figura 3.7: Taxonomia de entidades analógicas e relações de composição. 

Um sinal analógico digitalizado é caracterizado como um conjunto de pon-
tos ordenados por uma dimensão espacial. Uma entidade analógica é um conjun-
to de um ou mais pontos em alguma disposição. Neste trabalho, uma entidade 
visual pode ser um ponto, um conjunto de pontos, um intervalo ou um conjun-
to de intervalos (Figura 3.7). Entidade básica de um sinal, o Ponto é descrito pe-
las propriedades valor e índice: 

 
DatatypeProperty: valor 
  Annotations:  
    rdfs:comment “Valor da medida de um Ponto (y)” 
  Domain: Ponto 
  Range: float  
   Characteristics: Functional 
 
DatatypeProperty: indice 
  Annotations:  
 



47 

 

 

 
    rdfs:comment “Índice da medide de um Ponto (x)” 
  Domain: Ponto 
  Range: float  
   Characteristics: Functional 
 

Uma entidade analógica pode ser formada por outras. Essas relações de con-
juntos são capturadas por relações de composição, como a propriedade tem?
Ponto entre ConjuntoDePontos e Ponto. No caso da primitiva Intervalo, al-
guns pontos podem ter um papel especial de ponto inicial, final, central, entre 
outros. Abaixo segue uma lista destas relações. 

 
ObjectProperty: temPonto 
    Domain: ConjuntoDePontos 
    Range: Ponto 
 
ObjectProperty: temPontoInicial 
    SubPropertyOf: temPonto 
    Domain: Intervalo 
    Characteristics: Functional 
 
ObjectProperty: temPontoCentral 
    SubPropertyOf: temPonto 
    Domain: Intervalo 
    Characteristics: Functional 
 
ObjectProperty: temPontoFinal 
    SubPropertyOf: temPonto 
    Domain: Intervalo 
    Characteristics: Functional 
 
ObjectProperty: temPontoMáximo 
   Annotations:  
    rdfs:comment “Ponto de valor máximo de um Intervalo” 
    SubPropertyOf: temPonto 
    Domain: Intervalo 
    Characteristics: Functional 
 
ObjectProperty: temPontoMínimo 
   Annotations:  
    rdfs:comment “Ponto de valor mínimo de um Intervalo” 
    SubPropertyOf: temPonto 
    Domain: Intervalo 
    Characteristics: Functional 
     

Semelhante ao que o ocorre no nível visual, as entidades analógicas são ca-
racterizadas por propriedades analógicas, feições analógicas extraídas por algo-
ritmos de processamento numérico. Cada entidade analógica é definida pelo 
seu próprio conjunto de propriedades. Em geral, uma propriedade analógica é 
um índice numérico calculado sobre as informações analógicas contidas no si-
nal. Alguns exemplos: 

 



48 

 

 

 
DatatypeProperty: temPropriedadeAnalógica 
  Domain: EntidadeAnalógica 
  Range: float  
 
DatatypeProperty: temAreaAbsoluta 
  Domain: Intervalo 
  Annotations:  
    rdfs:comment “Valor de área total sob a curva de um Intervalo” 
  SubPropertyOf: temPropriedadeAnalógica 
 
DatatypeProperty: temArea 
  Domain: Intervalo 
  Annotations:  
    rdfs:comment “Valor da integral de um Intervalo” 
  SubPropertyOf: temPropriedadeAnalógica 
 
DatatypeProperty: temComprimento 
  Domain: Intervalo 
  Annotations:  
    rdfs:comment “Valor de comprimento de um Intervalo” 
  SubPropertyOf: temPropriedadeAnalógica 
 
DatatypeProperty: temTendencia 
  Domain: Intervalo 
  Annotations:  
    rdfs:comment “Valor de inclinação de uma reta entre Ponto inicial  
      e Ponto final de um Intervalo” 
  SubPropertyOf: temPropriedadeAnalógica 
 
DatatypeProperty: temMédia 
  Domain: Intervalo 
  Annotations:  
    rdfs:comment “Valor médio da curva em um Intervalo” 
  SubPropertyOf: temPropriedadeAnalógica 
 
 
DatatypeProperty: temGrauDeRuído 
  Annotations:  
    rdfs:comment “Grau de ruído em uma EntidadeAnalógica” 
  SubPropertyOf: temPropriedadeAnalógica 
 

Os algoritmos de processamento de sinal disponíveis também são modela-
dos no nível analógico, dado o importante papel que exercem na interpretação. 
Outros trabalhos buscam representar esse tipo de informação no nível analógi-
co, como é o caso do projeto Orion apresentado na seção 2.2.4. Dessa forma, é 
possível manter uma relação explícita entre as entidades visuais extraídas do 
sinal e os algoritmos que a extraíram. Essa informação é importante, uma vez 
que o algoritmo usado para extrair uma entidade pode dizer muito sobre a sua 
natureza. Aqui, os algoritmos são classificados em uma taxonomia simples, 
demonstrada na Figura 3.8. O relacionamento entre as entidades visuais e os al-
goritmos de processamento se dá propriedade pela propriedade éExtraídoCom. 
Uma entidade é extraída com um dado grau de certeza, capturado pela relação 
temForçaDeExtração. Mais especificamente: 



49 

 

 

 
ObjectProperty: éExtraídoComo 
    Domain: EntidadeAnalógica 
    Range: Segmentador 
 
DatatypeProperty: temForçaDeExtração 
  Domain: EntidadeAnalógica 
  Range: float  
 

Embora o nível analógico não busque representar todos os possíveis tipos de 
algoritmos, ele serve como uma base para futuras extensões, conforme as neces-
sidades de cada domínio de aplicação.  

 
Figura 3.8: Taxonomia básica de funcionalidades de processamento de sinal. 

3.2.4 Mapeamento e ancoramento simbólico 
Cada nível semântico é um modelo por si só, independente dos demais. No 

entanto, para que os níveis possam ser combinados, é necessário algum tipo de 
mapeamento. Essencialmente, o problema de ancoramento simbólico entre os 
níveis pode ser dividido em duas partes. A primeira tange ao mapeamento de 
primitivas mais abstratas em primitivas menos abstratas. Ou seja, como entida-
des básicas de um nível podem ser combinadas para gerar uma entidade no ní-
vel semântico superior. Por exemplo, como e quais primitivas analógicas (ex.: 
pixels e conjuntos de pixels) devem ser combinadas para indicar a existência de 
uma entidade no nível visual (ex.: uma curva de algum padrão específico). A 
segunda parte do problema consiste em, depois de realizada a extração de uma 
instância de entidade no nível superior, manter uma relação concreta desta com 
as instâncias que serviram como evidências no nível inferior.  

A fim de manter os modelos de mapeamento corretamente estruturados, é 
definido que entidades e objetos de um nível de abstração podem ser mapeados 
exclusivamente no nível imediatamente inferior. Ou seja, entidades do nível 
semântico podem ser mapeadas exclusivamente no nível visual e estas, por sua 
vez, no nível analógico. 

Uma peculiaridade deste modelo é que a formalização do mapeamento entre 
primitivas é definida com base no mapeamento entre instâncias. Embora essa 
relação pareça contra-intuitiva, ela se deve a forma como os algoritmos de in-
terpretação operam. A necessidade dessa relação é detalhada na seção 3.2.5 a 
seguir.  



50 

 

 

3.2.4.1 Mapeamento entre instâncias 

O ancoramento simbólico requer que entidades interpretadas sejam ligadas 
explicitamente às evidências que suportam essa interpretação. Como descrito 
anteriormente, neste trabalho essas entidades interpretadas – e as suas evidên-
cias – são representadas como instâncias de entidades de conhecimento visual. 
Assim, um modelo de ancoramento simbólico deve incorporar uma forma de 
mapeamento explícito entra as instâncias que representam as entidades detec-
tadas em um nível e as instâncias que representam as evidências encontradas 
no nível inferior. Para isso, o modelo de mapeamento incorpora uma relação 
básica e quatro relações específicas: 

 
ObjectProperty: temMapeamentoSimbólico 
 
ObjectProperty: mapeamentoSemânticoParaVisual 
  SubPropertyOf: temMapeamentoSimbólico 
  Domain: EntidadeSemântica 
  Range: EntidadeVisual 
  InverseOf: mapeamentoVisualParaSemântico 
 
ObjectProperty: mapeamentoVisualParaSemântico 
  SubPropertyOf: temMapeamentoSimbólico 
  InverseOf: mapeamentoSemânticoParaVisual 
 
ObjectProperty: mapeamentoVisualParaAnalógico 
  SubPropertyOf: temMapeamentoSimbólico 
  Domain: EntidadeVisual 
  Range: EntidadeAnalógica 
  InverseOf: mapeamentoAnalógicoParaVisual 
 
ObjectProperty: mapeamentoAnalógicoParaVisual 
  SubPropertyOf: temMapeamentoSimbólico 
  InverseOf: mapeamentoVisualParaAnalógico 
 

As quatro relações ajudam a definir limites claros entre os níveis, estrutu-
rando-os. Ou seja, entidades do nível semântico podem ser mapeadas somente 
para entidades do nível visual e estas somente para entidade no nível analógico. 
Não é possível mapear o nível semântico diretamente no nível analógico. 

3.2.4.2 Mapeamento entre primitivas: detectores simbólicos 

O mapeamento entre primitivas de níveis de abstração distintos é definido 
por detectores simbólicos. Um detector simbólico define quais são as condições 
necessárias para que uma dada primitiva semântica ou visual seja corretamente 
detectada. Os mecanismos de interpretação utilizam essas condições para testar 
as hipóteses de entidades semânticas e visuais. Por exemplo, um detector sim-
bólico para a primitiva semântica Sequência define que uma instância de se-
quência será interpretada somente se uma instância da primitiva visual Curva?
Gaussiana estiver presente e sua propriedade temComprimento tiver o valor 
longo. 



51 

 

 

Cada primitiva detectável é mapeada por um conjunto com um ou mais de-
tectores especializados. Da mesma forma que o mapeamento entre instâncias, 
um detector relaciona primitivas somente entre níveis de abstração adjacentes 
(semântico/visual e visual/analógico). Cada detector é modelado como uma 
única regra. O formalismo escolhido aqui para a definição de regras é a lingua-
gem SWRL (Semantic Web Rule Language), uma linguagem de regras definida 
sobre a linguagem OWL (HORROCKS et al., 2004). Para ser eficiente na mode-
lagem de detectores simbólicos, a definição de SWRL deve ser restringida. Ini-
cialmente, uma regra SWRL tem a forma 

, 

onde ambos – antecedente e conseqüente – são compostos por uma conjunção 
de átomos na forma … . Os átomos são predicados n-ários (ou 
datalogs) semelhantes aos da lógica de primeira ordem. Eles são de três tipos bá-
sicos: (i) predicados unários representando classes do modelo (ex.: Pessoa(?x)); 
(ii) predicados binários, representando propriedades do modelo (ex.: temFi-
lho(?x, ?y)); e (iii) predicados n-ários, representando funções embutidas da lin-
guagem SWRL (ex.: somar(?r, ?op1, ?op2)). Um exemplo de regra SWRL é 

Pessoa(?x) ? temFilho(?x, João) ? temPai(João, ?x)  

que infere a propriedade temPai sobre o individuo de nome João, com base na 
classe Pessoa e na propriedade temFilho. Uma regra SWRL é satisfeita se, para 
uma dada combinação de valores possíveis para as variáveis (símbolos prece-
didos de ponto-de-interrogação), o antecedente for satisfeito. Se isso acontecer, 
o mecanismo de inferência SWRL infere que o conseqüente também é verdadei-
ro. 

Para aplicação em detectores simbólicos, os possíveis predicados que com-
põem o antecedente e o conseqüente devem ser restritos, a fim de não permitir 
mapeamentos entre níveis não adjacentes. Assim, dado um conjunto P de pre-
dicados unários (conceitos), binários (propriedades) e predicados n-ários (fun-
ções), sobre primitivas de um modelo de conhecimento visual. Dados os con-
juntos de predicados Pinf de primitivas de um dado nível (ex.: n. analógico), Psup 
de primitivas do nível de semântico imediatamente superior (ex.: n. visual), Pmap 
de primitivas de mapeamento entre instâncias e Pfun de funções de SWRL, tal 
que inf sup map funP P P P P? ? ? ? . Dada uma regra r, um conjunto A de predicados 
do seu antecedente e um conjunto C de predicados do seu conseqüente. Ou seja, 
se { }, , , ,1 2 3 nA a a a ... h=  e { }, , , ,1 2 3 mC c c c ... c= , a regra r é dada por 

1 2 3... ...1 2 3 n ma a a a c c c c? ? ? ? ? ? ? ? ? . 

A regra r é um detector simbólico se inf sup( )map funA P P P P? ? ? ?  e 
sup( )funC P P? ? . Nessa forma, um detector simbólico pode ter como anteceden-

te um conjunto de primitivas em ambos os níveis relacionados, porém pode ge-
rar novas conclusões somente no nível de abstração superior considerado. Isso, 
em conjunto com as relações de mapeamento entre instâncias da seção anterior, 



52 

 

 

organiza o domínio e evita que entidades do domínio sejam inferidas direta-
mente a partir do sinal. 

Como mencionado anteriormente, cada primitiva detectável é mapeada por 
um conjunto de detectores simbólicos. Essa relação é formalizada restringindo-
se o conjunto de axiomas C da seguinte forma. Dado um conjunto Pd composto 
de predicados representando primitivas detectáveis de um modelo, um predi-
cado ? ? Pd e um conjunto dS P? ?  contendo ? e todas as suas subprimitivas. 
Dado um conjunto DS de todas as regras de detecção desse modelo, uma regra r 
? DS e um conjunto Cr corresponde ao conjunto de predicados do conseqüente 
da regra r. Então existe uma relação , pDS?  tal que ?DS DS?  e 

( )r pr s r DS s C s S?? ? ? ? ? ? ? . Dessa forma, toda regra de detecção associada a 
uma primitiva deve conter no seu conseqüente essa mesma primitiva ou algu-
ma de suas subprimitivas (subclasse ou subpropriedade). Por exemplo, consi-
derando um detector simbólico que mapeia a primitiva semântica Sequência 
nas primitivas visuais CurvaGaussiana e temComprimento com valor longo. 
Para atender a todas restrições apresentadas anteriormente, ele teria a forma:  

 
EntidadeSemântica(?es) ? mapeamentoSemânticoParaVisual(?es, ?ev)  
  ? EntidadeVisual(?ev) ? CurvaGaussiana(?ev)  
  ? temComprimento(?ev,longo) 
? Sequencia(?es)  
 

É importante que os detectores possam ser integrados diretamente nos mo-
delos OWL utilizados para representar o conhecimento visual, uma vez que co-
nectam os níveis de abstração representados nesse formalismo. O padrão SWRL 
define ainda um metamodelo para representação de regras integradas a mode-
los OWL. Ele associa explicitamente átomos que compõem as regras às classes, 
propriedades e funções correspondentes. Assim, classes e propriedades utiliza-
das nas regras podem ser checadas contra o modelo onde estão representadas. 

 Além disso, o metamodelo pode ser estendido para adicionar novas caracte-
rísticas às regras. Essa característica é importante, pois permite estender a lin-
guagem SWRL para dar suporte ao agrupamento de regras em conjuntos de de-
tectores simbólicos e ao relacionamento explícito destes conjuntos com as primi-
tivas de conhecimento visual que devem ser extraídas. Na prática, Martin 
O’Connor (O’CONNOR et al., 2005) propõe uma extensão no metamodelo de 
SWRL, incluindo alguns construtos para agrupamento de regras. De forma 
simplificada, eles são:  

 
Class: RuleGroup 
  SubClassOf: Entity 
 
ObjectProperty: hasRuleGroup 
  Domain: swrl:Imp 
  Range: RuleGroup 
 



53 

 

 

onde swrl:Imp caracteriza a meta-entidade que captura uma regra SWRL. Ou 
seja, por esse modelo, regras SWRL pode ser agrupados em instâncias de Rule?
Group. Os serviços de raciocínio de regras que dão suporte a esse construto po-
dem executar conjuntos de regras separadamente. 

Ainda sim, não é possível representar a relação (?, DS?) entre uma primitiva 
e um conjunto de regras. Para isso, este trabalho define as seguintes metapro-
priedades: 

 
ObjectProperty: extraídoPorDS 
  Annotations:  
    rdfs:comment “Relaciona primitivas com grupos de  
      detectores simólicos. Domínio pode ser qualquer construto” 
  Range: swrla:RuleGroup 
  InverseOf: extraiPrimitiva 
  Characteristics: Functional, InverseFunctional 
 
ObjectProperty: extraiPrimitiva 
  Domain: swrla:RuleGroup 
  Range: owl:Thing 
  InverseOf: extraídoPorDS 
 

O mecanismo de interpretação utiliza a relação extraídoPorDS junto com os 
grupos de regras SWRL para acionar somente o conjunto de detectores simbóli-
cos necessários a extração de uma primitiva. 

Os detectores simbólicos são utilizados para mapeamento entre quaisquer 
níveis. No entanto, o mapeamento entre do nível visual e o nível analógico deve 
levar em conta a conversão de valores numéricos em objetos simbólicos. Por e-
xemplo, dever ser possível representar explicitamente que o valor MuitoLongo 
para a propriedade visual comprimento corresponde a um comprimento analó-
gico em um dado intervalo de valores (ex.: entre 200m e  1500m). 

De fato, o problema de quantização de valores é recorrente na Inteligência 
Artificial. Entre as diversas possíveis soluções existentes na literatura, está a so-
lução de mapeamento por conjuntos fuzzy proposta inicialmente por Corades-
chi e colegas (CORADESCHI et al., 2001). No seu trabalho, Coradeschi define 
que uma propriedade analógica pode ser convertida para valores simbólicos a-
través de conjuntos fuzzy definidos sobre o seu contradomínio. Um conjunto 
fuzzy é definido como um par ,  onde D é um conjunto de valores possíveis 
e m uma função tal que : 0,1 , chamada função de associação. Para cada 
x D? , m(x) dá o grau de associação de x ao conjunto D. Se m(x) = 0, então diz-se 
que o x não pertence ao conjunto fuzzy ,D m . Dando rótulos simbólicos para 
os conjuntos e agrupando-os sobre um mesmo espaço de valores D, é possível 
quantizar esse espaço de valores. 

A inclusão de conjuntos fuzzy para mapeamento entre os níveis visual e ana-
lógico requer que a definição dos detectores simbólicos seja sutilmente amplia-
da. Em especial, o conjunto de predicados A definido anteriormente, que cor-



54 

 

 

responde às primitivas de hipótese, deve prover algum tipo operador fuzzy. Es-
se operador é definido como uma função SWRL n-ária na forma  

1 1 2 2: (? , ? , ? , ? , ? , ? , , ? , ? )n nfuzzy associa v r fa tc fa tc fa tcL , 

onde as variáveis ?v e ?r são interpretadas como o valor testado pela função e o 
termo resultante, respectivamente. Cada dupla ?fan e ?tcn define uma função de 
associação trapezoidal com a sintaxe [v1 v2 v3 v4] ({ }1, 2 , 3, 4v v v v D D? ? ? ) e 
respectivo termo que representa o conjunto. Os termos devem pertencer ao 
mesmo domínio de valores de uma propriedade visual. Por exemplo, o detector 
simbólico  

 
EntidadeVisual(?ev) ? EntidadeAnalógica(?ea)  
  ? mapeamentoVisualParaAnalógico(?ev, ?ea)  
  ? Intervalo(?ea) ? comprimento(?vn)  
  ? fuzzy:associa(?vn, ?r, “[0,1 5 10 15]”, comprimentoCurto, 
     “[10 15 20 25]”, comprimentoLongo)  
  ? temComprimento(?ev, ?r) 
 

converte valores analógicos de comprimento para valores simbólicos de tem?
Comprimento no nível visual. A Tabela 3.2 abaixo mostra uma série de possíveis 
valores para as variáveis ?vn e ?r do exemplo anterior e o resultado da avalia-
ção do predicado. 

Tabela 3.2: Avaliação da função fuzzy:associa. 
?vn ?r Avaliação 
0,0 comprimentoCurto Falso 

1,2 
comprimentoCurto Verdadeiro 
comprimentoLongo Falso 

11,0 
comprimentoCurto Verdadeiro 
comprimentoLongo Verdadeiro 

21 
comprimentoCurto Falso 
comprimentoLongo Verdadeiro 

30 comprimentoLongo Falso 

Os detectores simbólicos permitem, então, que se estruturem os modelos 
de conhecimento organizadamente. Eles fornecem meios para facilitar a infe-
rência entre os níveis semânticos, ao mesmo tempo em que mantém o nível de 
expressividade dos modelos. 

3.2.5 Componente de Interpretação 
A máquina de interpretação semântica da abordagem S-Chart é operaciona-

lizada por dois algoritmos que, juntos, realizam a extração de feições simbólicas 
do sinal por geração e teste de hipótese. O seu funcionamento é baseado na ma-
nipulação dos modelos de conhecimento visual através dos seus três níveis se-
mânticos. Eles também fazem uso dos detectores simbólicos e dos mapeamen-
tos entre instâncias do componente de ancoramento descrito anteriormente, a-
lém de acionar os algoritmos de processamento analógico do sinal.  



55 

 

 

A arquitetura da máquina de interpretação é demonstrada na Figura 3.9. A 
sua entrada é composta pelo sinal digitalizado e um conceito do domínio que 
deve ser encontrado no sinal. Esse conceito faz o papel de hipótese inicial, uma 
vez que a leva em conta também suas subclasses e subpartes. O resultado final 
da interpretação são instâncias de conceitos do domínio relacionados à hipótese 
inicial de busca. 

 
Figura 3.9: Arquitetura do componente de interpretação. 

Essa arquitetura se divide em dois componentes básicos, conforme mostra a 
Figura 3.9. A máquina de interpretação semântica (MIS) operacionaliza o ancora-
mento dos conceitos do nível semântico para o nível visual. Ela tenta confirmar 
a presença de conceitos no domínio com base na suas extensões no nível visual, 
definidos por detectores simbólicos. Ao mesmo tempo, a MIS coloca a cargo da 
máquina de interpretação visual (MIV) a extração das primitivas visuais direta-
mente do nível analógico. O seu papel é acionar os algoritmos de processamen-
to de sinal para extração de feições analógicas no sinal dado como entrada no 
componente de interpretação e interpretá-los em termos de primitivas visuais 
via os seus próprios detectores simbólicos.  

A representação dos algoritmos neste trabalho requer algumas explicações. 
Ambos os algoritmos utilizam informações de contexto durante raciocínio, co-
mo os modelos de conhecimento visual, instâncias do modelo e o sinal propri-
amente dito. O contexto é proporcionado por uma estrutura Contexto definida 
na Tabela 3.3. Essa estrutura é utilizada para armazenar os resultados das infe-
rências e troca de informações entre as máquinas MIS e MIV. 

Além disso, em alguns lugares, os algoritmos se referem a primitivas de re-
presentação de conhecimento do OWL (na forma prefixo:primitiva). Essas 



56 

 

 

primitivas se organizam em uma ontologia de representação de conhecimento 
(ver Anexo). 

Tabela 3.3: Variáveis de contexto para interpretação semântica 
Contexto.NivelSemântico.TBox  Conjunto de primitivas do nível semântico 
Contexto.NivelSemântico.ABox  Conjunto de instâncias de primitivas do nível semântico 

Contexto.NívelVisual.TBox  Conjunto de primitivas do nível visual 
Contexto.NívelVisual.ABox  Conjunto de instâncias de primitivas do nível visual 

Contexto.NívelAnalógico.Box  Conjunto de primitivas do nível analógico 
Contexto.NívelAnalógico.ABox  Conjunto de instâncias de primitivas do nível analógico 

Contexto.Sinal  Representação digital do sinal analisado 

Nas seções seguintes, as máquinas MIS e MIV são descritas em maior deta-
lhe. 

3.2.5.1 Máquina de interpretação semântica (MIS) 

A MIS busca combinar feições visuais extraídas do nível analógico para de-
tectar entidades semânticas que tenham expressão visual. A sua entrada corres-
ponde a um conjunto de conceitos do nível semântico que indica uma hipótese 
inicial de entidade a ser extraída. As saídas correspondem às instâncias de enti-
dades do domínio efetivamente encontradas no sinal. O processo de interpreta-
ção visual é operacionalizado pelo Algoritmo 3.1. 

Mais especificamente, a sua entrada corresponde a um conjunto de classes 
do nível semântico (no caso, classes do tipo owl:Class) em hipoInicial. Elas 
correspondem às primitivas do nível semântico que formam as hipóteses de en-
tidades presentes no sinal. O primeiro laço do algoritmo (linhas 5-8) verifica se 
todas as primitivas de hipoInicial são mesmo primitivas do nível semântico.  

O segundo laço (linhas 12-14) busca por todos os conceitos relacionados às 
primitivas iniciais através da função buscaRelacionados(owl:Class primi?
tiva). Essa função busca o fecho transitivo de todas as subclasses e subpartes 
de primitiva. O laço resulta no conjunto hipoCompleta de todas as primitivas 
que podem ser encontradas no sinal. 

O terceiro laço (linhas 22-30)  separa as primitivas de hipoCompleta que a-
presentam uma expressão visual no nível visual. Isso é realizado buscando por 
primitivas do domínio que estão relacionadas a conjuntos de detectores simbó-
licos pela relação de mapeamento extraídoPorDS. Os detectores simbólicos de 
cada conjunto são tomados um a um para processamento (linha 24).  Eles são 
agrupados no conjunto detectoresSimbólicosSelecionados para posterior 
execução. A processo de separação das primitivas visuais (linhas 26-29) consiste 
em procurar no antecedente de cada detector simbólico átomos que represen-
tem primitivas pertencentes ao nível visual. As primitivas encontradas (concei-
tos ou propriedades visuais) são separadas no conjunto primitivasVisuai?
sEsperadas. 

As primitivas visuais esperadas são então encaminhadas para processamen-
to na interpretaçãoAnalógica (linha 35). O resultado da interpretação é 
composto de instâncias de primitivas visuais que podem ou não ser significan-



57 

 

 

tes para interpretação. Após, cada uma dessas mesmas instâncias são mapeadas 
em um única instância de EntidadeSmântica através do procedimento mapVi?
sualSemântico(), descrito no Algoritmo 3.2. 

O procedimento executaDetectoresSimbólicos(swrl:Imp cds{})  apli-
ca as regras que constituem os detectores simbólicos selecionados sobre o mo-
delo de conhecimento simbólico (modelos mais instâncias), realizando a infe-
rência propriamente dita. Uma vez que os detectores são formalizados em na 
linguagem SWRL, essa função pode corresponder à execução de uma ferramen-
ta de interpretação de regras SWRL. O resultado será a classificação das instân-
cias de EntidadeSemântica em classes do domínio mais específicas. Ao fim, 
instâncias do nível semântico não classificadas como alguma entidade do do-
mínio – portanto instâncias somente da classe EntidadeSemântica – são remo-
vidas no ultimo laço do algoritmo (linhas 45-50). 

O resultado de todo o processo de interpretação visual pode ser encontrado 
em Contexto.NívelSemântico.Inst. No entanto, por conveniência, a função 
de interpretação retorna diretamente as instâncias resultantes do algoritmo. 
1 interpretaçãoSemântica (owl:Class hipoInicial{}) : EntidadeSemântica{} 
2  
3   //verifica se todas os conceitos de hipotese  
4   //pertencem ao domínio 
5   owl:Class hipoInicialVerificada{}; 
6   para?cada c ? hipoInicial faça  
7     se (c ? Contexto.NivelSemântico.TBox) então 
8       hipoInicialVerificada = hipoInicialVerificada ? c; 
9  
10  
11    //busca todas os conceitos relacionados com as hipoteses inicial 
12   owl:Class hipoCompleta{}; 
13   para?cada c ? hipoInicialVerificada faça  
14     hipoCompleta = hipoCompleta ? buscaRelacionadosSemântico(c); 
15  
16   //seleciona quais entidades do domínio têm expressão visual, 
17   //ou seja, quais são mapeadas por detectores simbólicos 
18   //e quais primitivas visuais (classes e propriedades) são utilizadas 
19   owl:Class entidadesMapeadas{}; 
20   swrl:Imp detectoresSimbólicosSelecionados{}; 
21   rdf:Resource primitivasVisuaisEsperadas{}; 
22   para?cada c ? hipoCompleta faça  
23     se :extraídoPorDS(c, DS) então 
24       para?cada r ? DS faça { 
25         detectoresSimbólicosSelecionados = detectoresSimbólicosSelecionados ? r; 
26         rdf:Resource atomosC = r.antecedente; 
27         para?cada atomo ? DS faça  
28           se atomo ? Contexto.NivelVisual.TBox então 
29               primitivasVisuaisEsperadas = primitivasVisuaisEsperadas ? atomo; 
30       } 
31        
32   //aciona o componente de interpretação analógica  
33   // as entidade visuais resultantes são acessiveis  
34   // em Contexto.NivelVisual.Inst 
35   interpretaçãoAnalógica(primitivasVisuaisEsperadas); 
36  
37   //mapeia cada entidade visual em Contexto.NivelVisual.Inst  
38   //como uma instancia de :EntidadeSemântica em Contexto.NivelSemântico.Inst 
39   mapVisualSemântico(); 



58 

 

 

40  
41   //executa detectores simbólicos das entidades semânticas separados anteriormente 
42   executaDetectoresSimbólicos(detectoresSimbólicosSelecionados); 
43    
44   //remove instâncias não classificadas, ou seja, hipoteses que não se confirmaram 
45   EntidadeSemântica resultado{}; 
46   para?cada es ? Contexto.NívelSemântico.ABox faça 
47     se ?c(rdf:type(es, c) ? c ? EntidadeSemântica) então 
48       Contexto.NívelSemântico.ABox = Contexto.NívelSemântico.ABox ? es 
49     senão 
50       resultado = resultado ? es; 
51   //para conveniencias, retorna entidades classificadas 
52   //elas podem ser acessadas fora do algortimo em  
53   retorna resultado; 
54 } 

Algoritmo 3.1: função de operação da máquina de interpretação semântica 
55 mapVisualSemântico (){ 
56  
57   //agrupa instâncias de EntidadeVisuais 
58   :EntidadeVisual entidadesDetectadas{}; 
59   para?cada inst ? Contexto.NivelVisual.ABox faça 
60     se owl:SubClassOf(inst, :EntidadeVisual) então 
61       entidadesDetectadas = entidadesDetectadas ? r; 
62    
63   //cria uma EntidadeSemântica para EntidadeVisual não mapeada 
64   para?cada iev ? entidadesDetectadas faça 
65     se !(?x(mapeamentoSemânticoParaVisual(x, iev))) então { 
66       EntidadeSemantica novaEntidade = novaInstancia(:EntidadeSemantica); 
67       Contexto.NivelVisual.ABox = Contexto.NivelVisual.ABox  
68         ? novaEntidade ? mapeamentoSemânticoParaVisual(novaEntidade, iev); 
69     } 
70 } 

Algoritmo 3.2: procedimento de mapeamento visual/semântico 

3.2.5.2 Máquina de interpretação visual (MIV) 

A máquina MIV é executada a partir de requisições da máquina MIS. Ela re-
cebe como entrada um conjunto de primitivas visuais que possivelmente exis-
tem no sinal e tenta confirmá-las através dos seus detectores simbólicos mapea-
dos em primitivas do nível analógico. A MIV se encarrega ainda de acionar os 
mecanismos de processamento de sinal para extração das características analó-
gicas do sinal. Embora o algoritmo não tenha uma saída propriamente dita, os 
resultados da sua execução (instâncias de EntidadeVisual) são alocados em 
Contexto.NívelVisual.Abox para que a maquina MIS tenha acesso. O algo-
ritmo que operacionaliza a máquina MIV é descrito pelo Algoritmo 3.3. 

O primeiro laço (linhas 4-6) busca por primitiva relacionadas na taxonomia 
onde elas estão inseridas. Essa entidade pode ser uma classe ou uma relação. 
Mais especificamente, a função buscaRelacionadosVisual(rdf:Resource 
res) retorna o conjunto de todas as subclasses de res e todas as suas superclas-
ses, caso res seja uma classe. Se res é uma relação, então o retorno da função é 
um conjunto com todas as sub-relações e super-relações de res.  

Com a hipótese visual completa, busca-se por detectores simbólicos associa-
dos às primitivas visuais em hipoVisualCompleta (linhas 12-24). De forma 



59 

 

 

semelhante ao que ocorre do terceiro laço do algoritmo de interpretação semân-
tica, o algoritmo analisa os detectores simbólicos encontrados em busca de pri-
mitivas analógicas referenciadas (classes ou relações). Essas entidade são guar-
dadas no conjunto primitivasAnalógicasEsperadas. 

As linhas 28-34 operacionalizam a extração das primitivas analógicas espe-
radas. Cada uma das primitivas é repassada ao componente de processamento 
de sinal pela função extrairEntidadeAnalógica(rdf:Resource  p).  Essa 
função deve acionar um algoritmo de processamento de sinal (armazenada na 
estrutura Contexto.Sinal) específico para extração da primitiva p. As instân-
cias de primitivas extraídas devem ser guardadas em Contex?
to.NívelSemântico.ABox. A função retorna a quantidade de instâncias extra-
ídas. De fato, a implementação dessa funcionalidade pode corresponder a um 
componente de software complexo por si só. Uma vez que essa função não de-
pende de aspectos de representação e interpretação de conhecimento visual, a 
sua descrição detalhada foge do escopo deste trabalho e é omitida. No entanto, 
no estudo de caso descrito no Capítulo 5 é apresentada uma versão simplificada 
– mas efetiva – para ela. O laço é executado até que nenhum novo resultado seja 
gerado. Isso garante que possíveis relações de dependência entre primitivas não 
influenciem nos resultados. 

O procedimento mapAnalógicoVisual() executa função semelhante ao 
procedimento descrito em Algoritmo 3.3, porem dessa vez mapeando instâncias 
de EntidadeAnalógica para instâncias de EntidadeVisual. Já o procedimento 
executaDetectoresSimbólicos(swrl:Imp  cds{}) executa os detectores 
simbólicos de entidade visuais para classificá-las em primitivas mais específi-
cas.  Por fim, o algoritmo retorna o controle da execução para a máquina MIS, 
sendo que o resultado da execução está armazenado em Contex?
to.NivelVisual.TBox.  
1 interpretaçãoVisual(rdf:Resource hipoVisualInicial{}){ 
2  
3    //busca todas os conceitos relacionados com as hipoteses inicial 
4   rdf:Resource hipoVisualCompleta{}; 
5   para?cada c ? hipoVisualInicial faça  
6     hipoVisualCompleta = hipoVisualCompleta ? buscaRelacionadosVisual(c); 
7    
8      
9   //seleciona as entidades visuais que realmente têm expressão analógica, 
10   //ou seja, quais são mapeadas por detectores simbólicos 
11   //e quais primitivas analógicas (classes e propriedades) são utilizadas 
12   rdf:Resource entidadesMapeadas{}; 
13   swrl:Imp detectoresSimbólicosSelecionados{}; 
14   rdf:Resource primitivasAnalógicasEsperadas{}; 
15   para?cada c ? hipoVisualCompleta faça  
16     se :extraídoPorDS(c, DS) então 
17       para?cada r ? DS faça { 
18         detectoresSimbólicosSelecionados = detectoresSimbólicosSelecionados ? r; 
19         rdf:Resource atomosC = r.antecedente; 
20         para?cada atomo ? DS faça  
21           se atomo ? Contexto.NívelAnalógico.TBox então 
22             primitivasAnalógicasEsperadas = primitivasAnalógicasEsperadas  
23               ? atomo; 
24       } 



60 

 

 

25  
26   //Aciona os algoritmos de processamento de sinal para extração  
27   // de entidades analógicas esperadas 
28   Inteiro resultado; 
29   faça{ 
30     resultado = 0; 
31     para?cada p ? hipoVisualCompleta faça 
32       resultado = extrairEntidadeAnalógica(p); 
33      
34   } enquanto resultado ? 0; 
35  
36   //mapeia cada EntidadeAnalógica em Contexto.NivelAnalógico.Inst  
37   //como uma instancia de EntidadeVisual em Contexto.Nivelvisual.Inst 
38   mapAnalógicoVisual(); 
39  
40   //executa detectores simbólicos das entidades visuais separados anteriormente 
41   executaDetectoresSimbólicos(detectoresSimbólicosSelecionados); 
42 } 

Algoritmo 3.3: procedimento de operação da máquina de interpretação visual 

3.2.5.3 Considerações sobre ancoramento simbólico 

O funcionamento geral do componente de interpretação pode ser classifica-
do com uma abordagem mista de ancoramento simbólico. Ele operacionaliza o 
raciocínio top-down, ao restringir as possíveis interpretações a partir da hipótese 
inicial informada. Somente os detectores simbólicos relacionados às hipóteses 
iniciais – por relações de taxonomia ou partonomia – serão acionados. Ou seja, a 
geração de hipóteses no “topo” da arquitetura é restrita pela estrutura do mo-
delo de domínio e pela hipótese inicial. O caminho até a “base” é realizado a-
través do processamento dos detectores simbólicos. 

Por outro lado, o processo bottom-up é iniciado no momento que a máquina 
MIV extrai informações sensórias ao acionar os algoritmos de processamento de 
sinal. As informações extraídas são propagadas aos níveis superiores aplicando-
se os detectores simbólicos previamente selecionados no processamento top-
down. 

 



 

 

 

4 ESTRATIGRAFIA DE SEQUÊNCIAS 

Este capítulo descreve brevemente o domínio de aplicação deste trabalho, a 
Estratigrafia de Sequências, sub-área da Geologia. Os conceitos apresentados 
aqui suportam a discussão sobre o sistema InteliStrata, apresentado no capítulo 
seguinte. O sistema InteliStrata é uma aplicação do framework S-Chart para in-
terpretação de gráficos no domínio da Estratigrafia de Sequências. 

O foco da discussão sobre a Estratigrafia de Sequencias feita aqui está na ta-
refa interpretação estratigráfica, que consiste na identificação de feições geoló-
gicas em perfis de poço, mais especificamente, perfis de raios gama, como o re-
presentado na Figura 4.1. A identificação das feições baseia-se na busca por pa-
drões específicos na curva de raio gama e nas suas inter-relações espaciais. 

 
Figura 4.1: Perfil típico em diferentes escalas. Raios gama em destaque. 

Inicialmente, será apresentado o domínio da Estratigrafia de Sequências, A 
seguir o foco é direcionado a interpretação estratigráfica. Por fim, é discutida a 
eficácia da utilização de wavelets para análise básica de perfis de raios gama.  

4.1 Apresentação do domínio 
A Estratigrafia de Sequências é uma área de Geologia que estuda o histórico 

de formação dos estratos que compõem o subsolo terrestre em termos de varia-
ções do nível do mar e taxas de sedimentação. Nessa área, diferentes sequências 



62 

 

 

de estratos indicam processos de formação distintos. O correto entendimento 
destes processos é importante para diversas outras áreas, especialmente para a 
Geologia de Petróleo, onde a interpretação dos processos que controlaram a 
formação de um reservatório define as estratégias de exploração e produção de 
óleo. 

O objetivo principal do sistema InteliStrata é sugerir interpretações de se-
quências. Uma sequência corresponde a um ciclo completo de descida e subida 
do nível do mar. Em geral, esse fenômeno afeta uma região localizada (800 a 80 
mil quilômetros quadrados) e pode levar de 100 mil a 5 milhões de anos para se 
completar.  Os diversos momentos de um ciclo influenciam diferentemente na 
ordem de deposição dos sedimentos na região onde ele acontece. Por exemplo, 
à medida que o nível do mar sobe em um dado local, ocorre a deposição de um 
sedimento mais lamoso e menos poroso. Quando o nível do mar desce no mes-
mo local, ocorre a deposição de um sedimento mais grosso, formando uma ro-
cha mais porosa. De fato, variações de porosidade da rocha têm uma grande 
correlação com variações no nível do mar. Dessa forma, estudando-se as varia-
ções de porosidade ao longo da profundidade do subsolo terrestre, é possível se 
ter uma ideia aproximada dos ciclos variação do nível do mar naquela região, e 
logo, das sequências. 

A interpretação de sequências é feita com base em dados coletados em poços 
de sondagem com até 8000 metros de profundidade, perfurados ao longo de 
uma região de interesse. Desses poços, dois tipos básicos de dados são coleta-
dos: testemunhos e perfis de poço. Durante a perfuração do poço, um tipo espe-
cial de broca extrai amostras de rochas na forma de pequenos cilindros. Estas 
amostras dão ao geólogo a possibilidade de analisar diretamente a rocha que 
constitui o terreno. Ele descreve precisamente as características de cada estrato 
da amostra (como tipo de rocha, porosidade, tamanho de grão, estrutura e etc.) 
e as organiza visualmente em uma coluna estratigráfica. Com base na análise 
visual da coluna estratigráfica, o geólogo consegue identificar visualmente os 
padrões de empilhamento de sedimentos e, a partir daí, identificar a sequência 
com um grande grau de precisão. Esse processo é feito em diversos poços, de 
forma que, após as informações serem correlacionadas, é possível identificar 
como uma mesma sequência afeta toda uma região. 

No entanto, o processo de perfuração com testemunhagem é consideravel-
mente caro. Durante o estudo de uma região, poucos poços são testemunhados. 
Os demais são analisados através de perfilagem sísmica (elétrica, raios gama, 
entre outras). Após ser perfurado, o poço é perpassado por uma sonda ao longo 
de toda a sua profundidade. Durante a descida, a sonda faz diversos tipos de 
medidas da rocha a sua volta, em intervalos de aproximadamente 20 cm (vari-
ando conforme a medida). E resultado é um perfil do poço, um gráfico que de-
monstra a variação das diversas medidas ao longo da sua profundidade. Exis-
tem vários tipos de medidas. Cada uma correlacionada com alguma caracterís-
tica. Padrões de variação nessas medidas refletem diferentes tipos de caracterís-
ticas do terreno. Ou seja, os perfis de poço servem como um instrumento indire-



63 

 

 

to de análise do terreno, quando, por exemplo, a análise direta da rocha pelo 
testemunho não é realizada. Um dos tipos de medida para interpretação de se-
quências é o perfil de raios gama. De forma simplificada, ele mede a radiação 
gama emitida pelas rochas de um poço. Através da radiação é possível inferir a 
porosidade de uma rocha. Em geral, quanto maior a leitura de raios gama, me-
nor é a porosidade e vice-versa. Como dito anteriormente, a porosidade da ro-
cha tem uma grande correlação com a ocorrência de sequências, por isso, geólo-
gos utilizam o perfil de raios gama como uma ferramenta para identificá-las.  

Em geral, a interpretação de sequências combina a análise de testemunhos 
com a análise de perfis. No entanto, por ser de custo inferior ao da testemunha-
gem, freqüentemente o perfil é o único dado disponível, sendo necessário extra-
ir interpretações geológicas úteis somente a partir dos perfis. A identificação 
dos tipos e limites de sequências é uma tarefa que requer ao geólogo um trei-
namento razoável em Estratigrafia. Mesmo quando domina o conhecimento ne-
cessário, a análise de perfilagens com centenas de metros de profundidade mos-
tra-se uma atividade manual morosa e consumidora de tempo.  Essa é a moti-
vação do sistema InteliStrata. Ele deve identificar sequências possíveis a partir 
de um perfil de raios gama e sugeri-las para o usuário. Outra vantagem desse 
sistema é que ele possibilita ao geólogo estratigráfico economizar tempo, suge-
rindo uma interpretação inicial.  

Na seção seguinte, a interpretação de sequência por perfil de raios gama é 
detalhada e é demonstrada a efetividade de um algoritmo de processamento 
para identificação de sequências e parassequências.  

4.2 Interpretação de sequências em perfis de raios gama 
A identificação de sequências em perfis de raios gama é um raciocínio pre-

dominantemente visual. O geólogo especialista analisa o gráfico do perfil visu-
almente, buscando padrões de variação na curva que casem com padrões de va-
riação conhecidos e que indicam a presença de determinadas feições geológicas. 
Além disso, a análise visual é condicionada pelo contexto e conhecimento pré-
vio a respeito da geologia da região, visto que o padrão visual de uma feição 
geológica pode mudar conforme a bacia sedimentar onde ela se insere. 

Existem inúmeras feições geológicas que podem ser interpretadas em um 
perfil. Neste trabalho, o foco é na interpretação de três tipos de feições: sequên-
cias, parassequências e superfícies de inundação máxima. 

Sequências, como mencionando anteriormente, são formadas por grandes 
ciclos de variação do nível do mar, que influenciam na formação dos estratos de 
sedimentos. Em um perfil de raios-gama, uma sequência tem a expressão seme-
lhante a uma curva gaussiana (Figura 4.2a): um movimento de subida e descida 
do nível do mar relativamente suaves. Uma sequência pode ter, no perfil, uma 
espessura entre 15 e 1500 metros aproximadamente. 

Já parassequência são, simplificadamente, pequenos ciclos locais de variação 
no nível do mar dentro de uma sequência. Podem ser vistas como um ciclo de 



64 

 

 

menor ordem. No perfil, uma parassequência se caracteriza como uma curva 
que inicia por uma variação negativa abrupta e uma recuperação suave até o 
mesmo nível (Figura 4.2b). A sua espessura está em uma escala inferior à se-
quência, variando entre aproximadamente 3 e 60 metros.  

 
Figura 4.2: Expressões genéricas das feições geológicas no perfil. 

A superfície de inundação máxima é o ponto de uma sequência onde o nível 
do mar atinge o seu ponto máximo e começa a baixar. No perfil, ele é facilmente 
identificado como o ponto mais alto da curva que forma a sequência.  

O relacionamento entre essas três feições é utilizado para refinar a interpre-
tação. Por exemplo, parassequências geralmente estão inseridas em uma se-
quência. Logo, a interpretação de uma parassequência é realizada após a identi-
ficação dos limites da seqüência onde ela se insere. Da mesma forma, a escala 
da análise é determinante para a correta identificação de sequências ou paras-
sequências, uma vez que a assinatura das duas feições é muito similar, exceto 
pelo contexto e escala em que ocorrem.  

A máquina de interpretação do sistema InteliStrata depende de um algorit-
mo de processamento de sinal que faça a segmentação inicial do perfil, de acor-
do com a escala onde as feições a serem interpretadas ocorrem e as feições bus-
cadas. A seguir é analisada uma proposta de segmentação por wavelets, baseado 
na detecção dos padrões de curva que formam as feições geológicas procuradas.  

4.3 Análise de perfis por wavelets 
Até a data de escrita deste trabalho, não existe nenhuma solução automática 

para interpretação de sequências descrita na literatura. Da mesma forma, ne-
nhum sistema estratigráfico atualmente no mercado oferece essa facilidade. Por 
outro lado, alguns autores exploram a utilização de técnicas matemáticas e 
computacionais, em especial processamento de sinal, para auxilio da interpreta-
ção de fenômenos cíclicos em dados de perfil. Enquanto alguns autores focam 
na análise da transformada de fourier (SILVA, 2001), algumas propostas explo-
ram  a possibilidade da análise espectral por wavelets ser uma boa ferramenta 
para detecção manual de sequências (CHOUDHURY et al., 2007). 

Do ponto de vista empírico, o uso de wavelets é a opção mais interessante. A 
análise de sinais via transformada de Fourier é eficaz somente em sinais perió-
dicos. Ao contrário disso, sequências – e as demais feições geológicas aqui con-
sideradas – são não-periódicas quanto a sua ocorrência em um perfil, onde co-



65 

 

 

existem em diferentes escalas e freqüências. Exatamente neste ponto que a 
transformada wavelet leva vantagem. A transformada wavelet contínua (TWC) 
responde a essas variações de ciclos em diferentes escalas e freqüências.  

Conceitualmente, o processamento da TWC é relativamente direto. O espec-
tro resultante de um TWC pode ser visto como uma resposta de similaridade de 
um determinado pulso de onda periódica com todas as partes de um sinal, em 
várias escalas. As diferentes wavelets são caracterizadas pelo formato de pulso 
aplicado na transformada. Por exemplo, a Figura 4.3 representa o pulso utiliza-
do pela wavelet gaussiana 2. Já o espectro resultante de um TWC tem o mesmo 
comprimento do sinal e altura correspondente ao número de escalas definidas 
para análise pelo usuário. Ele demonstra a resposta do sinal em comparação ao 
pulso da wavelet.  Em geral, partes do sinal que se assemelham a wavelet têm 
uma resposta maior a partes que não se assemelham. Esse comportamento tor-
na a TWC uma ferramenta interessante para reconhecimento de padrões, se o 
padrão buscado puder ser codificado como uma wavelet. 

 
Figura 4.3: Wavelet gussiana 2. 

Com base nesta capacidade, este trabalho propõe a utilização de duas TWC 
para processamento inicial do sinal que compõem os gráficos dos perfis de raios 
gama. Os algoritmos propostos tomam o papel dos algoritmos de processamen-
to de imagem convencionais. O objetivo é segmentar o sinal em intervalos, con-
forme a resposta de similaridade dada pelo TWC. A escolha das wavelets utili-
zadas se deu através de testes empíricos de resposta aos padrões de curva dese-
jados. Um estudo mais sistemático de diferentes wavelets seria necessário para 
se garantir que a opção utilizada garante o melhor resultado na segmentação, 
mas a seleção do algoritmo de processamento de imagem é secundária para a 
contribuição buscada neste trabalho.  

Os testes foram realizados sobre um perfil já interpretado pelo especialista e 
que cobre aproximadamente 340 metros. O perfil digitalizado contém 16.465 
amostras, resultando em um período de amostragem de aproximadamente 
0,018 metro. A seguir as duas TWC testadas são detalhadas. 



66 

 

 

4.3.1 Detecção de sequências: wavelet gaussiana 
Como descrito anteriormente, uma sequência se apresenta no perfil de forma 

semelhante a uma curva gaussiana. Isso induz a ideia de que a TWC com a wa-
velet gaussiana possa ser eficiente na detecção de sequências em perfis. Para 
demonstrar isso, o perfil de teste foi analisado através da TWC disponível no 
software Matlab (MATHWORKS INC.). O teste é descrito a seguir. 

Inicialmente, é considerada a banda de freqüências correspondente ao inter-
valo de períodos típico de sequências. Como mencionado anteriormente, uma 

 
Figura 4.4: Transformada wavelet gaussiana 2 em um perfil de raios gama. 



67 

 

 

sequência pode ter um período que varia entre 15 e 1500 metros de espessura 
no perfil. Para simplificar a TWC, o perfil é re-amostrado para uma freqüência 
0,013 amostras por metro, correspondendo a um período de amostragem de 
7,62 metros aproximadamente. Isso minimiza a presença de ciclos com menos 
de 15 metros no perfil (ruídos de alta frenqüência). 

A TWC é aplicada sobre o sinal resultante, agora com 44 das 16.465 amostras 
anteriores. Os parâmetros utilizados para a TWC são os seguintes: 

• Wavelet: Gaussiana número 2 (disponível no pacote de wavelets do Ma-
tlab), escolhida com base na sua semelhança com o padrão de curva de 
uma sequência; 

• Escalas: determinadas pela fórmula ( ) 2ns n = , onde, no caso deste sinal, 
0 2 4n , , ,= L . Essas escalas garantem que o resultado da TWC fique res-

trito a ciclos com períodos de 15 a 1500 metros. 

• Extensão: simétrica em ambos os lados do sinal, a fim de minimizar as 
distorções de borda. 

O resultado da aplicação dessa wavelet sobre o perfil é demonstrado na Figu-
ra 4.4. À direita, o espectro de coeficientes resultando da TWC demonstra onde 
o padrão de curva gaussiana está mais evidente (regiões mais claras). A linha 
pontilhada marca a interpretação de limites de sequência conforme feita pelo 
especialista sobre o perfil de teste (esquerda). É possível notar que os picos no 
espectro (setas) correspondem a aproximadamente ao centro das sequências, e 
os seus correspondentes vales as bordas de sequência. Isso é mais evidente nas 
sequências de maior escala, como a Sequência 2 e, em certa medida, a Sequência 
1. Isso demonstra a boa resposta da wavelet gaussiana 2 ao padrão de curva 
gaussiano definido para sequência. 

Com base nesse resultado, este trabalho considera que a TWC é uma ferra-
menta suficiente para processamento inicial do perfil. Ela identifica padrões 
correspondentes a sequências, que podem ser utilizados para alimentar os algo-
ritmos simbólicos no sistema InteliStrata. No entanto, os algoritmos simbólicos 
devem garantir que ruídos sejam minimamente propagados no processo de in-
terpretação (especialmente ruídos de alta freqüência). 

4.3.2 Detecção de parassequências: wavelet smoothtooth 
A detecção de parassequências por wavelets se dá de forma semelhante à de-

tecção de sequências, porém com algumas diferenças. Inicialmente, o padrão de 
curva de uma parassequência é diferente de uma gaussiana, como demonstrado 
na Figura 4.2 anterior. Além disso, essa curva não tem correspondente entre as 
wavelets da biblioteca padrão. Por isso, este trabalho propõe uma nova wavelet 
específica pare detecção deste padrão de curva. Ela foi criada dentro da função 
de criação de wavelets do aplicativo Matlab, seguindo o perfil de uma parasse-
quência padrão, e batizada de wavelet smoothtooth, devido a semelhança do pa-
drão original com um período da onda periódica padrão dente-de-serra.  



68 

 

 

Neste teste, o sinal foi reamostrado para o período de 1,5 metros, conforme o 
período mínimo desejado. O resultado é um sinal com 217 amostras. Além dis-
so, os parâmetros utilizados para a TWC são: 

• Wavelet: Smoothtooth; 

• Escalas: determinadas pela fórmula ( ) 2ns n = , onde 1 2 5n , , ,= L . Essas 
escalas garantem que o resultado da TWC fique restrito a faixa de ciclos 
de parassequências, com períodos de 3 a 60 metros. 

 
Figura 4.5: Transformada wavelet sawtooth em um perfil de raios gama. 



69 

 

 

• Extensão: também simétrica em ambos os lados do sinal, a fim de mini-
mizar ruídos de borda. 

Os resultados na aplicação da TWC com essa wavelet em um perfil é demons-
trado na Figura 4.5. As setas claras marcam os picos que correspondem aproxi-
madamente às parassequências determinadas pelo especialista (ps). Embora 
parte delas tenha sido discriminada, algumas não o são, como é o caso ps1, on-
de não aconteceram picos (seta escura inferior). Há também falsos positivos, 
como as marcadas pelas duas setas escuras superiores. Isso ocorre devido a es-
cala relativamente pequena das parassequências. Uma vez que elas aparecem 
em freqüências maiores, elas se confundem com ruídos de alta freqüência do 
perfil. Ainda sim, pode-se concluir que a wavelet smoothtooth pode ser utiliza-
da para auxiliar na extração de parassequências do perfil. 



 

 

 

5 SISTEMA INTELISTRATA 

Este capítulo detalha a implementação do sistema InteliStrada, uma aplica-
ção do framework S-Chart para interpretação de gráficos no domínio da Estra-
tigrafia de Sequências O objetivo do sistema InteliStrata é, dado como entrada 
um perfil de raios gama, fornecer ao usuário uma sugestão de interpretação de 
sequências, parassequências e superfícies de inundação máxima. 

Naturalmente, o sistema InteliStrada complementa a definição do frame-
work S-Chart do ponto de vista da implementação. Nas seções seguintes, serão 
explorados aspectos da infra-estrutura de software necessária para operaciona-
lizar os conceitos apresentados no capítulo 3. Além disso, são também explora-
dos aspectos de implementação específicos da interpretação estratigráfica. 

5.1 Objetivos do sistema 
O objetivo do sistema InteliStrata é sugerir interpretações estratigráficas so-

bre perfis de raios gama. Padrões visuais de comportamento da curva em perfis 
de raio gama sugerem a presença de sequências, parassequências e superfícies 
de inundação máxima. O sistema captura e combina esses padrões com conhe-
cimento de domínio, em um processo de interpretação semântica. Mais especifi-
camente, o sistema sugere possíveis instâncias de sequências, parassequências e 
superfícies de inundação máxima presentes no perfil. Para realizar essa tarefa, o 
sistema InteliStrata implementa e especializa o framework S-Chart. 

Do ponto de vista de sistema, o desenvolvimento do InteliStrata segue dois 
objetivos principais: 

• Utilização de modelos semanticamente ricos na implementação. Um 
dos princípios mais importantes na implementação de sistema intensivos 
em conhecimento é o princípio da preservação de estrutura (SCHREIBER 
et al., 1999; FIORINI, 2006). Por ele, o modelo de implementação (arquite-
tura, modelo de classes e etc.) de um sistema de conhecimento deve bus-
car manter ao máximo a estrutura dos modelos de conhecimento. Isto fa-
cilita a manutenção em um cenário onde os próprios modelos de conhe-
cimento são suscetíveis a mudanças constantes, além de facilitar o traba-
lho de implementação e manutenção. Em um caso ideal, o modelo de co-
nhecimento deve ser especificado em uma linguagem possível de ser a-
cessada diretamente pelo sistema de conhecimento (como linguagens de 
representação baseadas em XML). Dessa forma, o código do sistema de 



71 

 

 

conhecimento pode se restringir ao máximo à implementação dos algo-
ritmos de raciocínios. 

• Emprego de ferramentas off the shelf. O sistema InteliStrata é baseado no 
framework S-Chart, cujos modelos de representação são definidos em 
linguagens formais de representação de conhecimento, como OWL e 
SWRL. Uma vez que elas são processáveis por software, existem diversas 
ferramentas que dão suporte a manipulação, manutenção e raciocínio so-
bre modelos descritos nessas linguagens. Assim, é possível que estas fer-
ramentas sejam utilizadas como componentes de infra-estrutura para o 
desenvolvimento de um sistema de interpretação semântica, minimizan-
do o esforço de implementação nessas áreas (como estrutura de dados, 
por exemplo). 

Estes objetivos influenciam diretamente a arquitetura do sistema InteliStrata 
e sua forma de implementação. 

5.2 Arquitetura do sistema 
Um visão geral da arquitetura do sistema InteliStrata é apresentada na Figu-

ra 5.1. Os componentes que a formam podem ser divididos em três conjuntos: 
(i) um conjunto de componentes que constituem o sistema de interpretação 
propriamente dito; (ii) com conjunto de componentes de software para infra-
estrutura; (iii) um conjunto de modelos OWL que representam os modelos de 
conhecimento utilizados pelo software.  

Nas próximas seções, cada um desses componentes é detalhado, bem como a 
sua interrelação. 

 
Figura 5.1: Arquitetura de componentes do sistema InteliStrata. 



72 

 

 

5.2.1 Modelos de representação e extensões 
Como apresentado anteriormente, o framework S-Chart define um conjunto 

de modelos de conhecimento visual para dar suporte a interpretação semântica. 
Alguns desses modelos devem ser supridos pelo sistema que implementa o 
framework (como é o caso do modelo de domínio no nível semântico) enquanto 
outros modelos são disponibilizados pelo próprio framework, com a possibili-
dade de extensão (modelos no nível visual e analógico).  

 
Figura 5.2: Dependências entre ontologias OWL do sistema InteliStrata. 

No sistema, cada um dos modelos é descrito em uma ontologia OWL8. A Fi-
gura 5.2 mostra a relação de dependência entre as ontologias e os seus respecti-
vos prefixos. As ontologias AnalogLevelOntology, VisualLevelOntology, 
SemanticLevelOntology e SignalProcessingOntology modelam, respecti-
vamente, os três níveis semânticos e a ontologia de funcionalidades de proces-
samento de sinal. A StratigraphyOntology captura o conhecimento de domí-
nio da estratigrafia de sequências. Todas elas são agregadas em uma ontologia 
de aplicação chamada InteliStrataOntology. A sua função é agregar as de-
mais em um só arcabouço, além de servir de repositório para as extensões reali-
zadas nos modelo de conhecimento visual. Nas seções seguintes, cada uma des-
sas extensões é detalhada. 

                                                 

 
8 Embora o OWL descreva qualquer modelo da sua linguagem como uma “ontologia”, cla-

ramente o termo não é o mais preciso. Um modelo OWL – e de qualquer outra linguagem –
pode ser caracterizado como uma ontologia somente se modela formalmente uma conceituali-
zação compartilhada (como a definição dada por Gruber e Borst (1998)). No entanto, mantém-
se o termo aqui para manter consistência com o vocabulário de OWL. 



73 

 

 

5.2.1.1 Ontologia de domínio: Estratigrafia de Sequências 

O conhecimento sobre Estratigrafia que dá suporte a interpretação estrati-
gráfica é capturado por uma ontologia modelada em StratigraphyOntology. 
Uma vez que esta ontologia não existe publicamente, parte do desenvolvimento 
do sistema InteliStrata focou em, junto com um especialista em Estratigrafia, e-
liciar o conhecimento de domínio mínimo necessário para realização da inter-
pretação estratigráfica. A principal parte do conhecimento eliciado é modelada 
na ontologia de domínio (cuja estrutura básica é apresentada na Figura 5.3). Ou-
tros conceitos são capturados na forma de extensões do nível visual e na defini-
ção dos detectores simbólicos (detalhados a seguir).  

Como mencionado anteriormente, o sistema utiliza conhecimento de domí-
nio para extrair instâncias de sequências, parassequências e superfícies de i-
nundação máxima. Estes três tipos de entidades são capturados na ontologia 
(Figura 5.3). O modelo também apresenta relações de composição entre estas as 
entidades do modelo, semelhantes à relação de composição existente entre os 
conceitos coração e pessoa. Neste modelo, a composição é representada intuiti-
vamente pela relação transitiva hasPart. Essa relação será inferida através das 
relações topológicas entre as entidades visuais que ancoram cada um desses 
conceitos.  

 

Figura 5.3: Ontologia da Estratigrafia. 



74 

 

 

A ontologia modelada certamente está longe de representar uma conceitua-
lização completa o suficiente para ser considerada uma ontologia da Estratigra-
fia. No entanto, o modelo mínimo apresentado é o suficiente para suportar a in-
terpretação estratigráfica a que este sistema se propõe fazer 9.  

5.2.1.2 Extensões: nível visual 

A ontologia no nível visual proposta pelo framework S-Chart é constituída 
por primitivas visuais básicas, utilizadas na interpretação de qualquer tipo de 
gráfico. No entanto, há a possibilidade de extensão do modelo com primitivas 
visuais específicas de cada domínio de interpretação. No caso do sistema Inte-
liStrata, foi detectada a necessidade de apenas um tipo de extensão. 

Como já mencionado anteriormente, em domínios complexos, propriedades 
visuais idênticas podem ter interpretações diferentes, dependendo do contexto 
onde são aplicadas. Tomando-se como exemplo sequências e parassequências. 
A extensão visual de ambas as entidades contém a propriedade que captura a 
ideia de comprimento. No entanto, a escala de interpretação da propriedade va-
ria entre elas. Uma sequência longa (entre 142 m e 845 metros) está em uma es-
cala muito diferente de uma parassequência longa (entre 43 metros e 76 me-
tros). Se essa diferença não for levada em conta, o algoritmo de interpretação 
não será capaz de diferenciar em qual escala se encontra uma dada entidade vi-
sual ainda não classificada como sequência ou parassequência. 

Dessa forma, a primitiva LengthValue é estendida com duas subclasses de-
finidas como: 

 
Class: SequenceLengthValue 
  EquivalentTo: 
    ValorDeComprimento  
    and ({VeryShortLengthSequence, 
      ShortLengthSequence, 
      MediumLengthSequence, 
      LongLengthSequence, 
      VeryLongLengthSequence}) 
 
Class: ParassequenceLengthValue 
  EquivalentTo: 
    ValorDeComprimento  
    and ({ShortLengthParassequence, 
      MediumLengthParassequence,  
      LongLengthParassequenc}) 
 

                                                 

 
9 A arquitetura aqui proposta deverá ser integrada a ontologia completa de domínio que es-

tá sendo elaborada pelo mestrando Alexandre Lorenzatti, neste mesmo programa de pós-
graduação. Resultados são previstos para o final do ano de 2009. 



75 

 

 

As instâncias que formam os dois contradomínios de valores de comprimen-
tos são modeladas também como diferentes entre si.  

Estas são as únicas extensões necessárias para o nível visual. Eventualmente, 
elas serão utilizadas para definição dos detectores simbólicos entre os níveis 
semânticos. 

5.2.1.3 Extensões: nível analógico e processamento de sinal 

O nível analógico, embora não receba uma extensão propriamente dita, re-
cebe novas entidades necessárias para suporte de aspectos ligados ao sistema. 
Essas extensões permitem que entidades do modelo referenciem de forma ho-
mogênea containeres dos dados brutos que compõem o sinal.  

Idealmente, os dados digitais que formam o sinal deveriam ser totalmente 
representados em termos de primitivas do nível analógico. Por exemplo, como 
uma cadeia de instâncias da primitiva Ponto. No entanto, tal solução é tecnica-
mente inviável. Uma vez que um sinal pode ter até dezenas de milhares de a-
mostras, seria necessário o mesmo número de instâncias da classe Ponto para 
representá-los. Isso sobrecarregaria desnecessariamente os algoritmos de inter-
pretação, dado que somente alguns desses pontos são significativos para o pro-
cesso de inferência. A solução para esse problema é a inclusão de primitivas que 
possam representar os dados digitais que compõem um sinal de forma eficiente, 
sem que se perca a ligação entre o modelo e esses dados. Para isso, são defini-
das as primitivas apresentadas na Figura 5.4. 

 
Figura 5.4: Modelo para representação de dados brutos.  

No modelo proposto, todas as entidades analógicas devem referenciar al-
gum container de dados pela propriedade hasRawDataContainer. Cabe aos al-
goritmos de processamento e interpretação carregar e interpretar estes dados de 
forma correta.  

A segunda extensão é simples e direta. O framework S-Chart disponibiliza 
um ontologia simples para representação de algoritmos de processamento de 
sinal. Algoritmos utilizados no sistema devem ser representados como instân-
cias desta ontologia. Evidentemente, essas instâncias não se preocupam em re-
presentar o funcionamento interno dos algoritmos (detalhados nas seções 4.3 e 
5.2.2.1). Elas existem para que os próprios algoritmos tenham a sua existência 
descrita no modelo, de modo que possam ser referenciados como discriminado-



76 

 

 

res no processo de interpretação, por exemplo. Os dois algoritmos são represen-
tados da seguinte forma: 

 
Individual: SmoothtoothSPF 
    Types: WaveletSegmentator 
 
Individual: Gaussian2SPF 
    Types: WaveletSegmentator 
 

Cabe ao sistema manter a ligação entre estas instâncias e os componentes de 
software que elas representam. De fato, isso é realizado pelo componente de ge-
renciamento de processamento de sinal, descrito na seção 5.2.3.1 a seguir. 

As novas extensões definidas das últimas seções são suficientes para permi-
tir a modelagem dos modelos de conhecimento visual. No entanto, ainda resta a 
modelagem dos detectores simbólicos sobre essas primitivas.  

5.2.1.4 Detectores simbólicos 

Os detectores simbólicos mapeiam explicitamente entidades entre os níveis 
semânticos do modelo de conhecimento visual. O framework S-Chart define 
que os detectores simbólicos sejam modelados como regras SWRL com uma es-
trutura bem definida. Em geral, os detectores são definidos especificamente pa-
ra cada domínio de aplicação, uma vez que a interpretação visual pode variar 
entre domínios distintos. 

O sistema InteliStrata contém um total de 14 detectores (regras), divididos 
em 9 conjuntos, que são responsáveis pelo mapeamento. Todos eles são apre-
sentados no Apendice. Neste capítulo é descrito o exemplo de mapeamento do 
conceito dom:Sequence da ontologia de domínio no nível semântico até as en-
tidades analógicas correspondentes. 

Inicialmente, o conceito dom:Sequence é mapeado para um grupo de detec-
tores simbólicos chamado sdgDomSequence. Esse grupo mantém referência pa-
ra os detectores formados para extrair a entidade semântica de entidades visu-
ais. No caso do conceito dom:Sequence, apenas um detector é utilizado (sdSe?
quence): 

 
mapo:mappingSemanticToVisual(?de, ?ve)   
  ? vlo:GaussianCurve(?ve)   
 ? vlo:gaussianCurveCharacteristic(?ve, ?pattern)   
 ? swrlineq:greaterThanOrEqual(?pattern, vlo:StrongPattern)   
 ? vlo:length(?ve, ?length)   
 ? swrlineq:greaterThanOrEqual(?length, MediumLengthSequence)  
? dom:Sequence(?de) 
  

 De acordo com o detector, uma sequência tem como expressão visual uma 
curva gaussiana com característica forte, com um comprimento maior ou igual a 
curto dentro da escala de tamanho de sequência. Cada uma dessas primitivas 
visuais, por sua vez, deve ser extraída do nível analógico. Por esse motivo, elas 



77 

 

 

também apresentam detectores específicos definidos em termos de primitivas 
analógicas. A primitiva vlo:GaussianCurve, por exemplo, é associada a um 
grupo de detectores (sdgGaussianCurve). Esse grupo contém o seguinte detec-
tor. 

 
mapo:mappingVisualToAnalogic(?ve, ?al)   
  ? al:Interval(?al)   
  ? spo:apExtractedWith(?al, spo:Gaussian2SPF)   
  ? spo:apExtractionStrength(?al, ?vstrength)   
  ? fuzzy:match(?vstrength, ?strength,  
     "[0, 0, 0.2, 0.3]", vlo:VeryWeakPattern,  
     "[0.2, 0.3, 0.4, 0.5]", vlo:WeakPattern,  
     "[0.4, 0.5, 0.6, 0.7]", vlo:MediumPattern,  
     "[0.6, 0.7, 0.8, 0.9]", vlo:StrongPattern,  
     "[0.8, 0.9, 1, 1]", vlo:VeryStrongPattern) 
 ? vlo:GaussianCurve(?ve)   
  ? vlo:gaussianCurveCharacteristic(?ve, ?strength) 
 

Ele define que uma curva gaussiana é expressa no sinal na forma de um a?
lo:Interval extraído pelo algoritmo spo:Gaussian2SPF. O mesmo detector 
se encarrega de inferir o valor característico da curva. O valor é inferido através 
da função fuzzy embutida fuzzy:match, que atribui os valores característicos 
conforme o valor de intensidade (spo:apExtractionStrength) corresponden-
te. Em outro exemplo, a propriedade visual vlo:hasLength é extraída por um 
conjunto contento dois detectores (sdSequenceLength e sdParassequence?
Length): 

 
mapo:mappingVisualToAnalogic(?ve, ?al)   
  ? al:Interval(?al)   
  ? al:apLength(?al, ?value)   
  ? fuzzy:match(?value, ?fv,  
     "[0, 0, 66, 82]", VeryShortLengthSequence,  
     "[66, 82, 146, 210]", ShortLengthSequence,  
     "[146, 210, 466, 722]", MediumLengthSequence,  
     "[466, 722, 1746, 2770]", LongLengthSequence,  
     "[1746, 2770, 5000, 7500]", VeryLongLengthSequence) 
? vlo:length(?ve, ?fv) 
 
mapo:mappingVisualToAnalogic(?ve, ?al)   
  ? al:Interval(?al)   
  ? al:apLength(?al, ?value)   
  ? fuzzy:match(?value, ?fv,  
     "[5, 10, 50, 60]", ShortLengthParassequence,  
     "[50, 60, 140, 150]", MediumLengthParassequence,  
     "[140, 150, 200, 250]", LongLengthSequence)  
? vlo:length(?ve, ?fv) 
 

Ambos extraem valores de comprimento contextualizados – para sequência 
ou parassequência. Ambos também utilizam a função fuzzy:match para inferir 



78 

 

 

os valores lingüísticos para propriedade visual. Os valores utilizados para os 
conjuntos foram escolhidos com base no conhecimento eliciado do domínio. 

 Embora ausentes no exemplo anterior, as relações espaciais também são an-
coradas no nível analógico. Os seus detectores simbólicos são relativamente in-
dependentes de domínio, pois são definidos em função de entidades genéricas 
no nível analógico. Todos eles são agrupados em um conjunto relacionado à 
propriedade vlo:hasSpatialRelationWith, que inclui todas as propriedades 
espaciais. Um exemplo de detector para inferência da relação topológica de par-
te própria sobre intervalos: 

 
mapo:mappingVisualToAnalogic(?ve1, ?al1)   
  ? alo:Interval(?al1)   
  ? alo:hasStartPoint(?al1, ?sp1)   
  ? alo:hasEndPoint(?al1, ?ep1)   
  ? alo:index(?sp1, ?vsx1)   
  ? alo:index(?ep1, ?vex1)   
  ? mapo:mappingVisualToAnalogic(?ve2, ?al2)   
  ? alo:Interval(?al2)   
  ? alo:hasStartPoint(?al2, ?sp2)   
  ? alo:hasEndPoint(?al2, ?ep2)   
  ? alo:index (?sp2, ?vsx2)   
  ? alo:index (?ep2, ?vex2)   
  ? swrlb:greaterThan(?vsx2, ?vsx1)   
  ? swrlb:greaterThan(?vex1, ?vex2)  
 
? vlo:isNonTangentialProperPartOf(?ve2, ?ve1)   
  ? vlo:hasForNonTangentialProperPartOf(?ve1, ?ve2) 
 

Esse detector relaciona intervalos que estão propriamente contidos em ou-
tros intervalos, testando os limites de cada um. O resultado é uma relação is?
NonTangentialProperPartOf (e a sua inversa hasForNonTangentialProper?
PartOf) entre o intervalo contido e o intervalo que contém. 

Os detectores definidos no sistema InteliStrata, bem como os demais mode-
los de conhecimento visual estão completamente descritos no Anexo. 

5.2.2 Componentes de infra-estrutura 
Para dar suporte a todos os serviços de raciocínio e processamento de sinal, 

o sistema InteliStrata emprega componentes desenvolvidos por terceiros. Sobre 
estes componentes de infra-estrutura (Figura 5.5) são construídos os demais al-
goritmos de interpretação. A fim de justificar os componentes escolhidos, al-
gumas considerações são feitas. 



79 

 

 

 
Figura 5.5: Componentes de infra-estrutura do sistema InteliStrata. 

A primeira se refere à representação dos modelos de conhecimento. Como 
discutido na seção 3.1, os modelos de conhecimento do framework S-Chart usão 
formalizados na linguagem OWL. Essa linguagem tem suporte de algumas fer-
ramentas modelagem, sendo o software Protégé (KNUBLAUCH et al., 2004) o 
escolhido para utilização no InteliStrata. Essa escolha é pautada por alguns mo-
tivos, sendo os principais: 

• No momento da escrita deste trabalho, o Protégé é certamente a mais po-
pular e estável ferramenta para modelagem a manipulação de OWL. Ela 
conta com uma grande comunidade de usuários e sua distribuição é livre. 

• A ferramenta disponibiliza uma interface de programação que permite a 
manipulação e consulta de modelos OWL diretamente de outras aplica-
ções. Esta facilidade permite que o sistema InteliStrata manipule os mo-
delos de conhecimento visual diretamente, sem a necessidade da sua 
conversão para paradigmas de representação de informação menos ex-
pressivos (como paradigma relacional). Isso atende diretamente ao obje-
tivo de desenvolvimento com preservação de estrutura. 

• A arquitetura da própria ferramenta é extensível, permitindo a criação e 
integração de extensões e a sua utilização via a interface de software. Pa-
ra o sistema InteliStrata é utilizada a extensão SWRLTab (O’CONNOR et 
al., 2005), direcionado manipulação de regras SWRL. Essa extensão tem a 
sua importância em si, de forma que permite a definição e implementa-
ção de extensões sobre a própria linguagem SWRL, uma característica 
importante para definição dos detectores fuzzy. 

A execução das regras SWRL é garantida pelo software Jess (FRIEDMAN, 
2003), uma máquina para execução de regras também extensível. Embora o Jess 
seja uma pacote de software independente, a sua interface de programação po-
de ser facilmente integrada ao Protégé, via SWRLTab. De fato, atualmente a 
máquina Jess é a única máquina que permite a execução de regras SWRL fun-
ções embutidas definidas pelo usuário. Adicionalmente, é importante ressaltar 
que o sistema InteliStrata não faz uso de raciocinadores , como os implemen-
tados nas ferramentas Pellet (PARSIA et al., 2005) ou FaCT++ (TSARKOV; 
HORROCKS, 2006), uma vez que os modelos do sistema não apresentam rela-
ções implícitas e as instâncias dos modelos são geradas já na sua classificação 
definitiva.  



80 

 

 

A escolha das ferramentas Protégé e Jess também implica que a implementa-
ção seja baseada no ambiente Java (LINDHOLM; YELLIN, 1999), uma vez que 
essas duas disponibilizam as suas interfaces de programação nessa linguagem. 

Qualquer sistema de interpretação como este requer algum tipo serviço para 
processamento numérico que seja eficiente. No caso do sistema InteliStrata, es-
ses serviços de processamento devem dar suporte ao desenvolvimento de pro-
cessamento de sinal de uma dimensão. Existe pouco suporte nativo para a lin-
guagem Java. Por isso, escolheu-se utilizar o software matemático Matlab 
R2008a já mencionado anteriormente, que disponibiliza diversos pacotes para 
processamento de dados numéricos e, em especial, processamento de sinal. Por 
isso, o terceiro e último grande componente de infra-estrutura é o software Ma-
tlab. Os processamentos realizados com ele são melhor detalhados na subseção 
seguinte. 

Por fim, as ferramentas apresentadas garantem uma infra-estrutura adequa-
da que atende aos objetivos do sistema InteliStrada. O suporte da ferramenta 
Protégé à manipulação direta dos modelos de conhecimento ajuda na manuten-
ção da estrutura.  

5.2.2.1 Algoritmos de processamento por wavelet 

O processamento inicial do sinal é realizado dentro do software Matlab a-
través da aplicação da TWC com as wavelets apresentadas nas seções 4.3. No en-
tanto, a aplicação não é direta. O sinal necessita de tratamento pré-
processamento e pós-processamento, onde o resultado bruto da TWC é tratado 
antes de ser disponibilizado aos níveis superiores de raciocínio. 

A TWC para ambas as wavelets passam por processamentos semelhantes, 
variando em alguns parâmetros. Os passos do algoritmo são os seguintes: 

• O sinal é re-amostrado para freqüência mínima necessária para detecção 
de sequências (wavelet gaussiana) ou parassequências (wavelet smoothto-
oth). A freqüência mínima é derivada através do período mínimo de uma 
sequência (15 metros) ou parassequência (3 metros). O resultado é um si-
nal com menor quantidade de amostras, menor ruído da alta freqüência e 
mais eficiente para processamento; 

• A TWC é aplicada sobre o sinal re-amostrado. Em ambos os casos, a 
transformada é configurada para utilizar extensões espelhadas no sinal, a 
fim de amenizar os problemas de processamento na borda; 

Neste pontos, os o algoritmo para a wavelet guassiana se diferencia da wave-
let smoothtooth. Para wavelet gaussiana: 

• O espectro resultante é normalizado por nível; 

• Considera-se o espectro como uma superfície e procura-se pelos máximos 
locais na superfície.  



81 

 

 

• Sendo que cada máximo esta contida em uma das escalas, são procurados 
os mínimos à direita e a esquerda dentro da escala, considerando apenas 
essa escala. Os pares de mínimos formam os limites de segmentação 

No caso da wavelet sawtooth: 

• O espectro é normalizado e invertido; 

• Consideram-se cada escala do espectro e procura-se pelos máximos locais 
em cada uma. 

• A seguir, são encontrados os mínimos locais de cada lado dos máximos. 
Os pares de mínimas formam os limites de segmentação. 

Nesse ponto os algoritmos voltam a ser iguais: 

• Em seguida, um laço iterativo busca pelos limites de cada intervalo en-
contrado e segmenta o sinal de acordo com esses limites. O pico de cada 
intervalo é dado como a força padrão (que acabada dando origem à pro-
priedade característica no nível visual). Quanto mais alto o número, mais 
a curva para o intervalo se parece com o padrão da wavelet. 

Ao final, os dados são organizados e repassados ao ambiente Java para con-
versão em entidades do modelo de conhecimento (alo:Interval em geral).  

5.2.3 Componentes de interpretação 
Os componentes de interpretação caracterizam a máquina de interpretação 

semântica em si. Estes componentes são três (Figura 5.6): (i) o componente geren-
ciador de processamento de sinal, que trata de gerenciar a segmentação e extração 
do sinal a ser analisado pelo sistema; (ii) o componente de interpretação, que im-
plementa as máquinas MIS e MIV descritas no capítulo anterior; e (iii) o compo-
nente de interface gráfica com o usuário (GUI), que disponibiliza o resultado da 
interpretação ao usuário final. 

Os três componentes trocam informações dentro de um contexto comparti-
lhado. Esse contexto é dado pela ontologia OWL InteliStrataOntology. De 
fato, essa é uma extensão da utilização da estrutura Contexto das máquinas 
MIS e MIV, descritas na seção 3.2.5 no capítulo anterior. 

 
Figura 5.6: Componentes de interpretação do sistema InteliStrata. 



82 

 

 

5.2.3.1 Gerenciador de processamento de sinal 

O gerenciador de processamento de sinal tem por função delegar e gerenciar 
a aplicação dos algoritmos de processamento presentes no sistema. Ele recebe 
primitivas analógicas que devem ser extraídas diretamente do sinal e aciona o 
algoritmo adequado para a sua extração. O resultado da extração são instâncias 
dessas primitivas incluídas no modelo OWL para que sejam utilizados no pro-
cesso de interpretação simbólico. 

Cada algoritmo de processamento pode processar mais de uma primitiva 
por vez. Por exemplo, um algoritmo que extrai um conjunto de intervalos pode 
ao mesmo tempo calcular outras propriedades como comprimento, ponto inici-
al e final e etc. 

Grande parte dos algoritmos é implementado dentro do ambiente Java do 
sistema InteliStrata. No entanto, a extração por wavelets é repassada, via Java 
RMI, para módulos nativos do sistema Matlab. Nele, os algoritmos de extração 
utilizam as funcionalidades dos pacotes de processamento de sinal para ajusta o 
sinal que compõe o perfil de poço e extrair as feições visuais conforme descrito 
na seção anterior. O resultado bruto é repassado para o ambiente Java e conver-
tido em entidades analógicas dentro da ontologia OWL. 

5.2.3.2 Componente de interpretação 

O componente de interpretação implementa as máquinas MIS e MIV, con-
forme os algoritmos especificados na seção 3.2.5. Embora a implementação seja 
bastante direta em relação à especificação, alguns aspectos necessitam uma a-
tenção especial. 

Inicialmente, todas as manipulações de entidades dos modelos de conheci-
mento são realizadas diretamente sobre as ontologias OWL que representam 
esses modelos. Isso é feito através da interface de programação do aplicativo 
Protégé. Dessa forma, as entidades manipuladas não precisam ser convertidas 
para algum outro formalismo (como relacional, por exemplo) a fim de serem u-
tilizadas. 

Os algoritmos das máquinas MIS e MIV necessitam de algum mecanismo 
para execução dos detectores simbólicos, definidos como regras SWRL. No sis-
tema InteliStrata, a execução das regras é feita pelo componente Jess. A interface 
do componente Jess com o sistema InteliStrata é realizada pelo componente 
SWRLTab. 

5.2.3.3 Componente de interface 

O componente de interface gráfica (GUI) fornece ao usuário um meio para 
manipular e visualizar os resultados gerados pela interpretação. Idealmente, a 
interface deve prover as seguintes funcionalidades: 

• Filtragem interativa das informações interpretadas. A interface deve 
permitir ao usuário filtrar os resultados da interpretação, seja para 



83 

 

 

melhor visualização, seja para excluir interpretações espúrias que po-
dem ocorrer; 

• Carregamento de perfis de poço e salvamento de interpretação: o 
sistema deve permitir que dados de perfis de poços sejam importados 
e que o resultado das interpretações seja salvo; 

• Exibição gráfica interativa: a exibição dos resultados deve ser gráfica 
e interativa (com operações de zoom e etc.) 

Definidos esses requisitos, é necessário ressaltar que o foco principal deste 
trabalho é o desenvolvimento de um aplicativo protótipo que demonstre a via-
bilidade do framework S-Chart para interpretação de gráficos. Devido a esse fa-
to, o protótipo implementado aqui apenas demonstra como estas funcionalida-
des podem ser alcançados. Uma versão operacional deste sistema demandaria 
um estudo de interface e estratégias de visualização de dados.   

O componente de interface projetado para o protótipo permite a visualiza-
ção de todos os resultados da interpretação. Funções de carregamento de dados, 
salvamento e filtragem foram implementadas diretamente no código, como 
uma prova de conceito. A interface, com o resultado da interpretação para o 
perfil do poço Tenneco Rattlesnake State 2-12 é demonstrado na Figura 5.7. 

 
Figura 5.7: Interpretação de parte do perfil Tenneco Rattlesnake State 2-12. 

Um dos aspectos importantes do componente de interface é que os resulta-
dos da interpretação são recuperados diretamente dos modelos OWL, via 
Protègè. A consulta é realizada com a linguagem SPARQL, linguagem padrão 
de consulta para bases de dados RDF e OWL. Ela representa uma forma consis-
tente para pesquisa de informações geradas nos modelos. Além disso, os pró-



84 

 

 

prios modelos podem orientar a pesquisa. Por exemplo, para exibição das enti-
dades na Figura 5.7, todas as sequências são recuperadas inicialmente. Para exi-
bição, os atributos analógicos associados com cada sequência são recuperados. 
A seguir, conforme o modelo, as partes de cada sequência (parassequência e 
superfície de inundação máxima) são recuperadas e também exibidas com base 
nos dados analógicos onde estão ancoradas. Outro aspecto, é que consultas 
SPARQL podem ser geradas dinamicamente. Em um sistema completo, elas 
podem ser usadas para implementar um mecanismo de filtragem dinâmica das 
informações do modelo. 

5.3 Validação do sistema InteliStrata 
Aqui será demonstrada a avaliação de dois perfis de poços para os quais e-

xiste interpretação detalhada. As duas seções escolhidas para estudo de caso 
deste trabalho tem significado histórico para a Geologia.  Seu estudo deu ori-
gem às propostas originais de interpretação de bacias sedimentares a partir da 
identificação da oscilação do nível dos oceanos ao longo do tempo geológico, ou 
seja, deu origem à própria área de Estratigrafia de Sequências. Elas foram utili-
zadas como exemplos na publicação introdutória da área de Estratigrafia de Se-
quências de Van Wagoner e colegas (1990) e constituem a escolha ideal para tes-
te do sistema InteliStrata pois a sua interpretação estratigráfica já é consolidada 
e reconhecida. 

A metodologia de avaliação é direta e típica em sistemas de conhecimento: o 
sistema é válido se, para um dado perfil e uma dada interpretação desse perfil, 
o especialista no domínio considera o resultado da interpretação relevante. 

Para validação do sistema InteliStrata foram utilizadas duas seções retiradas 
de dois perfis de raios gama distintos. As seções são: 

• Seção 1 de raios gama com aproximadamente 340 metros, retirada a par-
tir da profundidade 820m do perfil medido no poço Tenneco Rattlesnake 
State 2-12, localizado na região de Book Cliffs, em Utah, EUA.  

• Seção 2 de raios gama com aproximadamente 360 metros, retirada a par-
tir da profundidade 30m do perfil medido no poço Exxon Production Rese-
arch Co. Sego Canyon nº 2, também localizado na região de Book Cliffs.
  

5.3.1 Preparação dos perfis 
As versões originais dos perfis foram obtidas no site do Departamento de 

Recursos Naturais do governo do estado de Utah, EUA (DEPARTMENT OF 
NATURAL RESOURCES, 2009). Eles são disponibilizados como imagens digi-
talizadas dos perfis em papel, no formato TIFF com 1 bit de cor, como a Figura 
4.1 no início deste capítulo.  

Para utilização no sistema InteliStrata, o perfil deve ser totalmente digitali-
zado. O processo de digitalização das imagens dos perfis foi implementado da 
seguinte forma: 



85 

 

 

• A imagem do perfil é limpa de outras informações até restar somente a 
área que representa o perfil de raios gama. Em seguida a imagem é ero-
dida para eliminar a maior parte de ruídos e imperfeições. 

• Da imagem do perfil é derivada uma máscara que descreve a somente 
curva do perfil com uma espessura maior. Esse processo é realizado ma-
nualmente. 

• A imagem limpa e a máscara são carregadas no programa Matlab. Um 
algoritmo desenvolvido como parte deste trabalho faz a digitalização. A 
cada linha de pixels da imagem limpa, encontram-se os limites da másca-
ra. Dentro deste limite, o algoritmo busca pela maior linha de pixels pin-
tados e calcula o seu centro. Este deve ser o centro da linha que caracteri-
za o perfil. Esse valor é gravado num vetor e repete-se o processo para a 
próxima linha. Posteriormente, os valores de posição do vetor são con-
vertidos para a escala de leitura do perfil de raios gama.   

• Eventualmente, algumas leituras são rejeitadas e o sinal ganha o valor ze-
ro em alguns pontos. Estes problemas de digitalização são eliminados 
com algoritmos de interpolação. 

O resultado do processo é uma representação vetorial do sinal original do 
perfil. Para utilização no sistema InteliStrata, uma instância de FileRawData?
Holder é criada, apontando para um arquivo com esse vetor. 

Numa aplicação real, os próprios sistemas de perfilagem de poço geram o 
arquivo com o vetor de dados que seria a entrada do sistema InteliStrata.    

5.3.2 Avaliação dos resultados e validação 
O primeiro fator importante na avaliação das interpretações obtidas pelo sis-

tema InteliStrata se refere a forma como é feita a interpretação estratigráfica. 
Como descrito anteriormente, uma interpretação completa é obtida através da 
correlação de diversas informações, não somente do perfil de raios gama. De 
acordo com o especialista, a interpretação baseada puramente no perfil ocorre 
quando as informações complementares ainda não estão disponíveis. O resul-
tado da interpretação do perfil tem o papel de dar a primeira ideia de quais são 
as feições geológicas presentes (sequência, parassequências e etc.), para que, 
posteriormente, sejam ajustadas conforme a disponibilidade de dados de teste-
munho, sísmica ou estudos geológicos anteriores da bacia sedimentar. Dessa 
forma, o resultado da interpretação realizada pelo sistema InteliStrata deve ser 
comparado com o resultado da análise preliminar do perfil, feita pelo especia-
lista sem a complementação de outros dados geológicos. 

O primeiro teste foi realizado com o perfil da Seção 1 e é demonstrado na Fi-
gura 5.8. A Interpretação A apresenta a interpretação realizada pelo especialista 
levando em conta somente o perfil. Já as interpretações B (B1, B2, B3, B4 e B5) 
são as obtidas pelo sistema InteliStrata.  

Inicialmente, é possível notar uma discordância entre o número de interpre-
tações do especialista e do sistema. De fato, o sistema sugere interpretações al-



86 

 

 

ternativas para as mesmas sequências. Três delas (B1, B2 e B4) concordam razo-
avelmente com os limites de sequência da interpretação gabarito. No entanto, 
não há nenhuma interpretação do sistema que sugira a presença da Sequência 3. 
É possível ajustar os níveis de ruído para possibilitar a sua interpretação. No en-
tanto, tal ajuste possibilita a passagem de sequências espúrias na interpretação 
final.  

No caso das superfícies de inundação máxima (mfs), todas as interpretações 
do sistema concordam com a do especialista (ex.: seta B). 

Já a interpretação das parassequências é mais ruidosa, gerando interpreta-
ções duplas e (ex.: seta A) falsos positivos (ex: seta C). No entanto, em alguns 
casos, há concordância quase exata, como as parasequências 8 e 9 na Sequência 
2 e, em um menor grau, a parasequências 3 na Sequência 1. Essa maior discor-
dância na interpretação ocorre devido a menor escala das parassequências. Fei-
ções em escalas muito pequenas se confundem com ruídos de alta freqüência 
durante segmentação do sinal, sendo, por isso, eliminadas no processamento 
simbólico.  

O segundo teste foi realizado com o perfil da Seção 2, demonstrado na Figu-
ra 5.9. A Interpretação A apresenta a interpretação realizada pelo especialista 
levando em conta somente o perfil. Já as interpretações B (B1, B2, B3 e B4) são as 
obtidas pelo sistema InteliStrata.  

Do ponto de vista qualitativo, a interpretação da Seção 2 é semelhante a da 
Seção 1, com destaque positivo para a grande coincidência da interpretação B3 
com a Sequência 1. 

Por fim, os resultados apresentados aqui foram descritos como satisfatórios 
pelo especialista, mesmo havendo discrepâncias entre a interpretação sugerida 
e a feita pelo sistema. A racionalização por trás desse fato é a imprecisão ineren-
te à interpretação estratigráfica por perfis de raios gama. A marcação precisa 
dos limites das feições é uma tarefa significativamente complexa e um tanto 
subjetiva, especialmente quando se leva em conta somente a informação do per-
fil. Um fato que corrobora com essa análise é a afirmação feita pelo especialista 
ao confrontar os resultados, declarando que algumas das feições interpretadas 
pelo sistema são visões válidas sobre o perfil, mesmo contrariando a sua pró-
pria interpretação. Um exemplo é a diferença de interpretação da mfs apontada 
pela seta D na figura Figura 5.9. O especialista afirma que a interpretação dada 
pelo sistema para a mfs também é uma interpretação possível, quando não es-
tao disponíveis outros dados geológicos. 

Os resultados mostram-se ainda mais válidos quando se considera o tempo 
demandado para a interpretação manual dos perfis pelos profissionais aptos a 
realizar interpretação de sequências. O processamento automático associado ao 
ajustes manuais necessários para obter um conjunto válido de interpretações 
requerem um tempo significativamente menor para a tarefa de interpretação 
quando comparado ao necessário para realizar a atividade na realidade atual.  



 

 

 

 
Figura 5.8: Comparação das interpretações do especialista e do sistema InteliStrata para Seção 1. 



88 

 

 

B

 
Figura 5.9: Comparação das interpretações do especialista e do sistema InteliStrata para Seção 2. 



 

 

 

6 CONCLUSÃO 

A área de visão computacional é de grande importância para computação. A 
tarefa de interpretar o conteúdo de dados pictóricos é bastante desafiadora e 
tem sido abordada de diversas maneiras nas últimas duas décadas. Uma dessas 
maneiras é a interpretação semântica de imagens, área deste trabalho. Ela busca 
unir os esforços de pesquisa alcançados nas áreas de processamento de imagens 
e de representação de conhecimento, especialmente conhecimento visual. 

Este trabalho propôs um framework completo para interpretação semântica 
de gráficos, um tipo específico de imagem que representa visualmente um sinal 
de uma dimensão. O S-Chart, como é chamado, define modelos de conhecimen-
to visual em três níveis semânticos e aplica conceitos de ancoramento simbólico 
para realizar o mapeamento entres entidades destes níveis. Além de propor al-
goritmos genéricos para operacionalizar a interpretação visual com base nos 
modelos de conhecimento. 

Este trabalho também descreveu o sistema InteliStrata, uma implementação 
do framework S-Chart para interpretação de gráficos no domínio da Geologia, 
mais especificamente, no domínio da Estratigrafia de Sequências. O objetivo do 
sistema InteliStrata é identificar feições geológicas (sequências, parassequências 
e superfícies de inundação máxima) com base na interpretação semântica de 
gráficos de perfis de raios gama. Os resultados obtidos foram considerados pelo 
especialista em Estratigrafia de Sequências que acompanhou o projeto como 
compatíveis com a interpretação dada por um geólogo com experiência nesse 
tipo de interpretação a partir dos mesmos dados. 

6.1 Framework S-Chart e modelos de conhecimento visual 
A análise de propostas de interpretação semântica existentes na literatura 

demonstrou que poucas delas são voltadas para representação e interpretação 
de dados pictóricos em domínios imagísticos. Além disso, nenhuma das pro-
postas aborda a interpretação de gráficos, cuja importância é relevante em di-
versos domínios. Tampouco foram encontradas abordagens que se baseiam em 
padrões de representação abertos, o que dificulta a integração dos resultados da 
interpretação com outros sistema de conhecimento, por exemplo. Por esses mo-
tivos, desenvolveu-se uma nova abordagem de interpretação semântica de ima-
gens voltada para interpretação de gráficos. 



90 

 

 

Os resultados positivos do framework S-Chart se dividem em duas partes. 
Primeiramente, o modelo de representação de conhecimento visual em três ní-
veis conseguiu capturar o conhecimento necessário para interpretação de gráfi-
cos de forma independente da aplicação, garantindo a reusabilidade da aborda-
gem em outros domínios. Isso se deu na forma de primitivas de representação 
em todos os níveis semânticos, com destaque para as primitivas no nível visual. 
Outro grande avanço no framework S-Chart foi a definição de uma maneira 
homogênea de mapeamento entre níveis semânticos, na forma dos detectores 
simbólicos. 

Em segundo lugar, a escolha da linguagem OWL e SWRL como formalismo 
de representação dos modelos de conhecimento facilita a implementação do 
framework em sistema de interpretação, como se demonstrou com o sistema In-
teliStrata. As vantagens sobre a utilização de formalismos específicos são diver-
sas, desde a utilização de uma linguagem já bem formalizada até a integração 
com outros sistemas que utilizam a mesma linguagem.  

O sistema InteliStrata demonstrou como o framework S-Chart  pode ser im-
plementado em um sistema real para interpretação de perfis de poço, no domí-
nio Estratigrafia de Sequência. O sistema complementa partes do framework 
definidas como dependente de domínio. Essas partes são um modelo de domí-
nio simples descrevendo os principais conceitos da Estratigrafia de Sequência, 
primitivas visuais dependentes do domínio e alguns detectores simbólicos. A-
lém disso, foi demonstrado como componentes de software e sistemas de tercei-
ros podem ser integrados para dar suporte a interpretação semântica. Os resul-
tados apresentados pelo sistema InteliStrata foram validados e considerados sa-
tisfatórios pelo especialista no domínio. 

6.2 Sugestão para trabalhos futuros 
Durante o desenvolvimento do framework, foram propostas diversas primi-

tivas de representação, especialmente no nível visual. Embora o sistema InteliS-
trata faça uso de apenas algumas delas, claramente novas primitivas podem ser 
definidas. De fato, ao contrário do que acontece com imagens reais, não existem 
trabalhos que definam parâmetros visuais genéricos em gráficos. Assim, à me-
dida que novas aplicações sejam desenvolvidas, a ontologia que modela o nível 
visual pode ser incrementada como novas primitivas. 

Um problema encontrado durante o desenvolvimento do modelo do nível 
visual foi dar uma representação simbólica textual adequada a alguns conceitos. 
A representação textual nem sempre se mostrou eficiente pra capturar o signifi-
cado completo do conceito. Como no caso dos tipos de curvas modeladas e, em 
especial, da entidade CurvaSigmoide. Uma curva sigmóide tem o mesmo for-
mato de uma curva logística. Assim, se ambos os conceitos forem modelados, 
eles serão equivalentes, gerando redundâncias no modelo. Entretanto, se, ao in-
vés de um símbolo textual, fosse usado um símbolo gráfico para representar o 
conceito (ex.: um desenho da curva), a sua representação se tornaria mais natu-
ral. Esse problema pode ser resolvido através do desenvolvimento de primiti-



91 

 

 

vas de representação de conhecimento que aceitem o uso de ícones e figuras 
como representação simbólica10.  

Uma lacuna deixada no desenvolvimento do framework foi o tratamento 
dos índices de pertinência resultantes das funções fuzzy utilizadas pelos descri-
tores simbólicos. De fato, os índices de característica utilizados no nível visual 
consistem na alternativa encontrada para modelar os índices de pertinências re-
sultados das funções fuzzy no modelo visual. Embora não a ideal, essa modela-
gem se mostrou eficiente para os fins deste trabalho. No entanto, no momento 
da escrita desta dissertação, uma nova versão da linguagem OWL está sendo 
definida e esta poderia modelar facilmente essas informações. As novas primi-
tivas antevistas permitiriam a modelagem de relações de anotação sobre qual-
quer entidade do modelo, até mesmo sobre axiomas entre instâncias. Dessa 
forma, seria possível anotar, por exemplo, um grau de certeza na relação entre 
uma instância e a sua classe. Deste modo, as primitiva temCaracteristica u-
sada no framework S-Chart poderia passar a ser uma relação de anotação sobre 
entidades visuais extraídas do nível analógico. 

Por fim, como foi afirmado no desenvolvimento do trabalho, a definição 
completa de um módulo para gerenciar a aplicação de algoritmos de processa-
mento de imagem fogem do escopo deste trabalho. No entanto, ele é de grande 
importância para um sistema de interpretação semântica. Um módulo deste ti-
po deveria ser capaz de, dado um conjunto de primitivas analógicas que devem 
ser extraídas do sinal, decidir quais algoritmos aplicar e em qual ordem. Defini-
dos como serviços de processamento, suas interfaces devem ser formalizadas 
dentro dos modelos, presumivelmente em uma extensão do modelo de proces-
samento de sinal descrito anteriormente.  

 

                                                 

 
10 No momento da escrita deste trabalho, esta alternativa de representação está também 

sendo pesquisada por Alexandre Lorenzatti (vide note anterior). 



 

 

 

REFERÊNCIAS 

BARSALOU, L. W. Perceptual symbol systems. Behavioral and Brain Sciences, 
New York. v. 22, n. 4, p. 577-660, 1999. 

CHELLA, A.; FRIXIONE, M.; GAGLIO, S. A cognitive architecture for artificial 
vision. Artificial Intelligence, Amsterdan. v. 89, n. 1-2, p. 73-111, 1997. 

CHELLA, A.; FRIXIONE, M.; GAGLIO, S. Conceptual Spaces for Computer Vi-
sion Representations. Artificial Intelligence Review, Dordrecht. v. 16, n. 2, p. 
137-152, 2001. 

CHOUDHURY, S. et al. Use of Wavelet Transformation for Geophysical Well-
Log Data Analysis.  In: 15th International Conference on Digital Signal Process-
ing.  Proceedings... , Cadiff (UK) 2007. p. 647-650. 

COHN, A. G. et al. Qualitative Spatial Representation and Reasoning with the 
Region Connection Calculus. GeoInformatica, Boston. v. 1, n. 3, p. 275-316, 
1997. 

COHN, A. G. et al. Towards an Architecture for Cognitive Vision Using Quali-
tative Spatio-temporal Representations and Abduction. Spatial Cognition III, 
Berlin: Springer, 2003. p. 246-262. (Lecture Notes in Computer Science, 2685). 

COLOMB, R. et al. The Object Management Group Ontology Definition Meta-
model. Ontologies for Software Engineering and Software Technology, Ber-
lin: Springer, 2006. p. 217-247. 

CORADESCHI, S. et al. Fuzzy anchoring.  In: The 10th IEEE International Con-
ference on Fuzzy Systems.  Proceedings... , [S.l.]. v. 1, 2001. p. 111-114. 

CREVIER, D.; LEPAGE, R. Knowledge-based image understanding systems: a 
survey. Comput. Vis. Image Underst., [S.l.]. v. 67, n. 2, p. 160-185, 1997. 

CRUBÉZY, M.; MARCOS, M.; MOISAN, S. Experiments in Building Program 
Supervision Engines from Reusable Components.  In: WORKSHOP on Applica-
tions of Ontologies and Problem-Solving Methods.  Proceedings... , Brighton, 
England 1998. p. 44-53. 

DELLA FÁVERA, J. C. Fundamentos de estratigrafia moderna. Rio de Janeiro: 
EdUERJ, 2001, 

DEPARTMENT OF NATURAL RESOURCES. State of Utah - Oil and Gas Pro-
gram - Home Page. 2009. Disponível em:&amp;lt;http://oilgas.ogm.utah.gov/&gt;. A-
cesso em: 25 Fev 2009. 



93 

 

 

FAN, J. et al. Statistical modeling and conceptualization of natural images. Pat-
tern Recognition, [S.l.]. v. 38, n. 6, p. 865-885, 2005. 

FIORINI, S. R. Uma Proposta de Arquitetura de Componentes pra Sistemas de 
Conhecimento para Avaliação de Reservatórios de Petróleo. 2006. 65 p. Mono-
grafia (Bacharelado em Ciência da Computação) - Instituto de Informática, Uni-
versidade Federal do Rio Grande do Sul. 

FRIEDMAN, E. Jess in action: rule-based systems in java. Greenwich, US: 
Manning Publications Co., 2003, 

GANGEMI, A. et al. Sweetening WORDNET with DOLCE. AI Magazine, 
Menlo Park, US. v. 24, n. 3, p. 13-24, 2003. 

GÄRDENFORS, P. Conceptual Spaces: The Geometry of Thought. Cambridge, 
Massachussetts: The MIT Press, 2004, 

GOMEZ, I.; DATCU, M. A Bayesian multi-class image content retrieval.  In: In-
ternational Geoscience and Remote Sensing Symposium.  Proceedings... , Los 
Alamitos, USA: IEEE, 2007. p. 326-329. 

HANSON, A.; RISEMAN, E. VISIONS: A computer system for interpreting 
scenes. Computer Vision Systems, [S.l.]. 1978. 

HARNAD, S. The symbol grounding problem. Phys. D, [S.l.]. v. 42, n. 1-3, p. 
335-346, 1990. 

HARNAD, S. Computation Is Just Interpretable Symbol Manipulation: Cog-
nition Isn't. 1994. Disponível em:&amp;lt;http://cogprints.org/1592/&gt;. Acesso em: 29 
Jan 2009. 

HERZOG, G. From visual input to verbal output in the visual translator.  In: 
AAAI Fall Symposium on Computational Models for Integrating Language and 
Vision.  Proceedings... , Cambridge, USA: AAAI, 1995. 

HORRIDGE, M. et al. The Manchester OWL Syntax.  In: OWLED*06 Workshop 
on OWL: Experiences and Directions.  Proceedings... , Athens, USA v. 216, 
2006. 

HORROCKS, I. et al. SWRL: A Semantic Web Rule Language Combining 
OWL and RuleML. 2004. Disponível em: 
&lt;http://www.w3.org/Submission/SWRL/&gt;. Acesso em: 17 Fev 2009. 

HUDELOT, C. Towards a Cognitive Vision Platform for Semantic Image In-
terpretation; Application to the Recognition of Biological Organisms. 2005. 
275 p. Thesis (Phd in Computer Science) - Université de Nice Sophia Antipolis. 

HUDELOT, C.; MAILLOT, N.; THONNAT, M. Symbol Grounding for Semantic 
Image Interpretation: From Image Data to Semantics.  In: Tenth IEEE Interna-
tional Conference on Computer Vision, 2005.  Proceedings... , Los Alamitos, 
USA: IEEE, 2005. p. 1875. 



94 

 

 

JARKE, M. et al. KBMS requirements of knowledge-based systems. Founda-
tions of knowledge base management, New York: Springer-Verlag, 1989. p. 
381-394. 

KNUBLAUCH, H. et al. The Protégé OWL Plugin: An Open Development En-
vironment for Semantic Web Applications. The Semantic Web – ISWC 2004, 
[S.l.]. 2004. p. 229-243. 

LINDHOLM, T.; YELLIN, F. Java Virtual Machine Specification. Addison-
Wesley Longman Publishing Co., Inc., 1999, 

MAILLOT, N. Ontology Based Object Learning and Recognition. 2005. 166 p. 
Thesis (Phd in Computer Science) - École Doctorale STIC, Université de Nice. 

MATHWORKS INC. Matlab 2008a. Natick, USA 

MATSUYAMA, T. Knowledge-Based Aerial Image Understanding Systems and 
Expert Systems for Image Processing. IEEE Transactions on Geoscience and 
Remote Sensing, New York. v. GE-25, n. 3, p. 305-316, 1987. 

MCGUINNESS, D. L.; HARMELEN, F. V. OWL Web Ontology: Language 
Overview. 2004. Disponível em:&amp;lt;http://www.w3.org/TR/owl-features/&gt;. 
Acesso em: 17 Fev 2009. 

NEUMANN, B.; MOLLER, R. On scene interpretation with description logics. 
Image and Vision Computing, [S.l.]. v. 26, n. 1, p. 82-101, 2008. 

O’CONNOR, M. et al. Supporting Rule System Interoperability on the Semantic 
Web with SWRL. The Semantic Web – ISWC 2005, [S.l.]. 2005. p. 974-986. 

OGIELA, M. R.; TADEUSIEWICZ, R. Artificial intelligence structural imaging 
techniques in visual pattern analysis and medical data understanding. Pattern 
Recognition, Oxford. v. 36, n. 10, p. 2441-2452, 2003. 

OSSOLA, J.; BRÉMOND, F.; THONNAT, M. A Communication Level in a Dis-
tributed Architecture for Object Recognition.  In: 8th International Conference 
on Systems Research Informatics and Cybernetics.  Proceedings... , [S.l.]. 1996. 

PARSIA, B.; SIRIN, E.; KALYANPUR, A. Debugging OWL ontologies.  In: 14th 
International Conference on World Wide Web.  Proceedings... , USA: ACM, 
2005. p. 633-640. 

RANDELL, D. A.; CUI, Z.; COHN, A. G. A Spatial Logic based on Regions and 
Connection.  In: 3rd Internation Conference on Knowledge Representation and 
Reasoning.  Proceedings... , San Mateo 1992. p. 165–176. 

RECTOR, A. Representing Specified Values in OWL: "value partitions" and 
"value sets". 2005. Disponível em:&amp;lt;http://www.w3.org/TR/swbp-specified-
values/&gt;. Acesso em: 17 Fev 2009. 

SANDAKLY, F.; GIRAUDON, G. Scene analysis system.  In: IEEE International 
Conference in Image Processing, 1994.  Proceedings... , [S.l.]. v. 3, 1994. p. 806-
810 vol.3. 



95 

 

 

SANTIN, C. E. Construtos ontológicos para representação simbólica de co-
nhecimento visual. 2008. 103 p. Dissertação (Mestrado em Computação) - Insti-
tuto de Informática, Universidade Federal do Rio Grande do Sul. 

SCHREIBER, G. et al. Knowledge Engineering and Management: The Com-
monKADS Methodology. Cambridge, USA: The MIT Press, 1999, 

SHANAHAN, M. Robotics and the Common Sense Informatics Situation.  In: 
Spring Symposium on Planning with Incomplete Information for Robot Prob-
lems.  Proceedings... , Menlo Park, USA: AAAI, 1996. p. 95-106. 

SHANAHAN, M.; RANDELL, D. A Logic-Based Formulation of Active Visual 
Perception.  In: Principles of Knowledge Representation and Reasoning: Pro-
ceedings of the Ninth International Conference (KR2004).  Proceedings... , Men-
lo Park, Calif. 2004. p. 64-72. 

SILVA, J. G. R. D. Estudo de cicloestratigrafia nos depósitos eopermianos do 
Grupo Itararé, Bacia do Paraná, nos Estados de Santa Catarina e Rio Grande 
do Sul, baseado em dados de testemunho e de perfis de raios gama. 2001. Dis-
sertação (Mestrado em Geologia) - Instituto de Geociências, Universidade Fede-
ral do Rio Grande do Sul. 

STUDER, R.; BENJAMINS, V. R.; FENSEL, D. Knowledge engineering: princi-
ples and methods. Data &amp;amp; Knowledge Engineering, [S.l.]. v. 25, n. 1-2, p. 161-
197, 1998. 

TSARKOV, D.; HORROCKS, I. FaCT++ Description Logic Reasoner: System 
Description. Automated Reasoning, Berlin: Springer, 2006. p. 292-297. (Lecture 
Notes in Computer Science, 4130). 

VAN WAGONER, J. C. et al. Siliciclastic sequence stratigraphy in well logs, 
cores, and outcrops. Tulsa, US: AAPG, 1990, 

WITHAGEN, P. J. Object detection and segmentation for visual surveillance. 
2006. 173 p. Thesis (PhD in Computer Science) - University of Amsterdam. 

YIP, K.; ZHAO, F. Spatial Aggregation: Theory and Applications. Journal of Ar-
tificial Intelligence Research, [S.l.]. v. 5, p. 1--26, 1996. 

 



 

 

 

ANEXO  METAMODELO DA LINUGAGEM OWL 

Os algoritmos das máquinas MIS e MIV descritos na seção 3.2.5 fazem refe-
rencia a objetos do metamodelo de OWL a RDF. Embora este ele esteja implícito 
na definição da própria linguagem, um metamodelo para OWL está sendo es-
tandardizado pela OMG (Object Managemnt Group) em conjunto com outras 
linguagens de representação (COLOMB et al., 2006). A Figura A.1 apresenta os 
objetos do metamodelo que permitem um melhor entendimento dos algoritmos 
descritos. 

 
Figura A.1: Hierarquia de construtos de representação de OWL. 

As primitivas da linguagem SWRL não são contemplados pela OMG. No en-
tanto, segundo a descrição da linguagem, o construto swrl:Imp que aparece 
nos algoritmos das máquinas MIS e MIV descritas na seção 3.2.5 é uma instân-
cia de OWLClass. A sua função é descrever um instâncias de uma regra SWRL 

 



 

 

 

APÊNDICE  DESCRIÇÃO RDF/XML DOS MODELOS 
OWL DE CONHECIMENTO VISUAL DO SISTEMA 

INTELISTRATA 

Neste apêndice encontra-se os modelos OWL que são utilizados pelo sistema 
InteliStrata. A representação utilizada é a RDF/XML padrão de OWL.  

Todos os modelos apresentados, com exceção dos apresentados na seção B.5, 
são independentes de domínio e podem ser aplicados diretamente em outros 
sistemas. 

B.1 Modelo do nível analógico  

B.1.1 Modelo OWL do nível analógico 
&lt;?xml version="1.0"?&gt; 
&lt;rdf:RDF 
    xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" 
    xmlns:protege="http://protege.stanford.edu/plugins/owl/protege#" 
    xmlns:xsp="http://www.owl-ontologies.com/2005/08/07/xsp.owl#" 
    xmlns:owl="http://www.w3.org/2002/07/owl#" 
    xmlns:xsd="http://www.w3.org/2001/XMLSchema#" 
    xmlns:swrl="http://www.w3.org/2003/11/swrl#" 
    xmlns:swrlb="http://www.w3.org/2003/11/swrlb#" 
    xmlns="http://www.owl-ontologies.com/AnalogLevelOntology.owl#" 
    xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#" 
  xml:base="http://www.owl-ontologies.com/AnalogLevelOntology.owl"&gt; 
 &amp;lt;owl:Ontology rdf:about=""/&gt; 
 &amp;lt;owl:Class rdf:ID="PointSet"&gt; 
   &amp;lt;rdfs:subClassOf&gt; 
     &amp;lt;owl:Class rdf:ID="AnalogEntity"/&gt; 
   &amp;lt;/rdfs:subClassOf&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class rdf:ID="SignalRawDataContainer"&gt; 
   &amp;lt;rdfs:subClassOf&gt; 
     &amp;lt;owl:Class rdf:ID="RawDataContainer"/&gt; 
   &amp;lt;/rdfs:subClassOf&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class rdf:ID="IntervalSet"&gt; 
   &amp;lt;rdfs:subClassOf rdf:resource="#PointSet"/&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class rdf:ID="Interval"&gt; 
   &amp;lt;rdfs:subClassOf&gt; 



98 

 

 

     &amp;lt;owl:Restriction&gt; 
       &amp;lt;owl:onProperty&gt; 
         &amp;lt;owl:DatatypeProperty rdf:ID="hasArea"/&gt; 
       &amp;lt;/owl:onProperty&gt; 
       &amp;lt;owl:maxCardinality rdf:datatype="http://www.w3.org/2001/XMLSchema#int" 
        &gt;1&lt;/owl:maxCardinality&gt; 
     &amp;lt;/owl:Restriction&gt; 
   &amp;lt;/rdfs:subClassOf&gt; 
   &amp;lt;rdfs:subClassOf&gt; 
     &amp;lt;owl:Restriction&gt; 
       &amp;lt;owl:onProperty&gt; 
         &amp;lt;owl:DatatypeProperty rdf:ID="hasAbsoluteArea"/&gt; 
       &amp;lt;/owl:onProperty&gt; 
       &amp;lt;owl:maxCardinality rdf:datatype="http://www.w3.org/2001/XMLSchema#int" 
        &gt;1&lt;/owl:maxCardinality&gt; 
     &amp;lt;/owl:Restriction&gt; 
   &amp;lt;/rdfs:subClassOf&gt; 
   &amp;lt;rdfs:subClassOf rdf:resource="#PointSet"/&gt; 
   &amp;lt;rdfs:subClassOf&gt; 
     &amp;lt;owl:Restriction&gt; 
       &amp;lt;owl:onProperty&gt; 
         &amp;lt;owl:DatatypeProperty rdf:ID="hasNoiseRate"/&gt; 
       &amp;lt;/owl:onProperty&gt; 
       &amp;lt;owl:maxCardinality rdf:datatype="http://www.w3.org/2001/XMLSchema#int" 
        &gt;1&lt;/owl:maxCardinality&gt; 
     &amp;lt;/owl:Restriction&gt; 
   &amp;lt;/rdfs:subClassOf&gt; 
   &amp;lt;rdfs:subClassOf&gt; 
     &amp;lt;owl:Restriction&gt; 
       &amp;lt;owl:onProperty&gt; 
         &amp;lt;owl:DatatypeProperty rdf:ID="hasLength"/&gt; 
       &amp;lt;/owl:onProperty&gt; 
       &amp;lt;owl:maxCardinality rdf:datatype="http://www.w3.org/2001/XMLSchema#int" 
        &gt;1&lt;/owl:maxCardinality&gt; 
     &amp;lt;/owl:Restriction&gt; 
   &amp;lt;/rdfs:subClassOf&gt; 
   &amp;lt;rdfs:subClassOf&gt; 
     &amp;lt;owl:Restriction&gt; 
       &amp;lt;owl:onProperty&gt; 
         &amp;lt;owl:DatatypeProperty rdf:ID="hasTendence"/&gt; 
       &amp;lt;/owl:onProperty&gt; 
       &amp;lt;owl:maxCardinality rdf:datatype="http://www.w3.org/2001/XMLSchema#int" 
        &gt;1&lt;/owl:maxCardinality&gt; 
     &amp;lt;/owl:Restriction&gt; 
   &amp;lt;/rdfs:subClassOf&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class rdf:ID="FileSignalRawDataContainer"&gt; 
   &amp;lt;rdfs:subClassOf rdf:resource="#SignalRawDataContainer"/&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class rdf:ID="Point"&gt; 
   &amp;lt;rdfs:subClassOf rdf:resource="#AnalogEntity"/&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:ObjectProperty rdf:ID="hasRawDataContainer"&gt; 
   &amp;lt;rdfs:range rdf:resource="#SignalRawDataContainer"/&gt; 
   &amp;lt;rdfs:domain rdf:resource="#AnalogEntity"/&gt; 



99 

 

 

 &amp;lt;/owl:ObjectProperty&gt; 
 &amp;lt;owl:ObjectProperty rdf:ID="hasPoint"&gt; 
   &amp;lt;rdfs:range rdf:resource="#Point"/&gt; 
   &amp;lt;rdfs:domain rdf:resource="#PointSet"/&gt; 
 &amp;lt;/owl:ObjectProperty&gt; 
 &amp;lt;owl:ObjectProperty rdf:ID="hasStartPoint"&gt; 
   &amp;lt;rdfs:domain rdf:resource="#Interval"/&gt; 
   &amp;lt;rdfs:subPropertyOf rdf:resource="#hasPoint"/&gt; 
   &amp;lt;rdf:type rdf:resource="http://www.w3.org/2002/07/owl#FunctionalProperty"/&gt; 
 &amp;lt;/owl:ObjectProperty&gt; 
 &amp;lt;owl:ObjectProperty rdf:ID="hasCenterPoint"&gt; 
   &amp;lt;rdfs:domain rdf:resource="#Interval"/&gt; 
   &amp;lt;rdfs:subPropertyOf rdf:resource="#hasPoint"/&gt; 
   &amp;lt;rdf:type rdf:resource="http://www.w3.org/2002/07/owl#FunctionalProperty"/&gt; 
 &amp;lt;/owl:ObjectProperty&gt; 
 &amp;lt;owl:ObjectProperty rdf:ID="hasMaximumPoint"&gt; 
   &amp;lt;rdfs:subPropertyOf rdf:resource="#hasPoint"/&gt; 
   &amp;lt;rdfs:domain rdf:resource="#PointSet"/&gt; 
   &amp;lt;rdf:type rdf:resource="http://www.w3.org/2002/07/owl#FunctionalProperty"/&gt; 
   &amp;lt;rdfs:range rdf:resource="#Point"/&gt; 
 &amp;lt;/owl:ObjectProperty&gt; 
 &amp;lt;owl:ObjectProperty rdf:ID="hasEndPoint"&gt; 
   &amp;lt;rdfs:domain rdf:resource="#Interval"/&gt; 
   &amp;lt;rdf:type rdf:resource="http://www.w3.org/2002/07/owl#FunctionalProperty"/&gt; 
   &amp;lt;rdfs:subPropertyOf rdf:resource="#hasPoint"/&gt; 
 &amp;lt;/owl:ObjectProperty&gt; 
 &amp;lt;owl:ObjectProperty rdf:ID="hasMinimumPoint"&gt; 
   &amp;lt;rdf:type rdf:resource="http://www.w3.org/2002/07/owl#FunctionalProperty"/&gt; 
   &amp;lt;rdfs:subPropertyOf rdf:resource="#hasPoint"/&gt; 
   &amp;lt;rdfs:range rdf:resource="#Point"/&gt; 
   &amp;lt;rdfs:domain rdf:resource="#PointSet"/&gt; 
 &amp;lt;/owl:ObjectProperty&gt; 
 &amp;lt;owl:DatatypeProperty rdf:about="#hasAbsoluteArea"&gt; 
   &amp;lt;rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#float"/&gt; 
   &amp;lt;rdfs:subPropertyOf&gt; 
     &amp;lt;owl:DatatypeProperty rdf:ID="hasAnalogicProperty"/&gt; 
   &amp;lt;/rdfs:subPropertyOf&gt; 
   &amp;lt;rdfs:domain rdf:resource="#Interval"/&gt; 
 &amp;lt;/owl:DatatypeProperty&gt; 
 &amp;lt;owl:DatatypeProperty rdf:ID="value"&gt; 
   &amp;lt;rdfs:domain rdf:resource="#Point"/&gt; 
   &amp;lt;rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#float"/&gt; 
   &amp;lt;rdf:type rdf:resource="http://www.w3.org/2002/07/owl#FunctionalProperty"/&gt; 
 &amp;lt;/owl:DatatypeProperty&gt; 
 &amp;lt;owl:DatatypeProperty rdf:about="#hasLength"&gt; 
   &amp;lt;rdfs:subPropertyOf&gt; 
     &amp;lt;owl:DatatypeProperty rdf:about="#hasAnalogicProperty"/&gt; 
   &amp;lt;/rdfs:subPropertyOf&gt; 
   &amp;lt;rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#float"/&gt; 
   &amp;lt;rdfs:domain rdf:resource="#Interval"/&gt; 
 &amp;lt;/owl:DatatypeProperty&gt; 
 &amp;lt;owl:DatatypeProperty rdf:about="#hasArea"&gt; 
   &amp;lt;rdfs:domain rdf:resource="#Interval"/&gt; 
   &amp;lt;rdfs:subPropertyOf&gt; 
     &amp;lt;owl:DatatypeProperty rdf:about="#hasAnalogicProperty"/&gt; 



100 

 

 

   &amp;lt;/rdfs:subPropertyOf&gt; 
   &amp;lt;rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#float"/&gt; 
 &amp;lt;/owl:DatatypeProperty&gt; 
 &amp;lt;owl:DatatypeProperty rdf:ID="index"&gt; 
   &amp;lt;rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#float"/&gt; 
   &amp;lt;rdfs:domain rdf:resource="#Point"/&gt; 
   &amp;lt;rdf:type rdf:resource="http://www.w3.org/2002/07/owl#FunctionalProperty"/&gt; 
 &amp;lt;/owl:DatatypeProperty&gt; 
 &amp;lt;owl:DatatypeProperty rdf:about="#hasAnalogicProperty"&gt; 
   &amp;lt;rdfs:domain rdf:resource="#AnalogEntity"/&gt; 
 &amp;lt;/owl:DatatypeProperty&gt; 
 &amp;lt;owl:DatatypeProperty rdf:about="#hasNoiseRate"&gt; 
   &amp;lt;rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#float"/&gt; 
   &amp;lt;rdfs:domain rdf:resource="#Interval"/&gt; 
   &amp;lt;rdfs:subPropertyOf rdf:resource="#hasAnalogicProperty"/&gt; 
 &amp;lt;/owl:DatatypeProperty&gt; 
 &amp;lt;owl:DatatypeProperty rdf:about="#hasTendence"&gt; 
   &amp;lt;rdfs:subPropertyOf rdf:resource="#hasAnalogicProperty"/&gt; 
   &amp;lt;rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#float"/&gt; 
   &amp;lt;rdfs:domain rdf:resource="#Interval"/&gt; 
 &amp;lt;/owl:DatatypeProperty&gt; 
 &amp;lt;owl:DatatypeProperty rdf:ID="hasLocationPath"&gt; 
   &amp;lt;rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#anyURI"/&gt; 
   &amp;lt;rdfs:domain rdf:resource="#FileSignalRawDataContainer"/&gt; 
 &amp;lt;/owl:DatatypeProperty&gt; 
&lt;/rdf:RDF&gt; 

B.1.2 Modelo OWL para processamento de sinais 
&lt;?xml version="1.0"?&gt; 
&lt;rdf:RDF 
    xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" 
    xmlns:protege="http://protege.stanford.edu/plugins/owl/protege#" 
    xmlns:xsp="http://www.owl-ontologies.com/2005/08/07/xsp.owl#" 
    xmlns="http://www.owl-ontologies.com/SignalProcessingOntology.owl#" 
    xmlns:owl="http://www.w3.org/2002/07/owl#" 
    xmlns:xsd="http://www.w3.org/2001/XMLSchema#" 
    xmlns:swrl="http://www.w3.org/2003/11/swrl#" 
    xmlns:swrlb="http://www.w3.org/2003/11/swrlb#" 
    xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#" 
    xmlns:alo="http://www.owl-ontologies.com/AnalogLevelOntology.owl#" 
  xml:base="http://www.owl-ontologies.com/SignalProcessingOntology.owl"&gt; 
 &amp;lt;owl:Ontology rdf:about=""&gt; 
   &amp;lt;owl:imports  

rdf:resource="http://www.owl-ontologies.com/AnalogLevelOntology.owl"/&gt; 
 &amp;lt;/owl:Ontology&gt; 
 &amp;lt;owl:Class rdf:ID="PropertyExtractor"&gt; 
   &amp;lt;rdfs:subClassOf&gt; 
     &amp;lt;owl:Class rdf:ID="SignalProcessingFunctionality"/&gt; 
   &amp;lt;/rdfs:subClassOf&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class rdf:ID="SizePropertyExtractor"&gt; 
   &amp;lt;rdfs:subClassOf rdf:resource="#PropertyExtractor"/&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class rdf:ID="Segmentator"&gt; 



101 

 

 

   &amp;lt;rdfs:subClassOf rdf:resource="#SignalProcessingFunctionality"/&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class rdf:ID="FourierSegmentator"&gt; 
   &amp;lt;rdfs:subClassOf&gt; 
     &amp;lt;owl:Class rdf:ID="BasicSegmentator"/&gt; 
   &amp;lt;/rdfs:subClassOf&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class rdf:about="#BasicSegmentator"&gt; 
   &amp;lt;rdfs:subClassOf rdf:resource="#Segmentator"/&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class rdf:ID="WaveletSegmentator"&gt; 
   &amp;lt;rdfs:subClassOf rdf:resource="#BasicSegmentator"/&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:ObjectProperty rdf:ID="isExtractedWith"&gt; 
   &amp;lt;rdfs:range rdf:resource="#SignalProcessingFunctionality"/&gt; 
   &amp;lt;rdfs:domain  

rdf:resource="http://www.owl-ontologies.com/ 
AnalogLevelOntology.owl#AnalogEntity"/&gt; 

 &amp;lt;/owl:ObjectProperty&gt; 
 &amp;lt;owl:DatatypeProperty rdf:ID="hasExtractionStrength"&gt; 
   &amp;lt;rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#float"/&gt; 
   &amp;lt;rdfs:subPropertyOf 

 rdf:resource="http://www.owl-ontologies.com/ 
AnalogLevelOntology.owl#hasAnalogicProperty"/&gt; 

 &amp;lt;/owl:DatatypeProperty&gt; 
 &amp;lt;WaveletSegmentator rdf:ID="SmoothToothCurveSPF"/&gt; 
 &amp;lt;WaveletSegmentator rdf:ID="Symlet8SPF"/&gt; 
 &amp;lt;WaveletSegmentator rdf:ID="Gaussian2SPF"/&gt; 
&lt;/rdf:RDF&gt; 

B.2 Modelo do nível visual 
O modelo no nível visual consiste em somente um modelo OWL. 
&lt;?xml version="1.0"?&gt; 
&lt;rdf:RDF 
    xmlns:ineq="http://www.owl-ontologies.com/InequalityOntology.owl#" 
    xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" 
    xmlns:protege="http://protege.stanford.edu/plugins/owl/protege#" 
    xmlns:xsp="http://www.owl-ontologies.com/2005/08/07/xsp.owl#" 
    xmlns:owl="http://www.w3.org/2002/07/owl#" 
    xmlns:xsd="http://www.w3.org/2001/XMLSchema#" 
    xmlns:swrl="http://www.w3.org/2003/11/swrl#" 
    xmlns:swrlb="http://www.w3.org/2003/11/swrlb#" 
    xmlns="http://www.owl-ontologies.com/VisualLevel.owl#" 
    xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#" 
  xml:base="http://www.owl-ontologies.com/VisualLevel.owl"&gt; 
 &amp;lt;owl:Ontology rdf:about=""&gt; 
   &amp;lt;owl:imports 

 rdf:resource="http://www.owl-ontologies.com/InequalityOntology.owl"/&gt; 
 &amp;lt;/owl:Ontology&gt; 
 &amp;lt;owl:Class rdf:ID="VerticalLine"&gt; 
   &amp;lt;rdfs:subClassOf&gt; 
     &amp;lt;owl:Class rdf:ID="Line"/&gt; 
   &amp;lt;/rdfs:subClassOf&gt; 



102 

 

 

 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class rdf:ID="GenericCharacteristicValue"&gt; 
   &amp;lt;owl:equivalentClass&gt; 
     &amp;lt;owl:Class&gt; 
       &amp;lt;owl:oneOf rdf:parseType="Collection"&gt; 
         &amp;lt;GenericCharacteristicValue rdf:ID="VeryStrongPattern"&gt; 
           &amp;lt;ineq:greaterThan&gt; 
             &amp;lt;GenericCharacteristicValue rdf:ID="StrongPattern"&gt; 
               &amp;lt;ineq:smallerThan rdf:resource="#VeryStrongPattern"/&gt; 
               &amp;lt;ineq:greaterThan&gt; 
                 &amp;lt;GenericCharacteristicValue rdf:ID="MediumPattern"&gt; 
                   &amp;lt;ineq:greaterThan&gt; 
                     &amp;lt;GenericCharacteristicValue rdf:ID="WeakPattern"&gt; 
                       &amp;lt;ineq:smallerThan rdf:resource="#MediumPattern"/&gt; 
                       &amp;lt;ineq:greaterThan&gt; 
                         &amp;lt;GenericCharacteristicValue rdf:ID="VeryWeakPattern"&gt; 
                           &amp;lt;ineq:smallerThan rdf:resource="#WeakPattern"/&gt; 
                         &amp;lt;/GenericCharacteristicValue&gt; 
                       &amp;lt;/ineq:greaterThan&gt; 
                     &amp;lt;/GenericCharacteristicValue&gt; 
                   &amp;lt;/ineq:greaterThan&gt; 
                   &amp;lt;ineq:smallerThan rdf:resource="#StrongPattern"/&gt; 
                 &amp;lt;/GenericCharacteristicValue&gt; 
               &amp;lt;/ineq:greaterThan&gt; 
             &amp;lt;/GenericCharacteristicValue&gt; 
           &amp;lt;/ineq:greaterThan&gt; 
         &amp;lt;/GenericCharacteristicValue&gt; 
         &amp;lt;GenericCharacteristicValue rdf:about="#StrongPattern"/&gt; 
         &amp;lt;GenericCharacteristicValue rdf:about="#MediumPattern"/&gt; 
         &amp;lt;GenericCharacteristicValue rdf:about="#WeakPattern"/&gt; 
         &amp;lt;GenericCharacteristicValue rdf:about="#VeryWeakPattern"/&gt; 
       &amp;lt;/owl:oneOf&gt; 
     &amp;lt;/owl:Class&gt; 
   &amp;lt;/owl:equivalentClass&gt; 
   &amp;lt;rdfs:subClassOf&gt; 
     &amp;lt;owl:Class rdf:ID="CharacteristicValue"/&gt; 
   &amp;lt;/rdfs:subClassOf&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class&gt; 
   &amp;lt;owl:oneOf rdf:parseType="Collection"&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#VeryStrongPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#MediumPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#WeakPattern"/&gt; 
   &amp;lt;/owl:oneOf&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class&gt; 
   &amp;lt;owl:oneOf rdf:parseType="Collection"&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#VeryStrongPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#StrongPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#MediumPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#WeakPattern"/&gt; 
   &amp;lt;/owl:oneOf&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class rdf:about="#CharacteristicValue"&gt; 
   &amp;lt;rdfs:subClassOf&gt; 



103 

 

 

     &amp;lt;owl:Class rdf:ID="VisualPropertyValue"/&gt; 
   &amp;lt;/rdfs:subClassOf&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class&gt; 
   &amp;lt;owl:oneOf rdf:parseType="Collection"&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#VeryStrongPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#StrongPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#MediumPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#WeakPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#VeryWeakPattern"/&gt; 
   &amp;lt;/owl:oneOf&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class&gt; 
   &amp;lt;owl:oneOf rdf:parseType="Collection"&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#VeryStrongPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#StrongPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#MediumPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#WeakPattern"/&gt; 
   &amp;lt;/owl:oneOf&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class&gt; 
   &amp;lt;owl:oneOf rdf:parseType="Collection"&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#StrongPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#MediumPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#WeakPattern"/&gt; 
   &amp;lt;/owl:oneOf&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class&gt; 
   &amp;lt;owl:oneOf rdf:parseType="Collection"&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#VeryStrongPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#StrongPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#MediumPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#WeakPattern"/&gt; 
   &amp;lt;/owl:oneOf&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class rdf:ID="GeometricEntity"&gt; 
   &amp;lt;rdfs:subClassOf&gt; 
     &amp;lt;owl:Class rdf:ID="VisualEntity"/&gt; 
   &amp;lt;/rdfs:subClassOf&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class&gt; 
   &amp;lt;owl:oneOf rdf:parseType="Collection"&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#VeryStrongPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#StrongPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#MediumPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#WeakPattern"/&gt; 
   &amp;lt;/owl:oneOf&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class rdf:ID="Point"&gt; 
   &amp;lt;rdfs:subClassOf rdf:resource="#GeometricEntity"/&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class&gt; 
   &amp;lt;owl:oneOf rdf:parseType="Collection"&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#VeryStrongPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#StrongPattern"/&gt; 



104 

 

 

     &amp;lt;GenericCharacteristicValue rdf:about="#MediumPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#WeakPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#VeryWeakPattern"/&gt; 
   &amp;lt;/owl:oneOf&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class rdf:about="#Line"&gt; 
   &amp;lt;rdfs:subClassOf&gt; 
     &amp;lt;owl:Class rdf:ID="OpenCurve"/&gt; 
   &amp;lt;/rdfs:subClassOf&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class rdf:ID="InclinedLine"&gt; 
   &amp;lt;rdfs:subClassOf rdf:resource="#Line"/&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class rdf:ID="SmoothToothCurve"&gt; 
   &amp;lt;rdfs:subClassOf&gt; 
     &amp;lt;owl:Class rdf:about="#OpenCurve"/&gt; 
   &amp;lt;/rdfs:subClassOf&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class rdf:ID="LengthValue"&gt; 
   &amp;lt;rdfs:subClassOf&gt; 
     &amp;lt;owl:Class rdf:about="#VisualPropertyValue"/&gt; 
   &amp;lt;/rdfs:subClassOf&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class&gt; 
   &amp;lt;owl:oneOf rdf:parseType="Collection"&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#VeryStrongPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#StrongPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#MediumPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#WeakPattern"/&gt; 
   &amp;lt;/owl:oneOf&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class&gt; 
   &amp;lt;owl:oneOf rdf:parseType="Collection"&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#VeryStrongPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#StrongPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#MediumPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#WeakPattern"/&gt; 
   &amp;lt;/owl:oneOf&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class&gt; 
   &amp;lt;owl:oneOf rdf:parseType="Collection"&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#VeryStrongPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#StrongPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#MediumPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#WeakPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#VeryWeakPattern"/&gt; 
   &amp;lt;/owl:oneOf&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class rdf:ID="SinusoidCurve"&gt; 
   &amp;lt;rdfs:subClassOf&gt; 
     &amp;lt;owl:Class rdf:about="#OpenCurve"/&gt; 
   &amp;lt;/rdfs:subClassOf&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class rdf:ID="GenericLengthValue"&gt; 
   &amp;lt;owl:equivalentClass&gt; 



105 

 

 

     &amp;lt;owl:Class&gt; 
       &amp;lt;owl:oneOf rdf:parseType="Collection"&gt; 
         &amp;lt;GenericLengthValue rdf:ID="ShortLength"&gt; 
           &amp;lt;ineq:greaterThan&gt; 
             &amp;lt;GenericLengthValue rdf:ID="VeryShortLength"&gt; 
               &amp;lt;ineq:smallerThan rdf:resource="#ShortLength"/&gt; 
             &amp;lt;/GenericLengthValue&gt; 
           &amp;lt;/ineq:greaterThan&gt; 
           &amp;lt;ineq:smallerThan&gt; 
             &amp;lt;GenericLengthValue rdf:ID="MediumLength"&gt; 
               &amp;lt;ineq:greaterThan rdf:resource="#ShortLength"/&gt; 
               &amp;lt;ineq:smallerThan&gt; 
                 &amp;lt;GenericLengthValue rdf:ID="LongLength"&gt; 
                   &amp;lt;ineq:smallerThan&gt; 
                     &amp;lt;GenericLengthValue rdf:ID="VeryLongLength"&gt; 
                       &amp;lt;ineq:greaterThan rdf:resource="#LongLength"/&gt; 
                     &amp;lt;/GenericLengthValue&gt; 
                   &amp;lt;/ineq:smallerThan&gt; 
                   &amp;lt;ineq:greaterThan rdf:resource="#MediumLength"/&gt; 
                 &amp;lt;/GenericLengthValue&gt; 
               &amp;lt;/ineq:smallerThan&gt; 
             &amp;lt;/GenericLengthValue&gt; 
           &amp;lt;/ineq:smallerThan&gt; 
         &amp;lt;/GenericLengthValue&gt; 
         &amp;lt;GenericLengthValue rdf:about="#VeryLongLength"/&gt; 
         &amp;lt;GenericLengthValue rdf:about="#LongLength"/&gt; 
         &amp;lt;GenericLengthValue rdf:about="#MediumLength"/&gt; 
         &amp;lt;GenericLengthValue rdf:about="#VeryShortLength"/&gt; 
       &amp;lt;/owl:oneOf&gt; 
     &amp;lt;/owl:Class&gt; 
   &amp;lt;/owl:equivalentClass&gt; 
   &amp;lt;rdfs:subClassOf rdf:resource="#LengthValue"/&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class&gt; 
   &amp;lt;owl:oneOf rdf:parseType="Collection"&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#VeryStrongPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#StrongPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#MediumPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#WeakPattern"/&gt; 
   &amp;lt;/owl:oneOf&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class&gt; 
   &amp;lt;owl:oneOf rdf:parseType="Collection"&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#VeryStrongPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#StrongPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#MediumPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#WeakPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#VeryWeakPattern"/&gt; 
   &amp;lt;/owl:oneOf&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class&gt; 
   &amp;lt;owl:oneOf rdf:parseType="Collection"&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#StrongPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#MediumPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#WeakPattern"/&gt; 



106 

 

 

   &amp;lt;/owl:oneOf&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class&gt; 
   &amp;lt;owl:oneOf rdf:parseType="Collection"&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#VeryStrongPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#MediumPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#WeakPattern"/&gt; 
   &amp;lt;/owl:oneOf&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class rdf:ID="MinimumPoint"&gt; 
   &amp;lt;rdfs:subClassOf rdf:resource="#Point"/&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class rdf:ID="PointSet"&gt; 
   &amp;lt;rdfs:subClassOf rdf:resource="#GeometricEntity"/&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class rdf:ID="MaximumPoint"&gt; 
   &amp;lt;rdfs:subClassOf rdf:resource="#Point"/&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class rdf:about="#VisualPropertyValue"&gt; 
   &amp;lt;owl:equivalentClass&gt; 
     &amp;lt;owl:Class&gt; 
       &amp;lt;owl:unionOf rdf:parseType="Collection"&gt; 
         &amp;lt;owl:Class rdf:about="#LengthValue"/&gt; 
         &amp;lt;owl:Class rdf:about="#CharacteristicValue"/&gt; 
       &amp;lt;/owl:unionOf&gt; 
     &amp;lt;/owl:Class&gt; 
   &amp;lt;/owl:equivalentClass&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class rdf:about="#OpenCurve"&gt; 
   &amp;lt;rdfs:subClassOf&gt; 
     &amp;lt;owl:Class rdf:ID="Curve"/&gt; 
   &amp;lt;/rdfs:subClassOf&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class&gt; 
   &amp;lt;owl:oneOf rdf:parseType="Collection"&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#StrongPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#MediumPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#WeakPattern"/&gt; 
   &amp;lt;/owl:oneOf&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class&gt; 
   &amp;lt;owl:oneOf rdf:parseType="Collection"&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#VeryStrongPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#StrongPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#MediumPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#WeakPattern"/&gt; 
   &amp;lt;/owl:oneOf&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class&gt; 
   &amp;lt;owl:oneOf rdf:parseType="Collection"&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#VeryStrongPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#StrongPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#MediumPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#WeakPattern"/&gt; 
   &amp;lt;/owl:oneOf&gt; 



107 

 

 

 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class&gt; 
   &amp;lt;owl:oneOf rdf:parseType="Collection"&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#StrongPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#MediumPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#WeakPattern"/&gt; 
   &amp;lt;/owl:oneOf&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class rdf:ID="SigmoidCurve"&gt; 
   &amp;lt;rdfs:subClassOf rdf:resource="#OpenCurve"/&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class&gt; 
   &amp;lt;owl:oneOf rdf:parseType="Collection"&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#VeryStrongPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#StrongPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#MediumPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#WeakPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#VeryWeakPattern"/&gt; 
   &amp;lt;/owl:oneOf&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class&gt; 
   &amp;lt;owl:oneOf rdf:parseType="Collection"&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#StrongPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#MediumPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#WeakPattern"/&gt; 
   &amp;lt;/owl:oneOf&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class rdf:ID="GaussianCurve"&gt; 
   &amp;lt;rdfs:subClassOf rdf:resource="#OpenCurve"/&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class&gt; 
   &amp;lt;owl:oneOf rdf:parseType="Collection"&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#VeryStrongPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#StrongPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#MediumPattern"/&gt; 
     &amp;lt;GenericCharacteristicValue rdf:about="#WeakPattern"/&gt; 
   &amp;lt;/owl:oneOf&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class rdf:about="#Curve"&gt; 
   &amp;lt;rdfs:subClassOf rdf:resource="#PointSet"/&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class rdf:ID="TriangularCurve"&gt; 
   &amp;lt;rdfs:subClassOf rdf:resource="#OpenCurve"/&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class rdf:ID="HorizontalLine"&gt; 
   &amp;lt;rdfs:subClassOf rdf:resource="#Line"/&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:ObjectProperty rdf:ID="isOver"&gt; 
   &amp;lt;owl:inverseOf&gt; 
     &amp;lt;owl:ObjectProperty rdf:ID="isUnder"/&gt; 
   &amp;lt;/owl:inverseOf&gt; 
   &amp;lt;rdfs:subPropertyOf&gt; 
     &amp;lt;owl:ObjectProperty rdf:ID="hasOrientationPositionTo"/&gt; 
   &amp;lt;/rdfs:subPropertyOf&gt; 
 &amp;lt;/owl:ObjectProperty&gt; 



108 

 

 

 &amp;lt;owl:ObjectProperty rdf:ID="isAtRightOf"&gt; 
   &amp;lt;owl:inverseOf&gt; 
     &amp;lt;owl:ObjectProperty rdf:ID="isAtLeftOf"/&gt; 
   &amp;lt;/owl:inverseOf&gt; 
   &amp;lt;rdfs:subPropertyOf&gt; 
     &amp;lt;owl:ObjectProperty rdf:about="#hasOrientationPositionTo"/&gt; 
   &amp;lt;/rdfs:subPropertyOf&gt; 
 &amp;lt;/owl:ObjectProperty&gt; 
 &amp;lt;owl:ObjectProperty rdf:ID="isDiscreteOf"&gt; 
   &amp;lt;rdfs:domain rdf:resource="#VisualEntity"/&gt; 
   &amp;lt;rdfs:subPropertyOf&gt; 
     &amp;lt;owl:ObjectProperty rdf:ID="hasTopologicalRelationWith"/&gt; 
   &amp;lt;/rdfs:subPropertyOf&gt; 
 &amp;lt;/owl:ObjectProperty&gt; 
 &amp;lt;owl:ObjectProperty rdf:ID="hasSpatialRelationWith"&gt; 
   &amp;lt;rdfs:range rdf:resource="#VisualEntity"/&gt; 
   &amp;lt;rdfs:domain rdf:resource="#VisualEntity"/&gt; 
 &amp;lt;/owl:ObjectProperty&gt; 
 &amp;lt;owl:ObjectProperty rdf:ID="hasSmoothness"&gt; 
   &amp;lt;rdfs:subPropertyOf&gt; 
     &amp;lt;owl:ObjectProperty rdf:ID="hasVisualProperty"/&gt; 
   &amp;lt;/rdfs:subPropertyOf&gt; 
 &amp;lt;/owl:ObjectProperty&gt; 
 &amp;lt;owl:ObjectProperty rdf:ID="hasConvexity"&gt; 
   &amp;lt;rdfs:subPropertyOf&gt; 
     &amp;lt;owl:ObjectProperty rdf:about="#hasVisualProperty"/&gt; 
   &amp;lt;/rdfs:subPropertyOf&gt; 
 &amp;lt;/owl:ObjectProperty&gt; 
 &amp;lt;owl:ObjectProperty rdf:about="#hasOrientationPositionTo"&gt; 
   &amp;lt;rdfs:subPropertyOf rdf:resource="#hasSpatialRelationWith"/&gt; 
 &amp;lt;/owl:ObjectProperty&gt; 
 &amp;lt;owl:ObjectProperty rdf:about="#isAtLeftOf"&gt; 
   &amp;lt;rdfs:subPropertyOf rdf:resource="#hasOrientationPositionTo"/&gt; 
   &amp;lt;owl:inverseOf rdf:resource="#isAtRightOf"/&gt; 
   &amp;lt;rdf:type rdf:resource="http://www.w3.org/2002/07/owl#TransitiveProperty"/&gt; 
 &amp;lt;/owl:ObjectProperty&gt; 
 &amp;lt;owl:ObjectProperty rdf:about="#hasTopologicalRelationWith"&gt; 
   &amp;lt;rdfs:subPropertyOf rdf:resource="#hasSpatialRelationWith"/&gt; 
 &amp;lt;/owl:ObjectProperty&gt; 
 &amp;lt;owl:ObjectProperty rdf:ID="hasForNonTangentialProperPartOf"&gt; 
   &amp;lt;rdfs:subPropertyOf&gt; 
     &amp;lt;owl:ObjectProperty rdf:ID="hasForProperPart"/&gt; 
   &amp;lt;/rdfs:subPropertyOf&gt; 
   &amp;lt;owl:inverseOf&gt; 
     &amp;lt;owl:ObjectProperty rdf:ID="isNonTangentialProperPartOf"/&gt; 
   &amp;lt;/owl:inverseOf&gt; 
   &amp;lt;rdfs:domain rdf:resource="#VisualEntity"/&gt; 
 &amp;lt;/owl:ObjectProperty&gt; 
 &amp;lt;owl:ObjectProperty rdf:about="#hasForProperPart"&gt; 
   &amp;lt;rdfs:subPropertyOf rdf:resource="#hasTopologicalRelationWith"/&gt; 
   &amp;lt;owl:inverseOf&gt; 
     &amp;lt;owl:ObjectProperty rdf:ID="isProperPartOf"/&gt; 
   &amp;lt;/owl:inverseOf&gt; 
   &amp;lt;rdfs:domain rdf:resource="#VisualEntity"/&gt; 
 &amp;lt;/owl:ObjectProperty&gt; 



109 

 

 

 &amp;lt;owl:ObjectProperty rdf:about="#isUnder"&gt; 
   &amp;lt;owl:inverseOf rdf:resource="#isOver"/&gt; 
   &amp;lt;rdfs:subPropertyOf rdf:resource="#hasOrientationPositionTo"/&gt; 
 &amp;lt;/owl:ObjectProperty&gt; 
 &amp;lt;owl:ObjectProperty rdf:ID="hasCurvature"&gt; 
   &amp;lt;rdfs:subPropertyOf&gt; 
     &amp;lt;owl:ObjectProperty rdf:about="#hasVisualProperty"/&gt; 
   &amp;lt;/rdfs:subPropertyOf&gt; 
 &amp;lt;/owl:ObjectProperty&gt; 
 &amp;lt;owl:ObjectProperty rdf:ID="hasHeight"&gt; 
   &amp;lt;rdfs:subPropertyOf&gt; 
     &amp;lt;owl:ObjectProperty rdf:about="#hasVisualProperty"/&gt; 
   &amp;lt;/rdfs:subPropertyOf&gt; 
 &amp;lt;/owl:ObjectProperty&gt; 
 &amp;lt;owl:ObjectProperty rdf:ID="hasLength"&gt; 
   &amp;lt;rdfs:subPropertyOf&gt; 
     &amp;lt;owl:ObjectProperty rdf:about="#hasVisualProperty"/&gt; 
   &amp;lt;/rdfs:subPropertyOf&gt; 
   &amp;lt;rdfs:range rdf:resource="#LengthValue"/&gt; 
   &amp;lt;rdfs:domain rdf:resource="#Curve"/&gt; 
 &amp;lt;/owl:ObjectProperty&gt; 
 &amp;lt;owl:ObjectProperty rdf:ID="isTopologicallyEqualsTo"&gt; 
   &amp;lt;rdfs:subPropertyOf rdf:resource="#hasTopologicalRelationWith"/&gt; 
 &amp;lt;/owl:ObjectProperty&gt; 
 &amp;lt;owl:ObjectProperty rdf:ID="partialOverlaps"&gt; 
   &amp;lt;rdfs:subPropertyOf rdf:resource="#hasTopologicalRelationWith"/&gt; 
 &amp;lt;/owl:ObjectProperty&gt; 
 &amp;lt;owl:ObjectProperty rdf:about="#isNonTangentialProperPartOf"&gt; 
   &amp;lt;rdfs:subPropertyOf&gt; 
     &amp;lt;owl:ObjectProperty rdf:about="#isProperPartOf"/&gt; 
   &amp;lt;/rdfs:subPropertyOf&gt; 
   &amp;lt;owl:inverseOf rdf:resource="#hasForNonTangentialProperPartOf"/&gt; 
 &amp;lt;/owl:ObjectProperty&gt; 
 &amp;lt;owl:ObjectProperty rdf:about="#isProperPartOf"&gt; 
   &amp;lt;owl:inverseOf rdf:resource="#hasForProperPart"/&gt; 
   &amp;lt;rdfs:subPropertyOf rdf:resource="#hasTopologicalRelationWith"/&gt; 
 &amp;lt;/owl:ObjectProperty&gt; 
 &amp;lt;owl:ObjectProperty rdf:ID="isExternallyConnectedTo"&gt; 
   &amp;lt;rdfs:subPropertyOf rdf:resource="#isDiscreteOf"/&gt; 
 &amp;lt;/owl:ObjectProperty&gt; 
 &amp;lt;owl:ObjectProperty rdf:about="#hasVisualProperty"&gt; 
   &amp;lt;rdfs:range rdf:resource="#VisualPropertyValue"/&gt; 
   &amp;lt;rdfs:domain rdf:resource="#VisualEntity"/&gt; 
 &amp;lt;/owl:ObjectProperty&gt; 
 &amp;lt;owl:ObjectProperty rdf:ID="isTangentialProperPartOf"&gt; 
   &amp;lt;rdfs:subPropertyOf rdf:resource="#isProperPartOf"/&gt; 
   &amp;lt;owl:inverseOf&gt; 
     &amp;lt;owl:ObjectProperty rdf:ID="hasForTangentialProperPart"/&gt; 
   &amp;lt;/owl:inverseOf&gt; 
 &amp;lt;/owl:ObjectProperty&gt; 
 &amp;lt;owl:ObjectProperty rdf:ID="idDisconnectedOf"&gt; 
   &amp;lt;rdfs:subPropertyOf rdf:resource="#isDiscreteOf"/&gt; 
   &amp;lt;rdfs:domain rdf:resource="#VisualEntity"/&gt; 
 &amp;lt;/owl:ObjectProperty&gt; 
 &amp;lt;owl:ObjectProperty rdf:about="#hasForTangentialProperPart"&gt; 



110 

 

 

   &amp;lt;rdfs:domain rdf:resource="#VisualEntity"/&gt; 
   &amp;lt;owl:inverseOf rdf:resource="#isTangentialProperPartOf"/&gt; 
   &amp;lt;rdfs:subPropertyOf rdf:resource="#hasForProperPart"/&gt; 
 &amp;lt;/owl:ObjectProperty&gt; 
 &amp;lt;owl:FunctionalProperty rdf:ID="hasCharacteristic"&gt; 
   &amp;lt;rdfs:range rdf:resource="#GenericCharacteristicValue"/&gt; 
   &amp;lt;rdf:type rdf:resource="http://www.w3.org/2002/07/owl#ObjectProperty"/&gt; 
   &amp;lt;rdfs:subPropertyOf rdf:resource="#hasVisualProperty"/&gt; 
   &amp;lt;rdfs:domain rdf:resource="#GeometricEntity"/&gt; 
 &amp;lt;/owl:FunctionalProperty&gt; 
 &amp;lt;owl:FunctionalProperty rdf:ID="hasGaussianCurveCharacteristic"&gt; 
   &amp;lt;rdfs:subPropertyOf rdf:resource="#hasCharacteristic"/&gt; 
   &amp;lt;rdf:type rdf:resource="http://www.w3.org/2002/07/owl#ObjectProperty"/&gt; 
   &amp;lt;rdfs:domain rdf:resource="#GaussianCurve"/&gt; 
 &amp;lt;/owl:FunctionalProperty&gt; 
 &amp;lt;owl:FunctionalProperty rdf:ID="hasSmoothToothCurveCharacteristic"&gt; 
   &amp;lt;rdfs:domain rdf:resource="#SmoothToothCurve"/&gt; 
   &amp;lt;rdf:type rdf:resource="http://www.w3.org/2002/07/owl#ObjectProperty"/&gt; 
   &amp;lt;rdfs:subPropertyOf rdf:resource="#hasCharacteristic"/&gt; 
 &amp;lt;/owl:FunctionalProperty&gt; 
 &amp;lt;owl:AllDifferent/&gt; 
&lt;/rdf:RDF&gt; 

B.3 Modelo do nível semântico 
O modelo nível semântico consiste em dois modelos OWL. 

B.3.1 Modelo OWL do nível semântico 
&lt;?xml version="1.0"?&gt; 
&lt;rdf:RDF 
    xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" 
    xmlns:protege="http://protege.stanford.edu/plugins/owl/protege#" 
    xmlns:xsp="http://www.owl-ontologies.com/2005/08/07/xsp.owl#" 
    xmlns:owl="http://www.w3.org/2002/07/owl#" 
    xmlns:xsd="http://www.w3.org/2001/XMLSchema#" 
    xmlns:swrl="http://www.w3.org/2003/11/swrl#" 
    xmlns:swrlb="http://www.w3.org/2003/11/swrlb#" 
    xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#" 
    xmlns="http://www.owl-ontologies.com/SemanticLevelOntology.owl#" 
  xml:base="http://www.owl-ontologies.com/SemanticLevelOntology.owl"&gt; 
 &amp;lt;owl:Ontology rdf:about=""/&gt; 
 &amp;lt;owl:Class rdf:ID="SemanticEntity"/&gt; 
&lt;/rdf:RDF&gt; 

B.3.2 Modelo OWL do domínio 
&lt;?xml version="1.0"?&gt; 
&lt;rdf:RDF 
    xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" 
    xmlns:protege="http://protege.stanford.edu/plugins/owl/protege#" 
    xmlns:xsp="http://www.owl-ontologies.com/2005/08/07/xsp.owl#" 
    xmlns:part 

="http://www.w3.org/2001/sw/BestPractices/OEP/SimplePartWhole/part.owl#" 
    xmlns:owl="http://www.w3.org/2002/07/owl#" 



111 

 

 

    xmlns="http://www.owl-ontologies.com/StratigraphyOntology.owl#" 
    xmlns:xsd="http://www.w3.org/2001/XMLSchema#" 
    xmlns:swrl="http://www.w3.org/2003/11/swrl#" 
    xmlns:swrlb="http://www.w3.org/2003/11/swrlb#" 
    xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#" 
  xml:base="http://www.owl-ontologies.com/StratigraphyOntology.owl"&gt; 
 &amp;lt;owl:Ontology rdf:about=""&gt; 
   &amp;lt;owl:imports 
rdf:resource="http://www.w3.org/2001/sw/BestPractices/OEP/SimplePartWhole/part.o
wl"/&gt; 
 &amp;lt;/owl:Ontology&gt; 
 &amp;lt;owl:Class rdf:ID="FloodingSurface"&gt; 
   &amp;lt;rdfs:subClassOf&gt; 
     &amp;lt;owl:Class rdf:ID="StratigraphicSurface"/&gt; 
   &amp;lt;/rdfs:subClassOf&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class&gt; 
   &amp;lt;owl:unionOf rdf:parseType="Collection"&gt; 
     &amp;lt;owl:Class rdf:ID="Parassequence"/&gt; 
     &amp;lt;owl:Class rdf:about="#FloodingSurface"/&gt; 
     &amp;lt;owl:Class rdf:ID="SequenceBoundary"/&gt; 
   &amp;lt;/owl:unionOf&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class rdf:ID="MaximumFloodingSurface"&gt; 
   &amp;lt;rdfs:subClassOf rdf:resource="#FloodingSurface"/&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class rdf:about="#Parassequence"&gt; 
   &amp;lt;owl:disjointWith&gt; 
     &amp;lt;owl:Class rdf:ID="Sequence"/&gt; 
   &amp;lt;/owl:disjointWith&gt; 
   &amp;lt;rdfs:subClassOf&gt; 
     &amp;lt;owl:Class rdf:ID="StratigraphicUnit"/&gt; 
   &amp;lt;/rdfs:subClassOf&gt; 
   &amp;lt;rdfs:subClassOf&gt; 
     &amp;lt;owl:Restriction&gt; 
       &amp;lt;owl:onProperty 
rdf:resource="http://www.w3.org/2001/sw/BestPractices/OEP/SimplePartWhole/part.o
wl#hasPart"/&gt; 
       &amp;lt;owl:allValuesFrom rdf:resource="#FloodingSurface"/&gt; 
     &amp;lt;/owl:Restriction&gt; 
   &amp;lt;/rdfs:subClassOf&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class rdf:ID="StratigraphicEntity"/&gt; 
 &amp;lt;owl:Class rdf:about="#SequenceBoundary"&gt; 
   &amp;lt;rdfs:subClassOf&gt; 
     &amp;lt;owl:Class rdf:about="#StratigraphicSurface"/&gt; 
   &amp;lt;/rdfs:subClassOf&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class&gt; 
   &amp;lt;owl:unionOf rdf:parseType="Collection"&gt; 
     &amp;lt;owl:Class rdf:about="#Parassequence"/&gt; 
     &amp;lt;owl:Class rdf:about="#MaximumFloodingSurface"/&gt; 
   &amp;lt;/owl:unionOf&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class rdf:about="#Sequence"&gt; 



112 

 

 

   &amp;lt;owl:disjointWith rdf:resource="#Parassequence"/&gt; 
   &amp;lt;rdfs:subClassOf&gt; 
     &amp;lt;owl:Class rdf:about="#StratigraphicUnit"/&gt; 
   &amp;lt;/rdfs:subClassOf&gt; 
   &amp;lt;rdfs:subClassOf&gt; 
     &amp;lt;owl:Restriction&gt; 
       &amp;lt;owl:allValuesFrom&gt; 
         &amp;lt;owl:Class&gt; 
           &amp;lt;owl:unionOf rdf:parseType="Collection"&gt; 
             &amp;lt;owl:Class rdf:ID="SystemTract"/&gt; 
             &amp;lt;owl:Class rdf:about="#MaximumFloodingSurface"/&gt; 
             &amp;lt;owl:Class rdf:about="#SequenceBoundary"/&gt; 
           &amp;lt;/owl:unionOf&gt; 
         &amp;lt;/owl:Class&gt; 
       &amp;lt;/owl:allValuesFrom&gt; 
       &amp;lt;owl:onProperty 
rdf:resource="http://www.w3.org/2001/sw/BestPractices/OEP/SimplePartWhole/part.o
wl#hasPart"/&gt; 
     &amp;lt;/owl:Restriction&gt; 
   &amp;lt;/rdfs:subClassOf&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class rdf:about="#StratigraphicSurface"&gt; 
   &amp;lt;rdfs:subClassOf rdf:resource="#StratigraphicEntity"/&gt; 
   &amp;lt;owl:disjointWith&gt; 
     &amp;lt;owl:Class rdf:about="#StratigraphicUnit"/&gt; 
   &amp;lt;/owl:disjointWith&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class rdf:about="#StratigraphicUnit"&gt; 
   &amp;lt;rdfs:subClassOf&gt; 
     &amp;lt;owl:Restriction&gt; 
       &amp;lt;owl:allValuesFrom&gt; 
         &amp;lt;owl:Class&gt; 
           &amp;lt;owl:unionOf rdf:parseType="Collection"&gt; 
             &amp;lt;owl:Class rdf:about="#StratigraphicSurface"/&gt; 
             &amp;lt;owl:Class rdf:about="#StratigraphicUnit"/&gt; 
           &amp;lt;/owl:unionOf&gt; 
         &amp;lt;/owl:Class&gt; 
       &amp;lt;/owl:allValuesFrom&gt; 
       &amp;lt;owl:onProperty 
rdf:resource="http://www.w3.org/2001/sw/BestPractices/OEP/SimplePartWhole/part.o
wl#hasPart"/&gt; 
     &amp;lt;/owl:Restriction&gt; 
   &amp;lt;/rdfs:subClassOf&gt; 
   &amp;lt;rdfs:subClassOf rdf:resource="#StratigraphicEntity"/&gt; 
   &amp;lt;owl:disjointWith rdf:resource="#StratigraphicSurface"/&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class&gt; 
   &amp;lt;owl:unionOf rdf:parseType="Collection"&gt; 
     &amp;lt;owl:Class rdf:about="#Parassequence"/&gt; 
     &amp;lt;owl:Class rdf:about="#MaximumFloodingSurface"/&gt; 
     &amp;lt;owl:Class rdf:about="#SequenceBoundary"/&gt; 
   &amp;lt;/owl:unionOf&gt; 
 &amp;lt;/owl:Class&gt; 
 &amp;lt;owl:Class rdf:about="#SystemTract"&gt; 
   &amp;lt;rdfs:subClassOf rdf:resource="#StratigraphicUnit"/&gt; 



113 

 

 

   &amp;lt;rdfs:subClassOf&gt; 
     &amp;lt;owl:Restriction&gt; 
       &amp;lt;owl:onProperty 
rdf:resource="http://www.w3.org/2001/sw/BestPractices/OEP/SimplePartWhole/part.o
wl#hasPart"/&gt; 
       &amp;lt;owl:allValuesFrom rdf:resource="#Parassequence"/&gt; 
     &amp;lt;/owl:Restriction&gt; 
   &amp;lt;/rdfs:subClassOf&gt; 
 &amp;lt;/owl:Class&gt; 
&lt;/rdf:RDF&gt; 

B.4 Modelo de ancoramento simbólico 
O modelo de ancoramento simbólico consiste em um modelo OWL. 
&lt;?xml version="1.0"?&gt; 
&lt;rdf:RDF 
    xmlns:protege="http://protege.stanford.edu/plugins/owl/protege#" 
    xmlns:xsp="http://www.owl-ontologies.com/2005/08/07/xsp.owl#" 
    xmlns:swrlx="http://swrl.stanford.edu/ontologies/built-ins/3.3/swrlx.owl#" 
    xmlns:spo="http://www.owl-ontologies.com/SignalProcessingOntology.owl#" 
    xmlns:swrlm="http://swrl.stanford.edu/ontologies/built-ins/3.4/swrlm.owl#" 
    xmlns:swrlb="http://www.w3.org/2003/11/swrlb#" 
    xmlns:vlo="http://www.owl-ontologies.com/VisualLevel.owl#" 
    xmlns:temporal="http://swrl.stanford.edu/ontologies/built-ins/3.3/temporal.owl#" 
    xmlns:tbox="http://swrl.stanford.edu/ontologies/built-ins/3.3/tbox.owl#" 
    xmlns:slo="http://www.owl-ontologies.com/SemanticLevelOntology.owl#" 
    xmlns:fuzzy="http://swrl.stanford.edu/ontologies/built-ins/3.3/fuzzy.owl#" 
    xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" 
    xmlns="http://www.owl-ontologies.com/MappingOntology.owl#" 
    xmlns:owl="http://www.w3.org/2002/07/owl#" 
    xmlns:sqwrl="http://sqwrl.stanford.edu/ontologies/built-ins/3.4/sqwrl.owl#" 
    xmlns:abox="http://swrl.stanford.edu/ontologies/built-ins/3.3/abox.owl#" 
    xmlns:xsd="http://www.w3.org/2001/XMLSchema#" 
    xmlns:swrl="http://www.w3.org/2003/11/swrl#" 
    xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#" 
    xmlns:swrla="http://swrl.stanford.edu/ontologies/3.3/swrla.owl#" 
  xml:base="http://www.owl-ontologies.com/MappingOntology.owl"&gt; 
 &amp;lt;owl:Ontology rdf:about=""&gt; 
   &amp;lt;owl:imports 

 rdf:resource="http://swrl.stanford.edu/ontologies/built-ins/3.3/fuzzy.owl"/&gt; 
   &amp;lt;owl:imports 

 rdf:resource="http://www.owl-ontologies.com/SemanticLevelOntology.owl"/&gt; 
   &amp;lt;owl:imports 

 rdf:resource="http://swrl.stanford.edu/ontologies/built-ins/3.3/swrlx.owl"/&gt; 
   &amp;lt;owl:imports 

 rdf:resource="http://swrl.stanford.edu/ontologies/built-ins/3.3/temporal.owl"/&gt; 
   &amp;lt;owl:imports 

 rdf:resource="http://swrl.stanford.edu/ontologies/built-ins/3.4/swrlm.owl"/&gt; 
   &amp;lt;owl:imports rdf:resource="http://swrl.stanford.edu/ontologies/3.3/swrla.owl"/&gt; 
   &amp;lt;owl:imports 

 rdf:resource="http://swrl.stanford.edu/ontologies/built-ins/3.3/abox.owl"/&gt; 
   &amp;lt;owl:imports 

 rdf:resource="http://sqwrl.stanford.edu/ontologies/built-ins/3.4/sqwrl.owl"/&gt; 
   &amp;lt;owl:imports  



114 

 

 

rdf:resource="http://www.owl-ontologies.com/VisualLevel.owl"/&gt; 
   &amp;lt;owl:imports 

 rdf:resource="http://www.owl-ontologies.com/SignalProcessingOntology.owl"/&gt; 
   &amp;lt;owl:imports 

 rdf:resource="http://swrl.stanford.edu/ontologies/built-ins/3.3/tbox.owl"/&gt; 
 &amp;lt;/owl:Ontology&gt; 
 &amp;lt;rdfs:Class 
rdf:about="http://swrl.stanford.edu/ontologies/3.3/swrla.owl#RuleGroup"/&gt; 
 &amp;lt;owl:ObjectProperty rdf:ID="groundedByDG"&gt; 
   &amp;lt;rdfs:range 
rdf:resource="http://swrl.stanford.edu/ontologies/3.3/swrla.owl#RuleGroup"/&gt; 
   &amp;lt;owl:inverseOf&gt; 
     &amp;lt;owl:ObjectProperty rdf:ID="groundingPrimitive"/&gt; 
   &amp;lt;/owl:inverseOf&gt; 
 &amp;lt;/owl:ObjectProperty&gt; 
 &amp;lt;owl:ObjectProperty rdf:ID="upwardMapLink"&gt; 
   &amp;lt;rdfs:subPropertyOf&gt; 
     &amp;lt;owl:ObjectProperty rdf:ID="mappingLink"/&gt; 
   &amp;lt;/rdfs:subPropertyOf&gt; 
 &amp;lt;/owl:ObjectProperty&gt; 
 &amp;lt;owl:ObjectProperty rdf:ID="downwardMapLink"&gt; 
   &amp;lt;rdfs:subPropertyOf rdf:resource="#mappingLink"/&gt; 
 &amp;lt;/owl:ObjectProperty&gt; 
 &amp;lt;owl:ObjectProperty rdf:ID="mappingAnalogicToVisual"&gt; 
   &amp;lt;owl:inverseOf&gt; 
     &amp;lt;owl:ObjectProperty rdf:ID="mappingVisualToAnalogic"/&gt; 
   &amp;lt;/owl:inverseOf&gt; 
   &amp;lt;rdfs:range 

 rdf:resource="http://www.owl-ontologies.com/VisualLevel.owl#VisualEntity"/&gt; 
   &amp;lt;rdfs:subPropertyOf rdf:resource="#upwardMapLink"/&gt; 
 &amp;lt;/owl:ObjectProperty&gt; 
 &amp;lt;owl:ObjectProperty rdf:about="#mappingVisualToAnalogic"&gt; 
   &amp;lt;owl:inverseOf rdf:resource="#mappingAnalogicToVisual"/&gt; 
   &amp;lt;rdfs:domain 

 rdf:resource="http://www.owl-ontologies.com/VisualLevel.owl#VisualEntity"/&gt; 
   &amp;lt;rdfs:subPropertyOf rdf:resource="#downwardMapLink"/&gt; 
 &amp;lt;/owl:ObjectProperty&gt; 
 &amp;lt;owl:ObjectProperty rdf:ID="mappingSemanticToVisual"&gt; 
   &amp;lt;rdfs:range 

 rdf:resource="http://www.owl-ontologies.com/VisualLevel.owl#VisualEntity"/&gt; 
   &amp;lt;owl:inverseOf&gt; 
     &amp;lt;owl:ObjectProperty rdf:ID="mappingVisualToSemantic"/&gt; 
   &amp;lt;/owl:inverseOf&gt; 
   &amp;lt;rdfs:domain rdf:resource 

="http://www.owl-ontologies.com/SemanticLevelOntology.owl#SemanticEntity"/&gt; 
   &amp;lt;rdfs:subPropertyOf rdf:resource="#downwardMapLink"/&gt; 
 &amp;lt;/owl:ObjectProperty&gt; 
 &amp;lt;owl:ObjectProperty rdf:about="#mappingVisualToSemantic"&gt; 
   &amp;lt;rdfs:range rdf:resource 

="http://www.owl-ontologies.com/SemanticLevelOntology.owl#SemanticEntity"/&gt; 
   &amp;lt;owl:inverseOf rdf:resource="#mappingSemanticToVisual"/&gt; 
   &amp;lt;rdfs:domain rdf:resource 

="http://www.owl-ontologies.com/VisualLevel.owl#VisualEntity"/&gt; 
   &amp;lt;rdfs:subPropertyOf rdf:resource="#upwardMapLink"/&gt; 
 &amp;lt;/owl:ObjectProperty&gt; 



115 

 

 

 &amp;lt;owl:ObjectProperty rdf:about="#groundingPrimitive"&gt; 
   &amp;lt;rdfs:domain  
 rdf:resource="http://swrl.stanford.edu/ontologies/3.3/swrla.owl#RuleGroup"/&gt; 
   &amp;lt;owl:inverseOf rdf:resource="#groundedByDG"/&gt; 
 &amp;lt;/owl:ObjectProperty&gt; 
&lt;/rdf:RDF&gt; 

B.5 Modelos Adicionais 
Os modelos adicionais constituem a ontologia de sistema e outra ontologia 

de relações de ordem.  

B.5.1 Modelo OWL do sistema InteliStrata 
&lt;?xml version="1.0"?&gt; 
&lt;rdf:RDF xmlns="http://www.owl-ontologies.com/InteliSrata.owl#" 
     xml:base="http://www.owl-ontologies.com/InteliSrata.owl" 
     xmlns:slo="http://www.owl-ontologies.com/SemanticLevelOntology.owl#" 
     xmlns:dom="http://www.owl-ontologies.com/StratigraphyOntology.owl#" 
     xmlns:spo="http://www.owl-ontologies.com/SignalProcessingOntology.owl#" 
     xmlns:abox="http://swrl.stanford.edu/ontologies/built-ins/3.3/abox.owl#" 
     xmlns:sqwrl="http://sqwrl.stanford.edu/ontologies/built-ins/3.4/sqwrl.owl#" 
     xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#" 
     xmlns:swrl="http://www.w3.org/2003/11/swrl#" 
     xmlns:fuzzy="http://swrl.stanford.edu/ontologies/built-ins/3.3/fuzzy.owl#" 
     xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" 
     xmlns:alo="http://www.owl-ontologies.com/AnalogLevelOntology.owl#" 
     xmlns:protege="http://protege.stanford.edu/plugins/owl/protege#" 
     xmlns:xsp="http://www.owl-ontologies.com/2005/08/07/xsp.owl#" 
     xmlns:swrlx="http://swrl.stanford.edu/ontologies/built-ins/3.3/swrlx.owl#" 
     xmlns:mapo="http://www.owl-ontologies.com/MappingOntology.owl#" 
     xmlns:vlo="http://www.owl-ontologies.com/VisualLevel.owl#" 
     xmlns:ineq="http://www.owl-ontologies.com/InequalityOntology.owl#" 
     xmlns:swrlm="http://swrl.stanford.edu/ontologies/built-ins/3.4/swrlm.owl#" 
     xmlns:temporal="http://swrl.stanford.edu/ontologies/built-ins/3.3/temporal.owl#" 
     xmlns:swrli="http://swrl.stanford.edu/ontologies/built-ins/3.4/swrli.owl#" 
     xmlns:swrlb="http://www.w3.org/2003/11/swrlb#" 
     xmlns:xsd="http://www.w3.org/2001/XMLSchema#" 
     xmlns:owl="http://www.w3.org/2002/07/owl#" 
     xmlns:tbox="http://swrl.stanford.edu/ontologies/built-ins/3.3/tbox.owl#" 
     xmlns:part= 

"http://www.w3.org/2001/sw/BestPractices/OEP/SimplePartWhole/part.owl#" 
     xmlns:swrlineq="http://swrl.stanford.edu/ontologies/built-ins/3.3/swrli.owl#" 
     xmlns:swrla="http://swrl.stanford.edu/ontologies/3.3/swrla.owl#"&gt; 
   &amp;lt;owl:Ontology rdf:about=""&gt; 
       &amp;lt;owl:imports 

 rdf:resource="http://swrl.stanford.edu/ontologies/built-ins/3.3/abox.owl"/&gt; 
       &amp;lt;owl:imports 

 rdf:resource="http://sqwrl.stanford.edu/ontologies/built-ins/3.4/sqwrl.owl"/&gt; 
       &amp;lt;owl:imports  

rdf:resource="http://swrl.stanford.edu/ontologies/3.3/swrla.owl"/&gt; 
       &amp;lt;owl:imports 

 rdf:resource="http://swrl.stanford.edu/ontologies/built-ins/3.3/temporal.owl"/&gt; 
       &amp;lt;owl:imports 



116 

 

 

 rdf:resource="http://www.owl-ontologies.com/StratigraphyOntology.owl"/&gt; 
       &amp;lt;owl:imports 

 rdf:resource="http://swrl.stanford.edu/ontologies/built-ins/3.3/swrli.owl"/&gt; 
       &amp;lt;owl:imports 

 rdf:resource="http://swrl.stanford.edu/ontologies/built-ins/3.3/swrlx.owl"/&gt; 
       &amp;lt;owl:imports  
  rdf:resource="http://www.owl-ontologies.com/MappingOntology.owl"/&gt; 
       &amp;lt;owl:imports 

 rdf:resource="http://swrl.stanford.edu/ontologies/built-ins/3.3/tbox.owl"/&gt; 
       &amp;lt;owl:imports 

 rdf:resource="http://swrl.stanford.edu/ontologies/built-ins/3.3/fuzzy.owl"/&gt; 
   &amp;lt;/owl:Ontology&gt; 
       &amp;lt;rdf:Description rdf:about="&amp;amp;dom;MaximumFloodingSurface"&gt; 
       &amp;lt;mapo:groundedByDG rdf:resource="#sdgMaximumFloodingSurface"/&gt; 
   &amp;lt;/rdf:Description&gt; 
   &amp;lt;rdf:Description rdf:about="&amp;amp;dom;Parassequence"&gt; 
       &amp;lt;mapo:groundedByDG rdf:resource="#sdgParassequence"/&gt; 
   &amp;lt;/rdf:Description&gt; 
   &amp;lt;rdf:Description rdf:about="&amp;amp;dom;Sequence"&gt; 
       &amp;lt;mapo:groundedByDG rdf:resource="#sdgDomSequence"/&gt; 
   &amp;lt;/rdf:Description&gt; 
   &amp;lt;ParassequenceLengthValue rdf:ID="LongLengthParassequence"&gt; 
       &amp;lt;ineq:greaterThan rdf:resource="#MediumLengthParassequence"/&gt; 
   &amp;lt;/ParassequenceLengthValue&gt; 
   &amp;lt;SequenceLengthValue rdf:ID="LongLengthSequence"&gt; 
       &amp;lt;ineq:smallerThan rdf:resource="#VeryLongLengthSequence"/&gt; 
       &amp;lt;ineq:greaterThan rdf:resource="#MediumLengthSequence"/&gt; 
   &amp;lt;/SequenceLengthValue&gt; 
   &amp;lt;ParassequenceLengthValue rdf:ID="MediumLengthParassequence"&gt; 
       &amp;lt;ineq:smallerThan rdf:resource="#LongLengthParassequence"/&gt; 
       &amp;lt;ineq:greaterThan rdf:resource="#ShortLengthParassequence"/&gt; 
   &amp;lt;/ParassequenceLengthValue&gt; 
   &amp;lt;SequenceLengthValue rdf:ID="MediumLengthSequence"&gt; 
       &amp;lt;ineq:smallerThan rdf:resource="#LongLengthSequence"/&gt; 
       &amp;lt;ineq:greaterThan rdf:resource="#ShortLengthSequence"/&gt; 
   &amp;lt;/SequenceLengthValue&gt; 
   &amp;lt;owl:Class rdf:ID="ParassequenceLengthValue"&gt; 
       &amp;lt;owl:equivalentClass&gt; 
           &amp;lt;owl:Class&gt; 
               &amp;lt;owl:oneOf rdf:parseType="Collection"&gt; 
                   &amp;lt;rdf:Description rdf:about="#LongLengthParassequence"/&gt; 
                   &amp;lt;rdf:Description rdf:about="#MediumLengthParassequence"/&gt; 
                   &amp;lt;rdf:Description rdf:about="#ShortLengthParassequence"/&gt; 
               &amp;lt;/owl:oneOf&gt; 
           &amp;lt;/owl:Class&gt; 
       &amp;lt;/owl:equivalentClass&gt; 
       &amp;lt;rdfs:subClassOf rdf:resource="&amp;amp;vlo;LengthValue"/&gt; 
   &amp;lt;/owl:Class&gt; 
   &amp;lt;swrla:RuleGroup rdf:ID="sdgDomSequence"&gt; 
       &amp;lt;swrla:isRuleGroupEnabled  
  rdf:datatype="&amp;amp;xsd;boolean"&gt;true&lt;/swrla:isRuleGroupEnabled&gt; 
       &amp;lt;mapo:groundingPrimitive rdf:resource="&amp;amp;dom;Sequence"/&gt; 
   &amp;lt;/swrla:RuleGroup&gt; 
   &amp;lt;swrla:RuleGroup rdf:ID="sdgGaussianCurve"&gt; 
       &amp;lt;swrla:isRuleGroupEnabled 



117 

 

 

 rdf:datatype="&amp;amp;xsd;boolean"&gt;true&lt;/swrla:isRuleGroupEnabled&gt; 
       &amp;lt;mapo:groundingPrimitive rdf:resource="&amp;amp;vlo;GaussianCurve"/&gt; 
   &amp;lt;/swrla:RuleGroup&gt; 
   &amp;lt;swrla:RuleGroup rdf:ID="sdgMaximumFloodingSurface"&gt; 
       &amp;lt;swrla:isRuleGroupEnabled  
  rdf:datatype="&amp;amp;xsd;boolean"&gt;true&lt;/swrla:isRuleGroupEnabled&gt; 
       &amp;lt;mapo:groundingPrimitive rdf:resource="&amp;amp;dom;MaximumFloodingSurface"/&gt; 
   &amp;lt;/swrla:RuleGroup&gt; 
   &amp;lt;swrla:RuleGroup rdf:ID="sdgMaximumPoint"&gt; 
       &amp;lt;swrla:isRuleGroupEnabled  
  rdf:datatype="&amp;amp;xsd;boolean"&gt;true&lt;/swrla:isRuleGroupEnabled&gt; 
       &amp;lt;mapo:groundingPrimitive rdf:resource="&amp;amp;vlo;MaximumPoint"/&gt; 
   &amp;lt;/swrla:RuleGroup&gt; 
   &amp;lt;swrla:RuleGroup rdf:ID="sdgParassequence"&gt; 
       &amp;lt;swrla:isRuleGroupEnabled  
  rdf:datatype="&amp;amp;xsd;boolean"&gt;true&lt;/swrla:isRuleGroupEnabled&gt; 
       &amp;lt;mapo:groundingPrimitive rdf:resource="&amp;amp;dom;Parassequence"/&gt; 
   &amp;lt;/swrla:RuleGroup&gt; 
   &amp;lt;swrla:RuleGroup rdf:ID="sdgSmoothToothCurve"&gt; 
       &amp;lt;swrla:isRuleGroupEnabled 

 rdf:datatype="&amp;amp;xsd;boolean"&gt;true&lt;/swrla:isRuleGroupEnabled&gt; 
       &amp;lt;mapo:groundingPrimitive rdf:resource="&amp;amp;vlo;SmoothToothCurve"/&gt; 
   &amp;lt;/swrla:RuleGroup&gt; 
   &amp;lt;swrla:RuleGroup rdf:ID="sdgVloLength"&gt; 
       &amp;lt;swrla:isRuleGroupEnabled  
  rdf:datatype="&amp;amp;xsd;boolean"&gt;true&lt;/swrla:isRuleGroupEnabled&gt; 
       &amp;lt;mapo:groundingPrimitive rdf:resource="&amp;amp;vlo;hasLength"/&gt; 
   &amp;lt;/swrla:RuleGroup&gt; 
   &amp;lt;swrla:RuleGroup rdf:ID="sdgVloSpatialRelation"&gt; 
       &amp;lt;swrla:isRuleGroupEnabled 

 rdf:datatype="&amp;amp;xsd;boolean"&gt;true&lt;/swrla:isRuleGroupEnabled&gt; 
       &amp;lt;mapo:groundingPrimitive rdf:resource="&amp;amp;vlo;hasVisualProperty"/&gt; 
   &amp;lt;/swrla:RuleGroup&gt; 
   &amp;lt;owl:Class rdf:ID="SequenceLengthValue"&gt; 
       &amp;lt;owl:equivalentClass&gt; 
           &amp;lt;owl:Class&gt; 
               &amp;lt;owl:oneOf rdf:parseType="Collection"&gt; 
                   &amp;lt;rdf:Description rdf:about="#LongLengthSequence"/&gt; 
                   &amp;lt;rdf:Description rdf:about="#MediumLengthSequence"/&gt; 
                   &amp;lt;rdf:Description rdf:about="#ShortLengthSequence"/&gt; 
                   &amp;lt;rdf:Description rdf:about="#VeryLongLengthSequence"/&gt; 
                   &amp;lt;rdf:Description rdf:about="#VeryShortLengthSequence"/&gt; 
               &amp;lt;/owl:oneOf&gt; 
           &amp;lt;/owl:Class&gt; 
       &amp;lt;/owl:equivalentClass&gt; 
       &amp;lt;rdfs:subClassOf rdf:resource="&amp;amp;vlo;LengthValue"/&gt; 
   &amp;lt;/owl:Class&gt; 
   &amp;lt;ParassequenceLengthValue rdf:ID="ShortLengthParassequence"&gt; 
       &amp;lt;ineq:smallerThan rdf:resource="#MediumLengthParassequence"/&gt; 
   &amp;lt;/ParassequenceLengthValue&gt; 
   &amp;lt;SequenceLengthValue rdf:ID="ShortLengthSequence"&gt; 
       &amp;lt;ineq:smallerThan rdf:resource="#MediumLengthSequence"/&gt; 
       &amp;lt;ineq:greaterThan rdf:resource="#VeryShortLengthSequence"/&gt; 
   &amp;lt;/SequenceLengthValue&gt; 
   &amp;lt;SequenceLengthValue rdf:ID="VeryLongLengthSequence"&gt; 



118 

 

 

       &amp;lt;ineq:greaterThan rdf:resource="#LongLengthSequence"/&gt; 
   &amp;lt;/SequenceLengthValue&gt; 
   &amp;lt;SequenceLengthValue rdf:ID="VeryShortLengthSequence"&gt; 
       &amp;lt;ineq:smallerThan rdf:resource="#ShortLengthSequence"/&gt; 
   &amp;lt;/SequenceLengthValue&gt; 
   &amp;lt;rdf:Description rdf:about="&amp;amp;vlo;GaussianCurve"&gt; 
       &amp;lt;mapo:groundedByDG rdf:resource="#sdgGaussianCurve"/&gt; 
   &amp;lt;/rdf:Description&gt; 
   &amp;lt;rdf:Description rdf:about="&amp;amp;vlo;hasLength"&gt; 
       &amp;lt;rdfs:domain rdf:resource="&amp;amp;vlo;Curve"/&gt; 
       &amp;lt;mapo:groundedByDG rdf:resource="#sdgVloLength"/&gt; 
   &amp;lt;/rdf:Description&gt; 
   &amp;lt;rdf:Description rdf:about="&amp;amp;vlo;hasVisualProperty"&gt; 
       &amp;lt;rdfs:domain rdf:resource="&amp;amp;vlo;VisualEntity"/&gt; 
       &amp;lt;mapo:groundedByDG rdf:resource="#sdgVloSpatialRelation"/&gt; 
   &amp;lt;/rdf:Description&gt; 
   &amp;lt;rdf:Description rdf:about="&amp;amp;vlo;hasSmoothToothCurveCharacteristic"/&gt; 
   &amp;lt;rdf:Description rdf:about="&amp;amp;vlo;MaximumPoint"&gt; 
       &amp;lt;mapo:groundedByDG rdf:resource="#sdgMaximumPoint"/&gt; 
   &amp;lt;/rdf:Description&gt; 
   &amp;lt;rdf:Description rdf:about="&amp;amp;vlo;SmoothToothCurve"&gt; 
       &amp;lt;mapo:groundedByDG rdf:resource="#sdgSmoothToothCurve"/&gt; 
   &amp;lt;/rdf:Description&gt; 
&lt;/rdf:RDF&gt; 

B.5.2 Modelo OWL de relações de ordem (desigualdades) 
&lt;?xml version="1.0"?&gt; 
&lt;rdf:RDF 
    xmlns="http://www.owl-ontologies.com/InequalityOntology.owl#" 
    xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" 
    xmlns:protege="http://protege.stanford.edu/plugins/owl/protege#" 
    xmlns:xsp="http://www.owl-ontologies.com/2005/08/07/xsp.owl#" 
    xmlns:owl="http://www.w3.org/2002/07/owl#" 
    xmlns:xsd="http://www.w3.org/2001/XMLSchema#" 
    xmlns:swrl="http://www.w3.org/2003/11/swrl#" 
    xmlns:swrlb="http://www.w3.org/2003/11/swrlb#" 
    xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#" 
  xml:base="http://www.owl-ontologies.com/InequalityOntology.owl"&gt; 
 &amp;lt;owl:Ontology rdf:about=""/&gt; 
 &amp;lt;owl:TransitiveProperty rdf:ID="greaterThan"&gt; 
   &amp;lt;rdfs:subPropertyOf&gt; 
     &amp;lt;owl:TransitiveProperty rdf:ID="inequalityRelation"/&gt; 
   &amp;lt;/rdfs:subPropertyOf&gt; 
   &amp;lt;owl:inverseOf&gt; 
     &amp;lt;owl:TransitiveProperty rdf:ID="smallerThan"/&gt; 
   &amp;lt;/owl:inverseOf&gt; 
   &amp;lt;rdf:type rdf:resource="http://www.w3.org/2002/07/owl#ObjectProperty"/&gt; 
 &amp;lt;/owl:TransitiveProperty&gt; 
 &amp;lt;owl:TransitiveProperty rdf:about="#smallerThan"&gt; 
   &amp;lt;rdfs:subPropertyOf&gt; 
     &amp;lt;owl:TransitiveProperty rdf:about="#inequalityRelation"/&gt; 
   &amp;lt;/rdfs:subPropertyOf&gt; 
   &amp;lt;rdf:type rdf:resource="http://www.w3.org/2002/07/owl#ObjectProperty"/&gt; 
   &amp;lt;owl:inverseOf rdf:resource="#greaterThan"/&gt; 



119 

 

 

 &amp;lt;/owl:TransitiveProperty&gt; 
 &amp;lt;owl:TransitiveProperty rdf:about="#inequalityRelation"&gt; 
   &amp;lt;rdf:type rdf:resource="http://www.w3.org/2002/07/owl#ObjectProperty"/&gt; 
 &amp;lt;/owl:TransitiveProperty&gt; 
 &amp;lt;owl:SymmetricProperty rdf:ID="equals"&gt; 
   &amp;lt;owl:inverseOf rdf:resource="#equals"/&gt; 
   &amp;lt;rdfs:subPropertyOf rdf:resource="#inequalityRelation"/&gt; 
   &amp;lt;rdf:type rdf:resource="http://www.w3.org/2002/07/owl#TransitiveProperty"/&gt; 
   &amp;lt;rdf:type rdf:resource="http://www.w3.org/2002/07/owl#ObjectProperty"/&gt; 
 &amp;lt;/owl:SymmetricProperty&gt; 
&lt;/rdf:RDF&gt; 

B.5.3 Detectores simbólicos 
Embora os detectores simbólicos pertençam ao modelo OWL do sistema In-

teliStrata, a sua codificação RDF/XML é consideravelmente prolixa. Por isso, 
somente a representação direta dos detectores é descrita aqui. 

A Tabela B.1 na página seguinte apresenta os detectores e a qual primitiva 
estão relacionados via grupos de detectores. 



 

 

 

Tabela B.1: Detectores simbólicos e relação com primitivas ancoradas. 

Primitiva Detector Simbólico (regra SWRL) 

dom:Sequence 
mapo:mappingSemanticToVisual(?de, ?ve)  ?  vlo:GaussianCurve(?ve)   
?  vlo:hasGaussianCurveCharacteristic(?ve, ?pattern)  ?  swrlineq:greaterThanOrEqual(?pattern, vlo:StrongPattern)   
?  vlo:hasLength(?ve, ?length)  ?  swrlineq:greaterThanOrEqual(?length, MediumLengthSequence) 
 ? dom:Sequence(?de) 

dom:Parassequence 

mapo:mappingSemanticToVisual(?de, ?ve)  ?  vlo:SmoothToothCurve(?ve)   
?  vlo:hasSmoothToothCurveCurveCharacteristic(?ve, ?pattern)   
?  swrlineq:greaterThanOrEqual(?pattern, vlo:MediumPattern)   
?  vlo:hasLength(?ve, ?length)  ?  swrlineq:greaterThanOrEqual(?length, ShortLengthParassequence)   
?  vlo:isProperPartOf(?ve, ?sequenceVe)  ?  mapo:mappingSemanticToVisual(?sequenceDe, ?sequenceVe)  
?  dom:Sequence(?sequenceDe)  
? dom:Parassequence(?de)  ?  part:hasPart_directly(?sequenceDe, ?de) 

dom:MaximumFloodingSurface 
mapo:mappingSemanticToVisual(?de, ?ve)  ?  vlo:MaximumPoint(?ve)  ?  vlo:isProperPartOf(?ve, ?sequenceVe)   
?  mapo:mappingSemanticToVisual(?sequenceDe, ?sequenceVe)  ?  dom:Sequence(?sequenceDe)  
? dom:MaximumFloodingSurface(?de)  ?  part:hasPart_directly(?sequenceDe, ?de) 

vlo:MaximumPoint 
mapo:mappingVisualToAnalogic(?ve, ?ae)  ?  alo:Interval(?ae2)  ?  alo:hasMaximumPoint(?ae2, ?ae)  
? vlo:MaximumPoint(?ve) 

vlo:hasLength 

mapo:mappingVisualToAnalogic(?ve, ?al)  ?  alo:Interval(?al)  ?  alo:hasLength(?al, ?value)   
?  fuzzy:match(?value, ?fv, "[5, 10, 50, 60]", ShortLengthParassequence, "[50, 60, 140, 150]", MediumLengthParassequence, 

"[140, 150, 200, 250]", LongLengthSequence)  
? vlo:hasLength(?ve, ?fv) 
mapo:mappingVisualToAnalogic(?ve, ?al)  ?  alo:Interval(?al)  ?  alo:hasLength(?al, ?value)   
?  fuzzy:match(?value, ?fv, "[0, 0, 66, 82]", VeryShortLengthSequence, "[66, 82, 146, 210]", ShortLengthSequence, "[146, 210, 

466, 722]", MediumLengthSequence, "[466, 722, 1746, 2770]", LongLengthSequence, "[1746, 2770, 5000, 7500]", 
VeryLongLengthSequence)  

? vlo:hasLength(?ve, ?fv) 

vlo:SmoothToothCurve 

mapo:mappingVisualToAnalogic(?ve, ?al)  ?  alo:Interval(?al)  ?  spo:isExtractedWith(?al, spo:SmoothToothCurveSPF)   
?  spo:hasExtractionStrength(?al, ?vstrength)   
?  fuzzy:match(?vstrength, ?strength, "[0, 0, 0.2, 0.3]", vlo:VeryWeakPattern, "[0.2, 0.3, 0.4, 0.5]", vlo:WeakPattern, "[0.4, 0.5, 

0.6, 0.7]", vlo:MediumPattern, "[0.6, 0.7, 0.8, 0.9]", vlo:StrongPattern, "[0.8, 0.9, 1, 1]", vlo:VeryStrongPattern) 
 ? vlo:SmoothToothCurve(?ve)  ?  vlo:hasSmoothToothCurveCharacteristic(?ve, ?strength) 

vlo:GaussianCurve 

mapo:mappingVisualToAnalogic(?ve, ?al)  ?  alo:Interval(?al)  ?  spo:isExtractedWith(?al, spo:Gaussian2SPF)   
?  spo:hasExtractionStrength(?al, ?vstrength)   
?  fuzzy:match(?vstrength, ?strength, "[0, 0, 0.2, 0.3]", vlo:VeryWeakPattern, "[0.2, 0.3, 0.4, 0.5]", vlo:WeakPattern, "[0.4, 0.5, 

0.6, 0.7]", vlo:MediumPattern, "[0.6, 0.7, 0.8, 0.9]", vlo:StrongPattern, "[0.8, 0.9, 1, 1]", vlo:VeryStrongPattern)  
? vlo:GaussianCurve(?ve)  ?  vlo:hasGaussianCurveCharacteristic(?ve, ?strength) 



121 

 

 

vlo:hasSpatialRelationWith 

mapo:mappingVisualToAnalogic(?ve1, ?al1)  ?  alo:Interval(?al1)  ?  alo:hasCenterPoint(?al1, ?cp1)   
?  alo:index(?cp1, ?vx1)  ?  mapo:mappingVisualToAnalogic(?ve2, ?al2)  ?  alo:Interval(?al2)   
?  alo:hasCenterPoint(?al2, ?cp2)  ?  alo:index(?cp2, ?vx2)  ?  swrlb:greaterThanOrEqual(?vx1, ?vx2)  
? vlo:isAtRightOf(?ve1, ?ve2)  ?  vlo:isAtLeftOf(?ve2, ?ve1) 
mapo:mappingVisualToAnalogic(?ve1, ?al1)  ?  alo:Interval(?al1)  ?  alo:hasStartPoint(?al1, ?sp1)   
?  alo:hasEndPoint(?al1, ?ep1)  ?  alo:index(?sp1, ?vsx1)  ?  alo:index(?ep1, ?vex1)   
?  mapo:mappingVisualToAnalogic(?ve2, ?al2)  ?  alo:Interval(?al2)  ?  alo:hasStartPoint(?al2, ?sp2)   
?  alo:hasEndPoint(?al2, ?ep2)  ?  alo:index(?sp2, ?vsx2)  ?  alo:index(?ep2, ?vex2)  ?  swrlb:greaterThan(?vsx2, ?vex1)  
? vlo:isDiscreteOf(?ve1, ?ve2) 
mapo:mappingVisualToAnalogic(?ve1, ?al1)  ?  alo:Interval(?al1)  ?  alo:hasStartPoint(?al1, ?sp1)   
?  alo:hasEndPoint(?al1, ?ep1)  ?  alo:index(?sp1, ?vsx1)  ?  alo:index(?ep1, ?vex1)   
?  mapo:mappingVisualToAnalogic(?ve2, ?al2)  ?  alo:Interval(?al2)  ?  alo:hasStartPoint(?al2, ?sp2)  
 ?  alo:hasEndPoint(?al2, ?ep2)  ?  alo:index(?sp2, ?vsx2)  ?  alo:index(?ep2, ?vex2)  ?  swrlb:equal(?vsx2, ?vsx1)   
?  swrlb:equal(?vex2, ?vex1)  
? vlo:isTopologicallyEqualsTo(?ve1, ?ve2) 
mapo:mappingVisualToAnalogic(?ve1, ?al1)  ?  alo:Interval(?al1)  ?  alo:hasEndPoint(?al1, ?ep1)   
?  alo:index(?ep1, ?vex1)  ?  mapo:mappingVisualToAnalogic(?ve2, ?al2)  ?  alo:Interval(?al2)   
?  alo:hasStartPoint(?al2, ?sp2)  ?  alo:index(?sp2, ?vsx2)  ?  swrlb:equal(?vsx2, ?vex1) 
 ? vlo:isExternallyConnectedTo(?ve1, ?ve2)  ?  vlo:isExternallyConnectedTo(?ve2, ?ve1) 
mapo:mappingVisualToAnalogic(?ve1, ?al1)  ?  alo:Interval(?al1)  ?  alo:hasStartPoint(?al1, ?sp1)   
?  alo:hasEndPoint(?al1, ?ep1)  ?  alo:index(?sp1, ?vsx1)  ?  alo:index(?ep1, ?vex1)   
?  mapo:mappingVisualToAnalogic(?ve2, ?al2)  ?  alo:Interval(?al2)  ?  alo:hasStartPoint(?al2, ?sp2)   
?  alo:hasEndPoint(?al2, ?ep2)  ?  alo:index(?sp2, ?vsx2)  ?  alo:index(?ep2, ?vex2)  ?  swrlb:greaterThan(?vsx2, ?vsx1)   
?  swrlb:greaterThan(?vex1, ?vex2)  
? vlo:isNonTangentialProperPartOf(?ve2, ?ve1)  ?  vlo:hasForNonTangentialProperPartOf(?ve1, ?ve2) 
mapo:mappingVisualToAnalogic(?ve1, ?al1)  ?  alo:Interval(?al1)  ?  alo:hasStartPoint(?al1, ?sp1)   
?  alo:hasEndPoint(?al1, ?ep1)  ?  alo:index(?sp1, ?vsx1)  ?  alo:index(?ep1, ?vex1)   
?  mapo:mappingVisualToAnalogic(?ve2, ?al2)  ?  alo:Interval(?al2)  ?  alo:hasStartPoint(?al2, ?sp2)   
?  alo:hasEndPoint(?al2, ?ep2)  ?  alo:index(?sp2, ?vsx2)  ?  alo:index(?ep2, ?vex2)   
?  swrlb:equal(?vsx2, ?vsx1)  ?  swrlb:greaterThan(?vex1, ?vex2) 
 ? vlo:isTangentialProperPartOf(?ve2, ?ve1)  ?  vlo:hasForTangentialProperPart(?ve1, ?ve2) 
mapo:mappingVisualToAnalogic(?ve1, ?al1)  ?  alo:Interval(?al1)  ?  alo:hasStartPoint(?al1, ?sp1)   
?  alo:hasEndPoint(?al1, ?ep1)  ?  alo:index(?sp1, ?vsx1)  ?  alo:index(?ep1, ?vex1)   
?  mapo:mappingVisualToAnalogic(?ve2, ?al2)  ?  alo:Interval(?al2)  ?  alo:hasStartPoint(?al2, ?sp2)   
?  alo:hasEndPoint(?al2, ?ep2)  ?  alo:index(?sp2, ?vsx2)  ?  alo:index(?ep2, ?vex2)  ?  swrlb:greaterThan(?vsx2, ?vsx1)   
?  swrlb:equal(?vex1, ?vex2)  
? vlo:isTangentialProperPartOf(?ve2, ?ve1)  ?  vlo:hasForTangentialProperPart(?ve1, ?ve2) 



122 

 

 

mapo:mappingVisualToAnalogic(?ve1, ?al1)  ?  alo:Interval(?al1)  ?  alo:hasStartPoint(?al1, ?sp1)   
?  alo:hasEndPoint(?al1, ?ep1)  ?  alo:index(?sp1, ?vsx1)  ?  alo:index(?ep1, ?vex1)   
?  mapo:mappingVisualToAnalogic(?ve2, ?al2)  ?  alo:Interval(?al2)  ?  alo:hasStartPoint(?al2, ?sp2)   
?  alo:hasEndPoint(?al2, ?ep2)  ?  alo:index(?sp2, ?vsx2)  ?  alo:index(?ep2, ?vex2)   
?  swrlb:greaterThan(?vex1, ?vsx2)  ?  swrlb:greaterThan(?vex2, ?vex1)  
? vlo:partialOverlaps(?ve1, ?ve2) 
mapo:mappingVisualToAnalogic(?ve1, ?al1)  ?  alo:Interval(?al1)  ?  alo:hasStartPoint(?al1, ?sp1)   
?  alo:hasEndPoint(?al1, ?ep1)  ?  alo:index(?sp1, ?vsx1)  ?  alo:index(?ep1, ?vex1)   
?  mapo:mappingVisualToAnalogic(?ve2, ?al2)  ?  alo:Point(?al2)  ?  alo:hasPoint(?al1, ?al2)   
?  alo:index(?al2, ?vx2)  ?  swrlb:greaterThan(?vx2, ?vsx1)  ?  swrlb:greaterThan(?vex1, ?vx2) 
 ? vlo:isNonTangentialProperPartOf(?ve2, ?ve1)  ?  vlo:hasForNonTangentialProperPartOf(?ve1, ?ve2) 
mapo:mappingVisualToAnalogic(?ve1, ?al1)  ?  alo:Interval(?al1)  ?  alo:hasStartPoint(?al1, ?sp1)   
?  alo:index(?sp1, ?vsx1)  ?  mapo:mappingVisualToAnalogic(?ve2, ?al2)  ?  alo:Point(?al2)  ?  alo:hasPoint(?al1, ?al2)   
?  alo:index(?al2, ?vx2)  ?  swrlb:equal(?vsx1, ?vx2) ? vlo:isTangentialProperPartOf(?ve2, ?ve1)   
?  vlo:hasForTangentialProperPart(?ve1, ?ve2) 
mapo:mappingVisualToAnalogic(?ve1, ?al1)  ?  alo:Interval(?al1)  ?  alo:hasEndPoint(?al1, ?ep1)   
?  alo:index(?ep1, ?vex1)  ?  mapo:mappingVisualToAnalogic(?ve2, ?al2)  ?  alo:Point(?al2)   
?  alo:hasPoint(?al1, ?al2)  ?  alo:index(?al2, ?vx2)  ?  swrlb:equal(?vex1, ?vx2)  
? vlo:isTangentialProperPartOf(?ve2, ?ve1)  ?  vlo:hasForTangentialProperPart(?ve1, ?ve2) 

 


</field>
	</doc>
</add>