<?xml version="1.0" encoding="utf-8"?>
<add>
	<doc>
		<field name="docid">BR-TU.08379</field>
		<field name="filename">13215_TacianaKisakiOliveiraShimizu_revisada.pdf</field>
		<field name="filetype">PDF</field>
		<field name="text">
 

 

  

Taciana Kisaki Oliveira Shimizu 
Tese de Doutorado do Programa Interinstitucional de Pós-Graduação 
em Estatística (PIPGES) 

Penalized regression methods for compositional data 



SERVIÇO DE PÓS-GRADUAÇÃO DO ICMC-USP

Data de Depósito:

Assinatura: ______________________

Taciana Kisaki Oliveira Shimizu

Penalized regression methods for compositional data

Doctoral dissertation submitted to the Instituto de

Ciências Matemáticas e de Computação – ICMC-

USP and to the Departamento de Estatística – DEs-

UFSCar, in partial fulfillment of the requirements for

the degree of the Doctorate joint Graduate Program in

Statistics DEs-UFSCar/ICMC-USP. FINAL VERSION

Concentration Area: Statistics

Advisor: Prof. Dr. Francisco Louzada Neto

USP – São Carlos

February 2019



Ficha catalográfica elaborada pela Biblioteca Prof. Achille Bassi 
e Seção Técnica de Informática, ICMC/USP, 

com os dados inseridos pelo(a) autor(a)

                                       Bibliotecários responsáveis pela estrutura de catalogação da publicação de acordo com a AACR2: 
                                       Gláucia Maria Saia Cristianini - CRB - 8/4938 
                                       Juliana de Souza Moraes - CRB - 8/6176

S555p
Shimizu, Taciana Kisaki Oliveira
   Penalized regression methods for compositional
data / Taciana Kisaki Oliveira Shimizu; orientador
Francisco Louzada. -- São Carlos, 2019.
   95 p.

   Tese (Doutorado - Programa Interinstitucional de
Pós-graduação em Estatística) -- Instituto de Ciências
Matemáticas e de Computação, Universidade de São
Paulo, 2019.

   1. Compositional data. 2. Regression model. 3.
Isometric logratio coordinates. 4. Variable
selection. I. Louzada, Francisco, orient. II.
Título. 



Taciana Kisaki Oliveira Shimizu

Métodos de regressão penalizados para dados

composicionais

Tese apresentada ao Instituto de Ciências

Matemáticas e de Computação – ICMC-USP e

ao Departamento de Estatística – DEs-UFSCar,

como parte dos requisitos para obtenção do título

de Doutora em Estatística – Interinstitucional de

Pós-Graduação em Estatística. VERSÃO REVISADA

Área de Concentração: Estatística

Orientador: Prof. Dr. Francisco Louzada Neto

USP – São Carlos

Fevereiro de 2019





For my husband, Marcelo (Hiro), who has always been a constant source of support and
encouragement during the challenges of my life!





ACKNOWLEDGEMENTS

Firstly, I would like to thank God for the gift of my life, providing protection and wisdom
in all the moments of my life.

To my parents who devoted their love and care to my education. To my brothers who
always demonstrated love and fraternal fellowship.

To my husband Hiro, who has always supported me unconditionally in my personal and
professional projects with love and patience.

I am especially grateful to my supervisor Dr. Francisco Louzada for his patience,
comprehension, encouragement, professionalism and support in this study. I am also grateful to
Professor Adriano Kamimura Suzuki who always patient and willing to help in moments that I
needed.

I am thankful to my family and my husband’s family who helped us in crucial moments.

I express my sincere gratitude to Elizabeth Mie Hashimoto who always supported me
and encouraged me to go ahead.

I would also like to thank the Fundação de Amparo à Pesquisa do Estado de São Paulo
(FAPESP) - processo no 2014/16147-3, for the financial support, which enabled me to carry out
the project.





“O destino é uma questão de escolha.”
(Augusto Cury)





RESUMO

SHIMIZU, T. K. O. Métodos de regressão penalizados para dados composicionais. 2019.
95 p. Doctoral dissertation (Doctorate Candidate joint Graduate Program in Statistics DEs-
UFSCar/ICMC-USP) – Instituto de Ciências Matemáticas e de Computação, Universidade de
São Paulo, São Carlos – SP, 2019.

Dados composicionais consistem em vetores conhecidos como composições cujos componentes
são positivos e definidos no intervalo (0,1) representando proporções ou frações de um “todo”,
sendo que a soma desses componentes totalizam um. Tais dados estão presentes em diferentes
áreas, como na geologia, ecologia, economia, medicina entre outras. Desta forma, há um
grande interesse em ampliar os conhecimentos acerca da modelagem de dados composicionais,
principalmente quando há a influência de covariáveis nesse tipo de dado. Nesse contexto, a
presente tese tem por objetivo propor uma nova abordagem de modelos de regressão aplicada
em dados composicionais. A ideia central consiste no desenvolvimento de um método balizado
por regressão penalizada, em particular Lasso, do inglês least absolute shrinkage and selection
operator, elastic net e Spike-e-Slab Lasso (SSL) para a estimação dos parâmetros do modelo.
Em particular, visionamos o desenvolvimento dessa modelagem para dados composicionais,
com o número de variáveis explicativas excedendo o número de observações e na presença de
grandes bases de dados, e além disso, quando há restrição na variável resposta e nas covariáveis.

Palavras-chave: Dados composicionais, modelo de regressão, coordenadas log-razão isométri-
cas, seleção de variáveis.





ABSTRACT

SHIMIZU, T. K. O. Penalized regression methods for compositional data. 2019. 95 p. Doc-
toral dissertation (Doctorate Candidate joint Graduate Program in Statistics DEs-UFSCar/ICMC-
USP) – Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo, São
Carlos – SP, 2019.

Compositional data consist of known vectors such as compositions whose components are
positive and defined in the interval (0,1) representing proportions or fractions of a “whole”,
where the sum of these components must be equal to one. Compositional data is present in
different areas, such as in geology, ecology, economy, medicine, among many others. Thus,
there is great interest in new modeling approaches for compositional data, mainly when there
is an influence of covariates in this type of data. In this context, the main objective of this
thesis is to address the new approach of regression models applied in compositional data. The
main idea consists of developing a marked method by penalized regression, in particular the
Lasso (least absolute shrinkage and selection operator), elastic net and Spike-and-Slab Lasso
(SSL) for the estimation of parameters of the models. In particular, we envision developing
this modeling for compositional data, when the number of explanatory variables exceeds the
number of observations in the presence of large databases, and when there are constraints on the
dependent variables and covariates.

Keywords: Compositional data, regression model, isometric log-ratio coordinates, variable
selection.





LIST OF FIGURES

Figure 1 – Estimation picture for the Lasso (left) and ridge regression (right). . . . . . 34
Figure 2 – Spike and Slab distributions for l0 = 1,2,3 and l1 = 0.1. . . . . . . . . . . 36
Figure 3 – ICMS series disaggregated in three economic sectors. . . . . . . . . . . . . 42
Figure 4 – The solution path SSL (A, B, C), Lasso (D) and elastic net (E) for ilr(y1).

The colored points on the solution path represent the estimated values of the
coefficients. The vertical line (D) and (E) corresponds to the optimal model
Lasso and elastic net (cross-validation), respectively. . . . . . . . . . . . . . 45

Figure 5 – The solution path SSL (A, B, C), Lasso (D) and elastic net (E) for ilr(y2).
The colored points on the solution path represent the estimated values of the
coefficients. The vertical line (D) and (E) corresponds to the optimal model
Lasso and elastic net (cross-validation), respectively. . . . . . . . . . . . . . 46

Figure 6 – The parameter estimation averaged over 1000 replicates assuming r = 0.2
for the covariance matrix (n = 100, p = 1000). . . . . . . . . . . . . . . . . 51

Figure 7 – The parameter estimation averaged over 1000 replicates assuming r = 0.5
for the covariance matrix (n = 100, p = 1000). . . . . . . . . . . . . . . . . 52

Figure 8 – The SSL solution paths (A, B, C). . . . . . . . . . . . . . . . . . . . . . . . 53
Figure 9 – The Lasso solution path (D) and elastic net path (E). . . . . . . . . . . . . . 58
Figure 10 – The solution path SSL (A, B, C), lasso (D) and elastic net (E) for healthy

patients. The colored points on the solution path represent the estimated
values of the coefficients. The vertical line (D) and (E) corresponds to the
optimal model lasso and elastic net (cross-validation), respectively. . . . . . 59

Figure 11 – The solution path SSL (A, B, C), lasso (D) and elastic net (E) for patients with
pathologies. The colored points on the solution path represent the estimated
values of the coefficients. The vertical line (D) and (E) corresponds to the
optimal model lasso and elastic net (cross-validation), respectively. . . . . . 60

Figure 12 – The SSL solution paths (A, B, C) (for ilr(y1)). . . . . . . . . . . . . . . . . 68
Figure 13 – The lasso solution path (D) and elastic net path (E) (for ilr(y1)). . . . . . . . 69
Figure 14 – The SSL solution paths (A, B, C) (for ilr(y2)). . . . . . . . . . . . . . . . . 70
Figure 15 – The lasso solution path (D) and elastic net path (E) (for ilr(y2)). . . . . . . . 71
Figure 16 – The solution path SSL (A, B, C), lasso (D) and elastic net (E) for ilr(y1).

The colored points on the solution path represent the estimated values of the
coefficients. The vertical line (D) and (E) corresponds to the optimal model
lasso and elastic net (cross-validation), respectively. . . . . . . . . . . . . . 72



Figure 17 – The solution path SSL (A, B, C), lasso (D) and elastic net (E) for ilr(y2).
The colored points on the solution path represent the estimated values of the
coefficients. The vertical line (D) and (E) corresponds to the optimal model
lasso and elastic net (cross-validation), respectively. . . . . . . . . . . . . . 73



LIST OF TABLES

Table 1 – Example: Volleyball game score. . . . . . . . . . . . . . . . . . . . . . . . . 23
Table 2 – Elementary logistic transformations of SD for RD?1. . . . . . . . . . . . . . 25
Table 3 – Averages of some performance measures for penalized methods with compo-

sitional response variable (ilr(y1)). . . . . . . . . . . . . . . . . . . . . . . . 40
Table 4 – Averages of some performance measures for penalized methods with compo-

sitional response variable (ilr(y2)). . . . . . . . . . . . . . . . . . . . . . . . 41
Table 5 – Averages of some performance measures for penalized methods with compo-

sitional covariates. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
Table 6 – Averages of some performance measures for penalized methods with compo-

sitional dependent variables and covariates (ilr(y1)). . . . . . . . . . . . . . . 63
Table 7 – Averages of some performance measures for penalized methods with compo-

sitional dependent variables and covariates (ilr(y2)). . . . . . . . . . . . . . . 64





LIST OF ABBREVIATIONS AND ACRONYMS

alr additive logratio

BMI body mass index

clr centered logratio

EM Expectation-Maximization

FN false negative

FP false positive

HAM Hamming

ilr isometric logratio

LARS Least Angle Regression

MCMC Markov Chain Monte Carlo

MSE Mean Square Error

OLS Ordinary Least Squares

PA phase angle

SCAD Smoothly Clipped Absolute Deviations

SSL Spike-and-Slab Lasso

WHO World Health Organization





CONTENTS

1 INTRODUCTION . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23

2 PRELIMINARIES . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
2.1 Basic Concepts for compositional data . . . . . . . . . . . . . . . . . 27
2.1.1 Principles of compositional analysis . . . . . . . . . . . . . . . . . . . 28
2.1.1.1 Scale invariance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
2.1.1.2 Permutation invariance . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
2.1.1.3 Subcompositional coherence . . . . . . . . . . . . . . . . . . . . . . . . . . 28
2.1.2 The Aitchison geometry . . . . . . . . . . . . . . . . . . . . . . . . . . 28
2.1.3 Logratio coordinates . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
2.2 Shrinkage Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
2.2.1 Lasso . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
2.2.1.1 Orthonormal design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
2.2.1.2 K-fold Cross-validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
2.2.1.3 Coordinate descent algorithm . . . . . . . . . . . . . . . . . . . . . . . . . 35
2.2.2 Elastic net . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
2.2.3 Spike-and-Slab Lasso . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35

3 PENALIZED REGRESSION MODEL FOR COMPOSITIONAL RE-
SPONSE VARIABLES . . . . . . . . . . . . . . . . . . . . . . . . . . 37

3.1 Simulation Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
3.2 Real data application - ICMS dataset . . . . . . . . . . . . . . . . . . 40
3.3 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43

4 PENALIZED REGRESSION MODEL FOR COMPOSITIONAL CO-
VARIATES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47

4.1 Simulation Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
4.2 Artificial Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
4.3 Real data application - Brazilian children malnutrition dataset . . . 54
4.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57

5 PENALIZED REGRESSION MODEL FOR COMPOSITIONAL RE-
SPONSE VARIABLES AND COVARIATES . . . . . . . . . . . . . . 61

5.1 Simulation Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62



5.2 Toy example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
5.3 Real data application - ICMS dataset . . . . . . . . . . . . . . . . . . 65
5.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66

6 CONCLUSION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75

BIBLIOGRAPHY . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77

APPENDIX A APPENDIX 1 . . . . . . . . . . . . . . . . . . . . . . . 81



23

CHAPTER

1

INTRODUCTION

Appropriate study of the compositional data theory has been developed since the 1970s
from the contributions of Aitchison and Shen (1980) and Aitchison (1982a). Since then, its
applications have grown in different areas of knowledge, some examples include mineral com-
positions of rocks or sediment compositions such as (sand, silt, clay) compositions in geology,
species compositions of biological communities in ecology, household budget compositions in
economy, blood and urine compositions in medicine.

Compositional data are vectors of proportions that specify D fractions as a whole.
Therefore, for z = (z1,z2,...,zD)&gt; to be a compositional vector, zi &gt; 0 for i = 1,...,D and
z1 + z2 + ...+ zD = 1.

In order to exemplify, we can describe compositional data as follows in the Table 1. The
vector z in the simplex sample space as a composition (rows of the Table 1 - % attack, % block,
% serve and % opponent’s error of each match), the elements of such vector as components
(columns of the Table 1) and the set of these vectors represent compositional data (Table 1)
(AITCHISON, 1982b). Such data often result from the normalization of raw data or obtaining

Table 1 – Example: Volleyball game score.

Match % attack % block % serve % opponent’s error
1 48.00 12.00 2.67 37.33
2 53.06 14.29 7.14 25.51
3 44.00 13.33 8.00 34.67
4 52.63 14.74 7.37 25.26
5 56.00 8.00 5.33 30.67
6 65.63 10.16 2.34 21.88
...

...
...

...
...

data as proportions of a certain heterogeneous quantity. Standard methods for multivariate
data analysis under the usual assumption of multivariate normal distribution (see, for example,



24 Chapter 1. Introduction

(JOHNSON; WICHERN, 1998)) are not appropriate for compositional data, due to compositional
restrictions.

Different models have been adopted for the analysis of compositional data analysis. The
first was the Dirichlet distribution; however, it requires the correlation structure to be wholly
negative, a fact that is not observed for compositional data, in which some correlations are
positive (see, for example, (AITCHISON, 1982a; AITCHISON, 1986)).

An alternative for the analysis of compositional data was proposed by Aitchison Aitchison
(1986), who considered suitable transformations from restricted sample space Simplex to well-
defined real sample space. More specifically, Aitchison and Shen Aitchison and Shen (1980)
developed the logistic-Normal class of distributions transforming the D component vector x into
a vector y in RD?1 and considering the additive logratio (alr) function.

The alr and centered logratio (clr) transformations were introduced by Aitchison (1986)
in order to solve the constant sum constraint. These transformations are coordinates with respect
to the Aitchison geometry (PAWLOWSKY-GLAHN; EGOZCUE; TOLOSANA-DELGADO,
2015). However, a remarkable disadvantage about the clr transformation is that the variance
matrix of its transformed composition is unique. Furthermore, the alr coordinates are non-
isometric and asymmetric and the clr coordinates are isometric and symmetric (CHEN; ZHANG;
LI, 2017). Another transformation for compositional data is proposed by Egozcue et al. (2003)
called isometric logratio (ilr), which is calculated with respect to a given orthogonal basis,
allowing a simple manipulation of the geometric elements in the simplex sample space. Such a
transformation preserves all metric properties in the real coordinates. According to the Hron,
Filzmoser and Thompson (2012), the ilr transformation provides a way to obtain an interpretation
of the unknown parameters for regression model without constraints on the parameters.

Table 2 presents other elementary transformations: multiplicative logistic and hybrid
logistic, besides the alr mentioned above.

More recently, some contributions about the theory and applications of compositional
data have been developed, for example, in Pawlowsky-Glahn and Buccianti (2011), Boogaart
and Tolosana-Delgado (2013), Pawlowsky-Glahn, Egozcue and Tolosana-Delgado (2015). Hijazi
and Jernigan (2009) made a comparison between the Dirichlet regression model and the alr
transformation to verify which one is better in the presence of the covariate.

Aitchison and Egozcue (2005) reported a bibliography review about statistical model-
ing for compositional data in the last twenty years, where one of the tantalizing problems in
compositional data was how to deal with the presence of components equal to zero. One of the
few articles which addressed this situation was proposed by Martín-Fernandez, Barceló-Vidal
and Pawlowsky-Glahn (2003) who considered non-parametric imputation. Hijazi (2011) pro-
posed a novel technique based on the Expectation-Maximization (EM) algorithm to replace the
components containing zeros.



25

Table 2 – Elementary logistic transformations of SD for RD?1.

Transformations Inverses

alr yi = ln
?

zi
zD

?

multiplicative logistic yi = ln

0

BBB
@

zi

1?
i

Â
k=1

zk

1

CCC
A

hybrid logistic
y1 = ln

?
z1

1?z1

?
;

yi = ln

2

666
4

zi0

@1?
i?1
Â
k=1

zk

1

A

0

@1?
i

Â
k=1

zk

1

A

3

777
5

, i = 2,...,D?1

On the other hand, the increase in large datasets whose dimensionality is much larger
than the sample size establishes new challenges for current methodology of compositional data.
The fact that there is a situation of a low relation between the number of dependent variables and
the sample size makes the standard analysis unsuitable for a regression model with compositional
data. According to this situation, the high collinearity among the covariates can also be seen,
which is restricted by the dependence on each other. Whenever one of the above situations is
considered, a problem of poor conditioning is observed, and a contraction can be considered in
order to overcome this problem.

The advance of regularization techniques for variable selection and estimation in linear
regression have received much attention from many authors to handle high-dimensional datasets
and colinearity between the covariates of the model. Among the most popular penalization
approaches are the Lasso (TIBSHIRANI, 1996), the elastic net (ZOU; HASTIE, 2005), the
Smoothly Clipped Absolute Deviations (SCAD) (FAN; LI, 2001), among others and more
recently, the SSL (ROCKOVÁ; GEORGE, 2018).

The l1 regularization or Lasso, and its extensions, has become a popular method because
it achieves a sparse solution (TIBSHIRANI, 1996). This method shrinks many coefficients
exactly to zero through the fast optimization algorithms called Least Angle Regression (LARS)
and the cyclic coordinate descent proposed by Efron et al. (2004) and Friedman, Hastie and
Tibshirani (2010), respectively. Recently, Lin et al. (2014) have proposed an l1 regularization
method for variable selection and estimation in high-dimensional linear models with constraints
in the covariates, combining coordinate descent with the method of multipliers. Similar to Lasso,
Zou and Hastie (2005) proposed a new regularization and variable selection method that deal
with strong correlations among the covariates. The best advantage of the elastic net is that
it incorporates the ridge penalty with the Lasso penalty. This combination performs feature



26 Chapter 1. Introduction

selection and works with multicollinearity in the dataset together, which reveals important
attributes for the analysis where the number of observations is smaller than the number of
covariates in the model. The combination of ridge and Lasso performs feature selection and
handles multicollinearity within the dataset, which are important characteristics for analyzing
datasets with large numbers of features (many of which could be collinear) and relatively smaller
number of observations.

On the other hand, a Bayesian alternative is to adapt the amount of shrinkage applied to
the hierarchical model with mixture Spike-and-Slab priors (GEORGE; MCCULLOCH, 1993).
In this context, the Spike-and-Slab prior has been an important tool for most Bayesian variable
selection (CHIPMAN, 1996; ROCKOVÁ; GEORGE, 2014). Some studies have applied Spike-
and-Slab variable selection approaches using the mixture normal priors on coefficients by
Markov Chain Monte Carlo (MCMC) algorithms (see for example, Ishwaran and Rao (2005);
Shelton et al. (2015); among others). Although widely practical, the MCMC methods have a high
computational cost. Moreover, such methods cannot perform a variable selection, and the mixture
of normal priors does not shrink coefficients towards zero. An EM algorithm was developed
by Rocková and George (2014) to apply in large-scale linear models with the mixture normal
priors. Recently, Rocková and George (2018) developed a new structure for high-dimensional
normal linear models; the so-called SSL. Under this model, a new prior was applied to the
coefficients, that is, the Spike-and-Slab mixture double-exponential distribution. The SSL is a
fast-computable approximation to mode detection under the Spike-and-Slab mixture of a point
mass at 0 and ensures significant theoretical and practical properties. The SSL method applied to
Cox models and generalized linear models have received some attention in the literature, as can
be seen in Tang et al. (2017b) and Tang et al. (2017a), respectively. However, to the best of our
knowledge, the shrinkage methodology for compositional data needs to be developed further.

In this context, the main objective of this thesis is to introduce a proposal for regression
models based on regularization methods such as Lasso, elastic net and SSL, where responses
and/or covariates have a compositional character. It is worth pointing out that the study with
using these data is challenging due to the dependence and absence of parametric classes in the
simplex sample space.

The remainder of this work is organized as follows. Chapter 2 introduces the prelimi-
naries of some important topics of compositional data and the shrinkage methods adopted for
the analysis. Chapter 3 presents the penalized regression model for compositional dependent
variables under the application of Lasso, elastic net and SSL methods for penalization. Chapter 4
presents a penalized regression model when the restriction of compositional data exists in the
covariates. Chapter 5 presents the case of compositional constraints on both dependent variables
and covariates where the regularization methods applied are Lasso, elastic net and SSL. Finally,
Chapter 6 draws some general conclusions and possible extensions of this current work.



27

CHAPTER

2

PRELIMINARIES

In this Chapter, we present a literature review of some important topics and properties
about basic operations in the methodology of compositional data, logratio coordinates and some
regularization methods which were applied in the proposed models.

2.1 Basic Concepts for compositional data
Initially, we start by defining compositional data. According to Pawlowsky-Glahn,

Egozcue and Tolosana-Delgado (2015), a column vector zzz = (z1,z2,...,zD)&gt; is a D?part com-
position when all the components are positive real numbers and carry only relative information.

An important operation called closure assigns a constant sum representative to a compo-
sition. It divides each component of a vector by the sum of the components, rescaling the initial
vector to the constant sum k. In mathematical terms, the definition is given by

Definition 1 (Closure). For any vector of D strictly positive real components, zzz = [z1,...,zD] 2
RD+, zi &gt; 0 for all i = 1,...,D, the closure of zzz to c &gt; 0 is defined as

C(zzz) =
?

cz1
ÂDi=1 zi

,...,
czD

ÂDi=1 zi

?
,

where c is an arbitrary positive real number and is usually 1 (proportions) or 100 (%) depending
on the units of measurement.

An appropriate scaling factor can be used to represent compositional data as proportions.
Consequently, we can assume compositional data as proportions, that is, as vectors of constant
sum c.

The sample space of compositional data called simplex is denoted by

SD = {zzz = (z1,z2,...,zD)&gt; : zi &gt; 0,i = 1,2,...,D;
D

Â
i=1

zi = c},



28 Chapter 2. Preliminaries

More specifically, we can define a vector zzz in the simplex sample space as a composition,
the elements of such vector as components and the set of these vectors represent compositional
data (AITCHISON, 1982a).

2.1.1 Principles of compositional analysis
The definition of compositional data follows the natural principles of compositions and

they are called: scale invariance, permutation invariance and subcompositional coherence.

2.1.1.1 Scale invariance

Scale invariance refers to when a composition has information only about relative
values. According to Aitchison and Egozcue (2005), the concept is easily formalized into a
statement that all meaningful functions of a composition can be expressed in terms of a set of
component ratios. In other words, if a composition changes from parts per unit to percentages, for
example, the information carried is completely equivalent (PAWLOWSKY-GLAHN; EGOZCUE;
TOLOSANA-DELGADO, 2015).

Definition 2 (Scale invariance). Let f (.) be a function defined on RD+. Such a function is scale
invariant if for any positive real value n 2 R+ and for any composition zzz 2 SD it satisfies
f (n zzz) = f (zzz), that is, it yields the same result for all compositionally equivalent vectors.

2.1.1.2 Permutation invariance

The concept of permutation invariance is that it provides the same results when the
components in the composition are changed (see Pawlowsky-Glahn, Egozcue and Tolosana-
Delgado (2015) for a detailed discussion).

2.1.1.3 Subcompositional coherence

Finally, a definition for subcomposition is given by a subset of components or parts of
a composition. Thus, the subcompositional coherence can be summarized as: if we have two
compositions, in which one has full compositions and the other one a subcomposition of these
full compositions, the inference about the relations within the common parts should be the same
results, i.e., the scale invariance of the results is preserved within arbitrary subcompositions, that
is, the ratios between any parts in the subcomposition are equal to the corresponding ratios in the
original composition.

2.1.2 The Aitchison geometry
In Euclidian geometry, we work with operations in vectors in real space. This geometry

is familiar with its geometric structure because the real space is a linear vector space with a
metric structure. However, this geometry is not suitable for analyzing compositional data. A way



2.1. Basic Concepts for compositional data 29

to illustrate this statement is to consider four compositions [5,55,40], [15,45,40], [40,30,30],
[50,20,30]. The difference between [5,55,40] and [15,45,40] is not the same as the difference
between [40,30,30] and [50,20,30]. The Euclidean distance between them is the same, there is
a difference of 10 units both between the first and second components respectively. While in
the first case, the proportion in the first component is triplicated, in the second case, the relative
increase is about 25%. To describe compositional variability, it is more interesting to consider
this relative difference.

This is one of the reasons for dispensing the Euclidian geometry as an appropriate tool for
analyzing compositional data. Other problems might occur, such as those where outcomes finish
up outside the sample space simplex, or when translating compositional vectors, or determining
joint confidence regions for random compositions under assumptions of normality.

A wise geometry is needed to deal with compositional data. Indeed, it is possible to
obtain two operations that provide the simplex of a vector space structure. They are defined as
perturbation and powering. The first one is like an addition in real space and the second one is
like a multiplication by a scalar in real space. These basic operations required for a vector space
structure of the simplex are defined below.

Definition 3 (Perturbation). Consider the compositions zzz,yyy 2 SD. The perturbation of zzz with yyy
is given by

zzz?yyy = C[z1y1,z2y2,...,zDyD] 2 SD.

where C[.] is defined in Definition 1.

Definition 4 (Powering). Consider the compositions zzz,yyy 2 SD. The powering of zzz by a constant
a 2 R as the composition is given by

a ?zzz = C[za1 ,z
a
2 ,...,z

a
D] 2 S

D.

To obtain a Euclidian vector space structure (PAWLOWSKY-GLAHN; EGOZCUE,
2001), we take the following inner product with its related norm ||.||a and Aitchison distance
(the subindex a stands for Aitchison).

Definition 5 (Aitchison inner product). Inner product of zzz,yyy 2 SD,

hzzz,yyyia =
D

Â
i=1

ln
?

zi
gm(zzz)

??
ln

yi
gm(yyy)

?
,

where gm(zzz) denotes the geometric mean of the components of zzz.

Definition 6 (Aitchison norm).
||zzz||a =

p
hzzz,zzzia.



30 Chapter 2. Preliminaries

Definition 7 (Aitchison distance).

da(zzz,yyy) = ||zzz yyy||a,

where zzz yyy is equal to the perturbation zzz?((?1)?yyy).

2.1.3 Logratio coordinates
Aitchison (1986) proposed transformations based on ratios, including alr transformation

and clr transformation. By the Aitchison’s approach, it is possible to give an algebraic-geometric
foundation and based on this framework, a transformation of coefficients is equivalent to ex-
press observations in a different coordinate system (PAWLOWSKY-GLAHN; EGOZCUE;
TOLOSANA-DELGADO, 2015). The principal logratio coordinates (alr, clr and ilr) are defined
below. The alr transformation is defined as follows.

Definition 8 (Additive logratio coordinates). Let zzz = [z1,z2,...,zD] be a composition in SD and
consider zD as a reference part. Its alr transformation into RD?1 is

alr(zzz) =
?

ln
z1
zD

,ln
z2
zD

,...,ln
zD?1
zD

?
= zzz .

To recover zzz from zzz = [z1,z2,...,zD?1], the inverse alr transformation is given through by
closure definition

zzz = alr?1(zzz ) = C[exp(z1),exp(z2),...,exp(zD?1),1].

As the reference part zD is in the denominator of the components logratio, the alr
transformation is not symmetric in the components. Another option of the reference part can be
chosen, conducting it to different alr-transformations. On the other hand, alr coordinates cannot
compute the Aitchison inner products or distances in the standard Euclidean way, that is, the alr
does not supply an isometry between SD and RD?1. Each part of the composition except for the
part in the denominator of the alr is

zi =
exp(zi)

1 + ÂD?1j=1 exp(zi)
,

where the denominator is the effect of the closure. Its term additive comes from the denominator,
which is the sum of the exponentials.

The clr coordinates give the expression of a composition in centered logratio coefficients

clr(zzz) =
?

ln
z1

gm(zzz)
,ln

z2
gm(zzz)

,...,ln
zD

gm(zzz)

?
= xxx .

The clr transformation is symmetric in the components, but the sum of the components
is zero. In addition, the covariance matrix of clr(zzz) is singular, that is, the determinant is zero.



2.1. Basic Concepts for compositional data 31

Moreover, the clr coefficients are not subcompositionally coherent, because the geometric mean
of the parts of a subcomposition gm(zzz) is not necessarily equal to that of the full composition. A
formal definition of the clr coefficients is given as follows.

Definition 9 (Centered logratio coefficients). For a composition zzz 2 SD, the clr coefficients are
the components of the unique vector xxx = [x1,x2,...,xD] = clr(zzz), satisfying the two conditions

zzz = clr?1(xxx ) = C(exp(xxx )) and
D

Â
i=1

xxx i = 0.

The ith clr coefficient is
xi = ln

zi
gm(zzz)

.

The more recent logratio coordinates are ilr coordinates. The main idea of ilr coordinates
is to obtain an orthonormal basis on the simplex, and to apply the new coordinates in a linear
regression model. There are many ways to construct such a basis (CHEN; ZHANG; LI, 2017).
Specifically, an example of a basis for compositional data is called sequential binary partitioning
(HRON; FILZMOSER; THOMPSON, 2012). We obtain coordinates which are interpreted in
terms of the included compositional parts. For a given matrix

WWW D?(D?1) = (www1,www2,...,wwwD?1) =

0

BBBBBBBBBB
@

q
D?1

D 0 ... 0

? 1p
D(D?1)

q
D?2
D?1 ... 0

...
...

. . .
...

? 1p
D(D?1)

? 1p
(D?1)(D?2)

... 1p
2

? 1p
D(D?1)

? 1p
(D?1)(D?2)

... ? 1p
2

1

CCCCCCCCCC
A

, (2.1)

and
eeei = C(exp wwwi), i = 1,...,D?1,

is the corresponding orthonormal basis eee1,...,eeeD?1 and then the transformation of the compo-
sition zzz = (z1,...,zD)&gt; 2 SD to the ilr coordinates ilr(zzz) = (ilr(zzz)1,...,ilr(zzz)D?1)&gt; 2 RD?1 is
obtained by

vi = ilr(zzz)i =
r

D?i
D?i + 1

ln

0

@ zi
D?i
q

’Dj=i+1 z j

1

A, i = 1,...,D?1. (2.2)

The inverse ilr transformation of (2.2) is given by

z1 = exp

(r
D?1

D
v1

)
,

zi = exp

(
?

i?1
Â
j=1

1
p

(D? j + 1)(D? j)
v j +

r
D?i

D?i + 1
vi

)
, i = 2,...,D?1, (2.3)

zD = exp

(
?

D?1
Â
j=1

1
p

(D? j + 1)(D? j)
v j

)
.



32 Chapter 2. Preliminaries

Considering the scale invariance property, the composition zzz (2.3) can be represented
by vectors with a chosen constant sum constraint. An important relationship between ilr and clr
coordinates of composition zzz (CHEN; ZHANG; LI, 2017) is defined by

ilr(zzz) = WWW&gt;D clr(zzz) = WWW
&gt;
D log(zzz),

where WWW D is defined in (2.1). Specifically, the first coordinates of ilr(zzz) and clr(zzz) present a
linear relation as

ilr(zzz)1 =
r

D
D?1

clr(zzz)1 =
1

D(D?1)

?
ln
?

z1
z2

?
+ ...+ ln

?
z1
zD

??
.

The coordinate ilr(z)1 extracts all relative information regarding z1 and obtains the
relative contribution of z1 respecting all the other parts (HRON; FILZMOSER; THOMPSON,
2012). Some properties of the ilr coordinates are expressed below to explain their potential
application and computation. Let the function ilr: SD ! RD?1 an isometry of vector spaces and
the asterisk (?) denotes coordinates in an orthonormal basis.

Property 1. Consider zzzh 2 SD, h = 1,2 and real constants a,g ,

(a) ilr(a ?zzz1 ?g ?zzz2) = a.ilr(zzz1)+ g.ilr(zzz2) = a.zzz?1 + g.zzz
?
2;

(b) hzzz1,zzz2ia = hilr(zzz1),ilr(zzz2)i = hzzz?1,zzz
?
2i;

(c) ||zzz1||a = ||ilr(zzz1)|| = ||zzz?1||;

(d) da(zzz1,zzz2) = d(ilr(zzz1),ilr(zzz2)) = d(zzz?1,zzz
?
2).

In this thesis, we applied the ilr coordinates to the compositional data in order to avoid
numerical problems in the context of linear models. For clr and ilr coordinates, the scalar product
is preserved and they are isometric, but this fact is different with alr coordinates, that is,

hzzz,yyyi = clr(zzz).clr&gt;(yyy) = ilr(zzz).ilr&gt;(yyy) 6= alr(zzz).alr&gt;(yyy).

A disadvantage of using the alr coordinates is that they should not be applied when there are
distances, angles and shapes involved. In addition, the clr coordinates provide singular covariance
matrices, a problem for estimation in linear models (BOOGAART; TOLOSANA-DELGADO,
2013).

2.2 Shrinkage Methods
The current section considers the three penalization methods used to develop the proposed

models for compositional data. This involves the Lasso, elastic net and SSL.



2.2. Shrinkage Methods 33

First, we consider the generic and classical linear regression model

yyy = XXX bbb + eee, (2.4)

where yyy 2 Rn is a vector of responses, XXX 2 Rn?p is a regression matrix of p predictors, bbb =
(b1,...,bp)&gt; 2 Rp is a vector of unknown regression coefficients and eee 2 Rn is the independent
noise vector distributed as Nn(0,s 2IIIn) being IIIn an identity matrix with dimension n. To solve the
estimation problem, the ordinary least squares (OLS) method is often used where the parameters
are estimated by the minimization of the residual sum of squares ||yyy?XXX bbb ||22. This method can
be applied under specific conditions, that is, XXX&gt;XXX is nonsingular and consequently we obtain
b?bb = (XXX&gt;XXX)?1XXX&gt;yyy. However, problems with high-dimensional regression are common in a wide
range of applications, that is, when we have a large number of covariates p to a response of
interest, which exceeds the number of observations n, p &gt; n or even p &gt;&gt; n.

2.2.1 Lasso
Tibshirani (1996) proposed Lasso (least absolute shrinkage and selection operator)

method or l1 regularization which has become very popular for high-dimensional estimation
problems taking into account its statistical accuracy for prediction and variable selection jointly
with its computational feasibility. Furthermore, its theoretical properties in high-dimensional
regression are well-understood (LIN et al., 2014).

Lasso is known as a penalized likelihood approach that develops the methodology for
l1-penalization in high-dimensional settings with desirable properties for p &gt;&gt; n problems. In
such problems, lasso demonstrated its superiority compared to other existing methods.

Assuming the regression model (2.4), the convex optimization problem with the applica-
tion the Lasso is defined as

b?bb = argmin
b

?
1

2n
||yyy?XXX bbb ||22 + l ||bbb ||1

?
, (2.5)

where l is a tuning parameter dealing with the amount of shrinkage, and ||.||2 and ||.||1 are the
l2 and l1 norms, respectively. Not only does the l1 penalty shrink the coefficients toward zero, but
it also has some advantages in relation to the classical Ordinary Least Squares (OLS) methods
such as some criteria for model selection, resulting in convexity of the optimization problem and
solving large problems efficiently (HASTIE; TIBSHIRANI; WAINWRIGHT, 2015).

When the lasso estimates regression coefficients to zero, it is creating a sparse solution,
that is, only few of the regression coefficients are nonzero. This performance is important due to
the variable selection that determines relevant covariates showing the strongest effects. The Lasso
results in sparsity and ridge penalty is not sparse are presented in Figure ?? in a geometrical
picture when there are only two parameters, which is given by the constraint interpretation of
their penalties. We can observe that the Lasso estimate can be set to zero.



34 Chapter 2. Preliminaries

Figure 1 – Estimation picture for the Lasso (left) and ridge regression (right).

Source: (HASTIE; TIBSHIRANI; FRIEDMAN, 2009).

2.2.1.1 Orthonormal design

Following Buhlmann and Geer (2011), the lasso estimator can be derived for an orthonor-
mal design case. Assuming uncorrelated variables implies that XXX&gt;i XXX j = 0 for each i 6= j and
1
n XXX

&gt;XXX = III p. Thus, the lasso estimator is given by

b?bb = S(b?bb
LSE

; l ), (2.6)

where S(.; l ) is the soft-thresholding operator

S(t; l ) = sgn(t)(|t|?l )+ =

8
&gt;&gt;&gt;&lt;

&gt;&gt;&gt;:

t ?l , if t &gt; l ,

0, if |t| ? l ,

t + l , if t &amp;lt;?l ,

where sgn denotes the sign of its argument (±1), (t)+ = max(t,0) denotes the positive part and
b?bb

LSE
is the OLS estimator for bbb .

2.2.1.2 K-fold Cross-validation

The k-fold cross-validation scheme is commonly used to select a reasonable tuning
parameter l for the Lasso estimator. First, we randomly divide the observations into k groups.
One group is fixed as the test set, and the k?1 groups are designated as a training set. The model
is fitted to the training data for a range of values of l in a grid, and we predict the responses in
the test set based on each fitted model, saving the mean-squared prediction errors. We repeat
this process k times, where the k groups have a chance to be test data, in relation to k ?1 groups
used as a training set. Thus, we capture k different estimates of the prediction error over a range
of values of l (HASTIE; TIBSHIRANI; WAINWRIGHT, 2015). The choice of k is usually 5 or
10, where k = 10 is very common in the field of applied machine learning.



2.2. Shrinkage Methods 35

2.2.1.3 Coordinate descent algorithm

Based on the estimator (2.5), there is no closed form expression for the estimates for
the lasso. Indeed, the optimization problem become a convex problem with inequality con-
straints (FRIEDMAN et al., 2007). Since the seminal work of Tibshirani (1996), computational
developments have been approached to obtain efficiency and solutions to solve the lasso problem.

The LARS algorithm was proposed by Efron et al. (2004), which is a useful and less
greedy version of traditional forward selection methods.

On the other hand, another fast and popular approach used for estimation in regularization
methods is the coordinate descent algorithm (FU, 1998), which has shown to be a strong
competitor to the LARS algorithm (FRIEDMAN et al., 2007). The idea of the algorithm is to
fix the penalty parameter l in the Lagrangian form (2.5) and optimize successively over each
parameter, keeping the other parameters fixed at their actual values. For more details, see for
example Hastie, Tibshirani and Friedman (2009).

2.2.2 Elastic net
The elastic net approach was proposed by Zou and Hastie (2005). According to the

authors, the method is similar to the lasso, in view of being a variable selection and continuous
shrinkage. Therefore, this method selects groups of correlate variables.

Thus, the elastic net combines the ridge (HOERL; KENNARD, 1970) and lasso penalties
to solve the following convex problem

b?bb = argmin
b

?
1
n
||yyy?XXX bbb ||22 + l

?
1
2
(1?a)||bbb ||2 + a||bbb ||1

??
,

where a 2 [0,1] is an elastic net tuning parameter that controls the mixing between the l1 and
l2 penalties. There are many alternatives of algorithms to solve the elastic net problem. Within
them, the coordinate descent is efficient due to the fact that the updates will be a simple extension
of lasso (HASTIE; TIBSHIRANI; WAINWRIGHT, 2015).

2.2.3 Spike-and-Slab Lasso
Under a Bayesian perspective, Rocková and George (2018) proposed the SSL for high-

dimensional normal linear models, and showed that it has important properties.

The general form of the penalized likelihood approach estimates bbb is given by

b?bb = arg max
b2RD

?
?

1
2
||yyy?XXX bbb ||22 + penl (bbb )

?
,

where penl (bbb ) is a penalty function that prioritizes suitable solutions.

The SSL involves placing a mixture prior on the regression coefficients bbb , where each
b j, j = 1,...,D is assumed a priori to be drawn from either a Laplacian “Spike” concentrated



36 Chapter 2. Preliminaries

around zero (and hence is considered negligible), or a diffuse Laplacian “Slab” (and hence may
be large). Thus, the hierarchical prior over bbb and the latent indicator variables ggg = (g1,...,gD) is
given by

p(bbb |ggg) =
D

’
j=1

[g jy1(bi)+(1?g j)y0(b j)], ggg ? p(ggg), (2.7)

p(ggg|q ) =
D

’
j=1

q g j (1?q )1?g j , and qqq ? Beta(a,b),

where y1(b j) = (l1/2)e?|b j|l1 is the Slab distribution, y0(b j) = (l0/2)e?|b j|l0 is the Spike
distribution (l1&amp;lt;&amp;lt;l0) and the beta-binomial prior has been used for the latent indicators. The
Figure 2 illustrates the spike and slab distribution for different values of l0. In this thesis, we

Figure 2 – Spike and Slab distributions for l0 = 1,2,3 and l1 = 0.1.

applied two types of SSL penalties studied in Rocková and George (2018): separable SSL and
non-separable SSL. The first one is the separable SSL penalty that arises from an independent
product prior (2.7), assuming q known, that is, it is fixed. Its definition is given below.

penS(bbb |q ) =
D

Â
j=1

r(b j|q ) = ?l1|b j|+
D

Â
j=1

log
?

p?q (0)
p?q (b j)

?
,

where
p?q (b j) =

q y1(b j)
q y1(b j)+(1?q )y0(b j)

and
p?q (0) =

q y1(0)
q y1(0)+(1?q )y0(0)

=
q l1

q (l1 ?l0)+ l0
.

Another one is the non-separable SSL penalty with unknown variance proposed by
Moran, Rocková and George (2018). This penalty treats the q as a random, avoiding the need
for cross-validation over q . Thus, the non-separable SSL penalty with q ? p(q ) is defined by

penNS(bbb ) = log
?

p(bbb )
p(000p)

?
= ?l1|bbb |+ log

2

4

R q p
’pj=1 p

?
q (b j)

dp(q )
R q p

’pj=1 p
?
q (0)

dp(q )

3

5.

All the penalized methods presented in this section were implemented in the software R
(R Core Team, 2017). Some examples of the routines for each penalized method used in this
work were in Appendix A.



37

CHAPTER

3

PENALIZED REGRESSION MODEL FOR
COMPOSITIONAL RESPONSE VARIABLES

In this Chapter, we present the penalized regression model with restrictions in the
response variable, that is, in the vector yyy.

The model into a multivariate regression problem with compositional response is defined
as

yyy = XXX bbb + eee, (3.1)

where yyy is a vector (D ? 1) of compositional response variables, XXX is a matrix (D ? p) of p
covariates, where D is the number of the components, bbb = (b1,...,bp)&gt; is a vector (p?1) of
unknown parameters and eee is the noise vector with distribution ND(000,ID), with a known variance
s2 = 1. The intercept of the model is not included, since the response and predictor variables
can be centered.

Based on the principle of working in coordinates, we can rewrite the model (3.1) as

yyy = XXX ?bbb + eee

ilr(yyy) = XXX bbb + ilr(eee), (3.2)

where eee ? N(000D?1,Silr).

Here, we assume the following estimators for bbb of the regression models with composi-
tional responses focused on regularization methods presented in Section 2. We considered the
Lasso, elastic net, SSL with separable and non-separable penalty approaches, respectively, for
the model (3.2) as follows.

1. Lasso:
b?bb = argmin

bbb

?
||ilr(yyy)?XXX bbb ||22/n + l ||bbb ||1

?
(3.3)

where ||ilr(yyy)?XXX bbb ||22 = Â
n
i=1(ilr(yyy)?XXX bbb )

2 and ||bbb ||1 = Â
p
j=1 |b j|.



38 Chapter 3. Penalized Regression Model for compositional response variables

2. Elastic net:

b?bb = argmin
bbb

?
1
n
||ilr(yyy)?XXX bbb ||22 + l

?
1
2
(1?a)||bbb ||22 + a||bbb ||1

??
. (3.4)

3. SSL with separable penalty (known variance):

b?bb = argmax
bbb2RD?1

(
?

1
2
||ilr(yyy)?XXX bbb ||22 +

"
?l1|bbb |+

p

Â
j=1

log
?

p?q (0)
p?q (b j)

?#)
. (3.5)

4. SSL with non-separable penalty (unknown variance):

b?bb = argmax
bbb2RD?1

8
&lt;

:
?

1
2
||ilr(yyy)?XXX bbb ||22 +

0

@?l1|bbb |+ log

2

4

R q p
’pj=1 p

?
q (b j)

dp(q )
R q p

’pj=1 p
?
q (0)

dp(q )

3

5

1

A

9
=

;
. (3.6)

For the estimation of the bbb ’s, we implemented the estimators (3.3) and (3.4) through by
R package glmnet (FRIEDMAN; HASTIE; TIBSHIRANI, 2010). The algorithm used to find
the minimum was cyclical coordinate descent. Such algorithm computes a grid of possible value
of l and a sequence of models related to the loss function is provided as output. One advantage
of this algorithm is that it can implemented for generalized linear model. The estimators (3.5)
and (3.6) were obtained through by R package SSLASSO (MORAN; ROCKOVÁ; GEORGE,
2018). The coordinate descent algorithm is used to fit the sequence of models indexed by the
regularization parameter l0.

3.1 Simulation Analysis
Here, we provided the simulation studies to investigate the efficacy of the penalized

methods for the regression model with compositional responses variables. We replicated the
simulation 1000 times and the results were summarized based on these replicates (Table 3 for
ilr(y1) and Table 4 for ilr(y2)). We generated a data matrix XXX from a normal distribution with
mean 0 and s = 2 for each element of matrix XXX . The compositional response variable is generated
according to model (3.1), from a logistic normal distribution with mean 000D and covariance matrix
S = (r|i? j|), with r = 0.2 and r = 0.5, for j = 1,...,D. We assume D = 3, that is, we have
3 components (y1,y2,y3) of a composition. The fixed values for the parameters bbb

?
= (b?1 ,b

?
2 )

were b?1 = (?2,?1.5,?1,0,1,1.5,2,0,...,0)
&gt; and b?2 = (2,?1,?2.5,0,1,?1,0.5,0,...,0)

&gt;

to q = 6 random directions (non-zero coefficients).

We assumed three scenarios with a different number of sample sizes and covariates:
(n, p) = (50,30),(100,200) and (100,1000). The adopted performance measures for our com-
parisons were the Mean Square Error (MSE) given by MSE(b?bb?) = Var(b?bb?)+(Bias(b?bb?)), where
Var(b?bb?) is the variance of the estimates of bbb? and Bias(b?bb?) = b?bb?? bbb?; the number of false
positive (FP), the number of false negative (FN), where positive and negative refer to nonzero



3.1. Simulation Analysis 39

and zero coefficients, respectively; and the Hamming (HAM) distance between the support of
the estimated bbb and the true bbb?, that is, suppose two vectors x = (1,0,0) and w = (0,1,0), the
HAM distance d(x,w) (number of different elements) between this two vectors, being that in
this case d(x,w) = 2 because the first and second elements of these vectors are different from
each other. In this way, lower values of HAM indicate better performance of the method. The
Tables 3 and 4 report the averages of these performance measures for the five penalized methods
adopted in this work.

According to the results in Tables 3 and 4, we can see that, in general, the SSL Separable
(SSL(1,6/p) with s = 1 fixed) performs better than other methods in all settings based on the
HAM distance (lower values), for p = 30,200 and 1000 covariates. Therefore, this method tends
to select fewer FP compared with other penalized methods. Among the approaches studied, the
elastic net has a worse performance in almost all settings.



40 Chapter 3. Penalized Regression Model for compositional response variables

Table 3 – Averages of some performance measures for penalized methods with compositional response
variable (ilr(y1)).

(n, p) Method MSE FP FN HAM
r =0.2

SSL (l1, q )
(50, 30) SSL (1, 0.8) with s =1 fixed 0.4845 0.1850 5.9550 6.2020

SSL (1, 6/30) with s =1 fixed 0.4834 0.0020 6.0000 6.0020
SSL (1, 6/30) with unknown s 0.4834 0.0030 5.0190 6.0030
Lasso 0.4854 4.1180 5.7020 10.3090
Elastic Net 0.4850 6.3010 5.4880 12.6020

(100, 200) SSL (1, 0.8) with s = 1 fixed 0.0733 1.9700 5.9540 7.9780
SSL (1, 6/200) with s = 1 fixed 0.0725 0.0020 6.0000 6.0020
SSL (1, 6/200) with unknown s 0.0769 31.9060 6.0000 38.0670
Lasso 0.0726 9.9130 5.7020 15.9760
Elastic Net 0.0726 16.4320 5.4880 22.5360

(100, 1000) SSL (1, 0.8) with s = 1 fixed 0.0149 4.7620 5.9750 10.6770
SSL (1, 6/1000) with s = 1 fixed 0.0145 0.0000 6.0000 6.0010
SSL (1, 6/1000) with unknown s 0.0153 0.0160 6.0000 6.0160
Lasso 0.0145 12.2770 5.9280 18.2910
Elastic Net 0.0145 23.3260 5.8590 32.9320

r =0.5
SSL (l1, q )

(50, 30) SSL (1, 0.8) with s = 1 fixed 0.4645 1.6550 5.5600 7.7260
SSL (1, 6/30) with s = 1 fixed 0.4523 0.1620 5.9530 6.1680
SSL (1, 6/30) with unknown s 0.4500 0.0140 5.9960 6.0140
Lasso 0.4849 3.8260 5.0460 9.9790
Elastic Net 0.4551 7.4290 4.1100 13.7340

(100, 200) SSL (1, 0.8) with s = 1 fixed 0.0742 4.0100 5.8640 10.0330
SSL (1, 6/200) with s = 1 fixed 0.0725 0.0090 6.0000 6.0090
SSL (1, 6/200) with unknown s 0.0725 0.0090 6.0000 6.0090
Lasso 0.0726 9.0780 5.7120 15.1410
Elastic Net 0.0726 15.1490 5.5680 21.2220

(100, 1000) SSL (1, 0.8) with s = 1 fixed 0.0152 7.2320 5.9640 13.2410
SSL (1, 6/1000) with s = 1 fixed 0.0145 0.0010 6.0000 6.0010
SSL (1, 6/1000) with unknown s 0.0145 0.0030 6.0000 6.0030
Lasso 0.0145 11.1680 5.9370 17.1780
Elastic Net 0.0145 21.5300 5.8740 27.5530

3.2 Real data application - ICMS dataset

The following dataset was made available by the Secretaria da Fazenda of Sao Paulo
State. The dataset consists of ICMS (Imposto sobre Circulação de Mercadorias e Prestação de
Serviços), which is the main revenue source for the Brazilian states.

The challenge with this dataset is the development of models which are disaggregated in
three economic sectors: industry (y1), commerce (y2) and administered prices (y3). These sectors



3.2. Real data application - ICMS dataset 41

Table 4 – Averages of some performance measures for penalized methods with compositional response
variable (ilr(y2)).

(n, p) Method MSE FP FN HAM
r =0.2

SSL (l1, q )
(50, 30) SSL (1, 0.8) with s = 1 fixed 0.4645 1.6550 5.5600 7.7260

SSL (1, 6/30) with s = 1 fixed 0.4523 0.1620 5.9530 6.1680
SSL (1, 6/30) with unknown s 0.4500 0.0140 5.9960 6.0140
Lasso 0.4547 5.5720 4.5270 11.8300
Elastic Net 0.4551 7.4290 4.1100 13.7340

(100, 200) SSL (1, 0.8) with s = 1 fixed 0.0749 11.9850 5.6220 18.0480
SSL (1, 6/200) with s = 1 fixed 0.0677 0.1440 5.9950 6.1450
SSL (1, 6/200) with unknown s 0.0723 8.3520 5.7350 14.3940
Lasso 0.0679 11.5980 5.6290 17.6650
Elastic Net 0.0678 19.5060 5.3710 25.6010

(100, 1000) SSL (1, 0.8) with s = 1 fixed 0.0150 12.3520 5.9200 18.3610
SSL (1, 6/1000) with s = 1 fixed 0.0135 0.0840 6.0000 6.0850
SSL (1, 6/1000) with unknown s 0.0135 0.0000 6.0000 6.0000
Lasso 0.0136 14.8520 5.9120 20.8660
Elastic Net 0.0135 26.9050 5.8120 32.9320

r =0.5
SSL (l1, q )

(50, 30) SSL (1, 0.8) with s = 1 fixed 0.4525 0.4250 5.8870 6.4460
SSL (1, 6/30) with s = 1 fixed 0.4502 0.0240 5.9960 6.0260
SSL (1, 6/30) with unknown s 0.4501 0.0060 5.9980 6.0060
Lasso 0.4530 3.8230 4.9620 9.9770
Elastic Net 0.4541 8.4840 3.7240 14.8560

(100, 200) SSL (1, 0.8) with s = 1 fixed 0.0694 4.1730 5.8720 10.1910
SSL (1, 6/200) with s = 1 fixed 0.0675 0.0040 5.9990 6.0040
SSL (1, 6/200) with unknown s 0.0675 0.0050 5.9990 6.0050
Lasso 0.0678 13.9800 5.5570 20.0600
Elastic Net 0.0677 23.4400 5.3080 29.5410

(100, 1000) SSL (1, 0.8) with s = 1 fixed 0.0142 7.1470 5.9520 13.1520
SSL (1, 6/1000) with s = 1 fixed 0.0135 0.0000 6.0000 6.0000
SSL (1, 6/1000) with unknown s 0.0135 0.0010 6.0000 6.0010
Lasso 0.0136 17.7410 5.8700 23.7620
Elastic Net 0.0135 33.1620 5.7680 39.2000

represent a linear combination, that is, they are defined as compositional data. The importance
of disaggregating the sectors is to allow the government to forecast potential decreases in tax
collection and plan efficient actions. The data were extracted from August 2007 to April 2018,
that is, the sample size is n = 128 months. The Figure 3 presents the evolution of the ICMS
series disaggregated in these three economic sectors: industry, commerce and administered price.



42 Chapter 3. Penalized Regression Model for compositional response variables

Composition ICMS given the 3 main sectors

Time

0 20 40 60 80 100 120

15
00

20
00

25
00

30
00

35
00

40
00

45
00

Industry
Commerce
ADM_Prices

Figure 3 – ICMS series disaggregated in three economic sectors.

The exogenous variables or covariates provided by research institutes are:

• log of Monthly Industrial Survey (IBGE) - X1;

• log of Monthly Trade Survey - PMC/IBGE - X2;

• log of Monthly energy consumption in Sao Paulo State - X3;

• Index of Economic Activity of the Central Bank - X4;

• IGP-DI/FGV – General Price Index - X5.

Besides the covariates mentioned above, we also considered the lagged covariate in
the period of 12 months of the proportion of ICMS in the industry (X6) and commerce (X7)
and 6 months of the proportion of ICMC in the industry (X8) and commerce (X9). The total
of covariates in the model is p = 9. Figures 4 and 5 present the solution path by the SSL
(non-adaptative choice (separable), fixed q ; non-adaptative oracle choice (separable); adaptative
choice, q ? B(1, p) (non-separable)), Lasso and elastic net methods for modeling the ICMS
disaggregated in 3 parts: industry, commerce and administered prices. These sectors represent
compositional data, once they are dependent on each other. Thereby, the results showed the same
performance for ilr(y1) and ilr(y2) when the SSL with separable penalties (Figures 4A, 4B, 5A
and 5B), that is, these methods did not select any significant covariate for the model. On the other
hand, SSL non-separable presented three significant covariates (X6, X7 and X8). The optimal
l calculated by the 10-fold cross-validation were 0.0036 (ilr(y1)) and 0.0033 (ilr(y2)) for the



3.3. Discussion 43

Lasso method and 0.0076 (ilr(y1)) and 0.0078 (ilr(y2)) for the elastic net method (vertical line in
Figures 4D, 4E, 5D and 5E). Moreover, Lasso method selected the covariates X1, X2 and X5 and
elastic net method besides these covariates also selected X3 considering the response variable
ilr(y1). These results present the significant exogenous variables when the approached methods
are applied considering compositional restriction on the response variable.

The models for each applied method is given by

1. SSL non-separable:

y1 =0.268?ICMSindustry12months + 0.280?ICMScommerce12months

+ 0.500?ICMSindustry6months

y2 =?0.242?%ICMSindustry12months + 0.118?%ICMScommerce6months

2. Lasso:

y1 =0.002?MontlyIndustrialSurvey + 0.001?MontlyTradeSurvey?0.002?IGP?DI

y2 =0.001?MontlyIndustrialSurvey + 0.004?MontlyTradeSurvey

+ 0.003?Montlyenergyconsumption + 0.002?IGP?DI

3. Elastic net:

y1 =0.002?MontlyIndustrialSurvey + 0.002?MontlyTradeSurvey

+ 0.003?Montlyenergyconsumption?0.002?IGP?DI

y2 =0.003?MontlyTradeSurvey + 0.004?Montlyenergyconsumption + 0.003?IGP?DI

3.3 Discussion
In this chapter, we presented a compositional regression model with restriction in the

response variables under five penalties methods. We applied the ilr coordinates on the response
variables to remove the dependence among the components.

A simulation study for the proposed model (3.2) showed that the model with SSL non-
adaptative oracle choice (separable) performs better in terms of estimation if compared with the
other penalized methods. It is noteworthy that this situation occurs in moderate dimensionality
as in high-dimensionality.

In the case of application, the real data set involves the ICMC tax. As this data set is
considered with moderate dimensionality, the SSL non-separable showed a better performance
in relation to the other SSL penalties. The Lasso and elastic net estimators presented similar
results, with only a little difference between the optimal l . Based on these results, the SSL
non-separable method considered the lagged covariates X6, X7 and X8 significant, that is, the



44 Chapter 3. Penalized Regression Model for compositional response variables

proportion of ICMS in the industry, commerce in the period of 12 months and the proportion
of ICMS in the industry in the period of 6 months are relevant to explain the response variable
ilr(y1) (proportion of the ICMS in the industry). On the other hand, the Lasso method considered
only exogenous covariates significant, which are Monthly Industrial Survey, Monthly Trade
Survey and IGP-DI/FGV General Price Index and for the elastic net method, besides these
covariates mentioned above, including also the covariate monthly energy comsuption in Sao
Paulo State.



3.3. Discussion 45

Figure 4 – The solution path SSL (A, B, C), Lasso (D) and elastic net (E) for ilr(y1). The colored points
on the solution path represent the estimated values of the coefficients. The vertical line (D) and
(E) corresponds to the optimal model Lasso and elastic net (cross-validation), respectively.



46 Chapter 3. Penalized Regression Model for compositional response variables

Figure 5 – The solution path SSL (A, B, C), Lasso (D) and elastic net (E) for ilr(y2). The colored points
on the solution path represent the estimated values of the coefficients. The vertical line (D) and
(E) corresponds to the optimal model Lasso and elastic net (cross-validation), respectively.



47

CHAPTER

4

PENALIZED REGRESSION MODEL FOR
COMPOSITIONAL COVARIATES

In this section, we present the penalized regression model with compositional constraints
in the covariates. The regression model based on methodology of compositional data is given by

yyy = ilr(XXX)bbb + eee, (4.1)

where yyy is a vector (l ? 1) of response variables, XXX is a matrix (l ? D) of D compositional
covariates, where l = 1,...,L and D is the number of the components, bbb = (b1,...,bD)&gt; is a
vector (D?1) of unknown parameters and eee is the noise vector with distribution Nl(000,Ip), with
a known variance s 2 = 1. The intercept of the model is not included, equal to model 3.1.

Based on the principle of working in coordinates, we can rewrite the model (4.1) as

y = hbbb ,XXXiA + eee

= (ilr(bbb ),ilr(XXX))+ eee

=
D?1
Â
k=1

ilr(b )kilrk(X)+ eee

=
D?1
Â
k=1

bkilrk(X)+ eee, (4.2)

with a vector of parameters bbb = bk that afterwards might be mapped back to a composition
through the inverse ilr transformation.

Now, we considered the following estimators for bbb of the regression models with com-
positional covariates focused on regularization methods presented in Section 2. We considered
the lasso, elastic net, SSL with separable and non-separable penalty approaches, respectively, for
the model (4.2) as follows.



48 Chapter 4. Penalized Regression Model for compositional covariates

1. Lasso:

b?bb = argmin
bbb

 
||yyy?

D?1
Â
k=1

bkilrk(X)||22/n + l ||bbb ||1

!
, (4.3)

where ||yyy?ÂD?1k=1 bkilrk(Xi)||
2
2 = Â

n
i=1(yi ?Â

D?1
k=1 bkilrk(Xi))

2 and ||bbb ||1 = ÂD?1j=1 |b j|.

2. Elastic Net:

b?bb = argmin
bbb

 
1
n
||yyy?

D?1
Â
k=1

bkilrk(X)||22 + l
?

1
2
(1?a)||bbb ||22 + a||bbb ||1

?!
. (4.4)

3. SSL with separable penalty (known variance):

b?bb = argmax
bbb2RD?1

(
?

1
2
||yyy?

D?1
Â
k=1

bkilrk(X)||22 +
"
?l1|bbb |+

D?1
Â
j=1

log
?

p?q (0)
p?q (b j)

?#)
. (4.5)

4. SSL with non-separable penalty (unknown variance):

b?bb = argmax
bbb2RD?1

8
&gt;&lt;

&gt;:
?

1
2
||yyy?

D?1
Â
k=1

bkilrk(X)||22 +

0

B
@?l1|bbb |+ log

2

6
4

R q D?1
’D?1j=1 p

?
q (b j)

dp(q )
R q D?1

’D?1j=1 p
?
q (0)

dp(q )

3

7
5

1

C
A

9
&gt;=

&gt;;
.

(4.6)

For the estimation of the bbb ’s, we implemented the estimators (4.3) and (4.4) through
by R package glmnet (FRIEDMAN; HASTIE; TIBSHIRANI, 2010). The estimators (4.5) and
(4.6) were obtained through by R package SSLASSO (MORAN; ROCKOVÁ; GEORGE, 2018).

4.1 Simulation Analysis
We provided the simulation studies to investigate the efficacy of the penalized methods

for a regression model with compositional covariates. We replicated the simulation 1000 times
and the results were summarized based on these replicates (Table 5). We generated a data
compositional matrix XXX of covariates from a logistic normal distribution with mean 000D and S =
(r|i? j|) with r = 0.2 and r = 0.5. The response is generated according to model (4.1) with b? =

1p
3
(?2,?1.5,?1,0,1,1.5,2,0,...,0)&gt; to q = 6 random directions (non-zeros coefficients).

We assumed three scenarios with a different number of sample size and covariates:
(n, p) = (50,30),(100,200) and (100,1000). The adopted performance measures for our com-
parisons were the MSE, the number of FP, the number of FN, and the HAM measure between
the support of the estimated b and the true b?. Table 5 reports the averages of these performance
measures for the five methods adopted.

As can be seen in Table 5, the SSL Separable Oracle (1,6/p) for p = 30,200,1000,
performs better than other methods in all settings based on the mean squared error and false
positives. The SSL Separable tends to select fewer false negatives in high dimensions, which is



4.1. Simulation Analysis 49

acceptable because the omission of important variables is more relevant than the inclusion of
shrunk variables.

Figures 6 and 7 present the estimates of coefficients over 1,000 replicates and the red
circle represents the true value of coefficients. For both settings, Figures 6B and 7B show better
accurate estimations.



50 Chapter 4. Penalized Regression Model for compositional covariates

Table 5 – Averages of some performance measures for penalized methods with compositional covariates.

(n, p) Method MSE FP FN HAM
r =0.2

SSL (l1, q )
(50, 30) SSL Separable (1, 0.8) 0.0115 0.6190 0.0030 6.5620

SSL Separable Oracle (1, 6/30) 0.0070 0.0290 0.0240 6.0250
SSL (1, 6/30) with unknown s 0.0079 0.1260 0.0210 6.1290
Lasso 0.0258 4.1020 0.0030 10.1020
Elastic Net 0.0262 6.4000 0.0000 12.3990

(100, 200) SSL Separable (1, 0.8) 0.0032 4.7320 0.0000 10.7320
SSL Separable Oracle (1, 6/200) 0.0004 0.0030 0.0000 6.0030
SSL (1, 6/200) with unknown s 0.0150 0.0180 1.7050 6.0180
Lasso 0.0030 7.0350 0.0000 13.0350
Elastic Net 0.0030 11.5890 0.0000 17.5890

(100, 1000) SSL Separable (1, 0.8) 0.0010 7.4650 0.0000 13.4650
SSL Separable Oracle (1, 6/1000) 0.0001 0.0000 0.0030 6.0000
SSL (1, 6/1000) with unknown s 0.0096 0.0000 4.4750 6.0000
Lasso 0.0010 11.0290 0.0020 17.0290
Elastic Net 0.0010 16.8400 0.0020 22.8390

r =0.5
SSL (l1, q )

(50, 30) SSL Separable (1, 0.8) 0.0184 0.5920 0.0430 6.5920
SSL Separable Oracle (1, 6/30) 0.0165 0.0440 0.1840 6.0440
SSL (1, 6/30) with unknown s 0.0192 0.1430 0.2130 6.1430
Lasso 0.0420 3.7790 0.0400 9.7790
Elastic Net 0.0407 6.0690 0.0200 12.0690

(100, 200) SSL Separable (1, 0.8) 0.0050 4.6700 0.0000 10.6700
SSL Separable Oracle (1, 6/200) 0.0007 0.0030 0.0100 6.0030
SSL (1, 6/200) with unknown s 0.0092 0.0400 0.8090 6.0400
Lasso 0.0049 6.9680 0.0010 12.9680
Elastic Net 0.0049 11.9650 0.0000 17.9650

(100, 1000) SSL Separable (1, 0.8) 0.0017 7.5630 0.0130 13.5630
SSL Separable Oracle (1, 6/1000) 0.0002 0.0010 0.0510 6.0010
SSL (1, 6/1000) with unknown s 0.0078 0.0000 3.6610 6.0000
Lasso 0.0015 10.6440 0.0250 16.6440
Elastic Net 0.0016 16.6880 0.0140 22.6880



4.1. Simulation Analysis 51

?=0.2

?

?

?

?

?

?

?

?

?

?

?

?

?

?

?1

?2

?3

?4

?5

?6

?7

?2.8 ?1.8 ?0.8 0.2 1.2 2.2
 

 

SSL Separable (?=0.8)A

?

?

?

?

?

?

?

?

?

?

?

?

?

?

?1

?2

?3

?4

?5

?6

?7

?2.8 ?1.8 ?0.8 0.2 1.2 2.2
 

 

SSL Separable (?=6/1000)B

?

?

?

?

?

?

?

?

?

?

?

?

?

?

?1

?2

?3

?4

?5

?6

?7

?2.8 ?1.8 ?0.8 0.2 1.2 2.2
 

 

SSL Non?SeparableC

?

?

?

?

?

?

?

?

?

?

?

?

?

?

?1

?2

?3

?4

?5

?6

?7

?2.8 ?1.8 ?0.8 0.2 1.2 2.2
 

 

LassoD

?

?

?

?

?

?

?

?

?

?

?

?

?

?

?1

?2

?3

?4

?5

?6

?7

?2.8 ?1.8 ?0.8 0.2 1.2 2.2
 

 

Elastic netE

Figure 6 – The parameter estimation averaged over 1000 replicates assuming r = 0.2 for the covariance
matrix (n = 100, p = 1000).



52 Chapter 4. Penalized Regression Model for compositional covariates

?=0.5

?

?

?

?

?

?

?

?

?

?

?

?

?

?

?1

?2

?3

?4

?5

?6

?7

?3 ?2 ?1 0 1 2
 

 

SSL Separable (?=0.8)A

?

?

?

?

?

?

?

?

?

?

?

?

?

?

?1

?2

?3

?4

?5

?6

?7

?3 ?2 ?1 0 1 2
 

 

SSL Separable (?=6/1000)B

?

?

?

?

?

?

?

?

?

?

?

?

?

?

?1

?2

?3

?4

?5

?6

?7

?3 ?2 ?1 0 1 2
 

 

SSL Non?SeparableC

?

?

?

?

?

?

?

?

?

?

?

?

?

?

?1

?2

?3

?4

?5

?6

?7

?3 ?2 ?1 0 1 2
 

 

LassoD

?

?

?

?

?

?

?

?

?

?

?

?

?

?

?1

?2

?3

?4

?5

?6

?7

?3 ?2 ?1 0 1 2
 

 

Elastic netE

Figure 7 – The parameter estimation averaged over 1000 replicates assuming r = 0.5 for the covariance
matrix (n = 100, p = 1000).



4.2. Artificial Data 53

4.2 Artificial Data

A way of illustrating the proposed model (4.2) to compare the SSL, Lasso and elastic
net penalties, we considered the following example. For n = 100 individuals with D = 1000
compositional covariates, we considered one generated sample of the simulation study presented
in the last subsection.

We compared the SSL with a fixed variance with three settings: (i) separable choice
q = 0.8, to verify the over-estimating of the true non-zero fraction 6/1000, (ii) separable oracle
choice q = 6/1000 and (iii) non-adaptative choice q ? B(1,D). The slab parameter was set to
l1 = 0.1 and we used a ladder l0 2 I = {1,2,...,50} for the spike parameter. In addition, we
applied the generated data to the lasso and elastic net penalties implemented in the R package
glmnet (FRIEDMAN; HASTIE; TIBSHIRANI, 2010). For this approach, an optimal value of l
was selected by 10-fold cross-validation. According to Figures 8 and 9, we can see the solution

Figure 8 – The SSL solution paths (A, B, C).

paths for the five settings when we have the structure of compositional covariates. Each line
represents a single regression coefficient and the horizontal dotted lines corresponds to the levels
of true coefficients. The true coefficients are in blue and zero coefficients are in red. We can



54 Chapter 4. Penalized Regression Model for compositional covariates

observe that when q is too large, there are some false positives (Figure 8A). Comparing oracle
choice and when q with distribution Binomial (1, p), the solution path is similar between them.
Moreover, the Spike-and-Slab lasso will keep the larger coefficients in the models. On the other
hand, irrelevant coefficients are removed. Compared to Figure 9D, the lasso model included 6
nonzero coefficients (false negatives) when the optimal l was 0.285. For the elastic net penalty
for the model (4.2), we can see that it presents the same behaviour as the lasso penalty.

4.3 Real data application - Brazilian children malnutri-
tion dataset

The SSL approach for the regression model with compositional covariates was applied to
analyze the nutritional status of children treated at a tertiary university hospital. Our focus is to
verify children with some types of pathology, including the following: osteogenesis imperfecta,
cardiopathy, cystic fibrosis, respiratory disease and tumor who were hospitalized at the University
Hospital Medical School in Ribeiro Preto/SP, Brazil. Basically, the study is based on knowledge
of the prevalence of child malnutrition, where some information about the hospitalized children,
such as sex, age, weight, height, gestational age, body mass index (BMI), bioelectrical impedance,
among others. The BMI can be classified by cutoff points to age (BMI/A) that are determined
according to the Z-score of the World Health Organization (WHO) table of parameters, where
+2 means overweight and -2 means undernutrition. Through some measures, the phase angle
based on resistance and reactance values was also obtained. The phase angle (PA) in children is
a useful tool for evaluating nutritional assessment of body cell mass in stable pediatric patients
and an important alternative method for predicting malnutrition (low PA value) (PILEGGI et al.,
2016)). More information about these measures can be found in Pileggi et al. (2016).

The motivation of this study has been to compare the prevalence of malnutrition in the
pediatric wards based on the average of phase angle of patients with some specific diseases
(University hospital). The evaluation of the children was between February 2008 and February
2009. We were able to use the data from 93 pediatric ward patients. However, our sample size is
n = 12 months of search.

The predictors analyzed in the model were the number of patients: with Z-score BMI (di-
vided into 3 classes: underweight, normal weight and overweight, has a compositional structure)
(Z1, Z2, Z3), where these covariates became ilr coordinates (X1 and X2); birth normal weight
(X3); gestational age less than 37 weeks (X4); male (X5); with age less than 5 years (X6); cesarean
birth (X7). The PA was used for the response of the model. Figures 10 and 11 present the
solution path by the SSL model (non-adaptative choice (separable), fixed q ; non-adaptative
oracle choice (separable); adaptative choice, q ? Binomial(1, p) (non-separable)), Lasso and
elastic net penalties for modeling healthy patients and patients with some disease, respectively.
For the group of healthy children, the three settings of SSL (Figure 10 A, B and C) obtained



4.3. Real data application - Brazilian children malnutrition dataset 55

similar results, that is, the predictor number of patients who were born with normal weight (X3)
and number of patients who had gestational age less than 37 weeks (X4) were significant. This
result showed that the healthy children who did not have malnutrition are those who have a
normal weight at birth and gestational age less than 37 weeks (coefficients with positive values).
On the other hand, the Lasso and elastic net methods included X1 and X5 covariates in the model,
where it presented optimal l = 0.824 by a 10-fold cross-validation (Figure 10D), that is, the
healthy children tend to have malnutrition when there are underweight at birth and are female
(negative estimative of X5).

For the group of children with some diseases, the solution paths for q = 0.5 when it is
fixed (Figure 11A) and q is set to the separable penalty choice 2/7 (Figure 11B), the predictor is
the number of patients who had cesarean births, which was included in the model (coefficient
with negative value). However, the SSL with the non-separable (adaptative) choice (Figure 11C)
did not include no coefficient in the model. The Lasso and elastic net methods included the
number of patients who had cesarean births (X7) in the model, assuming the optimal l = 1.017
calculated by the 10-fold cross-validation (Figure 11D). If we observe the Figures (Figure 11A,
11B, 11D and 11E), the children with diseases who were not born by cesarean have prevalence
to a malnutrition based on the PA measure. This is an important fact to analyze the remarkable
question of malnutrition between healthy children and who have some type of pathology. The
same result can be seen in Figures 11D and 11E.

The models for each applied method (healthy children group) is given by

1. SSL separable (Figure 10A):

y = 10.040?birthnormalweight + 6.479?gestationalagelessthan37weeks

2. SSL separable (Figure 10B):

y = 10.940?birthnormalweight + 6.479?gestationalagelessthan37weeks

3. SSL non-separable (Figure 10C):

y = 10.940?birthnormalweight + 6.479?gestationalagelessthan37weeks

4. Lasso:

y = 0.589?ZscoreBMIunderweight ?0.197?male

5. Elastic net:

y = 0.275?ZscoreBMIunderweight ?1.084?male

The models for each applied method (children group with some pathologies) is given by



56 Chapter 4. Penalized Regression Model for compositional covariates

1. SSL separable (Figure 10A):

y = ?6.725?cesareanbirth

2. SSL separable (Figure 10B):

y = ?6.725?cesareanbirth

3. Lasso:

y = ?1.190?cesareanbirth

4. Elastic net:

y = ?2.158?cesareanbirth



4.4. Discussion 57

4.4 Discussion
In this chapter, we applied a new methodology for regression model with compositional

covariates for child malnutrition data. The SSL, Lasso and elastic net penalties were applied
in the model with constraint covariates assuming dependence among them. Such a modelling
approach had a motivation based on a real data set that focused on the nutrition status of children
with some pathologies and a control group of healthy children by some measures defined by the
WHO.

The main key is to apply such penalties in the regression model with compositional
constraints when n&amp;lt;&amp;lt;p. This methodology yields good solutions by the fact of removing
irrelevant predictors and keeping the larger coefficients, thus obtaining accuracy of coefficient
estimation. We compared the SSL method with Lasso and elastic net, which is similar when we do
not have the slab component, and the performance of Lasso and elastic net were different from the
SSL method for the healthy children, showing that this approach moves more coefficients toward
zero, even if we adopt a strong penalty. On the other hand, for the group of children with some
pathologies, the SSL methods (except SSL non-separable), Lasso and elastic net incorporated
the same significant covariate (X7) in the model, that is, children with some pathologies tend to
have malnutrition when they were not born by cesarean (negative value of estimative).



58 Chapter 4. Penalized Regression Model for compositional covariates

Figure 9 – The Lasso solution path (D) and elastic net path (E).



4.4. Discussion 59

Figure 10 – The solution path SSL (A, B, C), lasso (D) and elastic net (E) for healthy patients. The colored
points on the solution path represent the estimated values of the coefficients. The vertical
line (D) and (E) corresponds to the optimal model lasso and elastic net (cross-validation),
respectively.



60 Chapter 4. Penalized Regression Model for compositional covariates

Figure 11 – The solution path SSL (A, B, C), lasso (D) and elastic net (E) for patients with pathologies.
The colored points on the solution path represent the estimated values of the coefficients.
The vertical line (D) and (E) corresponds to the optimal model lasso and elastic net (cross-
validation), respectively.



61

CHAPTER

5

PENALIZED REGRESSION MODEL FOR
COMPOSITIONAL RESPONSE VARIABLES

AND COVARIATES

In this section, we present the penalized regression model with compositional response
and covariates. The regression model based on the methodology of compositional data is given
by

yyy = XXX bbb + eee, (5.1)

where yyy is a vector (D ? 1) of compositional response variables, XXX is a matrix (D ? D) of D
compositional covariates, where D is the number of components, bbb = (b1,...,bD)&gt; is a vector
(D?1) unknown parameters and eee is the noise vector with distribution ND(000,Ip), with a known
variance s 2 = 1. The intercept of the model is not included, equal to models (3.1) and (4.1).

Based on the principle of working in coordinates, we can rewrite the model (5.1) as

ilr(yyy) = hbbb ,XXXiA + eee, (5.2)

=
D?1
Â
k=1

bkilrk(X)+ eee,

where eee ? N(000D?1,Silr) and with a vector of parameters bbb = (bk) that afterwards might be
mapped back to a composition through the inverse ilr transformation.

Considering the same scheme of Chapter 3 and Chapter 4, we have the following
estimators for bbb of the regression models with compositional responses and covariates focused
on regularization methods presented in Section 2. We considered the Lasso, elastic net, SSL with
separable and non-separable penalty approaches, respectively, for model (5.2) as follows.

1. Lasso:

b?bb = argmin
bbb

 
||ilr(yyy)?

D?1
Â
k=1

bkilrk(X)||22/n + l ||bbb ||1

!
, (5.3)



62 Chapter 5. Penalized Regression Model for compositional response variables and covariates

where ||ilr(yyy)?ÂD?1k=1 bkilrk(Xi)||
2
2 = Â

n
i=1(ilr(yyy)?Â

D?1
k=1 bkilrk(X))

2 and ||bbb ||1 = ÂDj=1 |b j|.

2. Elastic Net:

b?bb = argmin
bbb

 
1
n
||ilr(yyy)?

D?1
Â
k=1

bkilrk(X)||22 + l
?

1
2
(1?a)||bbb ||22 + a||bbb ||1

?!
. (5.4)

3. SSL with separable penalty (known variance):

b?bb = argmax
bbb2RD?1

(
?

1
2
||ilr(yyy)?

D?1
Â
k=1

bkilrk(X)||2 +
"
?l1|bbb |+

D

Â
j=1

log
?

p?q (0)
p?q (b j)

?#)
. (5.5)

4. SSL with non-separable penalty (unknown variance):

b?bb = argmax
bbb2RD?1

8
&gt;&lt;

&gt;:
?

1
2
||ilr(yyy)?

D?1
Â
k=1

bkilrk(X)||2 +

0

B
@?l1|bbb |+ log

2

6
4

R q D
’Dj=1 p

?
q (b j)

dp(q )
R q D

’Dj=1 p
?
q (0)

dp(q )

3

7
5

1

C
A

9
&gt;=

&gt;;
.

(5.6)

For the estimation of the bbb ’s, we implemented the estimators (5.3) and (5.4) through
by R package glmnet (FRIEDMAN; HASTIE; TIBSHIRANI, 2010). The estimators (5.5) and
(5.6) were obtained through by R package SSLASSO (MORAN; ROCKOVÁ; GEORGE, 2018).

5.1 Simulation Analysis
We provided the simulation studies to investigate the efficacy of the penalized methods

for a regression model with compositional response variable and covariates. We replicated the
simulation 1000 times and the results were summarized based on these replicates (Tables 6
and 7). We generated compositional data matrix XXX from a logistic normal distribution with
mean 000D and covariance matrix S = (r|i? j|) with r = 0.2 and r = 0.5, for i, j = 1,...,D.
Moreover, the compositional response variable is generated according to model (5.2), from
a logistic normal distribution with mean 000D and S = (r|i? j|) with r = 0.2 and r = 0.5. We
assume D = 3, that is, we have 3 components (y1,y2,y3) of a composition. The fixed values
for the parameters bbb? = (b?1 ,b

?
2 ) were b

?
1 = (?2,?1.5,?1,0,1,1.5,2,0,...,0)

&gt; and b?2 =
(2,?1,?2.5,0,1,?1,0.5,0,...,0)&gt; to q = 6 random directions (non-zero coefficients).

We assumed three scenarios with different number of sample sizes and covariates:
(n, p) = (50,30),(100,200) and (100,1000). The adopted performance measures for our com-
parisons were the MSE, FP, FN, HAM measure. Tables 6 and 7 report the averages of these
performance measures for the five regularization methods adopted. As can be seen in Tables 6
and 7, similar results are presented for all the settings. It is worth highlighting that the lasso and
elastic net estimator perform slightly better than SSL penalties in high dimensions according to
the HAM measure.



5.2. Toy example 63

Table 6 – Averages of some performance measures for penalized methods with compositional dependent
variables and covariates (ilr(y1)).

(n, p) Method MSE FP FN HAM
r =0.2

SSL (l1, q )
(50, 30) SSL (1, 0.8) with s = 1 fixed 0.5007 0.1820 5.9610 6.1820

SSL (1, 6/30) with s = 1 fixed 0.5001 0.0060 5.9990 6.0060
SSL (1, 6/30) with unknown s 0.5001 0.0070 5.9990 6.0070
Lasso 0.5039 1.5170 5.3930 7.5170
Elastic Net 0.5028 1.9220 5.2400 7.9220

(100, 200) SSL (1, 0.8) with s = 1 fixed 0.0733 1.7710 5.9540 7.7710
SSL (1, 6/200) with s = 1 fixed 0.0729 0.0010 6.0000 6.0010
SSL (1, 6/200) with unknown s 0.0729 0.0080 6.0000 6.0080
Lasso 0.1465 1.1470 5.8790 7.1470
Elastic Net 0.0730 3.8840 5.7570 9.8840

(100, 1000) SSL (1, 0.8) with s = 1 fixed 0.1465 9.0870 5.4270 15.0870
SSL (1, 6/1000) with s = 1 fixed 0.1465 7.0500 5.5620 13.0500
SSL (1, 6/1000) with unknown s 0.1465 15.3680 5.0480 21.3680
Lasso 0.1465 0.7290 5.9390 6.7290
Elastic Net 0.1465 0.7270 5.9390 6.7270

r =0.5
SSL (l1, q )

(50, 30) SSL (1, 0.8) with s = 1 fixed 0.5017 0.4320 5.9050 6.4320
SSL (1, 6/30) with s = 1 fixed 0.5001 0.0200 5.9970 6.0200
SSL (1, 6/30) with unknown s 0.5000 0.0040 5.9980 6.0040
Lasso 0.4675 1.5170 5.3590 7.5170
Elastic Net 0.5027 1.6720 5.4050 7.6720

(100, 200) SSL (1, 0.8) with s = 1 fixed 0.0740 3.9720 5.8860 9.9720
SSL (1, 6/200) with s = 1 fixed 0.0729 0.0010 6.0000 6.0010
SSL (1, 6/200) with unknown s 0.0729 0.0050 6.0000 6.0050
Lasso 0.0730 2.2810 5.8380 8.2810
Elastic Net 0.0730 3.2900 5.7940 9.2900

(100, 1000) SSL (1, 0.8) with s = 1 fixed 0.1466 12.7020 5.1950 18.7020
SSL (1, 6/1000) with s = 1 fixed 0.1465 10.3150 5.3290 16.3150
SSL (1, 6/1000) with unknown s 0.1465 11.0040 5.3050 17.0040
Lasso 0.0145 2.8290 5.9610 8.8290
Elastic Net 0.1465 0.6470 5.9470 6.6470

5.2 Toy example

A way of illustrating the proposed model (5.2) to compare the SSL, lasso and elastic
net penalties, we considered the following example. For n = 100 individuals with D = 1000
compositional covariates, we considered one generated sample of the simulation study presented
in the last section.

We compared the SSL with fixed variance s 2 = 1 with three settings: (i) separable



64 Chapter 5. Penalized Regression Model for compositional response variables and covariates

Table 7 – Averages of some performance measures for penalized methods with compositional dependent
variables and covariates (ilr(y2)).

(n, p) Method MSE FP FN HAM
r =0.2

SSL (l1, q )
(50, 30) SSL (1, 0.8) with s = 1 fixed 0.4747 1.5680 5.5900 7,5680

SSL (1, 6/30) with s = 1 fixed 0.4671 0.1890 5.9460 6.1890
SSL (1, 6/30) with unknown s 0.4656 0.0100 5.9970 6.0100
Lasso 0.4675 1.5170 5.3590 7.5170
Elastic Net 0.4673 1.8500 5.3250 7.8500

(100, 200) SSL (1, 0.8) with s = 1 fixed 0.0725 12.1800 5.6400 18.1800
SSL (1, 6/200) with s = 1 fixed 0.0680 0.1500 5.9960 6.1500
SSL (1, 6/200) with unknown s 0.0678 0.0040 6.0000 6.0040
Lasso 0.1364 1.0600 5.8780 7.0600
Elastic Net 0.0679 3.5070 5.8180 9.5070

(100, 1000) SSL (1, 0.8) with s = 1 fixed 0.1366 21.7880 4.6050 27.7880
SSL (1, 6/1000) with s = 1 fixed 0.1364 18.9530 4.7720 24.9530
SSL (1, 6/1000) with unknown s 0.1364 4.4560 5.6820 10.4560
Lasso 0.1364 0.5970 5.9460 6.5970
Elastic Net 0.1364 0.5920 5.9460 6.5920

r =0.5
SSL (l1, q )

(50, 30) SSL (1, 0.8) with s = 1 fixed 0.4673 0.3940 5.8750 6.3940
SSL (1, 6/30) with s = 1 fixed 0.4657 0.0150 5.9960 6.0150
SSL (1, 6/30) with unknown s 0.4655 0.0030 5.9990 6.0030
Lasso 0.4692 1.8110 5.2710 7.8110
Elastic Net 0.4683 2.2020 5.1620 8.2020

(100, 200) SSL (1, 0.8) with s = 1 fixed 0.0690 4.0420 5.8800 10.0420
SSL (1, 6/200) with s = 1 fixed 0.0679 0.0050 5.9980 6.0050
SSL (1, 6/200) with unknown s 0.0678 0.0080 5.9990 6.0080
Lasso 0.0679 2.8000 5.7780 8.8000
Elastic Net 0.0679 4.0810 5.7780 10.0810

(100, 1000) SSL (1, 0.8) with s = 1 fixed 0.1364 12.6170 5.1660 18.6170
SSL (1, 6/1000) with s = 1 fixed 0.1364 10.2980 5.3290 16.2980
SSL (1, 6/1000) with unknown s 0.1364 11.0540 5.2670 17.0540
Lasso 0.0135 4.1300 5.9380 10.1300
Elastic Net 0.0135 0.6440 5.9360 6.6440

choice q = 0.8, to verify the over-estimating of the true non-zero fraction 6/1000, (ii) separable
oracle choice q = 6/1000 and (iii) non-separable (adaptative) choice q ? Binomial(1,D). The
slab parameter was set to l1 = 0.1 and we used a ladder l0 2 I = {1,2,...,50} for the spike
parameter. In addition, we applied the generated data to the Lasso and elastic net penalties
implemented in the R package glmnet (FRIEDMAN; HASTIE; TIBSHIRANI, 2010). For this
approach, an optimal value of l was selected by 10-fold cross-validation. According to Figures
12, 13, 14 and 15, we can see the solution paths for the five settings when we have the structure of



5.3. Real data application - ICMS dataset 65

compositional response variables and covariates together. Each line represents a single regression
coefficient and the horizontal dotted lines corresponds to the levels of true coefficients. The true
coefficients are in blue and zero coefficients are in red. We can observe that when q is too large,
there are more false positives than false negatives (Figures 12A, 12B and 12C; 14A, 14B and
14C). In comparison with the Figures 13D, 13E, 15D and 15E, the lasso and elastic net presented
more false negatives, that is, the model includes unimportant variables with shrunk coefficients.

5.3 Real data application - ICMS dataset

The description of the applied dataset is in Chapter 3. The focus on this Chapter is the
restriction in the regression model with compositional response and covariates.

In this case, we considered the same three economic sectors: industry (y1), commerce (y2)
and administered prices (y3), defined as compositional data. Besides the covariates mentioned
in Chapter 3, we add the lagged compositional covariate in the period of 12 months of the
proportion of ICMC in the industry (X6), commerce (X7) and administered prices (X8) and 6
months of the proportion of ICMC in the industry (X9), commerce (X10) and administered prices
(X11).

Figures 16 and 17 present the solution path by the SSL (non-adaptative choice (separable),
fixed q ; non-adaptative oracle choice (separable); adaptative choice, q ? Binomial(1, p) (non-
separable)), lasso and elastic net methods for modeling the ICMS disaggregated in 3 parts:
industry, commerce and administered prices considering compositional covariates (X6 to X11).
Thereby, the results showed the same performance for ilr(y1) and ilr(y2) when the SSL with
separable penalties (Figures 16A, 16B, 17A and 17B), that is, these methods did not select
any significant covariate for the model. On the other hand, SSL non-separable presented three
significant covariates (ilr(X6), ilr(X7) and ilr(X8)). The optimal l calculated by the 10-fold
cross-validation were 0.0043 (ilr(y1)) and 0.0020 (ilr(y2)) for the lasso method and 0.0069
(ilr(y1)) and 0.0054 (ilr(y2)) for the elastic net method (vertical line in Figures 16D, 16E, 17D
and 17E).

The models for each applied method is given by



66 Chapter 5. Penalized Regression Model for compositional response variables and covariates

1. SSL non-separable:

y1 =0.004?MonthlyIndustrialSurvey + 0.001?MonthlyTradeSurvey

?0.002?IndexEconomicActivity?0.001?IGP?DI/F GV + 0.080?ICMSindustry12months

+ 0.062?ICMScommerce12months + 0.116?ICMSadm12months

?0.007?ICMScommerce12months

y2 =?0.001?MonthlyIndustrialSurvey + 0.005?MonthlyTradeSurvey

+ 0.001?Monthlyenergyconsumption + 0.001?IGP?DI/F GV

?0.075?ICMSindustry12months + 0.0357?ICMSindustry6months

2. Lasso:

y1 =0.002?MonthlyIndustrialSurvey + 0.002?MonthlyTradeSurvey

+ 0.003?Monthlyenergyconsumption?0.001?IndexEconomicActivity?0.002?IGP?DI/F GV

y2 =0.001?MonthlyIndustrialSurvey + 0.003?MonthlyTradeSurvey

+ 0.004?Monthlyenergyconsumption?0.003?IndexEconomicActivity + 0.003?IGP?DI/F GV

3. Elastic net:

y1 =0.002?MonthlyIndustrialSurvey + 0.001?MonthlyTradeSurvey

+ 0.003?Monthlyenergyconsumption?0.001?IndexEconomicActivity?0.002?IGP?DI/F GV

y2 =0.001?MonthlyIndustrialSurvey + 0.004?MonthlyTradeSurvey

+ 0.004?Monthlyenergyconsumption?0.003?IndexEconomicActivity + 0.003?IGP?DI/F GV

5.4 Discussion
In this chapter, we presented a compositional regression model with restriction in the

response variables and covariates under five regularization methods presented in Chapter 2. We
applied the ilr coordinates on the response variables and covariates simultaneously to remove the
dependence among the components.

A simulation study for the proposed model (5.2) showed that the model with lasso and
elastic net estimators perform better in terms of estimation if comparable to the other penalized
methods in high-dimensions.

In order to illustrate the methodology, a toy example was presented. When the lasso and
elastic net estimators are applied, there are many more false negatives if compared with SSL
estimators. Clearly, for the ilr(y2), the SSL estimators obtained a performance better than lasso
and elastic net. Therefore, SSL non-separable (Figure 14C) has superior performance compared
with the other SSL estimators (separable).



5.4. Discussion 67

In the case of application, the real data set involves the ICMC tax as in Chapter 3. The
SSL non-separable showed a better performance in relation to the other SSL penalties. The
lasso and elastic net estimators presented similar results, with only a little difference between
the optimal l . Based on these results, the SSL non-separable method considered the lagged
covariates administered prices ilr(X6), ilr(X7) and ilr(X8) significant, that is, the proportion
of ICMS in the industry, commerce and administered prices in the period of 12 months are
relevant to explain the response variable ilr(y1) (proportion of the ICMS in the industry). On
the other hand, the lasso method considered only exogenous covariates significant, which are
Monthly Industrial Survey, Monthly Trade Survey and IGP-DI/FGV General Price Index and for
the elastic net method, besides these covariates mentioned above, including also the covariate
Monthly energy comsuption in Sao Paulo State. We observe that these results were similar with
obtained in Chapter 3.



68 Chapter 5. Penalized Regression Model for compositional response variables and covariates

Figure 12 – The SSL solution paths (A, B, C) (for ilr(y1)).



5.4. Discussion 69

Figure 13 – The lasso solution path (D) and elastic net path (E) (for ilr(y1)).



70 Chapter 5. Penalized Regression Model for compositional response variables and covariates

Figure 14 – The SSL solution paths (A, B, C) (for ilr(y2)).



5.4. Discussion 71

Figure 15 – The lasso solution path (D) and elastic net path (E) (for ilr(y2)).



72 Chapter 5. Penalized Regression Model for compositional response variables and covariates

Figure 16 – The solution path SSL (A, B, C), lasso (D) and elastic net (E) for ilr(y1). The colored points
on the solution path represent the estimated values of the coefficients. The vertical line (D)
and (E) corresponds to the optimal model lasso and elastic net (cross-validation), respectively.



5.4. Discussion 73

Figure 17 – The solution path SSL (A, B, C), lasso (D) and elastic net (E) for ilr(y2). The colored points
on the solution path represent the estimated values of the coefficients. The vertical line (D)
and (E) corresponds to the optimal model lasso and elastic net (cross-validation), respectively.





75

CHAPTER

6

CONCLUSION

In this thesis, we considered penalized regression methods, in particular the Lasso (least
absolute shrinkage and selection operator), elastic net and Spike-and-Slab lasso when there are
compositional restrictions in the response variable, covariates or both of them.

One of the principal constraints of compositional data is the nature of dependence among
the components, which cannot be ignored in order to obtain accurate inferences. Thus, the
increase in large datasets, whose dimensionality is much larger than sample size, poses new
challenges to the current methodology of compositional data.

For the context of regression models, we presented three novel models based on compo-
sitional data with an application of different regularization methods. We note that considering the
penalized model with compositional response variables, the simulation studies and application in
a real data set proved that the SSL estimators (oracle separable and non-separable) performed
better than the other regularization methods.

Considering the penalized regression model with compositional covariates, the analysis
under this approach in the child malnutrition data is an important contribution to the present
study. These data focus on the nutrition status of children with some pathologies with some
measures defined by the WHO. For this model, the SSL estimators presented good solutions
by the fact of removing irrelevant predictors and keeping the larger coefficients, thus obtaining
accuracy of coefficient estimation.

Finally, the last penalized regression model considered restrictions for both the response
variables and covariates. The lasso and elastic net estimators perform better if compared with the
other penalized methods in high-dimensions in the simulation study.

For the further development, there are several extensions of this current work. In particu-
lar, we can consider longitudinal and spatio-temporal longitudinal models with compositional
restriction under regularization methods, semiparametric or non-parametric approaches for re-
gression model with log-contrast, where new methods of the regularized estimation could be



76 Chapter 6. Conclusion

developed. Another extension could be to study appropriate models in the presence of zero in
compositional data in a high-dimensional setting.



77

BIBLIOGRAPHY

AITCHISON, J. The statistical analysis of compositional data. Journal of the Royal Statistical
Society. Series B (Methodological), v. 44, n. 2, p. 139–177, 1982. Citations on pages 23, 24,
and 28.

. The statistical analysis of compositional data. Journal of the Royal Statistical Society.
Series B (Methodological), p. 139–177, 1982. Citation on page 23.

. The statistical analysis of compositional data. [S.l.]: Chapman and Hall, 1986. Citations
on pages 24 and 30.

AITCHISON, J.; EGOZCUE, J. J. Compositional data analysis: Where are we and where should
we be heading? Mathematical Geology, v. 37, n. 7, p. 829–850, 2005. Citations on pages 24
and 28.

AITCHISON, J.; SHEN, S. M. Logistic-normal distributions: Some properties and uses.
Biometrika, v. 67, n. 2, p. 261–272, 1980. Citations on pages 23 and 24.

BOOGAART, K. G. van den; TOLOSANA-DELGADO, R. Analyzing Compositional
Data with R. [S.l.]: Springer Publishing Company, Incorporated, 2013. ISBN 3642368085,
9783642368080. Citations on pages 24 and 32.

BUHLMANN, P.; GEER, S. van de. Statistics for High-Dimensional Data: Methods, Theory
and Applications. 1st. ed. [S.l.]: Springer Publishing Company, Incorporated, 2011. ISBN
3642201911, 9783642201912. Citation on page 34.

CHEN, J.; ZHANG, X.; LI, S. Multiple linear regression with compositional response and
covariates. Journal of Applied Statistics, v. 44, n. 12, p. 2270–2285, 2017. Citations on pages
24, 31, and 32.

CHIPMAN, H. Bayesian variable selection with related predictions. Canadian Journal of
Statistics, v. 24, n. 1, p. 17–36, 1996. Citation on page 26.

EFRON, B.; HASTIE, T.; JOHNSTONE, I.; TIBSHIRANI, R. Least angle regression. The
Annals of Statistics, v. 32, n. 2, p. 407–499, 2004. Citations on pages 25 and 35.

EGOZCUE, J. J.; PAWLOWSKY-GLAHN, V.; MATEU-FIGUERAS, G.; BARCELÓ-VIDAL,
C. Isometric logratio transformations for compositional data analysis. Mathematical Geology,
v. 35, n. 3, p. 279–300, 2003. Citation on page 24.

FAN, J.; LI, R. Variable selection via nonconcave penalized likelihood and its oracle properties.
Journal of the American Statistical Association, Taylor &amp;amp; Francis, v. 96, n. 456, p. 1348–1360,
2001. Citation on page 25.

FRIEDMAN, J.; HASTIE, T.; HöFLING, H.; TIBSHIRANI, R. Pathwise coordinate optimization.
The Institute of Mathematical Statistics, v. 1, n. 2, p. 302–332, 12 2007. Available:&amp;lt;https:
//doi.org/10.1214/07-AOAS131&gt;. Citation on page 35.

https://doi.org/10.1214/07-AOAS131
https://doi.org/10.1214/07-AOAS131


78 Bibliography

FRIEDMAN, J.; HASTIE, T.; TIBSHIRANI, R. Regularization paths for generalized linear
models via coordinate descent. Journal of Statistical Software, v. 33, n. 1, p. 1–22, 2010.
Citations on pages 25, 38, 48, 53, 62, and 64.

FU, W. J. Penalized regressions: The bridge versus the lasso. Journal of Computational and
Graphical Statistics, v. 7, n. 3, p. 397–416, 1998. Citation on page 35.

GEORGE, E. I.; MCCULLOCH, R. E. Variable selection via gibbs sampling. Journal of the
American Statistical Association, v. 88, n. 423, p. 881–889, 1993. Citation on page 26.

HASTIE, T.; TIBSHIRANI, R.; FRIEDMAN, J. The elements of statistical learning: data
mining, inference and prediction. 2. ed. [S.l.]: Springer, 2009. Citations on pages 34 and 35.

HASTIE, T.; TIBSHIRANI, R.; WAINWRIGHT, M. Statistical Learning with Sparsity:
The Lasso and Generalizations. [S.l.]: Chapman &amp;amp; Hall/CRC, 2015. ISBN 1498712169,
9781498712163. Citations on pages 33, 34, and 35.

HIJAZI, R. An em-algorithm based method to deal with rounded zeros in compositional data
under dirichlet models. In: PROCEEDINGS OF THE 1TH INTERNATIONAL WORKSHOP
ON COMPOSITIONAL DATA ANALYSIS, 4. Giron, 2011. Citation on page 24.

HIJAZI, R.; JERNIGAN, R. W. Modelling compositional data using dirichlet regression models.
Journal of Applied Probability &amp;amp; Statistics, v. 4, n. 1, p. 77–91, 2009. Citation on page 24.

HOERL, A. E.; KENNARD, R. W. Ridge regression: Biased estimation for nonorthogonal
problems. Technometrics, v. 12, p. 55–67, 1970. Citation on page 35.

HRON, K.; FILZMOSER, P.; THOMPSON, K. Linear regression with compositional explanatory
variables. Journal of Applied Statistics, v. 39, n. 5, p. 1115–1128, 2012. Citations on pages
24, 31, and 32.

ISHWARAN, H.; RAO, J. S. Spike and slab gene selection for multigroup microarray data.
Journal of the American Statistical Association, v. 100, n. 471, p. 764–780, 2005. Citation
on page 26.

JOHNSON, R.; WICHERN, D. Applied multivariate statistical analysis. [S.l.]: New Jersey:
Prentice Hall, 1998. Citation on page 24.

LIN, W.; SHI, P.; FENG, R.; LI, H. Variable selection in regression with compositional covariates.
Biometrika, v. 101, n. 4, p. 1–13, 2014. Citations on pages 25 and 33.

MARTíN-FERNANDEZ, J.; BARCELó-VIDAL, C.; PAWLOWSKY-GLAHN, V. Dealing with
zeros and missing values in compositional data sets using nonparametric imputation. Mathe-
matical Geology, v. 35, n. 3, p. 253–278, 2003. Citation on page 24.

MORAN, G. E.; ROCKOVÁ, V.; GEORGE, E. I. On variance estimation for Bayesian variable
selection. ArXiv e-prints, Jan. 2018. Citations on pages 36, 38, 48, and 62.

PAWLOWSKY-GLAHN, V.; BUCCIANTI, A. Compositional data analysis: Theory and
applications. [S.l.]: John Wiley &amp;amp; Sons, 2011. Citation on page 24.

PAWLOWSKY-GLAHN, V.; EGOZCUE, J. Geometric approach to statistical analysis on the
simplex. Stochastic Environmental Research and Risk Assessment, v. 15, p. 384–398, 2001.
Citation on page 29.



Bibliography 79

PAWLOWSKY-GLAHN, V.; EGOZCUE, J. J.; TOLOSANA-DELGADO, R. Modeling and
analysis of compositional data. [S.l.]: John Wiley &amp;amp; Sons, 2015. Citations on pages 24, 27,
28, and 30.

PILEGGI, V. N.; MONTEIRO, J. P.; MARGUTTI, A. V. B.; JR., J. S. C. Prevalence of child
malnutrition at a university hospital using the world health organization criteria and bioelectrical
impedance data. Brazilian Journal of Medical and Biological Research, v. 49, n. 3, 2016.
Citation on page 54.

R Core Team. R: A Language and Environment for Statistical Computing. Vienna, Austria,
2017. Available:&amp;lt;https://www.R-project.org/&gt;. Citation on page 36.

ROCKOVÁ, V.; GEORGE, E. I. Emvs: The em approach to bayesian variable selection. Journal
of the American Statistical Association, v. 109, n. 506, p. 828–846, 2014. Citation on page
26.

. The spike-and-slab lasso. Journal of the American Statistical Association, v. 113, n. 521,
p. 431–444, 2018. Citations on pages 25, 26, 35, and 36.

SHELTON, J. A.; SHEIKH, A. S.; BORNSCHEIN, J.; STERNE, P.; LUCKE, J. Nonlinear
spike-and-slab sparse coding for interpretable image encoding. PLoS One, v. 10, p. e0124088,
2015. Citation on page 26.

TANG, Z.; SHEN, Y.; ZHANG, X.; YI, N. The spike-and-slab lasso cox model for survival
prediction and associated genes detection. Bioinformatics, v. 33, n. 18, p. 2799–2807, 2017.
Citation on page 26.

. The spike-and-slab lasso generalized linear models for prediction and associated genes
detection. Genetics, v. 205, n. 1, p. 77–88, 2017. Citation on page 26.

TIBSHIRANI, R. Regression shrinkage and selection via the lasso. Journal of the Royal
Statistical Society B, v. 58, p. 267–288, 1996. Citations on pages 25, 33, and 35.

ZOU, H.; HASTIE, T. Regularization and variable selection via the elastic net. Journal of the
Royal Statistical Society, Series B, v. 67, p. 301–320, 2005. Citations on pages 25 and 35.

https://www.R-project.org/




81

APPENDIX

A

APPENDIX 1

Computational Routines for estimation of parameters -
software R

1 # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
2 # S i m u l a t i o n C h a p t e r 3
3 # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
4 rm ( l i s t = l s ( ) )
5
6 # g e n e r a t e c o m p o s i t i o n a l d a t a
7 r c o m p o s&amp;lt;? f u n c t i o n ( n , l , p , b e t a , s i g m a ) {
8 y = m a t r i x (NA, nrow =n , n c o l = l )
9 Z = m a t r i x (NA, nrow =n , n c o l =p )

10 Y = m a t r i x (NA, nrow =n , n c o l = l )
11
12 f o r ( j i n 1 : p ) {
13 Z [ , j ]&amp;lt;? r n o r m ( n , 2 )
14 }
15
16 mu = Z%?%b e t a
17 mean = a p p l y ( mu , 2 , mean )
18 y = rcompnorm ( n , mean , s i g m a , t y p e = " a l r " )
19
20 r e t u r n ( l i s t ( y , Z ) )
21 }
22
23 o u t p u t 1 . 1 = l i s t ( )



82 APPENDIX A. Appendix 1

24 o u t p u t 1 . 2 = l i s t ( )
25 o u t p u t 1 . 3 = l i s t ( )
26 o u t p u t 2 . 1 = l i s t ( )
27 o u t p u t 2 . 2 = l i s t ( )
28 o u t p u t 2 . 3 = l i s t ( )
29 o u t p u t 3 . 1 = l i s t ( )
30 o u t p u t 3 . 2 = l i s t ( )
31 o u t p u t 3 . 3 = l i s t ( )
32 e s t 1 . 1 = m a t r i x ( 0 , nrow =p , n c o l =S?1)
33 e s t 1 . 2 = m a t r i x ( 0 , nrow =p , n c o l =S?1)
34 e s t 1 . 3 = m a t r i x ( 0 , nrow =p , n c o l =S?1)
35 e s t 2 . 1 = m a t r i x ( 0 , nrow =p , n c o l =S?1)
36 e s t 2 . 2 = m a t r i x ( 0 , nrow =p , n c o l =S?1)
37 e s t 2 . 3 = m a t r i x ( 0 , nrow =p , n c o l =S?1)
38 e s t 3 . 1 = m a t r i x ( 0 , nrow =p , n c o l =S?1)
39 e s t 3 . 2 = m a t r i x ( 0 , nrow =p , n c o l =S?1)
40 e s t 3 . 3 = m a t r i x ( 0 , nrow =p , n c o l =S?1)
41 e s t . l a s s o 1 = m a t r i x ( 0 , nrow =p , n c o l =S?1)
42 e s t . l a s s o 2 = m a t r i x ( 0 , nrow =p , n c o l =S?1)
43 e s t . l a s s o 3 = m a t r i x ( 0 , nrow =p , n c o l =S?1)
44 e s t . e l a s t i c 1 = m a t r i x ( 0 , nrow =p , n c o l =S?1)
45 e s t . e l a s t i c 2 = m a t r i x ( 0 , nrow =p , n c o l =S?1)
46 e s t . e l a s t i c 3 = m a t r i x ( 0 , nrow =p , n c o l =S?1)
47 ham . s s l 1 . 1 = c ( )
48 ham . s s l 1 . 2 = c ( )
49 ham . s s l 1 . 3 = c ( )
50 ham . s s l 2 . 1 = c ( )
51 ham . s s l 2 . 2 = c ( )
52 ham . s s l 2 . 3 = c ( )
53 ham . s s l 3 . 1 = c ( )
54 ham . s s l 3 . 2 = c ( )
55 ham . s s l 3 . 3 = c ( )
56 ham . l a s s o 1 = c ( )
57 ham . l a s s o 2 = c ( )
58 ham . l a s s o 3 = c ( )
59 ham . e l a s t i c 1 = c ( )
60 ham . e l a s t i c 2 = c ( )
61 ham . e l a s t i c 3 = c ( )
62



83

63 # # # # GENERATE COMPOSITIONAL DATA MATRIX ###
64 s e t . s e e d ( 2 0 1 8 )
65 w h i l e ( j &amp;lt;S ) {
66 Y&amp;lt;? r c o m p o s ( n , l , p , b e t a , s i g m a )
67 y&amp;lt;? a s . m a t r i x ( p i v o t C o o r d (Y [ [ 1 ] ] ) )
68 Z&amp;lt;? a s . m a t r i x (Y [ [ 2 ] ] )
69 # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
70 ## SSL
71 # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
72 l a m b d a 1&amp;lt;? 1
73 l a m b d a 0&amp;lt;? s e q ( l a m b d a 1 , 5 0 , l e n g t h = 1 0 )
74 L&amp;lt;? l e n g t h ( l a m b d a 0 )
75
76 # O r a c l e SSLASSO w i t h known v a r i a n c e ( S e p a r a b l e )
77 r e s u l t 1 . 1&amp;lt;? SSLASSO ( Z , y [ , 1 ] , p e n a l t y = " s e p a r a b l e " , v a r i a n c e

= " known " ,
78 l a m b d a 1 = l a m b d a 1 , l a m b d a 0 = l a m b d a 0 ,
79 t h e t a = 0 . 8 )
80
81 # O r a c l e SSLASSO w i t h known v a r i a n c e ( S e p a r a b l e O r a c l e )
82 r e s u l t 1 . 2&amp;lt;? SSLASSO ( Z , y [ , 1 ] , p e n a l t y = " s e p a r a b l e " , v a r i a n c e

= " known " ,
83 l a m b d a 1 = l a m b d a 1 , l a m b d a 0 = l a m b d a 0 ,
84 t h e t a = 6 / p )
85
86 # O r a c l e SSLASSO w i t h unknown v a r i a n c e ( Non?S e p a r a b l e )
87 r e s u l t 1 . 3&amp;lt;? SSLASSO ( Z , y [ , 1 ] , p e n a l t y = " a d a p t i v e " , v a r i a n c e =

" unknown " ,
88 l a m b d a 1 = l a m b d a 1 , l a m b d a 0 = l a m b d a 0 ,
89 t h e t a = 6 / p )
90
91 # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
92 ## L a s s o
93 # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
94 r e s u l t 1 . 4 = g l m n e t ( Z , y [ , 1 ] , f a m i l y = " g a u s s i a n " , a l p h a = 1 ,

s t a n d a r d i z e =TRUE , i n t e r c e p t =FALSE )
95 c v . l a s s o . m o d t o t a l 1 . 4 = c v . g l m n e t ( Z , y [ , 1 ] , f a m i l y = " g a u s s i a n " ,

a l p h a = 1 )
96 b e s t l a m . l a s s o 1 . 4 = c v . l a s s o . m o d t o t a l 1 . 4 $ l a m b d a . 1 s e



84 APPENDIX A. Appendix 1

97
98 r e s u l t 1 . 4 . o p t = g l m n e t ( Z , y [ , 1 ] , a l p h a = 1 , s t a n d a r d i z e =TRUE , l a m b d a =

b e s t l a m . l a s s o 1 . 4 , i n t e r c e p t =FALSE )
99 # p r i n t ( c o e f ( r e s u l t 1 . o p t ) , d i g i t = 3 )

100 c o e f . l a s s o 1 = r o u n d ( m a t r i x ( c o e f ( r e s u l t 1 . 4 . o p t ) [ ?1 , ] , nrow =p ) , 4 )
101
102 # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
103 ## E l a s t i c N e t
104 # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
105 a&amp;lt;? s e q ( 0 . 1 , 0 . 9 , 0 . 0 5 )
106 s e a r c h&amp;lt;? f o r e a c h ( i = a , . c o m b i n e = r b i n d ) %d o p a r% {
107 c v&amp;lt;? c v . g l m n e t ( Z , y [ , 1 ] , f a m i l y = " g a u s s i a n " , n f o l d = 1 0 ,

t y p e . m e a s u r e = " d e v i a n c e " , p a r a l l e = TRUE , a l p h a = i )
108 d a t a . f r a m e ( cvm = cv$cvm [ c v $ l a m b d a == c v $ l a m b d a . 1 s e ] , l a m b d a . 1

s e = c v $ l a m b d a . 1 s e , a l p h a = i )
109 }
110 c v 1&amp;lt;? s e a r c h [ s e a r c h $ c v m == min ( s e a r c h $ c v m ) , ]
111
112 r e s u l t 1 . 5 = g l m n e t ( Z , y [ , 1 ] , f a m i l y = " g a u s s i a n " , a l p h a = c v 1 $ a l p h a ,

l a m b d a = c v 1 $ l a m b d a . 1 s e , i n t e r c e p t =FALSE , s t a n d a r d i z e =TRUE )
113
114 c o e f . e l a s t i c 1 = r o u n d ( m a t r i x ( c o e f ( r e s u l t 1 . 5 ) [ ?1 , ] , nrow =p ) , 4 )

1 # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
2 # S i m u l a t i o n C h a p t e r 4
3 # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
4 # # # # GENERATE COMPOSITIONAL DATA MATRIX
5 s e e d =2018
6 s e t . s e e d ( 2 0 1 8 )
7
8 w h i l e ( j &amp;lt;S ) {
9 mean&amp;lt;? c ( r e p ( 0 , p?1) ) # means f o r e a c h c o m p o n e n t

10 s i g m a&amp;lt;? m a t r i x ( 0 . 2 , nrow =p?1 , n c o l =p?1)
11 d i a g ( s i g m a )&amp;lt;? 1
12
13 X&amp;lt;? rcompnorm ( n , m=mean , s = s i g m a , t y p e = " a l r " )
14 X . mean = a p p l y ( X , 2 , mean )
15 i n d&amp;lt;? o r d e r (X . mean , d e c r e a s i n g =T ) [ 1 : 3 ]
16 X . new&amp;lt;? ( c b i n d (X [ , i n d ] , X[ ,? i n d ] ) )



85

17
18 # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
19 ## SSL
20 # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
21 ### GENERATE RESPONSE VECTOR Y
22 b e t a = c ( ?2 , ?1 . 5 , ?1 , 0 , 1 , 1 . 5 , 2 , r e p ( 0 , p?8) )
23
24 y = Z [ , 1 ] ? b e t a [ 1 ] + Z [ , 2 ] ? b e t a [ 2 ] + Z [ , 3 ] ? b e t a [ 3 ] + Z [ , 4 ] ? b e t a [ 4 ] + Z

[ , 5 ] ? b e t a [ 5 ] + Z [ , 6 ] ? b e t a [ 6 ] + Z [ , 7 ] ? b e t a [ 7 ] + r n o r m ( n )
25
26
27 # O r a c l e SSLASSO w i t h known v a r i a n c e ( S e p a r a b l e )
28 r e s u l t 1&amp;lt;? SSLASSO ( Z , y , p e n a l t y = " s e p a r a b l e " , v a r i a n c e = "

known " ,
29 l a m b d a 1 = l a m b d a 1 , l a m b d a 0 = l a m b d a 0 ,
30 t h e t a = 0 . 8 )
31
32 # O r a c l e SSLASSO w i t h known v a r i a n c e ( S e p a r a b l e O r a c l e )
33 r e s u l t 2&amp;lt;? SSLASSO ( Z , y , p e n a l t y = " s e p a r a b l e " , v a r i a n c e = "

known " ,
34 l a m b d a 1 = l a m b d a 1 , l a m b d a 0 = l a m b d a 0 ,
35 t h e t a = 6 / p )
36
37 # O r a c l e SSLASSO w i t h unknown v a r i a n c e ( Non?S e p a r a b l e )
38 r e s u l t 3&amp;lt;? SSLASSO ( Z , y , p e n a l t y = " a d a p t i v e " , v a r i a n c e = "

unknown " ,
39 l a m b d a 1 = l a m b d a 1 , l a m b d a 0 = l a m b d a 0 ,
40 t h e t a = 6 / p )
41
42 o u t p u t 1 [ [ j ] ] = r e s u l t 1 $ b e t a [ , 1 0 ]
43 o u t p u t 2 [ [ j ] ] = r e s u l t 2 $ b e t a [ , 1 0 ]
44 o u t p u t 3 [ [ j ] ] = r e s u l t 3 $ b e t a [ , 1 0 ]
45
46 e s t 1 [ , j ] = a s . m a t r i x ( o u t p u t 1 [ [ j ] ] )
47 e s t 2 [ , j ] = a s . m a t r i x ( o u t p u t 2 [ [ j ] ] )
48 e s t 3 [ , j ] = a s . m a t r i x ( o u t p u t 3 [ [ j ] ] )
49
50 m e d i a s 1 = a p p l y ( e s t 1 , 1 , mean )
51 m e d i a s 2 = a p p l y ( e s t 2 , 1 , mean )



86 APPENDIX A. Appendix 1

52 m e d i a s 3 = a p p l y ( e s t 3 , 1 , mean )
53
54 v a r 1 = a p p l y ( e s t 1 , 1 , v a r )
55 v a r 2 = a p p l y ( e s t 2 , 1 , v a r )
56 v a r 3 = a p p l y ( e s t 3 , 1 , v a r )
57
58 s d 1 = a p p l y ( e s t 1 , 1 , s d )
59 s d 2 = a p p l y ( e s t 2 , 1 , s d )
60 s d 3 = a p p l y ( e s t 3 , 1 , s d )
61
62 # # # # # d i a g n o s t i c s s t a t i s t i c s
63 b i a s 1 = m e d i a s 1?b e t a
64 b i a s 2 = m e d i a s 2?b e t a
65 b i a s 3 = m e d i a s 3?b e t a
66
67 mse1 = mean ( v a r 1 + ( b i a s 1 ^ 2 ) )
68 mse2 = mean ( v a r 2 + ( b i a s 2 ^ 2 ) )
69 mse3 = mean ( v a r 3 + ( b i a s 3 ^ 2 ) )
70
71 ## e r r o r p r e d i c t i o n
72 p e 1 = sum ( y?Z%?%e s t 1 [ , j ] ) ^ 2 / n
73 p e 2 = sum ( y?Z%?%e s t 2 [ , j ] ) ^ 2 / n
74 p e 3 = sum ( y?Z%?%e s t 3 [ , j ] ) ^ 2 / n
75
76 j = j +1
77 c a t ( j , " " , i t e r + 1 , " \ n " )
78 i t e r&amp;lt;? i t e r +1
79 }
80
81 ## FP = f a l s e p o s i t i v e n u m b e r
82 f p 1 = sum ( e s t 1 [ 4 , ] ! = 0 , e s t 1 [ 8 : ( p?1) , ] ! = 0 ) / ( S?1)
83 f p 2 = sum ( e s t 2 [ 4 , ] ! = 0 , e s t 2 [ 8 : ( p?1) , ] ! = 0 ) / ( S?1)
84 f p 3 = sum ( e s t 3 [ 4 , ] ! = 0 , e s t 3 [ 8 : ( p?1) , ] ! = 0 ) / ( S?1)
85
86 ## FP = f a l s e n e g a t i v e n u m b e r
87 f n 1 = sum ( e s t 1 [ 1 : 3 , ] == 0 , e s t 1 [ 5 : 7 , ] == 0 ) / ( S?1)
88 f n 2 = sum ( e s t 2 [ 1 : 3 , ] == 0 , e s t 2 [ 5 : 7 , ] == 0 ) / ( S?1)
89 f n 3 = sum ( e s t 3 [ 1 : 3 , ] == 0 , e s t 3 [ 5 : 7 , ] == 0 ) / ( S?1)
90



87

91 ## Hamming d i s t a n c e
92 ham1 = sum ( e s t 1 [ 1 : ( p?1) , ] ! = b e t a [ 1 : ( p?1) ] ) / ( S?1)
93 ham2 = sum ( e s t 2 [ 1 : ( p?1) , ] ! = b e t a [ 1 : ( p?1) ] ) / ( S?1)
94 ham3 = sum ( e s t 3 [ 1 : ( p?1) , ] ! = b e t a [ 1 : ( p?1) ] ) / ( S?1)
95
96 d i a g n = r o u n d ( r b i n d ( mse1 , mse2 , mse3 , ham1 , ham2 ,
97 ham3 , f p 1 , f p 2 , f p 3 , f n 1 , f n 2 , f n 3 , pe1 , pe2 , p e 3 )

, 5 )
98 d i a g n

1
2 # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
3 # S i m u l a t i o n C h a p t e r 5
4 # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
5 rm ( l i s t = l s ( ) )
6 # g e n e r a t e c o m p o s i t i o n a l d a t a
7 r c o m p o s&amp;lt;? f u n c t i o n ( n , l , p , b e t a , s i g m a ) {
8 y = m a t r i x (NA, nrow =n , n c o l = l )
9 Z = m a t r i x (NA, nrow =n , n c o l =p )

10 Y = m a t r i x (NA, nrow =n , n c o l = l )
11
12 mean1&amp;lt;? c ( r e p ( 0 , p?1) ) # means f o r e a c h c o m p o n e n t
13 s i g m a&amp;lt;? m a t r i x ( 0 . 4 , nrow =p?1 , n c o l =p?1)
14 d i a g ( s i g m a )&amp;lt;? 2
15
16 X&amp;lt;? rcompnorm ( n , m=mean1 , s = s i g m a , t y p e = " a l r " )
17 X . mean = a p p l y ( X , 2 , mean )
18 i n d&amp;lt;? o r d e r (X . mean , d e c r e a s i n g =T ) [ 1 : 3 ] ## E x t r a i n d o o s 5

m a i o r e s c o m p o n e n t e s
19 X . new&amp;lt;? ( c b i n d (X [ , i n d ] , X[ ,? i n d ] ) )
20
21 Z&amp;lt;? a s . m a t r i x ( p i v o t C o o r d (X . new ) )
22
23 mu = Z%?%b e t a
24 mean2 = a p p l y ( mu , 2 , mean )
25 y = rcompnorm ( n , mean2 , s i g m a c , t y p e = " a l r " )
26
27 r e t u r n ( l i s t ( y , Z ) )
28 }



88 APPENDIX A. Appendix 1

29
30 # # # # # Y1 # # # # #
31 # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
32 ## SSL
33 # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
34 l a m b d a 1&amp;lt;? 1
35 l a m b d a 0&amp;lt;? s e q ( l a m b d a 1 , 5 0 , l e n g t h = 1 0 )
36 L&amp;lt;? l e n g t h ( l a m b d a 0 )
37
38 o u t p u t 1 . 1 = l i s t ( )
39 o u t p u t 1 . 2 = l i s t ( )
40 o u t p u t 1 . 3 = l i s t ( )
41 o u t p u t 2 . 1 = l i s t ( )
42 o u t p u t 2 . 2 = l i s t ( )
43 o u t p u t 2 . 3 = l i s t ( )
44 o u t p u t 3 . 1 = l i s t ( )
45 o u t p u t 3 . 2 = l i s t ( )
46 o u t p u t 3 . 3 = l i s t ( )
47 e s t 1 . 1 = m a t r i x ( 0 , nrow =p?1 , n c o l =S?1)
48 e s t 1 . 2 = m a t r i x ( 0 , nrow =p?1 , n c o l =S?1)
49 e s t 1 . 3 = m a t r i x ( 0 , nrow =p?1 , n c o l =S?1)
50 e s t 2 . 1 = m a t r i x ( 0 , nrow =p?1 , n c o l =S?1)
51 e s t 2 . 2 = m a t r i x ( 0 , nrow =p?1 , n c o l =S?1)
52 e s t 2 . 3 = m a t r i x ( 0 , nrow =p?1 , n c o l =S?1)
53 e s t 3 . 1 = m a t r i x ( 0 , nrow =p?1 , n c o l =S?1)
54 e s t 3 . 2 = m a t r i x ( 0 , nrow =p?1 , n c o l =S?1)
55 e s t 3 . 3 = m a t r i x ( 0 , nrow =p?1 , n c o l =S?1)
56 e s t . l a s s o 1 = m a t r i x ( 0 , nrow =p?1 , n c o l =S?1)
57 e s t . l a s s o 2 = m a t r i x ( 0 , nrow =p?1 , n c o l =S?1)
58 e s t . l a s s o 3 = m a t r i x ( 0 , nrow =p?1 , n c o l =S?1)
59 e s t . e l a s t i c 1 = m a t r i x ( 0 , nrow =p?1 , n c o l =S?1)
60 e s t . e l a s t i c 2 = m a t r i x ( 0 , nrow =p?1 , n c o l =S?1)
61 e s t . e l a s t i c 3 = m a t r i x ( 0 , nrow =p?1 , n c o l =S?1)
62 ham . s s l 1 . 1 = c ( )
63 ham . s s l 1 . 2 = c ( )
64 ham . s s l 1 . 3 = c ( )
65 ham . s s l 2 . 1 = c ( )
66 ham . s s l 2 . 2 = c ( )
67 ham . s s l 2 . 3 = c ( )



89

68 ham . s s l 3 . 1 = c ( )
69 ham . s s l 3 . 2 = c ( )
70 ham . s s l 3 . 3 = c ( )
71 ham . l a s s o 1 = c ( )
72 ham . l a s s o 2 = c ( )
73 ham . l a s s o 3 = c ( )
74 ham . e l a s t i c 1 = c ( )
75 ham . e l a s t i c 2 = c ( )
76 ham . e l a s t i c 3 = c ( )
77
78 # # # # GENERATE COMPOSITIONAL DATA MATRIX ###
79 w h i l e ( j &amp;lt;S ) {
80 j j = j j +1
81 s e t . s e e d ( j j )
82
83 Y&amp;lt;? r c o m p o s ( n , l , p , b e t a , s i g m a )
84 y&amp;lt;? a s . m a t r i x ( p i v o t C o o r d (Y [ [ 1 ] ] ) )
85 Z&amp;lt;? a s . m a t r i x (Y [ [ 2 ] ] )
86
87 # O r a c l e SSLASSO w i t h known v a r i a n c e ( S e p a r a b l e )
88 r e s u l t 1 . 1&amp;lt;? SSLASSO ( Z , y [ , 1 ] , p e n a l t y = " s e p a r a b l e " ,

v a r i a n c e = " known " ,
89 l a m b d a 1 = l a m b d a 1 , l a m b d a 0 = l a m b d a 0 ,
90 t h e t a = 0 . 8 )
91
92 # O r a c l e SSLASSO w i t h known v a r i a n c e ( S e p a r a b l e O r a c l e )
93 r e s u l t 1 . 2&amp;lt;? SSLASSO ( Z , y [ , 1 ] , p e n a l t y = " s e p a r a b l e " ,

v a r i a n c e = " known " ,
94 l a m b d a 1 = l a m b d a 1 , l a m b d a 0 = l a m b d a 0 ,
95 t h e t a = 6 / p )
96
97 # O r a c l e SSLASSO w i t h unknown v a r i a n c e ( Non?S e p a r a b l e )
98 r e s u l t 1 . 3&amp;lt;? SSLASSO ( Z , y [ , 1 ] , p e n a l t y = " a d a p t i v e " , v a r i a n c e

= " unknown " ,
99 l a m b d a 1 = l a m b d a 1 , l a m b d a 0 = l a m b d a 0 ,

100 t h e t a = 6 / p )
101
102 # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
103 ## L a s s o



90 APPENDIX A. Appendix 1

104 # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
105 r e s u l t 1 . 4 = g l m n e t ( Z , y [ , 1 ] , f a m i l y = " g a u s s i a n " , a l p h a = 1 ,

s t a n d a r d i z e =TRUE , i n t e r c e p t =FALSE )
106 c v . l a s s o . m o d t o t a l 1 . 4 = c v . g l m n e t ( Z , y [ , 1 ] , f a m i l y = " g a u s s i a n " ,

a l p h a = 1 )
107 b e s t l a m . l a s s o 1 . 4 = c v . l a s s o . m o d t o t a l 1 . 4 $ l a m b d a . 1 s e
108
109 r e s u l t 1 . 4 . o p t = g l m n e t ( Z , y [ , 1 ] , a l p h a = 1 , s t a n d a r d i z e =TRUE ,

l a m b d a = b e s t l a m . l a s s o 1 . 4 , i n t e r c e p t =FALSE )
110 # p r i n t ( c o e f ( r e s u l t 1 . o p t ) , d i g i t = 3 )
111 c o e f . l a s s o 1 = r o u n d ( m a t r i x ( c o e f ( r e s u l t 1 . 4 . o p t ) [ ?1 , ] , nrow =p?1)

, 4 )
112
113 # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
114 ## E l a s t i c n e t
115 # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
116 a&amp;lt;? s e q ( 0 . 1 , 0 . 9 , 0 . 0 5 )
117 s e a r c h&amp;lt;? f o r e a c h ( i = a , . c o m b i n e = r b i n d ) %d o p a r% {
118 c v&amp;lt;? c v . g l m n e t ( Z , y [ , 1 ] , f a m i l y = " g a u s s i a n " , n f o l d = 1 0 ,

t y p e . m e a s u r e = " d e v i a n c e " , p a r a l l e = TRUE , a l p h a = i )
119 d a t a . f r a m e ( cvm = cv$cvm [ c v $ l a m b d a == c v $ l a m b d a . 1 s e ] , l a m b d a

. 1 s e = c v $ l a m b d a . 1 s e , a l p h a = i )
120 }
121 c v 1&amp;lt;? s e a r c h [ s e a r c h $ c v m == min ( s e a r c h $ c v m ) , ]
122
123 r e s u l t 1 . 5 = g l m n e t ( Z , y [ , 1 ] , f a m i l y = " g a u s s i a n " , a l p h a = c v 1 $ a l p h a ,

l a m b d a = c v 1 $ l a m b d a . 1 s e , i n t e r c e p t =FALSE , s t a n d a r d i z e =TRUE )
124
125 c o e f . e l a s t i c 1 = r o u n d ( m a t r i x ( c o e f ( r e s u l t 1 . 5 ) [ ?1 , ] , nrow =p?1)

, 4 )
126
127 # # # # # Y2 # # # # #
128 # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
129 ## SSL
130 # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
131 # O r a c l e SSLASSO w i t h known v a r i a n c e ( S e p a r a b l e )
132 r e s u l t 2 . 1&amp;lt;? SSLASSO ( Z , y [ , 2 ] , p e n a l t y = " s e p a r a b l e " ,

v a r i a n c e = " known " ,
133 l a m b d a 1 = l a m b d a 1 , l a m b d a 0 = l a m b d a 0 ,



91

134 t h e t a = 0 . 8 )
135
136 # O r a c l e SSLASSO w i t h known v a r i a n c e ( S e p a r a b l e O r a c l e )
137 r e s u l t 2 . 2&amp;lt;? SSLASSO ( Z , y [ , 2 ] , p e n a l t y = " s e p a r a b l e " ,

v a r i a n c e = " known " ,
138 l a m b d a 1 = l a m b d a 1 , l a m b d a 0 = l a m b d a 0 ,
139 t h e t a = 6 / p )
140
141 # O r a c l e SSLASSO w i t h unknown v a r i a n c e ( Non?S e p a r a b l e )
142 r e s u l t 2 . 3&amp;lt;? SSLASSO ( Z , y [ , 2 ] , p e n a l t y = " a d a p t i v e " , v a r i a n c e

= " unknown " ,
143 l a m b d a 1 = l a m b d a 1 , l a m b d a 0 = l a m b d a 0 ,
144 t h e t a = 6 / p )
145
146 # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
147 ## L a s s o
148 # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
149 r e s u l t 2 . 4 = g l m n e t ( Z , y [ , 2 ] , f a m i l y = " g a u s s i a n " , a l p h a = 1 ,

s t a n d a r d i z e =TRUE , i n t e r c e p t =FALSE )
150 c v . l a s s o . m o d t o t a l 2 . 4 = c v . g l m n e t ( Z , y [ , 2 ] , f a m i l y = " g a u s s i a n " ,

a l p h a = 1 )
151 b e s t l a m . l a s s o 2 . 4 = c v . l a s s o . m o d t o t a l 2 . 4 $ l a m b d a . 1 s e
152
153 r e s u l t 2 . 4 . o p t = g l m n e t ( Z , y [ , 2 ] , a l p h a = 1 , s t a n d a r d i z e =TRUE ,

l a m b d a = b e s t l a m . l a s s o 2 . 4 , i n t e r c e p t =FALSE )
154 # p r i n t ( c o e f ( r e s u l t 1 . o p t ) , d i g i t = 3 )
155 c o e f . l a s s o 2 = r o u n d ( m a t r i x ( c o e f ( r e s u l t 2 . 4 . o p t ) [ ?1 , ] , nrow =p?1)

, 4 )
156
157 # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
158 ## E l a s t i c n e t
159 # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
160 a&amp;lt;? s e q ( 0 . 1 , 0 . 9 , 0 . 0 5 )
161 s e a r c h&amp;lt;? f o r e a c h ( i = a , . c o m b i n e = r b i n d ) %d o p a r% {
162 c v&amp;lt;? c v . g l m n e t ( Z , y [ , 2 ] , f a m i l y = " g a u s s i a n " , n f o l d = 1 0 ,

t y p e . m e a s u r e = " d e v i a n c e " , p a r a l l e = TRUE , a l p h a = i )
163 d a t a . f r a m e ( cvm = cv$cvm [ c v $ l a m b d a == c v $ l a m b d a . 1 s e ] , l a m b d a

. 1 s e = c v $ l a m b d a . 1 s e , a l p h a = i )
164 }



92 APPENDIX A. Appendix 1

165 c v 2&amp;lt;? s e a r c h [ s e a r c h $ c v m == min ( s e a r c h $ c v m ) , ]
166
167 r e s u l t 2 . 5 = g l m n e t ( Z , y [ , 2 ] , f a m i l y = " g a u s s i a n " , a l p h a = c v 2 $ a l p h a ,

l a m b d a = c v 2 $ l a m b d a . 1 s e , i n t e r c e p t =FALSE , s t a n d a r d i z e =TRUE )
168
169 c o e f . e l a s t i c 2 = r o u n d ( m a t r i x ( c o e f ( r e s u l t 2 . 5 ) [ ?1 , ] , nrow =p?1)

, 4 )
170
171 # # # # # # # # # # # OUTPUTS # # # # # # # # # # #
172 o u t p u t 1 . 1 [ [ j ] ] = r e s u l t 1 . 1 $ b e t a [ , 1 0 ]
173 o u t p u t 1 . 2 [ [ j ] ] = r e s u l t 1 . 2 $ b e t a [ , 1 0 ]
174 o u t p u t 1 . 3 [ [ j ] ] = r e s u l t 1 . 3 $ b e t a [ , 1 0 ]
175 o u t p u t 2 . 1 [ [ j ] ] = r e s u l t 2 . 1 $ b e t a [ , 1 0 ]
176 o u t p u t 2 . 2 [ [ j ] ] = r e s u l t 2 . 2 $ b e t a [ , 1 0 ]
177 o u t p u t 2 . 3 [ [ j ] ] = r e s u l t 2 . 3 $ b e t a [ , 1 0 ]
178
179 e s t 1 . 1 [ , j ] = a s . m a t r i x ( o u t p u t 1 . 1 [ [ j ] ] )
180 e s t 1 . 2 [ , j ] = a s . m a t r i x ( o u t p u t 1 . 2 [ [ j ] ] )
181 e s t 1 . 3 [ , j ] = a s . m a t r i x ( o u t p u t 1 . 3 [ [ j ] ] )
182 e s t 2 . 1 [ , j ] = a s . m a t r i x ( o u t p u t 2 . 1 [ [ j ] ] )
183 e s t 2 . 2 [ , j ] = a s . m a t r i x ( o u t p u t 2 . 2 [ [ j ] ] )
184 e s t 2 . 3 [ , j ] = a s . m a t r i x ( o u t p u t 2 . 3 [ [ j ] ] )
185 e s t . l a s s o 1 [ , j ] = c o e f . l a s s o 1
186 e s t . l a s s o 2 [ , j ] = c o e f . l a s s o 1
187 e s t . e l a s t i c 1 [ , j ] = c o e f . e l a s t i c 1
188 e s t . e l a s t i c 2 [ , j ] = c o e f . e l a s t i c 2
189
190 m e d i a s 1 . 1 = a p p l y ( e s t 1 . 1 , 1 , mean )
191 m e d i a s 1 . 2 = a p p l y ( e s t 1 . 2 , 1 , mean )
192 m e d i a s 1 . 3 = a p p l y ( e s t 1 . 3 , 1 , mean )
193 m e d i a s 2 . 1 = a p p l y ( e s t 2 . 1 , 1 , mean )
194 m e d i a s 2 . 2 = a p p l y ( e s t 2 . 2 , 1 , mean )
195 m e d i a s 2 . 3 = a p p l y ( e s t 2 . 3 , 1 , mean )
196 m e d i a s . l a s s o 1 = a p p l y ( e s t . l a s s o 1 , 1 , mean )
197 m e d i a s . l a s s o 2 = a p p l y ( e s t . l a s s o 2 , 1 , mean )
198 m e d i a s . e l a s t i c 1 = a p p l y ( e s t . e l a s t i c 1 , 1 , mean )
199 m e d i a s . e l a s t i c 2 = a p p l y ( e s t . e l a s t i c 2 , 1 , mean )
200
201 v a r 1 . 1 = a p p l y ( e s t 1 . 1 , 1 , v a r )



93

202 v a r 1 . 2 = a p p l y ( e s t 1 . 2 , 1 , v a r )
203 v a r 1 . 3 = a p p l y ( e s t 1 . 3 , 1 , v a r )
204 v a r 2 . 1 = a p p l y ( e s t 2 . 1 , 1 , v a r )
205 v a r 2 . 2 = a p p l y ( e s t 2 . 2 , 1 , v a r )
206 v a r 2 . 3 = a p p l y ( e s t 2 . 3 , 1 , v a r )
207 v a r . l a s s o 1 = a p p l y ( e s t . l a s s o 1 , 1 , v a r )
208 v a r . l a s s o 2 = a p p l y ( e s t . l a s s o 2 , 1 , v a r )
209 v a r . e l a s t i c 1 = a p p l y ( e s t . e l a s t i c 1 , 1 , v a r )
210 v a r . e l a s t i c 2 = a p p l y ( e s t . e l a s t i c 2 , 1 , v a r )
211
212 s d 1 . 1 = a p p l y ( e s t 1 . 1 , 1 , s d )
213 s d 1 . 2 = a p p l y ( e s t 1 . 2 , 1 , s d )
214 s d 1 . 3 = a p p l y ( e s t 1 . 3 , 1 , s d )
215 s d 2 . 1 = a p p l y ( e s t 2 . 1 , 1 , s d )
216 s d 2 . 2 = a p p l y ( e s t 2 . 2 , 1 , s d )
217 s d 2 . 3 = a p p l y ( e s t 2 . 3 , 1 , s d )
218 s d . l a s s o 1 = a p p l y ( e s t . l a s s o 1 , 1 , s d )
219 s d . l a s s o 2 = a p p l y ( e s t . l a s s o 2 , 1 , s d )
220 s d . e l a s t i c 1 = a p p l y ( e s t . e l a s t i c 1 , 1 , s d )
221 s d . e l a s t i c 2 = a p p l y ( e s t . e l a s t i c 2 , 1 , s d )
222
223 # # # # # d i a g n o s t i c s s t a t i s t i c s
224 b i a s 1 . 1 = m e d i a s 1 .1? b e t a 1
225 b i a s 1 . 2 = m e d i a s 1 .2? b e t a 1
226 b i a s 1 . 3 = m e d i a s 1 .3? b e t a 1
227 b i a s 2 . 1 = m e d i a s 2 .1? b e t a 2
228 b i a s 2 . 2 = m e d i a s 2 .2? b e t a 2
229 b i a s 2 . 3 = m e d i a s 2 .3? b e t a 2
230 b i a s . l a s s o 1 = m e d i a s . l a s s o 1 ?b e t a 1
231 b i a s . l a s s o 2 = m e d i a s . l a s s o 2 ?b e t a 2
232 b i a s . e l a s t i c 1 = m e d i a s . e l a s t i c 1 ?b e t a 1
233 b i a s . e l a s t i c 2 = m e d i a s . e l a s t i c 2 ?b e t a 2
234
235 mse1 . 1 = mean ( v a r 1 . 1 + ( b i a s 1 . 1 ^ 2 ) )
236 mse1 . 2 = mean ( v a r 1 . 2 + ( b i a s 1 . 2 ^ 2 ) )
237 mse1 . 3 = mean ( v a r 1 . 3 + ( b i a s 1 . 3 ^ 2 ) )
238 mse2 . 1 = mean ( v a r 2 . 1 + ( b i a s 2 . 1 ^ 2 ) )
239 mse2 . 2 = mean ( v a r 2 . 2 + ( b i a s 2 . 2 ^ 2 ) )
240 mse2 . 3 = mean ( v a r 2 . 3 + ( b i a s 2 . 3 ^ 2 ) )



94 APPENDIX A. Appendix 1

241 mse . l a s s o 1 = mean ( v a r . l a s s o 1 + ( b i a s . l a s s o 1 ^ 2 ) )
242 mse . l a s s o 2 = mean ( v a r . l a s s o 2 + ( b i a s . l a s s o 2 ^ 2 ) )
243 mse . e l a s t i c 1 = mean ( v a r . e l a s t i c 1 + ( b i a s . e l a s t i c 1 ^ 2 ) )
244 mse . e l a s t i c 2 = mean ( v a r . e l a s t i c 2 + ( b i a s . e l a s t i c 2 ^ 2 ) )
245
246 ## e r r o r p r e d i c t i o n
247 p e 1 . 1 = sum ( y [ , 1 ] ?Z%?%e s t 1 . 1 [ , j ] ) ^ 2 / n
248 p e 1 . 2 = sum ( y [ , 1 ] ?Z%?%e s t 1 . 2 [ , j ] ) ^ 2 / n
249 p e 1 . 3 = sum ( y [ , 1 ] ?Z%?%e s t 1 . 3 [ , j ] ) ^ 2 / n
250 p e 2 . 1 = sum ( y [ , 2 ] ?Z%?%e s t 2 . 1 [ , j ] ) ^ 2 / n
251 p e 2 . 2 = sum ( y [ , 2 ] ?Z%?%e s t 2 . 2 [ , j ] ) ^ 2 / n
252 p e 2 . 3 = sum ( y [ , 2 ] ?Z%?%e s t 2 . 3 [ , j ] ) ^ 2 / n
253 p e . l a s s o 1 = sum ( y [ , 1 ] ?Z%?% e s t . l a s s o 1 [ , j ] ) ^ 2 / n
254 p e . l a s s o 2 = sum ( y [ , 2 ] ?Z%?% e s t . l a s s o 2 [ , j ] ) ^ 2 / n
255 p e . e l a s t i c 1 = sum ( y [ , 1 ] ?Z%?% e s t . e l a s t i c 1 [ , j ] ) ^ 2 / n
256 p e . e l a s t i c 2 = sum ( y [ , 2 ] ?Z%?% e s t . e l a s t i c 2 [ , j ] ) ^ 2 / n
257
258 j = j +1
259 c a t ( j , " " , i t e r + 1 , " \ n " )
260 i t e r&amp;lt;? i t e r +1
261 }
262
263 ## FP = f a l s e p o s i t i v e n u m b e r
264 f p 1 . 1 = sum ( e s t 1 . 1 [ 4 , ] ! = 0 , e s t 1 . 1 [ 8 : ( p?1) , ] ! = 0 ) / ( S?1)
265 f p 1 . 2 = sum ( e s t 1 . 2 [ 4 , ] ! = 0 , e s t 1 . 2 [ 8 : ( p?1) , ] ! = 0 ) / ( S?1)
266 f p 1 . 3 = sum ( e s t 1 . 3 [ 4 , ] ! = 0 , e s t 1 . 3 [ 8 : ( p?1) , ] ! = 0 ) / ( S?1)
267 f p 2 . 1 = sum ( e s t 2 . 1 [ 4 , ] ! = 0 , e s t 2 . 1 [ 8 : ( p?1) , ] ! = 0 ) / ( S?1)
268 f p 2 . 2 = sum ( e s t 2 . 2 [ 4 , ] ! = 0 , e s t 2 . 2 [ 8 : ( p?1) , ] ! = 0 ) / ( S?1)
269 f p 2 . 3 = sum ( e s t 2 . 3 [ 4 , ] ! = 0 , e s t 2 . 3 [ 8 : ( p?1) , ] ! = 0 ) / ( S?1)
270 f p . l a s s o 1 = sum ( e s t . l a s s o 1 [ 4 , ] ! = 0 , e s t . l a s s o 1 [ 8 : ( p?1) , ] ! = 0 )

/ ( S?1)
271 f p . l a s s o 2 = sum ( e s t . l a s s o 2 [ 4 , ] ! = 0 , e s t . l a s s o 2 [ 8 : ( p?1) , ] ! = 0 )

/ ( S?1)
272 f p . e l a s t i c 1 = sum ( e s t . e l a s t i c 1 [ 4 , ] ! = 0 , e s t . e l a s t i c 1 [ 8 : ( p?1) , ]

! = 0 ) / ( S?1)
273 f p . e l a s t i c 2 = sum ( e s t . e l a s t i c 2 [ 4 , ] ! = 0 , e s t . e l a s t i c 2 [ 8 : ( p?1) , ]

! = 0 ) / ( S?1)
274
275 ## FP = f a l s e n e g a t i v e n u m b e r



95

276 f n 1 . 1 = sum ( e s t 1 . 1 [ 1 : 3 , ] == 0 , e s t 1 . 1 [ 5 : 7 , ] == 0 ) / ( S?1)
277 f n 1 . 2 = sum ( e s t 1 . 2 [ 1 : 3 , ] == 0 , e s t 1 . 2 [ 5 : 7 , ] == 0 ) / ( S?1)
278 f n 1 . 3 = sum ( e s t 1 . 3 [ 1 : 3 , ] == 0 , e s t 1 . 3 [ 5 : 7 , ] == 0 ) / ( S?1)
279 f n 2 . 1 = sum ( e s t 2 . 1 [ 1 : 3 , ] == 0 , e s t 2 . 1 [ 5 : 7 , ] == 0 ) / ( S?1)
280 f n 2 . 2 = sum ( e s t 2 . 2 [ 1 : 3 , ] == 0 , e s t 2 . 2 [ 5 : 7 , ] == 0 ) / ( S?1)
281 f n 2 . 3 = sum ( e s t 2 . 3 [ 1 : 3 , ] == 0 , e s t 2 . 3 [ 5 : 7 , ] == 0 ) / ( S?1)
282 f n . l a s s o 1 = sum ( e s t . l a s s o 1 [ 1 : 3 , ] == 0 , e s t . l a s s o 1 [ 5 : 7 , ] == 0 ) / (

S?1)
283 f n . l a s s o 2 = sum ( e s t . l a s s o 2 [ 1 : 3 , ] == 0 , e s t . l a s s o 2 [ 5 : 7 , ] == 0 ) / (

S?1)
284 f n . e l a s t i c 1 = sum ( e s t . e l a s t i c 1 [ 1 : 3 , ] == 0 , e s t . e l a s t i c 1 [ 5 : 7 , ]

== 0 ) / ( S?1)
285 f n . e l a s t i c 2 = sum ( e s t . e l a s t i c 2 [ 1 : 3 , ] == 0 , e s t . e l a s t i c 2 [ 5 : 7 , ]

== 0 ) / ( S?1)
286
287 ## Hamming d i s t a n c e
288 ham . s s l 1 . 1 = sum ( e s t 1 . 1 [ 1 : ( p?1) , ] ! = b e t a 1 [ 1 : ( p?1) ] ) / ( S?1)
289 ham . s s l 1 . 2 = sum ( e s t 1 . 2 [ 1 : ( p?1) , ] ! = b e t a 1 [ 1 : ( p?1) ] ) / ( S?1)
290 ham . s s l 1 . 3 = sum ( e s t 1 . 3 [ 1 : ( p?1) , ] ! = b e t a 1 [ 1 : ( p?1) ] ) / ( S?1)
291 ham . s s l 2 . 1 = sum ( e s t 2 . 1 [ 1 : ( p?1) , ] ! = b e t a 2 [ 1 : ( p?1) ] ) / ( S?1)
292 ham . s s l 2 . 2 = sum ( e s t 2 . 2 [ 1 : ( p?1) , ] ! = b e t a 2 [ 1 : ( p?1) ] ) / ( S?1)
293 ham . s s l 2 . 3 = sum ( e s t 2 . 3 [ 1 : ( p?1) , ] ! = b e t a 2 [ 1 : ( p?1) ] ) / ( S?1)
294 ham . l a s s o 1 = sum ( e s t . l a s s o 1 [ 1 : ( p?1) , ] ! = b e t a 1 [ 1 : ( p?1) ] ) / ( S?1)
295 ham . l a s s o 2 = sum ( e s t . l a s s o 2 [ 1 : ( p?1) , ] ! = b e t a 2 [ 1 : ( p?1) ] ) / ( S?1)
296 ham . e l a s t i c 1 = sum ( e s t . e l a s t i c 1 [ 1 : ( p?1) , ] ! = b e t a 1 [ 1 : ( p?1) ] ) / ( S

?1)
297 ham . e l a s t i c 2 = sum ( e s t . e l a s t i c 2 [ 1 : ( p?1) , ] ! = b e t a 2 [ 1 : ( p?1) ] ) / ( S

?1)


</field>
	</doc>
</add>