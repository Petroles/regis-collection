<?xml version="1.0" encoding="utf-8"?>
<add>
	<doc>
		<field name="docid">BR-TU.05990</field>
		<field name="filename">10475_juliogrimalt.pdf</field>
		<field name="filetype">PDF</field>
		<field name="text">FUNDAÇÃO GETULIO VARGAS
EPGE
Escola de Pós-Graduação em Economia
Cálculo do Value at Risk (VaR) para o Ibovespa, pós crise de 2008, por meio dos modelos de heterocedasticidade condicional (GARCH) e de volatilidade estocástica (Local Scale Model -LSM).
Julio Cesar Grimalt dos Santos
Dissertação apresentada à Escola de Pós-Graduação em Economia (EPGE-RJ) da Fundação Getúlio Vargas, como requisito parcial à obtenção do título de Mestre em Finanças e Economia Empresarial.
Orientador - Eduardo Lima Campos
Rio de Janeiro
2015
Santos, Julio Cesar Grimalt dos
Cálculo do Value at Risk (VaR) para o Ibovespa, pós crise de 2008, por meio dos modelos de heterocedasticídade condicional (GARCH) e de volatilidade estocástica (Local Scale Model - LSM) / Julio Cesar Grimalt dos Santos. - 2015.
55 f.
Dissertação (mestrado) - Fundação Getulio Vargas, Escola de Pós-Graduação em Economia.
Orientador: Eduardo Lima Campos.
Inclui bibliografia.
1.	Risco (Economia). 2. Processo estocástico. 3. Mercado financeiro. 4. Análise de séries temporais. I. Campos, Eduardo Lima. II. Fundação Getulio Vargas. Escola de Pós-Graduação em Economia. III. Título.
CDD-338.5
FGV
JULIO CESAR GRIMALT DOS SANTOS
“Cálculo do Value at Risk (VaR) para Ibovespa, pós crise de 2008, por meio dos modelos de heterocedasticidade condicional (GARCII) e de volatilidade estocástica (Local Scale Model - LSM)”
Dissertação apresentada ao Curso de Mestrado Profissional em Economia Empresarial e Finanças da Escola de Pós-Graduação cm Economia para obtenção do grau de Mestre em Economia Empresarial e Finanças.
Data da defesa: 10/02/2015
ASSINATURA DOS MEMBROS DA BANCA EXAMINADORA
Eduardo Lima campos
Orientador (a)
Aos meus pais, Edmundo Vidal e Dayse Grimalt, e irmã, Glaucia Grimalt, pela formação de meu caráter, pelos exemplos e conselhos. Por onde a vida me levar vocês serão meu maior patrimônio!
Dedicatória
4
Agradecimentos
Ao melhor orientador de todos os tempos, Eduardo Lima Campos, pela acessibilidade, correção de atitudes e paciência, mas, sobretudo, pela pessoa.
Aos amigos de profissão por terem suprido minhas ausências e pela energia sempre positiva emanada. Ao meu chefe e amigo, pela tolerância e compreensão ao longo dos últimos 19 meses.
Saúdo os membros da banca pela disponibilidade, pelas considerações e por terem aceitado o convite para fazer parte da avaliação.
Sou grato aos funcionários da secretaria do MFEE, Gisele Gammaro e Vitor Barros, pela boa vontade e pelo pronto atendimento nas horas mais complicadas.
Agradeço ao monitor Valdemar Neto, pelo valioso auxílio técnico, que muito contribuiu para o meu desenvolvimento acadêmico e aprendizado.
Aos companheiros de jornada, pelo convívio e pelas horas de estudo aos sábados e domingos, enquanto poderíamos estar na praia. E por falar em praia, destaco o também aluno, Paulo Roberto Miller, que com admirável didática, eloquência e abnegação, permitiu, através de seus valiosos ensinamentos, que eu tivesse algumas horas de lazer, entre uma prova e outra.
Aos familiares e amigos, pela compreensão de que nesta vida, nada é de graça, e que para se galgar nobres objetivos é preciso muita disposição, foco e resiliência.
Esta conquista também é de vocês!
5
Resumo
O objetivo deste estudo é propor a implementação de um modelo estatístico para cálculo da volatilidade, não difundido na literatura brasileira, o modelo de escala local (LSM), apresentando suas vantagens e desvantagens em relação aos modelos habitualmente utilizados para mensuração de risco.
Para estimação dos parâmetros serão usadas as cotações diárias do Ibovespa, no período de janeiro de 2009 a dezembro de 2014, e para a aferição da acurácia empírica dos modelos serão realizados testes fora da amostra, comparando os VaR obtidos para o período de janeiro a dezembro de 2014.
Foram introduzidas variáveis explicativas na tentativa de aprimorar os modelos e optou-se pelo correspondente americano do Ibovespa, o índice Dow Jones, por ter apresentado propriedades como: alta correlação, causalidade no sentido de Granger, e razão de log-verossimilhança significativa.
Uma das inovações do modelo de escala local é não utilizar diretamente a variância, mas sim a sua recíproca, chamada de “precisão” da série, que segue uma espécie de passeio aleatório multiplicativo.
O LSM captou todos os fatos estilizados das séries financeiras, e os resultados foram favoráveis a sua utilização, logo, o modelo torna-se uma alternativa de especificação eficiente e parcimoniosa para estimar e prever volatilidade, na medida em que possui apenas um parâmetro a ser estimado, o que representa uma mudança de paradigma em relação aos modelos de heterocedasticidade condicional.
Palavras-chave: Volatilidade, Modelo de escala local, heterocedasticidade condicional e Value at Risk.
6
Abstract
To estimate the parameters will be used daily prices of Ibovespa in the period from January 2009 to December 2014, and to measure the empirical accuracy of the models out of sample tests will be performed, comparing the VaR obtained for the period from January to December 2014.
Explanatory variables were introduced in an attempt to improve the models, and we chose to its corresponding American Ibovespa, the Dow Jones index, for presenting characteristics such as high correlation, causality in the Granger sense, and reason for significant log-likelihood.
One of the local scale model innovation is not directly use the variance, but its reciprocal, called "precision" series, which follows a kind of multiplicative random walk.
LSM captured all financial series of stylized facts, and the results were favorable to use, so the model becomes an efficient and economical alternative specification for estimating and predicting volatility, to the extent that only one parameter has to be estimated, which represents a paradigm shift in the models of conditional heteroscedasticity.
Keywords: Volatility, Local Scale Model, heteroscedasticity and Value at Risk.
7
LISTA DE TABELAS E ILUSTRAÇÕES
Gráficos
1	- Retornos diários Ibovespa 2009 a 2014
2	- Previsores 1 passo à frente para a variância
3	- VaR dentro da amostra LSM
4	- VaR dentro da amostra GARCH(1,1)
5	- VaR fora da amostra LSM
6	- VaR fora da amostra GARCH(1,1)
7	- VaR fora da amostra, com variáveis explicativas LSM
8	- VaR fora da amostra, com variáveis explicativas GARCH(1,1)
9	- Histograma dos retornos Ibovespa.
10	- Histograma dos resíduos padronizados
11	- Retornos ao quadrado
Tabelas
1	- Discrepância entre os valores de m e j
2	- Coeficientes estimados pelos modelos
3	- Teste de violações do VaR com preditiva Normal (amostra)
4	- Teste de violações do VaR com preditiva empírica (amostra)
5	- Teste de violações do VaR com preditiva t_Student (amostra)
6	- Teste de violações do VaR com preditiva Normal (amostra)
7	- Teste de violações do VaR com preditiva empírica (fora da amostra)
8	- Teste de violações do VaR com preditiva t_Student (fora da amostra)
9	- Teste de violações do VaR com preditiva Normal (fora da amostra, com variável explicativa)
10	- Teste de violações do VaR com preditiva empírica (fora da amostra, com variável explicativa)
11	- Teste violações do VaR com preditiva t_Student (fora da amostra, com variável explicativa)
12	- Teste de autocorrelação dos retornos
13	- Teste de autocorrelação dos retornos ao quadrado
14	- Teste de raiz unitária para os retornos
8
LISTA DE SIGLAS E ABREVIATURAS
ARCH	AUTOREGRESSIVE CONDITIONAL HETEROSKEDASTICITY
AR-SV	AUTOREGRESSIVE STOCHASTIC VOLATILITY
EMV	ESTIMADOR DE MÁXIMA VEROSSIMILHANÇA
EWMA	EXPONENTIALLY WEIGHTED MOVING AVERAGE
GARCH	GENERALIZED AUTOREGRESSIVE CONDITIONAL HETEROSKEDASTICITY
IGARCH	INTEGRATED GENERALIZED AUTOREGRESSIVE CONDITIONAL HETEROSKEDASTI-
CITY
JB	JARQUE-BERA
LLR	TESTE DA RAZÃO DE VEROSSIMILHANÇAS
LSM	MODELO DE ESCALA LOCAL
REQM	RAIZ QUADRADA DO ERRO MÉDIO
VaR	VALUE AT RISK
9
Sumário
1	Introduzindo séries financeiras	13
1.1	Dados utilizados.......................................................... 13
1.2	Volatilidade.............................................................. 14
1.3	Heterocedasticidade Condicional........................................... 15
1.4	Assimetria................................................................ 15
1.5	Curtose................................................................... 16
1.6	Teste de normalidade Jarque-Bera.......................................... 16
1.7	Retornos.................................................................. 16
1.8	Distribuição incondicional versus distribuição preditiva (condicional) ...	17
2	Value at Risk (VaR)	18
2.1	Controle do risco nos mercados financeiros................................ 18
2.2	Calculo do VaR............................................................ 19
2.3	Validando modelos com o VaR............................................... 19
2.4	O teste de Kupiec ........................................................ 20
2.5	Vantagens e desvantagens do Value at Risk................................. 20
2.6	Alternativa ao VaR ....................................................... 21
3	Modelos para volatilidade	22
3.1	Modelos de heterocedasticidade condicional................................ 22
3.1.1	ARCH (Autoregressive Conditional Heteroskedasticity) .............. 22
3.1.2	Modelos GARCH...................................................... 22
3.2	Modelos de volatilidade estocástica - (Stochastic Volatility Models) ..... 25
3.2.1	Abordagem Bayesiana ............................................... 25
3.2.2	O modelo de escala local: visão geral.............................. 26
3.2.3	Inicialização e solução “steady state” do filtro Gama.............. 33
3.2.4	Estimação de m..................................................... 34
3.2.5	Cálculo de ht ..................................................... 36
3.2.6	Variáveis explicativas na equação da precisão...................... 36
3.3	Aplicações do LSM a séries brasileiras.................................... 39
3.4	Modelo de escala local com variáveis explicativas: resultados ............ 40
10
4	Comparações entre o LSM e o GARCH(1,1)
4.1	Coeficientes estimados............................................... 41
4.2	Variâncias estimadas ................................................ 41
4.3	Cálculo do Value at Risk ............................................ 42
4.3.1	Testes contendo toda a amostra................................ 42
4.3.2	Teste fora da amostra......................................... 45
4.3.3	Considerações sobre	os VaR.................................... 48
4.4	Erros de previsão ................................................... 48
5	Conclusões	49
6	Extensão	49
7	Referências	50
Fim sumário
11
Apresentação
Dentre os modelos utilizados para a estimação e previsão da volatilidade a tempo discreto, devem ser destacados duas classes: modelos de heterocedasticidade condicional e modelos de volatilidade estocástica.
Os modelos de heterocedasticidade condicional partem do pressuposto que a variância da série, condicional à informação disponível até o instante anterior, varia ao longo do tempo, enquanto a variância incondicional permanece constante. A volatilidade aqui é uma variável observável, pois é completamente especificada, uma vez conhecidos os parâmetros do modelo.
Se, por outro lado, partirmos do pressuposto de que a variância da série é uma variável não observável, cuja evolução segue um processo estocástico, caímos na classe dos modelos de volatilidade estocástica. O modelo de escala local, a ser desenvolvido ao longo desta dissertação pertence a esta classe.
Desenvolvimento
No capítulo 1 serão tratados aspectos fundamentais para o estudo de séries financeiras: as principais definições, fatos estilizados e propriedades, assim como uma introdução ao cálculo da volatilidade. A metodologia a ser utilizada para a comparação dos modelos, o Value at Risk (VaR) será apresentada no capítulo 2.
Os modelos de escala local e ARCH/GARCH serão apresentados no capítulo 3. O capítulo 4 contém as análises e os resultados gerados, com e sem variáveis explicativas.
O capítulo 5 contém as conclusões e o capítulo 6, uma breve discussão sobre uma possível extensão das ideias desenvolvidas no trabalho.
Recursos computacionais
Para elaboração dos cálculos e gráficos decorrentes do estudo foram utilizados dois softwares:
Eviews- programa de estatística para Windows, usado para análises econométricas em geral; e “R” FoundationforStatisticalComputing - para a implementação do LSM.
12
-	se tomarmos a variância como medida desta dispersão em torno da média, o que é bastante razoável, podemos dizer que as séries financeiras apresentam variância variante no tempo.
-	retornos, em geral, não autocorrelacionados;
-	quadrados dos retornos autocorrelacionados;
-	agrupamentos de volatilidade;
-	distribuição incondicional dos retornos apresenta caudas mais pesadas que a distribuição Normal; e
-	embora aproximadamente simétricas, são leptocúrticas.
1.1	Dados utilizados
A escolha do período amostral pós crise 2008 deve-se à tentativa de se obter relativa estabilidade nos parâmetros, caso contrário, corria-se o risco de tornar os modelos mal especificados (ALEXANDER, 2005).
Análise da série de retornos
Através do gráfico dos retornos do Ibovespa para o período amostral foram identificadas algumas características peculiares às séries financeiras:
.08
.04
.00
-.04
-.08
-.12
I I I I I I IV I I I I I I IV I I I I I I IV I	I I I I I IV I I I I I I IV I I I I I I IV
2009	2010	2011	2012	2013	2014
Gráfico 1: Retornos diários Ibovespa 2009 a 2014
13
A primeira característica é que ao longo de todo o ano de 2009 observamos um agrupamento (cluster) de altas volatilidades, gerado pela instabilidade econômica pós crise de 2008.
Os picos isolados (outliers) também é uma fato estilizado de séries financeiras e pode representado no gráfico pela forte queda em 08/08/2011, primeiro dia de negociações após o rebaixamento da nota de crédito dos Estados Unidos no dia 05/08/11, pela agência Standard and Poor's. O Ibovespa acompanhou o movimento internacional e fechou com queda de 8,08%, aos 48.668 pontos.
Outra característica dos retornos, e que pode ser visualmente identificado, é que estes estão centrados na média (ou bem próximo), fato que será demonstrado ocorrer, quando analisarmos as estatísticas básicas da série, no anexo A deste estudo.
Antes de explorarmos os aspectos teóricos serão apresentadas as definições essenciais para o estudo de séries financeiras.
1.2	Volatilidade
Os modelos de volatilidade têm como principal objetivo prover uma métrica que pode ser utilizada na gestão de ativos, auxiliando na tomada de decisões financeiros, na montagem de carteiras e na formação de preço de derivativos, uma vez que serve como instrumento de orientação e mensuração dos riscos associados aos investimentos.
O desenvolvimento e o aprimoramento de modelos de volatilidade vêm se tornando uma das áreas de pesquisa mais importantes em finanças, provavelmente ter um extenso publico alvo, composto por investidores em portfólios, gestores de fundos de investimento, empresas com ações negociadas em bolsa, empresas que planejam lançamento de ações em bolsa e entidades reguladoras e fiscalizadoras dos mercados acionários.
Com o início da crise econômica mundial de 2008, que foi deflagrada inicialmente nos Estados Unidos, com a quebra do banco Lehman Brothers, a fragilidade das instituições financeiros tornou-se evidente. Com a falência de bancos houve a intervenção do governo a fim de evitar o colapso do sistema financeiro e uma recessão mais acentuada.
Se em 2008 os bancos sofreram com a exposição a hipotecas de alto risco, atualmente as instituições sentem os efeitos da exposição a títulos da dívida soberana de alguns países europeus, ou seja, a gestão do risco é uma preocupação constante, e no mercado brasileiro, previsão de volatilidade é particularmente importante, pois há evidências empíricas de que os ativos e índices brasileiros são consideravelmente mais voláteis do que seus correspondentes norte-americanos, europeus e japoneses.
O termo volatilidade designa a variabilidade temporal do grau de dispersão dos dados em torno de sua tendência central e considerando a medida de tendência central como a média, a variância (ou o desvio padrão) nos fornece este grau de dispersão, portanto, podemos definir volatilidade como a variação ao longo do tempo, da variância condicional de uma série.
14
O desvio padrão possui vantagem de ser expresso na mesma unidade da série analisada, todavia, para manter a consistência com a literatura, os modelos aqui discutidos serão expressos em termos da variância (precisão).
Uma definição geral calcula a volatilidade histórica para cada instante t, como uma média de k retornos passados, a saber:
r 1 k-1	q 1
U = k E I rt-J Ip '	(1)
kJ=0
ondep &gt; 0. Casos usuais são dep = 2 ep = 1.
No lugar de uma média, podemos calcular a volatilidade por meio de um procedimento EWMA (Exponentially Weighted Moving Average), procedimento usado pelo Riskmetrics.
1.3	Heterocedasticidade Condicional
Quando a variância é tratada como uma variável observável, condicional ao conjunto de informações disponíveis, e aos valores dos parâmetros estimados, é comum aludir à propriedade de volatilidade como heterocedasticidade condicional.
Os modelos clássicos de séries temporais são homocedásticos por construção, isto é, possuem variância condicional constante ao longo do tempo. Porém, em alguns casos, esta hipótese é inadequada, isto é, observa-se que a variância condicional da série varia no tempo.
Nas séries de retornos financeiros, observa-se um padrão específico de comportamento, expresso pelos chamados clusters (ou agrupamentos) de volatilidade.
A heterocedasticidade condicional será aprofundada na seção 3.1, que apresenta os modelos da família ARCH/GARCH.
1.4	Assimetria
Uma suposição muitas vezes utilizada é que os retornos rt sejam independentes, identicamente distribuídos e Normais (Gaussianos), contudo há argumentos contrários a essa suposição.
Para dados financeiros, quando se considera a distribuição amostrai dos retornos, nota-se que esta é aproximadamente simétrica.
Abaixo, a especificação da assimetria:
Seja X é uma variável aleatória qualquer, com média p e variância a2.
Então, a assimetria de X é definida por:
A(X) = E
(X - p )3 \
a3 J
(2)
15
1.5	Curtose
A curtose de X é definida por:
K (X) = E
)
(3)
K(X) — 3 será chamada de excesso de curtose, e uma característica das distribuições com caudas pesadas é ter curtose maior do que 3, podendo ser indefinida.
A distribuição Normal assumida para os retornos é a referência para a medição da curtose e possui curtose igual a 3, ou mesocurtose.
Valores de curtose menores que 3 são chamados de platicúrticos, e maiores, leptocúrticos.
O fato das séries financeiras serem bastante voláteis faz com que os retornos assumam valores extremos com razoável frequência.
A tradução deste fato, em termos probabilísticos é que a sua distribuição de probabilidade empírica possuirá caudas pesadas (fat tails), o que ocasionará excesso de curtose, ou leptocur-tose, sendo este, outro fato estilizado das séries financeiras.
Supondo-se que as séries financeiras sejam leptocúrticas, seria equivocado assumir que os retornos possuam distribuição Normal, embora por diversas vezes tal hipótese tenha sido considerada, por simplicidade.
Vis a vis, assumir uma distribuição Normal para os retornos equivale a assumir uma distribuição Log-normal para os preços, ou cotações.
1.6	Teste de normalidade Jarque-Bera
Sabemos que séries financeiras não apresentam distribuição Normal, no entanto, diversas teorias consideram tal distribuição por simplicidade. Neste trabalho, para verificação da normalidade dos retornos será realizado o teste de Jarque-Bera (JB).
A estatística do teste JB é definida em termos da amostra das estimativas da assimetria A(X) e do excesso de curtose K(X), baseadas no tamanho de amostra n :
JB = n[(A(X )2/6) + (K (X )2/24)]	(4)
que possui, assintoticamente, distribuição qui-quadrado com 2 graus de liberdade. A hipótese nula é de que os dados possuem distribuição Normal.
1.7	Retornos
Calculado em escala logarítmica, o retorno de uma ação rt é expresso como:
16
r =MP—O’
onde Pt é o preço do ativo em t.
1.8	Distribuição incondicional versus distribuição preditiva (condicional)
Em alguns casos, os retornos são observados condicionalmente Gaussianos. Isto significa que, dado o conjunto de informação até o instante t — 1, a distribuição no instante t será Normal. Porém, de um modo geral, a distribuição condicional também possui excesso de curtose.
De qualquer modo, a evidência empírica mais forte é a de leptocurtose da distribuição incondicional dos retornos. Um bom modelo para séries financeiras, portanto, deve possuir distribuição incondicional e, se possível, também a preditiva, leptocúrticas.
É importante saber diferenciar distribuição incondicional dos retornos (a qual sabemos possuir excesso de curtose) da distribuição preditiva (a qual, algumas vezes, é observada lepto-cúrtica).
Sabemos que séries temporais são geradas por realizações de processos estocásticos de parâmetro (tempo) discreto. Processos estocásticos, por sua vez, são conjuntos de variáveis aleatórias, as quais são supostas identicamente distribuídas para simplificar a análise.
Um caso particular importante ocorre quando as variáveis componentes do processo são descorrelatas. Neste caso, o processo é dito aleatório, ou ruído branco, e se além de descorrelatas, as variáveis seguem distribuição Normal, o processo é chamado ruído estritamente branco.
A distribuição incondicional dos retornos é a distribuição conjunta das variáveis do processo, isto é, dos elementos da série, ou dados dispostos horizontalmente.
A distribuição a cada instante de tempo, ou vertical, condicional ao conjunto de informação disponível até o instante anterior, é chamada distribuição preditiva ou condicional, na terminologia dos modelos ARCH/GARCH.
Neste estudo, o termo densidade preditiva será sempre referente à previsão 1 passo a frente das observações (retornos).
17
2	Value at Risk (VaR)
Intuitivamente, quanto mais volátil for a série de preços (ou retornos) associados a determinado ativo, maior será a incerteza quanto ao seu comportamento, e portanto, maior será o risco incorrido por sua negociação.
Prever corretamente a volatilidade significa dimensionar de modo acurado o risco assumido por cada operação, e por conseguinte, tomar decisões fundamentadas em estimativas cientificamente consistentes, de quanto se pode ganhar ou perder ao assumir determinada posição.
Na terminologia de análise de risco financeiro, isto significa dimensionar corretamente o seu Value At Risk, também conhecido no Brasil como Valor em Risco.
Melhorias estão sendo continuamente perseguidas para relacionar o capital regulatório, que deve estar disponível, com os riscos subjacentes incorridos pelas empresas. Desse modo, as regulações têm tido um papel muito importante no desenvolvimento das técnicas de medição do risco.
O Comitê da Basiléia de Supervisão Bancária, recomenda a adoção de dois tipos de modelos para se medir o risco de mercado em bases diárias.
Uma das abordagens é quantificar a perda máxima por meio de um grande conjunto de cenários dos movimentos dos fatores de risco ao longo de um determinado horizonte de tempo. A outra é ponderar os cenários com probabilidades e avaliar o nível de perda para uma dada probabilidade (baixa) que pode ser excedida ao longo de um horizonte de tempo fixado.
Ambas as abordagens assumem que a posição (ou o portifólio) não é alterado durante o horizonte de tempo fixado (ALEXANDER, 2005).
2.1	Controle do risco nos mercados financeiros
Os mercados financeiros são mais voláteis hoje em dia? Certamente sim. Uma das razões para isso é a habilidade crescente das instituições financeiras de criar alavancagem.
Os fundos de hedge podem alcançar posições altamente alavancadas porque seus modelos são supostamente projetos para diversificar a maior parte dos riscos. Novos produtos derivativos estão sendo continuamente estruturados para permitir que as companhias e os bancos possam aumentar a alavancagem de maneira mais engenhosas do que sempre fizeram.
Isso requer um acurado controle regulatório e a necessidade de sofisticação constante na metodologia de aferição de risco. Daí a relevância da volatilidade e do VaR para o mercado financeiro.
18
2.2	Calculo do VaR
O VaR é calculado a partir de um previsor (passos a frente) da variância, com base na estimativa intervalar obtida a partir dos percentis da distribuição preditiva, que fornece exatamente o queremos: os valores máximo e mínimo que o retorno pode atingir no instante seguinte, com determinada probabilidade.
Para modelos com distribuição preditiva simétrica (por exemplo: t-Student ou Normal), o VaR é simplesmente o produto do valor crítico para uma dada probabilidade de cobertura, pelo previsor do desvio padrão, ou desvio padrão condicional. Por exemplo, para a distribuição preditiva Normal, com probabilidade de cobertura de 95%, o VaR é dado por 1,96at|t—1 , onde 1,96 e o valor crítico da Normal a 2,5%, ou o valor ”x” para o qual P(X &gt; x) = 2,5%, sendo que ct2t_1 é o previsor da variância, obtido por um dos modelos de volatilidade disponíveis na literatura.
Para modelos cuja distribuição preditiva não é simétrica, é necessário calcular os seus percentis. Para uma probabilidade de 95%, por exemplo, devemos calcular os percentis de 2,5% e 97,5%.
Informalmente, o VaR é uma medida da variação potencial máxima do valor de um ativo (ou carteira), em um período pré-fixado, com dada probabilidade. Ou seja, quanto se pode perder, com probabilidade ”p”, em um horizonte ”s” fixado (em séries financeiras, geralmente 1 passo a frente).
Do ponto de vista de uma empresa, o VaR é uma medida de perda associada a um evento extremo, sob condições normais de mercado.
Maiores detalhes em Moretin (2011).
2.3	Validando modelos com o VaR
O número total de perdas excepcionais (violações em relação ao VaR) pode ser considerado como uma variável aleatória que tem distribuição binomial. A probabilidade de “sucesso” (perda excepcional) é de p = 0,01 para um VaR a 1%, e o número de n-tentativas é o número de dias do teste retroativo, por exemplo n = 1000. Então, o número esperado de perdas excepcionais é de np = 10 e a variância das perdas excepcionais é de np(1 — p) = 9,9. Assim, o desvio
padrão
146.
Usando-se o fato de que a distribuição binomial é aproximadamente Normal, quando n for grande e quando p for pequeno, então o intervalo de confiança a 99% do número de perdas excepcionais, assumindo-se que o modelo VaR seja preciso, será dado, aproximadamente por:
(np — Z0,00^/(np(1 — p)); np + Z0,00^/(np(1 — p))).
A Emenda de 1996 ao Acordo da Basiléia descreve a forma dos teste retroativos que devem ser adotados pelas empresas que desejam usar o modelo VaR.
19
It+i(a) = ’
Os reguladores recomendam o uso dos últimos 250 dias de dados no teste retroativo do VaR de um dia a 1%.
O modelo deve ser testado retroativamente contra perdas teóricas e verdadeiras e conforme será mencionado na seção seguinte, um modelo ajustado é aquele que fornece sistematicamente resultados de violações estatisticamente semelhantes ao da perda teórica.
2.4	O teste de Kupiec
Denote por xt,t-1 a perda/ganho em um período.
Defina a função indicadora por:
1 se xt t+1 &amp;lt;-VaR t (a) 0 se xt t+1 &gt; -VaR t (a)
O problema de acurácia de um modelo de VaR consiste em verificar se a sequência indicadora satisfaz as seguintes propriedades:
Cobertura Incondicional: P(It+1(a)) = 1) = a.
Independência: It+j(a) , It+k(a) devem ser independentes.
Essas duas propriedades são equivalentes a It (a) ~ B(a).
O teste de Kupiec concentra-se apenas na primeira propriedade. A estatística de teste (razão de Max verossimilhança) assintoticamente equivale a uma qui-quadrado com 1 grau de liberdade, onde o valor crítico para a x2(0.95%,1) = 3,84.
Maiores detalhes em Jorion (2006).
2.5	Vantagens e desvantagens do Value at Risk
Desde que os reguladores impuseram exigências de capital mínimo para cobrir os riscos de mercado, o VaR tornou-se uma medida de risco onipresente. Isso tem muitas vantagens.
O VaR pode ser usado para comparar os riscos de mercado de todos os tipos de atividade de uma empresa e fornece uma medida única, que é facilmente entendida pelos administradores. O conceito do VaR pode ser estendido a outros tipos de risco, especialmente os riscos de crédito e operacional.
Contudo, existem muitas desvantagens no uso do VaR. Por exemplo, ele não distingue as diferenças de liquidez das posições de mercado. De fato, este método só captura riscos de curto prazo, em circunstâncias normais de mercado.
Os modelos de VaR podem estar baseados em hipóteses não seguras e alguns riscos, tais como os custos das operações compromissadas são ignorados.
Além disso, pode-se observar que as medidas do VaR são muito imprecisas porque dependem de muitas hipóteses acerca dos parâmetros dos modelos que podem ser muito difíceis
20
de serem sustentadas.
2.6	Alternativa ao VaR
Em virtude das limitações apresentadas em 2.5 surgiram alternativas ao VaR tradicional.
Umas delas, proposta por Artzner (1997), introduziu uma medida de risco chamada VaR condicional, que é uma medida coerente de risco, e que tem uma relação simples com a VaR comum.
O VaR comum corresponde ao mais baixo percentil da distribuição teórica dos lucros e perdas, o nível limite de perda que divide a cauda mais baixa da distribuição. O VaR condicional é a perda esperada, dado que esta excede o limite do VaR. Ou seja:
VaR condicional = E(X/X &gt; VaR).
O VaR condicional representa a perda excessiva média geradas pelos picos do modelo. (ALEXANDER, 2005).
21
3	Modelos para volatilidade
Para os retornos: rt = ln(Pt) — ln(Pt—1),
Para a média condicional p.t = E(rt | Ft—1), e
Para a variância condicional at2 = Var(rt | Ft—1), onde Ft—1 denotará a informação até o instante t — 1, que consideraremos ser {rt—1,rt—2,...,r1}.
Em algumas situações iremos supor que u = 0, de modo que neste caso a2 = E(rj | Ft—1).
3.1	Modelos de heterocedasticidade condicional
Engle (1982) mostrou ser possível modelar, simultaneamente, a média e a variância de uma série, e para isso, lançou as bases do conceito de variância condicional, que deve ser modelada como um termo autorregressivo.
A ideia básica é que o retorno rt é não correlacionado serialmente, mas a volatilidade (variância condicional) depende de retornos passados por meio de uma função quadrática.
3.1.1	ARCH (Autoregressive Conditional Heteroskedastícity)
O processo ARCH(m) captura a heterocedasticidade condicional dos retornos financeiros admitindo que a variância condicional de hoje seja a média ponderada dos retornos não-esperados ao quadrado:
2 2 2
Cj = ao + «I fi2 I + ... + «m F—m	(5)
onde £t | lt ~ N(0, at2) e ao &gt; 0, ai &gt; 0, i = 1,..., m
Pela própria definição, valores grandes de rt são seguidos por outros valores grandes da séries, e devido ao fato de termos retornos ao quadrado, alguns retornos grandes e isolados podem conduzir a super previsões.
Os modelos ARCH não são frequentemente usados nos mercados financeiros porque o modelo GARCH simples funciona muito melhor.
3.1.2	Modelos GARCH
Uma generalização dos modelos ARCH é o chamado GARCH (de generalized ARCH), ou GARCH (m, n), no qual a variância condicional de n, no instante t, depende não somente de
22
perturbações passadas ao quadrado, mas também de variâncias condicionais passadas, ou seja, são heterocedásticos condicionais, e têm uma variância incondicional constante.
Assim como um modelo ARMA pode ser mais parcimonioso no sentido de apresentar menos parâmetros do que o modelo AR ou MA isoladamente, do mesmo modo, um modelo GARCH pode ser usado para descrever a volatilidade com menos parâmetros do que um modelo ARCH.
Alexander (2005) menciona que, em um modelo GARCH é admitida a hipótese de que os retornos sejam gerados por um processo estocástico. Em vez de se modelar os dados após eles terem sido colapsados em uma única distribuição de probabilidade incondicional, os modelos GARCH introduzem hipóteses mais detalhadas sobre as distribuições condicionais dos retornos. Essas distribuições condicionais mudam ao longo do tempo de um modo autocorrelacionado.
Equação da média e da variância condicional
A ideia fundamental do modelo GARCH consiste em adicionar uma segunda equação ao modelo de regressão padrão: a equação da variância condicional.
Essa equação descreve a evolução da variância condicional do processo do retorno não-esperado, Vt(et) = a2.
A variável dependente, o insumo do modelo GARCH da volatilidade é composta sempre por uma série de retornos. Desse modo, o modelo consiste em duas equações.
A primeira é a equação da média condicional, que usaremos a mais simples possível, ou seja:
rt = c + £t .
Nesse caso, o retorno não esperado £t é exatamente o desvio médio do retorno, pois a constante é a média dos retornos ao longo do período amostral. Se houvesse uma autocorrelação significativa dos retornos (que na seção 7 foi demonstrado não ocorrer), usaríamos uma média condicional autorregressiva, que na maioria dos casos, um modelo AR(1) seria suficiente.
A segunda equação do modelo GARCH é a equação da variância condicional, onde para um modelo GARCH (m, n):
= a0 + a1£t-1 + ... + amet-m +	+ ... + lpa' n
23
O GARCH (1,1)
A escolha do modelo GARCH a ser empregado não é simples, no entanto, a literatura consagra o modelo com uma defasagem para ambos os parâmetros, como o que melhor captura os grupamentos de volatilidade do mercado, e segundo Morettin (2011), a tipificação mais comum nos mercados é o modelo GARCH (1,1).
Ademais, o modelo GARCH(1,1) apresentou o melhor resultado nos testes de critério de informação dentre os modelos da família GARCH, e por esta razão será o modelo utilizado para a comparação com o modelo de escala local.
Sua especificação para média será:
rt = c + et,	(6)
e para a variância:
a,2 = rn + ae— + pa^,	(7)
onde os parâmetros são estimados por máxima verossimilhança.
Extensões do modelo GARCH
Os modelos ARCH e GARCH tratam simetricamente os retornos, pois a volatilidade é uma função quadrática dos mesmos. Mas também e sabido que a volatilidade reage de forma assimétrica aos retornos, tendendo a ser maior para retornos negativos. Por esta razão Nelson (1992), em “Filtring and forecasting with misspecified ARCH models”, introduziu os modelos EGARCH, um dos primeiros modelos GARCH assimétricos.
No pacote Eviews há como se verificar o impacto de informação assimétrica, onde esperase que “bad news” tenham impacto maior nas séries financeiras. Para o caso em que o processo da variância não é estacionária, utiliza-se o IGARCH, que tem a mesma especificação do GARCH, com uma restrição, a de que a soma dos coeficientes seja igual a 1.
Além dos modelos mencionados nessa seção, há uma variedade grande de modelos da família ARCH que são considerados na literatura, no entanto, o estudo das extensões do modelo GARCH não será abordado neste trabalho.
24
3.2	Modelos de volatilidade estocástica - (Stochastic Volatility Models)
Os modelos da família GARCH supõem que a variância condicional depende de retornos passados. Os modelo de volatilidade estocástica não fazem essa suposição. Esses modelos têm como premissa o fato de que a volatilidade presente depende de seus valores passados, mas é independente dos retornos passados, ou seja, a variância condicional segue um processo estocástico não observável (a volatilidade é considerada como variável latente).
Em termos da componente estocástica (ruído), não há uma conclusão fechada a respeito da forma que ela deva tomar. Pode-se pensar em ruídos aditivos (somados ao processo), como no AR(1)-SV, e multiplicativos. Uma das propostas subjacentes a este trabalho é testar a validade de um ruído multiplicativo.
Também quanto a distribuição probabilística a ser assumida para o ruído, pode-se fazer conjecturas originais, em especial quando se trabalha com ruído multiplicativo.
Na verdade, a especificação do filtro Gama impõe um processo multiplicativo e um ruído seguindo uma distribuição Beta. Tal fato resulta da propriedade de conjugação para as distribuições Gama e Beta.
De um modo geral, a ideia de volatilidade estocástica parece bastante atraente para descrever o comportamento de séries de retornos financeiros, uma vez que tais modelos incorporam, além do efeito da dependência em valores passados da volatilidade, o efeitos de choques aleatórios.
É sabido que uma diversidade de notícias, declarações e intervenções governamentais interferem fortemente no comportamento do mercado financeiro. Se observamos que tais fatores ocorrem com certa aleatoriedade, torna-se razoável considerá-los como choques aleatórios, evidenciando a importância dos referidos modelos.
Muitas formulações de modelos de volatilidade estocástica são encontradas na literatura, dentre as quais exploraremos o modelo de escala local.
3.2.1	Abordagem Bayesiana
A principal diferença entre o enfoque Bayesiano e o enfoque clássico é que, no enfoque clássico, os parâmetros sobre os quais queremos fazer a inferência são considerados constante. Isto é, obtemos estimadores pontuais e, a partir da distribuição de tais estimadores, e de uma medida de incerteza associada (erro padrão), obtemos intervalos de confiança para esses parâmetros.
Quando não conhecemos a distribuição exata do estimador, como foi o caso aqui, ou temos acesso a ela apenas assintoticamente, o Bootstrap é, quase sempre, uma solução viável para a obtenção de intervalos de confiança.
Já no enfoque Bayesiano, são especificadas distribuições à priori para os parâmetros. Isto é, m é agora uma variável aleatória. O tratamento Bayesiano do nosso problema, portanto,
25
seria considerar que m possui determinada distribuição de probabilidade, baseado em alguma intuição a respeito de seu comportamento.
A abordagem que será utilizada ao longo do trabalho será essencialmente clássica. Isto quer dizer que nenhuma incerteza foi incorporada a respeito do vetor paramétrico 8. Visando simplificar a análise, vamos concentrar a discussão sobre o parâmetro m.
3.2.2	O modelo de escala local: visão geral
Neil Shephard (1994) publicou “The Local Scale Models - State space alternative to integrated GARCH processes”, onde apresentou o modelo de escala local, e em “Stochastic Volatility: Likelihood Inference and comparasion with ARCH models”(1998), lançou as bases para a comparação entre os modelos de heterocedasticidade condicional e modelos de volatilidade estocástica.
Principais características
A densidade das observações, condicional à precisão (densidade de medida) é suposta Normal.
Sabendo-se que, para verossimilhança Normal, a densidade Gama é conjugada, torna-se possível obter um filtro exato. Portanto, condicional às observações, a precisão seguirá uma distribuição Gama, de tal forma que a especificação multiplicativa e a distribuição Beta para o ruído surgirão naturalmente.
Sendo a densidade das observações uma Normal, e a precisão Gama, a densidade preditiva resulta t-Student.
O fator de locação da densidade preditiva será zero, e o seu fator de escala, que é a previsão 1 passo à frente da variância será aproximadamente uma média móvel com amortecimento exponencial (EWMA), no quadrado das observações passadas, a mesma função de previsão 1 passo a frente para a variância condicional no método de amortecimento exponencial e no modelo IGARCH(1,1).
A grande vantagem do modelo de escala local reside no fato de que a solução encontrada para o filtro é exata, o que não é frequente entre os modelos de volatilidade estocástica.
A estimativa pontual para o fator de amortecimento m será obtida por máxima verossimilhança.
Neste sentido, o modelo pode ser visto como uma alternativa, no contexto dos modelos estruturais, ao modelo IGARCH(1,1), como propôs Shephard.
Na verdade, o modelo não trabalha diretamente com a variância, mas com a sua recíproca, a precisão, que será suposta evoluir segundo um processo estocástico markoviano, com ruído multiplicativo.
26
A incerteza a respeito da variância não observável, implica que a densidade preditiva das observações possua distribuição t de Student, cujos graus de liberdade serão determinados pelo fator de amortecimento exponencial estimado.
A distribuição Gama será adotada para a precisão, pois tal densidade possui a desejável propriedade de conjugação (isto é, priori e posteriori na mesma família). Além disso, existe uma propriedade fundamental de variáveis Gama e Beta, que fornece subsídio para a especificação multiplicativa.
A especificação multiplicativa subentende um passeio aleatório no logaritmo da precisão.
O modelo é capaz de lidar com espaçamento irregular de observações, o que permite efetuar previsões ”s” passos à frente, s &gt; 1, e pode ser generalizado para, por exemplo, lidar com variáveis explicativas na variância (precisão), o que será feito na seção 3.2.6.
As próximas seções serão dedicadas ao desenvolvimento e a discussão dos aspectos teóricos inerentes ao LSM.
A precisão
A precisão será tratada como componente não observável no modelo de escala local, e se pensarmos em termos de representação em espaço de estados, ela será a nossa variável de estado. Ao longo desta seção serão discutidas algumas características desta variável, tais como a sua função no modelo, sua distribuição e sua evolução.
Densidade de medida
No modelo de escala local, a densidade das observações, condicional à precisão é Normal, isto é:
rt | dt - N(0, 0-1),
onde rt é a observação no instante t e 0t, a precisão em t (cuja inversa é a variância). Esta densidade será chamada densidade de medida.
Para determinarmos a densidade preditiva das observações, que é o que realmente se pretende, precisamos da distribuição de 0t|t-1, isto é, a priori de 0.
Discutiremos esta distribuição.
O porquê da distribuição Gama: a propriedade de conjugação
A utilização de informação a priori em inferência Bayesiana requer a especificação de uma distribuição a priori para a quantidade de interesse 0. Esta distribuição deve representar (probabilisticamente) o conhecimento que se tem sobre 0 antes da realização do experimento.
27
Uma propriedade bastante desejável em modelos Bayesianos é a propriedade de conjugação, ou seja, que a priori e a posteriori pertençam a mesma família de distribuições.
Quando isto acontece, priori e a posteriori são ditas conjugadas.
Por verossimilhança Normal, onde a precisão é uma variável aleatória (como é o caso), é simples demonstrar que a distribuição Gama é conjugada (ou seja: priori Gama +verossimilhança Normal = posteriori Gama).
Além disso, uma variável Gama assume somente valores estritamente positivos, como de fato ocorre com a precisão.
Portanto é razoável considerar que a precisão condicional seja gerada por uma distribuição Gama.
A priori para a precisão será designada por:
|t-i - G(at\t-i,bt\t-i),
onde at|t—1 e bt|t—1 são respectivamente os parâmetros de forma, e de escala, da distribuição Gama à priori, que serão determinados nesta seção.
Da propriedade de conjugação, a posteriori fica:
fyt|t — G(at, bt)
(8)
agora at e bt são respectivamente os parâmetros de forma, e de escala, da distribuição Gama à posteriori, que também serão determinados nesta seção.
Aqui, dt |t e dt |t-1
estão designando, respectivamente, E(6t | Rt) e E(fy | Rt—1) , onde R
t-1
denota toda a informação disponível até o instante t — 1.
Ou seja: Rt—1 = {rt—1,rt—2,...,U}.
Da mesma forma, Rt = {rt, rt—1, rt—2,..., u}.
Equações de atualização e previsão
A partir das considerações anteriores, estamos aptos a escrever as equações do nosso filtro Gama. Isto equivale a escrever as expressões para a previsão e atualização dos parâmetros de forma e escala das distribuições para as densidades Gama a priori e a posteriori da precisão.
Se X ~ G(p, Â), Y ~ B(a, p — a) e Z = XY, então Z ~ G(a, Â), isto é:
X - G(p,Â),Y - B(a,p — a) =^ Z - G(a,Â)
(9)
28
Portanto, se especificarmos os parâmetros da posteriori, em t — 1, como at—1 e bt—1, como em 8, temos de 9 e 10 (equação de transição especificada em Shephard (1994), e transcrita abaixo):
dt = eht 6t—1Vt	nt ~ Beta(a&gt;at—1, (1 — a&gt;)at—1)	(10)
que os parâmetros da priori serão dados por:
at\t — = m at _i	(11)
bt\t-i = e-htbt-i	(12)
de tal forma que podemos reescrevê-la como: 0t|t-1 — G(at|t_i,bt|t-1) = G(rnat-1,e _hbt—1), então pelo teorema de Bayes, temos para os parâmetros da posteriori:
at = at |t_1 + 1/2	(13)
bt = bt\t _1 + (r2/2)	(14)
então, a equação 8 pode também ser reescrita com
e,|, - G(a,, b,) = G(a,|,_1 + 1/2, b,t,_1 + (r?/2)).	(15)
Os parâmetros de forma e escala da priori e da posteriori (no instante t), at\t_bbt|t_i,at e bt serão gerados sequencialmente (isto é, a cada observação chegada, ou iteração), pelo filtro definido pelas equações 11, 12, 13e 14.
Estimação da precisão
As médias a priori (previsores), e a posteriori (atualizadores) da precisão serão dadas por at|t_1/bt|t_1 e at/bt, pois o valor esperado de uma distribuição Gama é simplesmente a razão entre o parâmetro de forma e o parâmetro de escala.
Neste item vamos obter uma forma fechada para o estimador da precisão, dado por:
E(0t | Rt) = at/bt, Rt = {rt,rt_1,rí_2,...,rj.
Para isso, vamos obter expressões fechadas para at e bt, a partir das recursões do filtro, dadas pelas equações 11, 12, 13 e 14.
Se arbitrarmos a1 = 1/2 e b1 = r\/2, como valores iniciais, onde r1 é a primeira observação da série (a justificativa para impôr tais valores será vista na seção 3.2.3), e efetuarmos as recursões do filtro, obtemos:
29
t—1 at = (E a i)/2 = (1 — a1 )/2(1 — a)
i=0
t—1
b, = (E air2—)/2	(17)
i=0
Daí, pelo argumento apresentado no início deste item, podemos escrever uma forma fechada para o atualizados da precisão em t dada por:
t—1	t—1	i—1
E(dt | Rt) = at/bt = (E ai)/(E ^—ir—),	= n e—ht—j	(18)
i=0	i=0	j=0
Se a é maior do que 0,8, então e—ht será bem próximo de a, o que fornece uma aproximação para a equação 19, nos casos práticos, portanto, o erro em que se incorre por considerar a referida aproximação para valores de a maiores do que 0,8 (os quais ocorrerão na prática), é menor do que 5%.
a	e	ht—j	discrepância %
0,5	0,0007	-99,86%
0,7	0,6144	-12,23%
0,8	0,7609	-4,88%
0,9	0,8900	-1,11%
0,95	0,9475	-0,25%
0,99	0,9899	-0,01%
Tabela 1: Discrepância entre os valores de a e e ht—i
Para os valores obtidos para as séries utilizadas neste trabalho, em torno de 0,935, o erro será consideravelmente menor do que 1%.
A aproximação, portanto, é excelente, e substituindo e—ht—j por a na equação 18, ficamos com a expressão para o atualizador da precisão dada pela inversa de uma EWMA dos quadrados das observações (CAMPOS E., 1998).
Isto é, a equação 18 se torna:
E(dt | Rt) = at/bt = (Ei=0 al~)/(Ei=0 ^—ir—) , &amp;amp;—i = nj=o a = a1 então,
at/bt = (E= a i)/(Ei;0 a
= (1 — a)/(1 — a)/(Eti=0 air^‘—i)
30
= (1 - at )/[(1 - a )(Lt=0 ® ‘r2-)¡
= (1 — a‘)/[(1 - a)(r,2 + ar2-1 + a2r2-2 + -))•
Como (1 — ®t)/(1 — a) converge para 1/(1 — a), quando a &amp;lt;1 , a equação acima converge para:
1/[(1 — a )(r2 + a rt2—1 + a2 r;2—2 + •••)],
que é a inversa de uma EWMA, com fator de amortecimento exponencial dos quadrado dos retornos dado por a.
Então, podemos escrever:
a;/b; —&gt; 1/[(1 — a)(r2 + ar— 1 + a2r;2—2 +...)], a &gt; 0,8	(19)
Estimação da variância
Se a precisão segue distribuição Gama (a, p), a variância segue distribuição Gama inversa, cujo valor esperado é dado por p/(a — 1). Portanto, o estimador não tendencioso para a variância é dado por:
SNt = E (a2 = e~1) = b;/(a; — 1)	(20)
Porém, neste trabalho será utilizado um estimador tendencioso, dado pela inversa da estimativa da precisão. Isto é:
S2t = b;/a;	(21)
A razão para tal procedimento é que o Viés, ou (Bias) desse estimador é pequeno, ao passo que, utilizando esse estimador, percebemos que a variância não observável do modelo é estimada (se a for alto) via média móvel, conforme a equação 19.
Muitas instituições financeiras utilizam o método do amortecimento exponencial com fator de decaimento de 0,94 para estimar volatilidade. Valor muito próximo aos obtidos neste trabalho para o parâmetro a.
Também, os modelos IGARCH utilizam uma EWMA como função de previsão 1 passo à frente para a variância. Portanto, estimar a variância deste modo, significa manter a consistência com os demais modelos disponíveis na literatura, e que são utilizados pelos mercados na prática.
31
Da equação 17 obtemos o valor limite de at, isto é:
Viés assintótico do estimador da variância
t—1
at = (£ al)/2 = (1 — a)/2( 1 — a)	■ 1/[2( 1 — a)]	(22)
i=0
Se por exemplo, a = 0,9, at —&gt; 5 pela equação 22, de tal forma que bt/at = bt/5. Por outro lado, o estimador não tendencioso dado pela equação 20, resulta bt/4.
O Viés assintótico, neste caso, é dado por bt/4 — bt/5 = bt/20, que é um valor bastante reduzido, uma vez que bt é determinado pelas observações, cuja magnitude, no presente caso, é pequena (estamos trabalhando com retornos financeiros, expressos na linha dos números reais).
Em suma, utilizamos a inversa da média condicional da precisão para estimar a variância (um estimador tendencioso, pois esta não é a média condicional da variância), para obtermos uma EWMA nos quadrados das observações passadas.
Se utilizássemos o estimador não tendencioso, dado pela média da distribuição Gama inversa (a verdadeira distribuição da variância), não obteríamos tal função de previsão. Ademais, o erro incorrido por tal procedimento é consideravelmente pequeno.
Estimativa do desvio padrão
Como estimativa do desvio padrão, tomamos a raiz quadrada da estimativa da variância .Isto também é uma aproximação, que visa simplificar a análise, pois, a rigor, a raiz quadrada da média não é a média da distribuição da raiz quadrada, porém, tal procedimento é usual em modelos de volatilidade.
a e a série de precisões (variâncias) estimadas
A taxa de variação da precisão (e da variância) ao longo do tempo é monitorada pelo fator de amortecimento a.
Quanto maior o valor de a, menor será a taxa de variação da precisão, isto é, 0t mudará mais lentamente. Tal resultado é intuitivo, pois quanto maior o fator de amortecimento, maior ponderação relativa estará sendo dada às observações mais recentes, no cálculo da precisão.
Portanto, um valor de a muito próximo de 1 modela um padrão de heterocedasticidade bem comportado (suave), enquanto um valor muito baixo (menor do que 0,8) indica uma grande aleatoriedade do processo gerador da variância (comportamento nervoso).
Em outras palavras, quanto maior o valor de a, mais estável será a série de precisões (e variâncias) estimadas.
32
Particularmente, no caso de séries financeiras, é de se esperar que os atualizadores e previsores dos parâmetros da distribuição da precisão dos retornos sejam bastante estáveis, de modo que a evidência empírica sugere um M próximo (mas não muito) da unidade. No caso da série de variâncias estimadas de retornos financeiros, valores de m em torno de 0,94 são razoáveis.
Todavia, um método formal para a determinação de m será descrito na seção 3.2.4.
3.2.3	Inicialização e solução “steady state” do filtro Gama
Inicialização: priori difusa
A inicialização do filtro Gama faz parte de um problema genérico em modelos de espaço de estados.
No ambiente Gaussiano, este problema é conhecido como inicialização do filtro de Kalman, o qual possui várias soluções, dependendo das características de estacionariedade, ou não, das componentes do vetor de estado.
Em se tratando de componentes não estacionárias, uma das soluções adotadas é inicializar o filtro de Kalman com uma distribuição a priori difusa (priori dif use).
Distribuições difusas, bastante comuns em estatísticas Bayesiana são distribuições impróprias, ou seja, que não integram a um.
No caso do filtro de Kalman é utilizado uma Normal difusa, isto é:
00 ~ N(a.0,kI), k —&gt;
onde 00 é o vetor de estado em t = 0.
Sabemos, que a variância é uma medida de incerteza, portanto, utilizar uma Normal com variância infinita para inicializar o filtro Kalman, significa assumir total ignorância, a priori, acerca dos momentos da distribuição do vetor de estado.
Mais detalhes sobre a inicialização do filtro de Kalman em Harvey (1989).
No contexto do filtro Gama iremos adotar uma posição clássica, no sentido de que o valor inicial da precisão seja também difuso.
Uma solução possível é fazer:
00 = G(a0 = 0, b0 = 0).
Embora esta distribuição seja imprópria em t = 1, após t = 1, a densidade 0t | Rt_1 ~ G(at|t_1, bt|t-1) será bem definida, se r1 = 0. Assim podemos inicializar o cálculo da função de verossimilhança em t = 2, se r1 = 0. Se r1 = 0, podemos iniciar em t = 3, etc...
De um modo geral, o cálculo da função de verossimilhança deverá ser iniciado em t = t' + 1, onde t' é o instante que ocorre a primeira observação não nula.
33
Todavia, utilizando as equações de previsão 11 e 12, é fácil deduzir que a inicialização proposta em 3.2.2, ou seja, a1 = 1/2 e b1 = r^/2, é equivalente a inicializar o filtro como com:
00 = G(a.0 = 0, b0 = 0).
Em outras palavras, inicializar o filtro em t = 0, com a0 = 0 e b0 = 0, equivale a arbitrar em t = 1, a1 = 1/2 e b1 = r^/2, se r1 = 0.
Solução “steady state”
O equivalente à solução “steady state” do filtro de Kalman, obtida pela solução algébrica da equação de Riccatti, também pode ser obtido para o filtro Gama.
Da equação 22:
at —&gt; 1/2(1 — a).
A solução steady state para o filtro Gama é então:
at —&gt; 1/2(1 — a) = a.
Se a = 0,9, at converge para a = 5.
3.2.4	Estimação de a
O problema
Determinar o valor de a é de suma importância, pois tal valor monitora a forma da densidade preditiva.
Nesta seção será descrito brevemente um procedimento simples para a determinação do a.
Já foi visto que quando a é menor que a unidade, então podemos assegurar, conforme a equação abaixo sugere, que at|t—1convergirá, qualquer que seja o valor inicial.
df = 2at\t—1 = a (1 — at—1)/(1 — a)	(23)
onde df são os graus de liberdade da distribuição t-Student, conforme mencionado em 3.2.2.
Em outras palavras, para a &amp;lt;1, o filtro gera estimativas estáveis para at|t—1, e o número de graus de liberdade também convergirá.
Realmente, como visto em 23, a equação fornece o valor limite de df —&gt; «&gt;, por:
34
De qualquer modo, o que importa é que os graus de liberdade da distribuição preditiva dependem apenas de m.
Como a distribuição t-Student é completamente especificada por seus graus de liberdade, concluímos que, uma vez determinado ®, a preditiva estará completamente especificada.
Isto leva a pensar em construir a função de verossimilhança, em termos do parâmetro desconhecido m.
Expressão e maximização da log-verossimilhança
A partir das considerações efetuadas no item anterior, podemos escrever a forma analítica da função de verossimilhança.
Desta feita, pode-se maximizar a função de verossimilhança com respeito a m, utilizando um algoritmo de otimização adequado, e obter o estimador de máxima verossimilhança.
Este procedimento será efetuado neste trabalho.
Nota-se, no entanto, que o formato da densidade preditiva nos conduz, por simplicidade, a tratar com o logaritmo natural da função de verossimilhança, ou seja, a função de log-verossimilhança.
Como a função logarítmica é monotonicamente crescente, maximizar a log-verossimilhança é equivalente a maximizar a verossimilhança.
Portanto, a função de log-verossimilhança será a função objetivo, no procedimento de determinação do estimador de máxima verossimilhança (EMV) de m.
Obviamente, trata-se de um problema de maximização (não linear) restrita, pois m, por definição, está situado entre 0 e 1.
A expressão da função de log-verossimilhança, a menos de uma constante, é dada por:
f ÍM) = E[atlog(btlt-1/bt) - (1/2)log(bt|t-1) + log(r(at)/r(at|t-1))]	(25)
i=1
e o problema a ser solucionado é uma busca não linear unidimensional, isto é:
Max f (rn)
s.a m E (0,1).
35
3.2.5	Cálculo de ht
O algoritmo numérico para a determinação de ht é:
ht = -E (log(n)),
sendo que nt é um ruído multiplicativo.
O grande problema aqui é que os momentos do logaritmo de uma variável Beta (particularmente o primeiro momento, que é o que nos interessa aqui), não possuem uma expressão analítica fechada.
Desta feita, deve-se procurar um algoritmo que forneça tal expectância.
A expressão final para a aproximação de E(nt), é:
E (nt )= log(o) - (1/2)((1 - rn)/rnat-1) - £ ;= (B2n/2n)(1 - rn)/rnat-i))2n,	(28)
onde B2,B4,B6... são os números de Bernoulli, que é a expressão para -ht, a qual será computada a cada iteração.
Shephard (1994), demonstra que quatro termos no somatório deverão ser suficientes para produzir resultados acurados. Neste trabalho foram usados 6 termos.
Tendo em vista que at é função apenas de m (equação 16), veremos que -ht será apenas função deste parâmetro.
Os cálculos utilizados para cálculo de ht encontra-se no interior do algoritmo que maximiza a função de log-verossimilhança, que será apresentado na seção 3.3.
3.2.6	Variáveis explicativas na equação da precisão
O modelo de escala local simples (sem variáveis explicativas) e padrão (com densidade de medida Normal) possui apenas 1 parâmetro a ser estimado.
Podemos trivialmente incorporar variáveis explicativas na equação da precisão, e estimar o seu efeito, aumento a dimensão do espaço paramétrico.
Tal procedimento não é trivial nos modelos ARCH/GARCH, sendo esta uma das vantagem do modelo de escala local.
Na dedução que segue, Xt denota o vetor de variáveis explicativas, isto é, Xt = (x1t, x2t,..., xLt), onde os xit, (i = 1,2,...,L), são as L variáveis explicativas.
Y denota o vetor de coeficientes das variáveis, que representam seu efeito sobre a precisão, Y = (Y1, Y2,..., Yl) ', onde Yi é o coeficiente de xü.
A precisão explicada por Xt será denotada por 0t+.
36
Assim, podemos incorporar o efeito das variáveis explicativas em 0t, pela seguinte equa
ção:
e+ = OteYXt,	(29)
com —&amp;lt;» &amp;lt;Y &amp;lt;&lt;x&gt;. Ou seja, está sendo incorporado o efeito de Xt por uma especificação multiplicativa e utilizando uma função de ligação exponencial, bastante utilizada no contexto dos modelos lineares generalizados.
Existem 2 razões fundamentais para considerar a função de ligação exponencial.
Primeiro, a função exponencial nunca se anula. Caso isto ocorresse para algum valor de Xt, o efeito deste valor assumido por Xt seria anulado, o que não faria sentido.
O segundo motivo é que a especificação (29), impõe que a função de ligação somente assuma valores estritamente positivos (pois O e e+ &gt; o, para todo t, e a função exponencial obedece a tal restrição.
Mas o que realmente ocorre as equações de previsão e atualização do modelo, quando da introdução de variáveis explicativas?
A dedução aqui exposta será baseada na seguinte propriedade de uma variável aleatória com densidade Gama.
Seja X — G(a, p).
Se Y = kX, onde k é uma constante, então Y possui distribuição G(a, p/k). Isto é:
X — G(a,p), Y = kX — G(a,p/k).
Sabemos que Ot — G(a, b), onde a = at\t—1, b = bt\t—1, se estamos nos referindo à priori, e a = at, b = bt, se nos referimos à posteriori.
Então, da equação 29, concluímos que:
Ot+ = OteYXt — G(a, be—YXt)
assim como:
at\t—1 = at |t—1	(30)
ap = at	(31)
bpt—1 = e—YXtbt\t—1 = e—Xte~htbt—1	(32)
Então já temos as equações para os previsores de a e b, e para o atualizador de a. Falta apenas a equação para o atualizador de b.
Aqui, cabe uma ressalva: o termo bt—1 que aparece no membro direito da equação 32 não é exatamente o atualizador de b antigo, isto é, sem variáveis explicativas. Na verdade, ele é a
37
transformada inversa do novo atualizador de b, o que não é a mesma coisa. O referido termo é obtido pelo produto do atualizados antigo de b por eYXt.
A expressão para o atualizador de b fica:
b+ =bt\t-i+ (rt2/2),
que quando combinado com 32, fornece a expressão recursiva para o atualizador de b, isto é:
bt+ =e-YXt e-ht bt-1 + (r^/2)	(33)
Finalmente, efetuando a transformação inversa em bt+, ou seja, multiplicando por eYXt, obtemos a expressão recursiva para o atualizador que aparece na equação 32:
bt =bt+ eYXt = e-rtbt-1 + eYXt (rt2/2)	(34)
Observe que tal expressão difere da expressão para o atualizador antigo de bt.
A partir das equações 29, 32 e 33, podemos escrever as expressões finais para priori e posteriori da precisão, com variáveis explicativas. Isto é, a priori fica:
et+ | R_ - G(a+
bt\t_i) = G(®at_i,e yXte htbt_1)
e a posteriori:
\ Rt ~ G(at,b+) = G(a+ _1 + 1/2,e YXte htbt_i + (rt2/2)).
Em termos de custo computacional, aumentamos em L elementos o espaço paramétrico, onde L é o número de variáveis explicativas inseridas.
A log-verossimilhança agora é função de L + 1 parâmetros, de tal forma que será efetuada uma busca não linear multidimensional, em L + 1 dimensões, com restrições para um dos parâmetros (rn).
O problema a ser solucionado se torna:
Maxf (rn, y) = £t=1 [atlog(bt\t_1/bt) _ (1/2)log(bt\t_1) + log(r(at)/r(at\t_1))]
s.a m E (0,1) y E	i = 1,2,...,L
onde bt\t_1 e bt, como visto, dependem de Xt e y.
38
A significância das variáveis explicativas utilizadas será avaliada através do teste da razão de verossimilhanças, que possui a seguinte expressão:
LLR = 2(log-verossimilhança do modelo irrestrito/ log-verossimilhança do modelo restrito)
Sob hipótese nula de que o aumento da log-verossimilhança não é significante, a estatística LLR, definida acima, possui distribuição qui-quadrado, com 1 grau de liberdade, isto é: LLR - x2.
A lógica subjacente é que a distribuição qui-quadrado, com 1 grau de liberdade, possui média 1. Se o aumento não for significante, LLR possuirá distribuição qui-quadrado, e o valor da estatística deverá, na maioria das vezes, estar próximo de sua média, que é 1.
Faz sentido, pois se o aumento não for significativo, as verossimilhanças serão próximas, e realmente, a razão de seus logaritmos deverá estar próxima de 1.
A ideia básica deste teste é verificar a significância estatística da razão entre as log-verossimilhanças computadas para o modelo com a variável e sem a variável explicativa.
3.3	Aplicações do LSM a séries brasileiras
O modelo de escala local simples (sem variáveis explicativas) padrão (com densidade preditiva de medida Normal) foi implementado e rodado nesta seção, para a série do Ibovespa (1.565 retornos, de 01/01/2009 a 31/12/2014).
O fator de amortecimento ótimo foi obtido via maximização da função de verossimilhança utilizando-se toda a amostra.
Nesta seção serão apresentados o fator de amortecimento ótimo, a função de log-verossimilhança, bem como a evolução do preditor 1 passo à frente da variância fornecida pelo modelo.
O fator de amortecimento (a) e o valor correspondente (isto é, maximizado) da função de log-verossimilhança, obtidos no período ex-post foram:
a = 0,9351365 e log-verossimilhança f (a) = 5.896,138.
Para obtenção dos valores ótimos foi utilizado o código do Anexo B, implementado no “R”:
39
3.4	Modelo de escala local com variáveis explicativas: resultados
Este item se propõe a testar a significância empírica da especificação deduzida na seção
3.2.6.
Foi realizado o teste LLR para testar a razão de verossimilhança entre os modelos irrestrito e restrito.
O resultado para a variável explicativa “Risco Brasil” não foi satisfatório, pois as veros-similhanças dos modelos, com e sem a variável eram muito próximos, sendo por isso a LLR menor que 3,84, que é o valor crítico da X2.
Já para a variável explicativa “Índice Dow Jones”, os valores da log-verossimilhança foram:
Irrestrito = 5.898,31 Restrito = 5.896,13, que gerou um LLR = 4,36 &gt; 3,84, levando à adoção deste índice como variável explicativa, a ser introduzida ao modelo, de modo a melhorar a sua capacidade preditiva.
Os valores ótimos obtidos para os parâmetros, com a variável explicativa, foram:
a = 0.9335429 , y = _5,0225074 e f (a, y) = 5.898,311.
Para obtenção dos valores ótimos utilizou-se o algoritmo do Anexo B.
40
4	Comparações entre o LSM e o GARCH(1,1)
Basicamente os modelos serão testados, de acordo com a sua probabilidade de cobertura das volatilidades geradas
Serão obtidos os gráficos da evolução dos previsores da variância, e intervalos de confiança para as volatilidades da série do Ibovespa.
O desempenho do modelo de escala local será comparado com o desempenho do modelo GARCH(1,1) via Value at Risk, descrito na seção 2.
4.1	Coeficientes estimados
O LSM foi implementado no “R”, enquanto que para o modelo GARCH(1,1) foi utilizado o Eviews.
Os coeficientes estimados pelos modelos estão descritos na tabela abaixo:
Ativo	LSM	GARCH(1,1)	LSM - com variável explic.	GARCH(1,1) - com variável explic.
IBOV	a = 0,935	a = 0,059£ = 0,913	a = 0,934 y = —5,023	a = 0,040 P = 0,9398 p = 0,0014
Tabela 2
4.2	Variâncias estimadas
O gráfico 2 contém os previsores 1 passo à frente para a variância, gerados pelos dois modelos, plotados conjuntamente.
Podemos observar que as volatilidades geradas pelo LSM e pelo GARCH(1,1) são ligeiramente próximas, mas um aspecto torna-se evidente, principalmente no terceiro trimestre de 2011 e no quarto trimestre de 2014, é que em momentos de calmaria, o LSM subestima as previsões, se comparado ao GARCH, ao passo que, em momentos de picos de volatilidade, as previsões do LSM superam as do GARCH.
Este fato se deve, provavelmente, à introdução da variável ht na equação da precisão, pois esta variável não depende das observações, sendo classificada como um ruído externo, e portanto, deve captar o impacto de choques externos sobre a volatilidade. Em outras palavras, o humor de mercado esta sendo monitorado por esta variável, que transmite esta informação para a previsão da precisão (variância).
41
.0024-
.0020-
.0016-
2009	2010	2011	2012	2013	2014
--- LSM --GARCH(1,1)
Gráfico 2
Queremos, no entanto, comparar o desempenho dos modelos de uma maneira mais formal, e isso será feito através do cálculo do VaR.
Os algoritmos desenvolvidos no “R” para o cálculo da volatilidade (precisão) um passo a frente, encontram-se no anexo B deste trabalho.
4.3	Cálculo do Value at Risk
4.3.1	Testes contendo toda a amostra
Para o cálculo das violações foi atribuído 95% de confiança, então, espera-se que o per-centil de violações empíricas fique significativamente próximo de 5%, e para esta verificação foi realizado o teste de proporção populacional por meio da função prop.test (x, n, p, 0.05, correct = F, alternative = ”two.sided”) do “R”.
Esta função realiza o teste de Kupiec, mencionado na seção 2.4, onde a hipótese nula é que a proporção amostral é igual ao percentual de violações teórico, ou seja, 5%.
4.3.1.1	- VaR construídos a partir dos valores críticos da distribuição preditiva Normal:
A tabela abaixo contém os resultados das violações. Os resultados não rejeita a hipótese nula de que o número de violações é estatisticamente igual a 5%:
Modelo	Tam. amostra	Num. violações	%	p-valor (prop.test)	Resultado
LSM	1.565	70	4,47	0,3386	Não se rejeita H0
Garch	1.565	79	5,05	0,9307	Não se rejeita H0
Tabela 3
42
Abaixo, o gráfico gerado para 95% de confiança pelo LSM:
--- VaR LSM
Gráfico 3
Pelo GARCH(1,1):
---- VaR GARCH(1,1) ~~|
Gráfico 4
4.3.1.2	- VaR a partir da distribuição empírica:
A verdadeira distribuição preditiva não é conhecida, de forma que uma proxy para ela pode ser a distribuição empírica dos retornos.
Sendo assim, também foram calculados os VaR, considerando agora, os valores críticos correspondentes aos percentis de 2,5 e 97,5 da distribuição empírica para construção dos VaR.
O método Cornish-Fisher
O “R” permite o cálculo da distribuição empírica pelo método de Cornish-Fisher.
43
A função utilizada neste trabalho para este cálculo foi a quantile(Rpad,p), onde Rpad representa a série dos retornos padronizados, e p o percentil de interesse.
Os valores críticos para os percentuais de 2,5% e 97,5% da distribuição empírica, calculados pelo método de Cornish-Fisher, e os resultados das violações utilizando esses valores críticos estão na tabela abaixo:
Modelo	Tam. amostra	Num. violações	%	p-valor (prop.test)	Resultado
LSM	1.565	81	5,18	0,7498	Não se rejeita H0
Garch	1.565	85	5,43	0,4337	Não se rejeita H0
Tabela 4
4.3.1.3	- VaR considerando a distribuição t-Student para o LSM (proposto por Shephard):
Considerado a distribuição preditiva t-Student, os valores críticos a serem utilizados para construir o IC variam no tempo até a convergência do parâmetro a, e conforme previsto na equação 24, para a amostra utilizada, os graus de liberdade convergiram para 14 a partir da amostra 43, de forma que foram utilizados os inteiros mais próximos da tabela t-Student, quando os graus de liberdade não resultaram inteiros.
Após a convergência, o valor crítico utilizado passou a ser 2,145, que corresponde a 14 graus de liberdade. Segue o resultado:
Modelo	Tam. amostra	Num. violações	%	p-valor (prop.test)	Resultado
LSM	1.565	44	2,81	0,00007	Rejeita-se H 0
Tabela 5
Os resultados para os IC construídos com a distribuição t-Student tive seu teste de violações rejeitado para a hipótese de igualdade a 5%.
Comparando os resultados fica claro que o IC construído a partir destes valores críticos são mais largos que os produzidos utilizando-se a hipótese de distribuição Normal e a distribuição empírica, o que sugere que, especificado desta forma, o modelo gera IC mais conservadores, que indica aversão ao risco.
4.3.2.1	- VaR para a distribuição preditiva Normal
4.3.2	Teste fora da amostra
Modelo	Tam. amostra	Num. violações	%	p-valor (prop.test)	Resultado
LSM	261	13	4,98	0,9887	Não se rejeita Ho
Garch	261	14	5,36	0,7873	Não se rejeita H0
Tabela 6
Gráfico obtido para o VaR fora da amostra (LSM):
----VaR LSM - Fora da amostra
Gráfico 5
45
Gráfico gerado pelo GARCH(1,1):
----VaR Garch - Fora da amostra
Gráfico 6
4.3.2.2	- VaR com os percentis de 2,5 e 97,5 da distribuição empírica.
Modelo	Tam. amostra	Num. violações	%	p-valor (prop.test)	Resultado
LSM	261	17	6,51	0.2619	Não se rejeita H0
Garch	261	17	6,51	0.2619	Não se rejeita H0
Tabela 7
4.3.2.3 - VaR para o LSM com os valores críticos da distribuição t-Student
Modelo	Tam. amostra	Num. violações	%	p-valor (prop.test)	Resultado
LSM	261	10	3,83	0,3864	Não se rejeita H0
Tabela 8
O VaR construído a partir das três distribuições tiveram resultados satisfatórios, e levando-se em conta, que a probabilidade de cobertura pré-especificada para os intervalos foi de 95%, as coberturas obtidas levam a concluir que os IC fornecidos pelo LSM tem um comportamento ligeiramente mais conservador.
46
4.3.2.4	- VaR com variáveis explicativas, e distribuição Normal:
Modelo	Tam. amostra	Num. violações	%	p-valor (prop.test)	Resultado
LSM	261	13	4,98	0,9887	Não se rejeita H0
Garch	261	17	6,51	0,2619	Não se rejeita H0
Tabela 9
Gráfico do VaR com variáveis explicativas, e distribuição preditiva Normal (LSM):
-----VaR LSM com variável explicativa - Fora da amostra
Gráfico 7
Pelo GARCH(1,1):
-----VaR Garch com variável explicativa - Fora da amostra |
Gráfico 8
47
4.3.2.5 - VaR com variável explicativa, e considerando a distribuição empírica
Modelo	Tam. amostra	Num. violações	%	p-valor (prop.test)	Resultado
LSM	261	17	6,51	0.2619	Não se rejeita H0
Garch	261	18	6,89	0.1598	Não se rejeita H0
Tabela 10
4.3.2.6 - VaR com variáveis explicativas, considerando a distribuição t-Student para o LSM
Modelo	Tam. amostra	Num. violações	%	p-valor (prop.test)	Resultado
LSM	261	10	3,83	0,3864	Não se rejeita H0
Tabela 11
Podemos observar comparando os resultados das tabelas 6, 7 e 8, com os das tabelas 9, 10 e 11 (com variáveis explicativas), que os modelos não foram beneficiados pela inclusão do índice Dow Jones, uma vez que os p-valores do prop.test foram menores para os modelos com variáveis explicativas.
4.3.3	Considerações sobre os VaR
Pelos VaR calculados não foi possível constatar a superioridade de nenhum dos modelos testados quanto à capacidade de previsão da volatilidade 1 passo a frente.
Podemos observar, no entanto que, para o teste fora da amostra, o resultado considerando a distribuição preditiva t-Student, que o número de violações foi consideravelmente menor dos que obtidos considerando as outras duas distribuições teóricas.
4.4	Erros de previsão
A utilização do VaR pode ser questionada por alguns autores, então torna-se conveniente que esta metodologia seja complementada através do cálculo dos erros de previsão. Um método bastante difundido é dado pela raiz quadrada do erro médio (REQM), que foi utilizado de modo a aferir o quão distante estão as estimativas da variância dos retornos efetivamente observados.
REQM = [ 1 ET=1(r? - O2)2]2
48
onde, r2 é o retorno quadrático da série, ot2 é a volatilidade estimada pelo modelo e T é o número de observações da amostra.
O valor obtido para o REQM do LSM foi de 0,00039869, consideravelmente inferior que 0,000431493, obtido para o GARCH(1,1). Este resultado fornece forte evidencia empírica que o LSM se ajusta mais adequadamente à serie do Ibovespa.
5	Conclusões
A presente dissertação descreveu, implementou e comparou o LSM ao GARCH(1,1).
Na seção 3.2.2 foi constatado que a função de previsão para a variância no modelo de escala local é a mesma do modelo IGARCH(1,1), ou seja, uma EWMA, nos quadrados das observações passadas, onde o fator de amortecimento m é estimado por máxima verossimilhança. Neste sentido, o modelo pode ser visto como uma alternativa ao modelo IGARCH(1,1), como propôs Shephard (1994).
Foi observado que o LSM capta os principais fatos estilizados das séries financeiras, com a vantagem de que neste modelo, a introdução de variáveis explicativas é trivial, o que não ocorre nos modelos ARCH/GARCH.
Outra vantagem é que há apenas 1 parâmetro a ser estimado, o fator de amortecimento M, enquanto no GARCH(1,1) existem 3. Além disso, o erro de previsão calculado pelo REQM para o LSM foi menor que para o GARCH(1,1).
Para os testes fora da amostra de violações do VaR construídos a partir dos modelos, tanto o LSM quanto o GARCH(1,1) apresentaram resultados satisfatórios.
Em suma, foi desenvolvida uma abordagem estrutural, pouco difundida na literatura, e como os resultados esperados são semelhantes, o que credencia o LSM como uma alternativa parcimoniosa aos modelos tradicionais, em virtude das vantagens já mencionadas.
6	Extensão
Modelo de Escala Local Generalizado
O modelo de escala local padrão, pode parecer inadequado para modelar séries financeiras, pois apresenta o inconveniente da distribuição dos retornos, condicional à precisão (densidade de medida) ser Gaussiana, enquanto na prática, observa-se que a distribuição incondicional dos retornos financeiros possui excesso de curtose.
Shephard (1994) demonstra que tais distribuições não são exatamente as mesmas, e que trabalhar com uma densidade Gaussiana condicional à precisão, equivale a modelar indireta-
49
mente o excesso de curtose da distribuição incondicional (embora não se tenha obtido uma forma analítica para ela, no presente caso).
Ainda sim, o excesso de curtose induzido na distribuição preditiva parece insuficiente para modelar séries financeiras, cuja distribuição preditiva possui caudas muito pesadas. Se isto ocorre, a densidade de medida Normal é potencialmente inadequada.
Em virtude de tal possibilidade, Shephard implementou uma generalização do modelo de escala local, que trabalha com uma densidade de medida (Exponencial Power, ou Generalized Error Distribution), mais flexível, englobando distribuições com baixa curtose (platicurtose) e com excesso de curtose (leptocurtose), incluindo a Normal como um caso particular.
O modelo foi chamado de modelo de escala local generalizado, e não foi abordado nesta dissertação, de modo que seu estudo valeria de extensão ao tema apresentado.
7	Referências
Alexander C. - Modelos de Mercados, Saraiva, 2005.
Artzner P., Thinking Coherently, Risk, 1997.
Bollerslev T.- Generalized Autoregressive Conditional Heteroskedasticity, Journal of Econometrics, 1986.
Campos E. - Modelo de Escala Local: Uma alternativa de especificação multiplicativa para a estimação e previsão de volatilidade de séries financeiras, Dissertação de Mestrado PUC-RJ, 1998.
Engle R. F. - ARCH selected readings - Advanced texts in econometrics, 1982.
Harvey A. C., Forecasting, Structural Time series models and Kalman Filter, Cambridge Univ. Press, 1989.
Jorion, Philippe. Value-at-Risk - The New Benchmark for Managing Financial Risk, McGraw Hill, 2006.
Morettin P. - Econometria Financeira, Blucher, 2011.
Nelson D. B. - Filtering and forecasting with misspecified ARCH models, Journal of Econometrics, 1992.
Shephard N.G - State space alternative to integrated GARCH processes, Journal of Econometrics, 1994.
Shephard N.G - Stochastic Volatility: Likelihood Inference and comparasion with ARCH models, The Review of Economics Studies, 1998.
50
Anexo A
Estatísticas básicas
Teste de normalidade dos retornos
Gráfico 9: Histograma dos retornos Ibovespa.
Teste de autocorrelação dos retornos
Para que a modelagem pelo modelo GARCH seja adequada, é desejável que a série não apresente autocorrelação dos retornos e apresentem autocorrelação dos retornos ao quadrados. Para verificar estas condições foram extraídos pelo Eviews, os correlogramas dessas duas séries.
Sample: 1/01/2009 31/12/2014
Included observations: 1565
Autocorrelation Partial Correlation AC PAC O-Stat Prob
1	-0.046 -0.046 3.2537 0.071
2	0.025 0.023 4.2490 0.119
3	-0.038 -0.036 6.5105 0.089
4	0.010 0.006	6.6553	0.155
5	0.003 0.005	6.6696	0.246
6	-0.017 -0.018	7.1086	0.311
7	-0.035 -0.037	9.0737	0.247
8	-0.002 -0.004	9.0797	0.336
9	0.014 0.014	9.3966	0.402
10	-0.030 -0.031	10.832	0.371
11	-0.009 -0.012	10.972	0.446
12	-0.014 -0.013	11.288	0.504
13	0.014 0.009	11.585	0.562
14	-0.026 -0.027	12.679	0.552
15	0.019 0.016	13.226	0.585
16	-0.002 0.002	13.233	0.656
17	0.029 0.024	14.557	0.627
18	-0.003 -0.000	14.567	0.691
19	-0.024 -0.025	15.472	0.692
20	0.027 0.027	16.668	0.674
21	-0.016 -0.015	17.064	0.707
22	0.023 0.019	17.915	0.711
23	-0.040 -0.033	20.419	0.617
24	0.015 0.010	20.768	0.652
25	-0.009 -0.005	20.887	0.699
26	-0.011 -0.017	21.084	0.738
27	0.044 0.049	24.130	0.623
28	-0.001 0.003	24.132	0.675
29	0.009 0.006	24.267	0.716
30	0.008 0.010	24.363	0.755
Tabela 12
O resultado deste teste sugere que não há autocorrelação dos retornos da série.
51
Sample: 1/01/2009 31/12/2014
Included observations: 1565
Auto corre latí on		Partial Correlation			AC	PAC	Q-Stat	Prob
			]	1	0098	0.098	15.019	0.000
	□		3	2	0144	0136	47 700	0.000
	□	i	□	3	0173	0.152	94.766	0.000
				4	0 086	0.045	106.45	0.000
				5	0107	0 059	124 45	0.000
	□		]	6	0.133	0.086	152.36	0.000
				7	0.075	0.026	161 28	0.000
				8	0.088	0.033	173.55	0.000
				9	0.107	0.053	191.44	0.000
	J			10	0.1Q1	0.054	207.39	0.000
				11	0060	0001	213.00	0.000
				12	0.093	0.035	226.60	0.000
	3			13	0113	0065	24688	0.000
				14	0.030	-0.027	248.27	0.000
				15	0098	0.039	263.32	0.000
				16	0037	-0 020	265.49	0.000
				17	0066	0.026	272.40	0.000
				18	0 088	0.037	284 73	0.000
				19	0.045	-0.002	288.01	0.000
				20	0 087	0044	300.10	0.000
				21	0.057	0.004	305.26	0.000
				22	0.081	0.036	315.61	0.000
				23	0.060	0.006	321.31	0.000
				24	0072	0025	329.49	0.000
				25	0.067	0.015	336.61	0.000
				26	0026	-0027	337.70	0.000
				27	0.029	-0.019	339.05	0.000
				28	0 006	-0043	339.11	0.000
				29	0.061	0040	345.04	0.000
				30	0.079	0.044	354.90	0.000
Tabela 13
O resultado para os retornos ao quadrado apresenta forte autocorrelação, como era de se esperar, sendo assim adequada a modelagem por um modelo da família GARCH, ainda que o teste de normalidade para os resíduos padronizados indique que estes não são Normais:
Series: Standardized Residuals	
Sample 1/01/2009 31/12/2014	
Observations 1565	
Mean	0.005676
Median	0.000000
Maximum	3.913821
Minimum	-4.407372
Std. Dev.	1.000084
Skewness	-0.068038
Kurtosis	3.964039
Jarque-Bera	61.81014
Probability	0.000000
Gráfico 10: Histograma dos resíduos padronizados
52
Gráfico dos retornos ao quadrado
Gráfico 11
Este gráfico também evidencia a presença de clusters de volatilidade, ratificando a conveniência da utilização de um modelo de heterocedasticidade condicional para modelagem da volatilidade.
Teste de raiz unitária para os retornos
Foi realizado teste de raiz unitária para verificar a estacionariedade da série dos retornos.
Null Hypothesis: IBOV has a unit root
Exogenous: Constant
Lag Length: 0 (Automatic - based on SIC, maxlag=23)
		t-Statistic	Prob.*
Augmented Dickey-Fuller test statistic		-41.36542	0.0000
Test critical values:	1% level	-3.434325	
	5% level	-2.863183	
	10% level	-2.567693	
’MacKinnon (1996) one-sided p-values.
Augmented Dickey-Fuller Test Equation Dependent Variable: D(IBOV) Method: Least Squares Date: 16/01/15 Time: 12:26
Sample (adjusted): 2/01/2009 31/12/2014 Included observations: 1564 after adjustments
Variable	Coefficient	Std Error	t-Statistic	Prob.
IBOV(-1)	-1.045553	0.025276	-41.36542	0.0000
C	0.000309	0.000377	0.820031	0.4123
R-squared	0.522776	Mean dependentvar	-1.11E-20
Adjusted R-squared	0.522471	S.D. dependentvar	0.021574
S.E. of regression	0.014908	Akaike info criterion	-5.572541
Sum squared resid	0.347159	Schwarz criterion	-5.565693
Log likelihood	4359.727	Hannan-Quinn criter.	-5.569995
F-statistic	1711.098	Durbin-Watson stat	1.983085
Prob(F-statistic)	0.000000		
Tabela 14
O teste ADF rejeita a hipótese nula de que a série possui raiz unitária. Este mesmo teste não rejeitou a hipótese nula de raiz unitária para a série da pontuação do Ibovespa no nível.
53
Anexo B
Códigos para o modelo de escala local no “R”:
Otimização de a
M _otimo&lt;-function(ativo)
{ a&lt;-vector(mode = "numeric", length = length(ativo)); b&lt;-vector(mode = "numeric", length = length(ativo)); h&lt;-vector(mode = "numeric", length = length(ativo)); bern&lt;-c(1/30,1/30,691/2730, 3617/510,174611/330,236364091/2730); vetor_bern&lt;-vector(mode = "numeric", length = length(bern)); somat&lt;-vector(mode = "numeric", length = length(ativo)); vetor_vero&lt;-vector(mode = "numeric", length = length(ativo)); fr&lt;-function(a) {
a[0]&lt;-0; b[0]&lt;-0;
a[1]&lt;-1/2;
b[1]&lt;-((0.005)A2)/2;
for (n in 2:length(ativo)) {
a[n]&lt;-a *a[n-1]+0.5;
for (i in 1:length(bern)) {
vetor_bern[i]&lt;-((bern[i])/(2*i))*((1-a)/(a*a[n-1]))A(2*i); } somat[n]&lt;-sum(vetor_bern);
h[n]&lt;-(log(a )-(1/2)*((1-a )/(m *a[n-1]))-somat[n]); b[n]&lt;-(exp(-h[n]))*(b[n-1])+(ativo[n]A2)/2;} for (m in 2:length(ativo)) { vetor_vero[m]&lt;-a[m]*log((exp(-h[m])*b[m-1])/b[m]) -(1/2)*log(exp(-h[m])*b[m-1])
+log(gamma(a[m])/gamma(a*a[m-1])); }
-sum(vetor_vero) } optimise(fr,c(0,1))} a _otimo(IBOV);
54
Cálculo de 6
Calc 6&lt;-function(ativo) {
a&lt;-vector(mode = "numeric", length = length(ativo)); b&lt;-vector(mode = "numeric", length = length(ativo)); h&lt;-vector(mode = "numeric", length = length(ativo)); bern&lt;-c(1/30,1/30,691/2730, 3617/510,174611/330,236364091/2730); vetor_bern&lt;-vector(mode = "numeric", length = length(bern));
6&lt;-vector(mode = "numeric", length = length(ativo));
vari&lt;-vector(mode = "numeric", length = length(ativo)); precisao&lt;-vector(mode = "numeric", length = length(ativo)); vol&lt;-vector(mode = "numeric", length = length(ativo)); somat&lt;-vector(mode = "numeric", length = length(ativo)); a&lt;-0.9351365; # valor otimizado pelo código do Anexo B; a[0]&lt;-0; b[0]&lt;-0; a[1]&lt;-1/2; b[1]&lt;-((0.005)A2)/2;
for (n in 2:length(ativo)) {
a[n]&lt;-a *a[n-1]+0.5;
for (i in 1:length(bern)) {
vetor_bern[i]&lt;-((bern[i])/(2*i))*((1-a )/(a *a[n-1]))A(2*i);} somat[n]&lt;-sum(vetor_bern);
h[n]&lt;-(log(a )-(1/2)*((1-a )/(a *a[n-1]))-somat[n]); b[n]&lt;-(exp(-h[n]))*(b[n-1])+(ativo[n]A2)/2;
6	[n]&lt;-(a[n]/b[n]);
vari[n]&lt;-(b[n]/a[n]); precisao[n]&lt;-1/vari([n]);
vol[n]&lt;-sqrt(vari[n]); }
# plot(precisao,type="l",col="red",main="Gráfico",xlab="Dia",ylab="Precisão") return(6); # return(vari); # return((a-1/2)*2);}
Calc_6 (IBOV);
Otimização de a e y
a_Y_otimos&lt;-function(ativo,var_explic) { a&lt;-vector(mode = "numeric", length = length(ativo));
55
h&lt;-vector(mode = "numeric", length = length(ativo)); bern&lt;-c(1/30,1/30,691/2730, 3617/510,174611/330,236364091/2730); vetor_bern&lt;-vector(mode = "numeric", length = length(bern)); somat&lt;-vector(mode = "numeric", length = length(ativo)); vetor_vero&lt;-vector(mode = "numeric", length = length(ativo));
fr&lt;-function(y) {
o&amp;lt;-y[1];
Y&lt;-y[2];
a[0]&lt;-0;
b[0]&lt;-0;
a[1]&lt;-1/2;
b[1]&lt;-((0.005)A2)/2;
for (n in 2:length(ativo)) {
a[n]&lt;-o *a[n-1]+0.5;
for (i in 1:length(bern)) {
vetor_bern[i]&lt;-((bern[i])/(2*i))*((1-o)/(o*a[n-1]))A(2*i); } somat[n]&lt;-sum(vetor_bern);
h[n]&lt;-(log(o )-(1/2)*((1-o )/(o *a[n-1]))-somat[n]); b[n]&lt;-(exp(-h[n]))*(b[n-1])+(exp(y*var_explic[n]))*(ativo[n])A2/2; } for (m in 2:length(ativo)) { vetor_vero[m]&lt;-a[m]*log((exp(-h[m])*b[m-1])/b[m])-(1/2)*log(exp(-h[m])*b[m-1]) +log(gamma(a[m])/gamma(o*a[m-1])); }
-sum(vetor_vero) } optim(c(.9,-10),fr); } o _y_otimos(IBOV,D_Jones);
Cálculo de 0+
Calc 0+&lt;-function(ativo,var_explic) {
a&lt;-vector(mode = "numeric", length = length(ativo)); b&lt;-vector(mode = "numeric", length = length(ativo)); h&lt;-vector(mode = "numeric", length = length(ativo)); bern&lt;-c(1/30,1/30,691/2730, 3617/510,174611/330,236364091/2730);
56
vetor_bern&lt;-vector(mode = "numeric", length = length(bern));
somat&lt;-vector(mode = "numeric", length = length(ativo));
vetor_vero&lt;-vector(mode = "numeric", length = length(ativo));
vol&lt;-vector(mode = "numeric", length = length(ativo));
0+&lt;-vector(mode = "numeric", length = length(ativo));
precisao_explic&lt;-vector(mode = "numeric", length = length(ativo));
rn&amp;lt;-0.9277839;
y&lt;-4.9771622; # valores otimizados no Anexo B;
a[0]&lt;-0; b[0]&lt;-0; a[1]&lt;-1/2; b[1]&lt;-((0.005)A2)/2;
for (n in 2:length(ativo)) {
a[n]&lt;-® *a[n-1]+0.5;
for (i in 1:length(bern)) {
vetor_bern[i]&lt;-((bern[i])/(2*i))*((1-®)/(®*a[n-1]))A(2*i); } somat[n]&lt;-sum(vetor_bern);
h[n]&lt;-(log(® )-(1/2)*((1-®)/(® *a[n-1]))-somat[n]);
b[n]&lt;-(exp(-h[n]))*(b[n-1])+(exp(y*var_explic[n]))*(ativo[n])A2/2; vari_explic[n]&lt;-(b[n]/a[n]);
0+[n]&lt;-1/vari_explic[n];
vol[n]&lt;-sqrt(vari_explic[n]);}
#	plot(precisao_explic,type="l",col="red",main="Gráfico",xlab="Dia",ylab="Precisão - com variável explicativa")
#	return(vol);}
Calc_0+(IBOV,D_Jones).
57</field>
	</doc>
</add>