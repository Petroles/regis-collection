<?xml version="1.0" encoding="utf-8"?>
<add>
	<doc>
		<field name="docid">BR-TU.19793</field>
		<field name="filename">3213_Sanchetta_AlexandreCruz_M.pdf</field>
		<field name="filetype">PDF</field>
		<field name="text">
 0 

 

 

 

 

UNIVERSIDADE ESTADUAL DE CAMPINAS 

FACULDADE DE ENGENHARIA MECÂNICA E 

INSTITUTO DE GEOCIÊNCIAS 

COMISSÃO DE PROGRAMA MULTIDISCIPLINAR DE PÓS-

GRADUAÇÃO EM CIÊNCIAS E ENGENHARIA DE PETRÓLEO 
 

 

 

 

 

 

 

 Reconhecimento E Classificação De Fácies 

Geológicas Através Da Análise De 

Componentes Independentes 
 

 

 

 

 

 

Autor: Alexandre Cruz Sanchetta 

Orientador: Rodrigo de Souza Portugal 

Co-orientador: 

 

 

 

 

 

 

 

 

 

 

 



 i 

 

 

 

 

UNIVERSIDADE ESTADUAL DE CAMPINAS 

FACULDADE DE ENGENHARIA MECÂNICA E 

INSTITUTO DE GEOCIÊNCIAS 

COMISSÃO DE PROGRAMA MULTIDISCIPLINAR DE PÓS-

GRADUAÇÃO EM CIÊNCIAS E ENGENHARIA DE PETRÓLEO 
 

 

 

 

  

Reconhecimento E Classificação De Fácies 

Geológicas Através Da Análise De 

Componentes Independentes  
 

 

Autor: Alexandre Cruz Sanchetta 

Orientador: Rodrigo de Souza Portugal 

Co-orientador: 

 

 

 

 

 

Programa: Ciências e Engenharia de Petróleo 

Área de Concentração: Caracterização de Reservatórios  

 

 

Dissertação de mestrado acadêmico apresentada à Comissão de Pós Graduação em Ciências 

e Engenharia de Petróleo da Faculdade de Engenharia Mecânica e Instituto de Geociências, como 

requisito para a obtenção do título de Mestre em Ciências e Engenharia de Petróleo. 

 

 

 

Campinas, 2010 

SP – Brasil. 



 ii 

 

 

 

 

 

 

 

FICHA  CATALOGRÁFICA  ELABORADA  PELA  

  BIBLIOTECA  DA  ÁREA  DE  ENGENHARIA  E  ARQUITETURA  -  BAE  -  UNICAMP 

 

 

 

 

    C889r 

 

Cruz Sanchetta, Alexandre 

     Reconhecimento e classificação de fácies geológicas 

através da análise de componentes independentes / 

Alexandre Cruz Sanchetta. --Campinas, SP: [s.n.], 2010. 

 

     Orientador: Rodrigo de Souza Portugal. 

     Dissertação de Mestrado - Universidade Estadual de 

Campinas, Faculdade de Engenharia Mecânica e 

Instituto de Geociências. 

 

     1. Análise multivariada.  2. Fácies (Geologia).  3. 

Reconhecimento de padrões.  4. Classificação.  I. 

Portugal, Rodrigo de Souza.  II. Universidade Estadual 

de Campinas. Faculdade de Engenharia Mecânica e 

Instituto de Geociências.  III. Título. 

 

 

Título em Inglês: Recognition and classification of geological facies based on 

independent component analysis 

Palavras-chave em Inglês: Multivariate analysis, Facies (Geology), Recognition 

of patterns, Classification 

Área de concentração: Reservatórios e Gestão 

Titulação: Mestre em Ciências e Engenharia de Petróleo 

Banca examinadora: Emilson Pereira Leite, Liliana Alcazar Diogo 

Data da defesa: 02/12/2010 

Programa de Pós Graduação: Ciências e Engenharia de Petróleo 

 

 

 

 

 

 

 

 

 



 

 

 

iii 

 

 

UNIVERSIDADE ESTADUAL DE CAMPINAS 

FACULDADE DE ENGENHARIA MECÂNICA E 

INSTITUTO DE GEOCIÊNCIAS 

COMISSÃO DE PROGRAMA MULTIDISCIPLINAR DE PÓS-

GRADUAÇÃO EM CIÊNCIAS E ENGENHARIA DE PETRÓLEO 
 

 DISSERTAÇÃO DE MESTRADO ACADÊMICO 
 

 

 Reconhecimento E Classificação De Fácies 

Geológicas Através Da Análise De 

Componentes Independentes  

 
 

Autor: Alexandre Cruz Sanchetta 

Orientador: Rodrigo de Souza Portugal 

Co-orientador: 

 

A Banca Examinadora composta pelos membros abaixo aprovou esta Dissertação: 

 

 

____________________________________________________ 

Prof. Dr. Rodrigo de Souza Portugal, Presidente 

SHUMBLERGER 

 

 

____________________________________________________ 

Prof. Dr. Emilson Pereira Leite 

DGRN/IG/UNICAMP 

 

 

____________________________________________________ 

Profª. Drª. Liliana Alcazar Diogo 

IAG/USP 

 

Campinas, 02 de dezembro de 2010



 

 

 

v 

 

 

 

 

 

 

Dedicatória 

Dedico este trabalho a minha família, por sempre permitir que eu continuasse com meus 

objetivos. 

 

 

 

 



 

 

 

vii 

 

 

 

 

 

 

Agradecimentos 

Agradeço imensamente... 

À minha família, como dedicado previamente, esse trabalho é de vocês. 

Aos meus pais, Pedro e Wanda, pelo incentivo, apoio e carinho durante todo esse caminho. 

Ao meu orientador, Professor Rodrigo de Souza Portugal, pela orientação (praticamente) à 

distância e pela paciência durante os anos. 

Ao Professor Alexandre Campane Vidal, pelo acolhimento e confiança depositada. 

Aos Professores Emilson Pereira Lima e Rodrigo Duarte Drummond, pelas correções, 

dicas, e conversas. 

Aos amigos de ofício: Juliana, Michele, Ancila, e Bruno (Champz), pelo companheirismo, 

amizade e pela ajuda prestada. 

A meus companheiros da República Viracopos, pelo entretenimento e suporte nas horas em 

que a mente pedia um descanso. 

 

 

 

 



 

ix 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

“A música é um exercício inconsciente 

de cálculos”.  

 

Leibniz 

 

 



 

 

  

xi 

 

 

 

 

 

Resumo 

SANCHETTA, Alexandre Cruz, Reconhecimento E Classificação De Fácies Geológicas Através 

Da Análise De Componentes Independentes, Campinas, Faculdade de Engenharia 

Mecânica, Universidade Estadual de Campinas, 2010. 94 p. Dissertação de Mestrado. 

O uso método de análise multivariada ICA (Análise de Componentes Independentes), 

mais o método K-NN (K-vizinhos mais Próximos) aplicados em dados de poços e em dados 

sísmicos buscando classificar fácies geológicas e suas características. Esses dois métodos foram 

aplicados em dados retirados do Campo de Namorado, na Bacia de Campos, Brasil. A ICA 

encontra as componentes independentes dos dados, que quando treinadas pelo método K-NN para 

reconhecer padrões nos dados, predizem fácies geológicas e outras informações sobre as rochas, 

como as características de reservatório. Essas componentes independentes configuram uma nova 

opção de interpretação das informações disponíveis, pois nessas novas variáveis, o espaço de 

análise não apresenta dimensões dependentes e exclui informações repetidas ou dúbias da 

interpretação dos resultados. Além disso, a maior parte da informação é resumida em poucas 

dimensões, resultando em uma possível redução de variáveis referentes ao problema.  Um 

abundante número de testes foi feito procurando a taxa de sucesso desse método. Como taxa de 

sucesso, é compreendida a divisão do número de predições corretas dividido pelo número total de 

tentativas.  O que se observa é uma taxa de sucesso alta, em torno de 85% de acerto em algumas 

situações, ressaltando-se que as componentes têm distribuição gaussiana, sendo que o método 

funciona melhor em encontrar componentes não-gaussianas. Mesmo nessa situação adversa o 

método se mostrou robusto.  A solidez do método mostra-se uma alternativa para novas formas 

de interpretação geológicas e petrofísicas. Um dos trunfos desse método é que a base da sua 

aplicação pode ser estendida para outros tipos de dados, inclusive de naturezas físicas diferentes. 

 

 

Palavras Chave 

Análise de Multivariada; Fácies (Geologia); Reconhecimento de Padrões; Classificação 



 

 

  

xiii 

 

 

 

Abstract 

SANCHETTA, Alexandre Cruz, Recognition and Classification of Geological Facies Based on 

Independent Component Analysis, Campinas, Faculdade de Engenharia Mecânica, 

Universidade Estadual de Campinas, 2010. 94 p. Dissertação de Mestrado. 

The use of multivariate analysis method ICA (Independent Component Analysis), plus the 

K-NN method (K-nearest Neighbor) applied on well log data and seismic data to predict the 

classification of geological facies and their characteristics. These two methods were applied to 

data from the Campo de Namorado, in the Campos Basin, Brasil. The ICA  finds the independent 

components of the data that can be trained by K-NN method to recognize patterns in the data  and 

predict the geological facies or other information about the rocks, as the characteristics of the 

reservoir. These independent components make up a new option for interpretation of available 

information, because with these new variables, the space has no dependent dimensions and the 

duplicate information or dubious interpretation of results are excluded. Moreover, most of the 

information is summarized in a few dimensions, resulting in a possible reduction of variables 

related to the problem. An abundant number of tests were done looking for the success rate of 

this method. As success rate, it is understood by the division of the number of correct predictions 

divided by total attempts. What is observed is a high success rate, around 85% accuracy in some 

situations, pointing out that the components have a Gaussian distribution and the method works 

best in finding non-Gaussian components. Even in this adverse situation the method was robust. 

The robustness of the method proves that ICA can be an alternative to new forms of geological 

and petrophysical interpretation. One of the advantages of this method is that the basis of their 

application can be extended to other types of data, including datas with different physical natures. 

 

 

 

Key Words 

Multivariate Anaysis, Facies (Geology), Pattern Recognition; Classification  



 

 

  

xv 

 

 

 

Índice 

 

Lista de Figuras................................................................................................ xvii 

Lista de Tabelas ................................................................................................ xix 

Siglas ................................................................................................................ xxi 

Capítulo 1 ............................................................................................................ 1 

   Introdução ...........................................................................................................  

Capítulo 2 ............................................................................................................ 3 

   Separação Cega de Sinais ....................................................................................  

Capítulo 3 ............................................................................................................ 7 

   Análise de Componentes Principais ....................................................................  

Capítulo 4 .......................................................................................................... 11 

   Análise De Componentes Independentes.............................................................  

Capítulo 5 .......................................................................................................... 33 

   Reconhecimento de Padrões ................................................................................  

Capítulo 6 .......................................................................................................... 47 

   Metodologia ........................................................................................................  

Capítulo 7 .......................................................................................................... 57 

   Resultados e Discussões ......................................................................................  

Capítulo 8 .......................................................................................................... 77 

   Conclusões ..........................................................................................................  

Bibliografia ........................................................................................................ 81 

Apêndices .......................................................................................................... 87 

 



 

 

  

xvii 

 

 

 

 

 

 

Lista de Figuras 

 

Figura 2.1          Cocktail Party Problem................................................................................... 4 

Figura 5.1          Exemplos de Translação e Rotação de um Objeto-Padrão Inicial.................35 

Figura 5.2          Vizinhos Utilizados para Diferentes números de K.........................................43 

Figura 6.1          Fluxograma dos métodos............................................................................... 54 

Figura 6.2          Tabela de classificação Reservatório/Não Reservatório ............................... 56 

Figura 7.1          Primeira Bateria de Resultados...................................................................... 60 

Figura 7.2          Segunda Bateria de Resultados...................................................................... 62 

Figura 7.3          Terceira Bateria de Resultados  - Classificação Fácies.................................. 64 

Figura 7.4          Terceira Bateria de  Resultados - Classificação de Reservatórios................. 66 

Figura 7.5           Quarta Bateria de Resultados......................................................................... 68 

Figura 7.6          Teste entre ICA e FastICA – Classificação de Fácies.................................... 69 

Figura 7.7          Teste entre ICA e FastICA – Classificação de Reservatório.......................... 69 

Figura 7.8       Comparação entre velocidades de ICA e FastICA......................................... 70 

Figura 7.9           Comparação ICA e PCA – Classificação Fácies............................................ 71 

Figura 7.10           Comparação ICA e PCA – Classificação Fácies............................................ 72 

Figura 7.11  Predição de Poço NA01.................................................................................. 74 

Figura 7.12           Teste Par/Ímpar – Classificação de Fácies .................................................... 75 

Figura 7.13          Teste Menos-Um – Classificação de Reservatórios........................................ 76 



 

 

  

xix 

 

 

 

 

 

 

Lista de Tabelas 

 

Tabela 4.1 Rotina do algoritmo de gradiente através da negentropia - Adaptado de 

(Hyvärinen,2001).......................................................................................................................... 19 

 

Tabela 4.2 Rotina do FastICA através da negentropia - Adaptado de 

(Hyvärinen,2001).......................................................................................................................... 20 

 

Tabela 4.3 Rotina do FastICA através da Ortogonalização Deflacionária- Adaptado de 

(Hyvärinen,2001).......................................................................................................................... 21 

 

Tabela 4.4 Rotina do FastICA através da Ortogonalização Simétrica - Adaptado de 

(Hyvärinen,2001)......................................................................................................................... 22 

 

Tabela 4.5 Rotina do FastICA através da Estimativa de Máxima Probabilidade - Adaptado de 

(Hyvärinen,2001).......................................................................................................................... 28 

 

Tabela 5.1 Aplicações do Reconhecimento de Padrões.............................................................. 32 

 

Tabela 6.1 Fácies Litológicas...................................................................................................... 48 

 

Tabela 6.2 Perfis Geológicos...................................................................................................... 49 

 

Tabela 6.3 Exemplo de Amostra de Dado de Perfil....................................................................50  

 

Tabela 6.4 Dados Sísmicos......................................................................................................... 51 

 

Tabela 6.5 Exemplo de Amostra de Dado Sísmico..................................................................... 52 

 

Tabela 6.6 Separação de Testemunho nos Dados Sísmicos com Predominância...................... 53 

 

Tabela 6.7  Separação de Testemunho nos Dados Sísmicos sem Predominância...................... 53 

 

Tabela 7.1 Funções-Objetivo......................................................................................................67 

 

Tabela 7.2 Sequência Quarta Bateria de Resultados..................................................................67 

 

Tabela 7.3 Predição individual da parte testemunhada..............................................................73 

 

 



 

 

  

xxi 

 

 

 

 

 

Siglas 

 

BSS – Separação Cega de Sinais 

 

PCA – Análise de Componentes Principais 

 

ICA – Análise de Componentes Independentes 

 

K-NN – K-Vizinhos mais Próximos 

 

NMV – Número Mínimo de Vizinhos 

 

VMA – Valor de Máximo Acerto 

 

 



 

 

  

1 

 

 

 

 

 

Capítulo 1 

 

Introdução 

 

Na esfera da análise multivariada, uma das ferramentas que podem ser utilizadas em vários 

tipos de processamento de dados é a Análise de Componentes Independentes (Stone, 2005). Este 

artigo tem como objetivo aprofundar o conhecimento nesse método, além de alternativas para seu 

funcionamento, seja essas alternativas computacionais, ou conceituais. 

Como o ICA não carece de nenhuma informação sobre os dados, como qualquer Separação 

Cega de Sinais (Murata ET AL, 2001), a existência de sinais independentes nos dados é 

assumida, sem perda de aplicabilidade do método, visto que a independência é praticamente um 

preceito físico (Casey, 2001). Diferente dos outros métodos procura-se encontrar estimativas dos 

sinais independentes não-gaussianos, ou seja, que sua distribuição se afaste ao máximo da 

distribuição gaussiana (Comom, 1994). Logo, a busca por essa não-gaussianidade tem alto nível 

de importância no método. Existem vários métodos para encontrar tal atributo estatístico, porém é 

interessante balancear-se possíveis perdas e ganhos nessa procura. Métodos de robustez elevada 

podem levar a um preço computacional proporcionalmente elevado. Métodos rápidos podem ter 

grandes estimativas errôneas ou apresentar desvios perigosamente elevados. Essas preocupações, 

somadas à aplicabilidade do método aos dados podem traduzir um avanço na análise dos dados, 

assim como traduz a motivação dessa pesquisa em si.  

Medir a não-gaussianidade de um conjunto de dados, pode ser descrito como maximizar um 

conjunto de funções-objetivo procurando as estimativas das componentes independentes.  

Enquanto o ICA, como proposto em seu princípio (Hyvärinen, 2001), apesar de eficiente, tinha 

alto custo computacional. Por esse motivo, diversos autores procuraram novas alternativas para 

melhorar, acelerar ou otimizar tal processo [(Hyvärinen, 1999), (Marchini ET AL, 2009), 

(Cardoso ET AL, 2002)].  



 

 

  

2 

 

Dentre esses autores, um dos que se destacam é Aapo Hyvärinen da Universidade de 

Tecnologia de Helsinki, que criou o FastICA, um algoritmo baseado no esquema de iteração de 

ponto - fixo que maximiza a não-gaussianidade como uma medida de independência estatística.  

Entre as vantagens do FastICA, podemos destacar que o método é muito mais eficaz no que 

se diz respeito a custo computacional. A busca rápida dentro da função objetivo faz com que o 

FastICA seja mais rápido, como um todo, que o método convencional baseado em declínio de 

gradiente. Estima-se que a convergência paire na casa de 10 a 100 vezes mais rápida, de acordo 

com o próprio Hyvärinen. Além disso, esse método não requer que o usuário defina parâmetros 

para o funcionamento do mesmo, ao contrário do método convencional que necessita de algumas 

escolhas, como a taxa de aprendizado do gradiente para atualização da matriz.  

 Existem vários trabalhos que relacionam esta poderosa ferramenta com dados de perfil de 

poço [(Sancevero ET AL, 2008), (Landim, 1998), (Rosa ET AL, 2008)] e de forma robusta 

compõe um quadro já estabelecido desta utilização. 

 Este presente trabalho tem como objetivo estudar mais a fundo a técnica de FastICA, além 

de desenvolver vários testes que demonstrem a eficiência do método em cima de dados do 

Campo de Namorado, na Bacia de Campos. Com esse objetivo, foram buscados algoritmos para a 

utilização da Análise de Componentes Independentes. Diversos testes foram simulados e 

validados com mudanças de parâmetros dos métodos envolvidos, com a finalidade de observar o 

impacto dessas mudanças nos resultados observados. 

A forma de análise é apresenta de forma quantitativa, através de gráficos com porcentagem 

de acertos dentro do testemunho dos poços, podendo exprimir resultados que possam ser 

comparados e analisados, não só pelo ponto de vista teórico, mas também em simulações com 

dados reais. 

 



 

 

  

3 

 

 

 

 

 

Capítulo 2 

  

Separação Cega de Sinais 

 

A Separação Cega de Sinais (BSS, tradução de Blind Source/Signal Separation) é um 

método de processamento digital de sinais que consiste em encontrar, reconhecer ou separar 

fontes desconhecidas de um sinal captado por algum tipo de receptor (Jutten ET AL, 1991). A 

qualidade ?cega? que esse método carrega em seu nome, provém da desnecessidade de 

conhecimentos sobre essas fontes que se deseja encontrar. O que é um artifício tão simples para a 

compreensão humana, uma vez que o cérebro faz esse tipo de separação a todo o momento, a 

BSS encontra vários desafios na área computacional, desde o processamento até a construção do 

método, o que motiva muitos estudos sobre o caso.  

A separação cega de sinais teve início nos anos 1980 com o trabalho de Christian Jutten e 

consiste na separação de uma mistura de sinais, em sub-sinais que compõem a mistura inicial 

captada, com pouca, ou nenhuma informação adicional sobre as fontes envolvidas ou o processo 

que levou a mistura desses sinais. Logo, a BSS faz parte do grupo de métodos chamados não-

supervisionados, ou seja, métodos que não requerem treinos e classificações para serem 

propostos em um trabalho. 

Um exemplo largamente discutido e reproduzido é a festa de coquetel (?cocktail party‘) 

ilustrado na Figura 1: Em uma festa, num determinado ambiente, existem alguns microfones 

espalhados que captam os sons de pessoas conversando. O sinal recebido em um microfone, nada 

mais é do que todo ruído da festa mais a mistura das vozes de cada uma das pessoas, de modo 

que essas misturas são captadas de formas diferentes, pois os microfones são afetados de 

maneiras diferentes pelos sinais originais de voz. A explicação para que cada microfone registre 

uma mistura diferente está baseada no fato do som, como onda mecânica, interagir com os 

objetos na festa, recebendo a ação de reverberação e/ou obstáculos, além da distância entre as 

pessoas e os microfones, causando uma óbvia diferença entre cada receptor, dependendo da 

posição da pessoa na festa. Dentro desse sistema apresentado, o problema passa a ser a análise de 

todos os sinais gravados nos microfones e dessas misturas, extraírem-se os sinais originais de 



 

 

  

4 

 

cada interlocutor, sem ter informações sobre os dados objetos da festa, ou ainda sobre a posição 

de cada pessoa (fontes).  

Nesse exemplo, qualquer outro tipo de som, ou onda mecânica, capturado pelos receptores 

(música telefones tocando,...) é considerado ruído.  

 

 

Figura 2.1 Cocktail Party Problem 

 

Dado o exemplo, podemos extrapolar essa idéia de separação de sinais para várias outras 

áreas do conhecimento, como processamento de conjunto de dados; comunicações multiusuários; 

reconhecimento de voz e imagem; processamento de sinais biomédicos (Leite, 2004) e também 

para a Geoestatística (Hyvärinen, Karhunen &amp;amp; Oja, 2001). 

Como o método tenta separar fontes sem qualquer informação prévia, tal separação pode 

ser feita de várias formas, alterando-se vários parâmetros, como por exemplo, pode-se entender 

que os sinais observados podem ser reais ou complexos, contínuos ou discretos, de diferentes 

distribuições de probabilidade, entre outras características que influenciam nos cálculos de BSS 

(Cavalcante, 2004). Pela grande abrangência de características que um sinal pode ter, somada a 

complexidade do problema, a Separação Cega de Sinais necessita de estruturas matemáticas não-

lineares para que sua solução contemple a perfeita resposta dos sinais-fonte. Como tais estruturas 

frequentemente não têm solução conhecida, ou necessitam de ferramentas matemáticas 

demasiadas robustas para a obtenção da sua solução, opta-se, sem perda de generalidade, por 

construções lineares que simplificam o sistema de equações e incógnitas. Além dessa linearização 

do problema, algumas outras restrições são impostas à solução, dependendo do tipo de método de 



 

 

  

5 

 

BSS utilizado, a saber: fontes discretas, mesma distribuição de probabilidade, sub-gaussianas, 

não-gaussianas, independentes e estatisticamente independentes. (Cavalcante, 2004) 

 Matematicamente, a proposta do BSS pode ser descrita como um conjunto de sinais 

observados   , que na verdade são uma combinação de sinais-fontes, denotados por   e esta 

combinação esta relacionada através de uma matriz de mistura  , podendo o sistema ter a adição 

de algum tipo de ruído  , isto é 

       

A matriz de mistura  , no caso do cocktail party, seria o conjunto de fatores que constroem 

o sinal captado em um dado receptor. Nota-se que essa suposição linear da mistura é usada pela 

simplicidade, dita acima, sem que o método perca sua aplicabilidade. (Hyvärinen, Karhunen &amp;amp; 

Oja, 2001). 

Caso a matriz de mistura A fosse conhecida, o problema seria, simplesmente, resolver o 

sistema das equações envolvidas, não sendo necessário nenhum tipo de método de resolução. 

Contudo, como não existem informações sobre essa matriz A, o BSS deverá utilizar-se do 

conjunto de sinais observados para descobrir quais são os sinais-fonte. 

O objetivo desse método é encontrar os sinais S, que estão relacionados com os sinais 

captados  , através da matriz        , de forma que       . Essa matriz   é chamada de 

Matriz de separação e é uma aproximação da matriz pseudo-inversa    , pois como   pode ter 

qualquer tipo de dimensão, não necessariamente quadrada, não tendo, portando, uma verdadeira 

matriz inversa. 

A inversão da matriz A é sempre possível quando alguma característica estatística é 

aplicada. Por exemplo, no caso da Análise de Componentes Independentes (ICA), como o 

próprio nome denota, os sinais são estaticamente independentes, e se A é uma matriz quadrada, 

seu determinante é diferente de zero, logo A é invertível. Se A não é uma matriz quadrada, o 

cálculo é feito através da matriz pseudo-inversa. Quanto à parcela que cabe ao ruído, ela esta 

embutida nessa proposta, caso este seja tratado como uma fonte. Caso não seja, quando o vetor 

ruído r é considerado de forma aditiva no sistema de equações,este não fornece dificuldades para 

a inversão da matriz A, pois o ruído teria dimensionalidade compatível com a do sistema, e a 

soma e subtração de matrizes é uma operação simples e que não muda essa tal dimensionalidade. 

 A dimensão do sistema influencia no tipo de resolução que o método tem como resposta, 

pois a construção do sistema é dependente do número de captadores e do número de sinais-fonte 



 

 

  

6 

 

que estão colaborando para os sinais registrados. Sem uma análise profunda da matriz de mistura 

A, e levando em conta a matemática dessas funções e o número de fontes e de receptores, podem-

se dividir as situações em três possibilidades plausíveis: 

 

A) Número de Receptores menor que o Número de Fontes 

Também conhecido como caso sub-determinando. Quando o número de sinais originais é 

maior do que o número de receptores, na construção de equações, haveria menos 

equações ( ) que incógnitas ( ), logo nosso sistema é considerado incompleto. 

Matematicamente ainda é possível em alguns casos, construir uma noção de recorrência 

para determinar as fontes faltantes, mas talvez sem um significado físico. Um método 

para estimar essas fontes seria através da pseudo-inversa da matriz de misturas, ou outras 

técnicas mais sofisticadas (Hyvärinen, Karhunen &amp;amp; Oja, 2001). 

  

B) Número de Receptores maior que o Número de Fontes 

Conhecido como caso sobre-determinado. Quando o número de sinais é menor que o 

número de receptores, a construção de equações fica com um número maior de equações 

( ) que incógnitas ( ). De fato, se as equações não forem dependentes ou não se tratar de 

um sistema impossível, a resolução é a trivial do método, já que existe um número de 

dados maiores que o necessário para a construção do sistema de equações. 

  

C) Número de Receptores igual ao número de fontes 

É o caso clássico da resolução de BSS. Com o mesmo número de receptores e de fontes, o 

sistema de equações é um sistema de equações também com a mesma dimensão, logo   é 

quadrada e   também, deixando o problema envolvido       , mais simples e com 

uma solução bem definida, desde que as fontes não permitam graus de liberdade e 

dependência. 

  

 Vários métodos são usados para a Separação Cega de Sinais, entre os mais conhecidos 

estão a Análise de Componentes Principais (PCA), Análise de Componentes Independentes 

(ICA). Existem ainda as Redes Neurais, como os Mapas Auto-organizáveis(SOM, tradução de 

Self Organizing Maps) (Kohonen, 1997). 



 

                                                  

  

7 

 

 

 

 

 

Capítulo 3 

 

Análise de Componentes Principais 

 

A Análise de Componentes Principais (PCA, tradução de Principal Component Analysis) é 

um dos métodos mais conhecidos para o problema da Separação Cega de Sinais e é muito 

utilizada na área de processamento de dados que necessitam de extração de redundâncias 

(dimensões dependentes) de algum conjunto de informações (Hyvärinen ET AL, 2001). Essas 

extrações podem ser entendidas como uma redução de dimensionalidade dos dados, caso seja 

necessário diminuir a carga de informação disponível. 

Seja  , um vetor aleatório com   elementos, de forma que sejam conhecidos   elementos 

desse vetor, por exemplo,            A primeira hipótese que deve ser considerada sobre o vetor 

 , para a utilização do PCA, é de que esses vetores, necessariamente, devam apresentar alguma 

redundância, ou seja, devam ser mutuamente correlacionados. No caso de elementos 

independentes (a hipótese do ICA), o PCA não consegue resultados. 

Para o início do método, os dados são centralizados e é aplicado um pré-processamento 

chamado Branqueamento (Apêndice A). Depois desse pré-processamento, a Análise de 

Componentes Principais aplica uma mudança de base que mantenha as características espaciais 

do vetor aleatório  , agora padronizado e sem redundâncias. Nessa O vetor   apresenta    

elementos, onde    , pois o vetor   não apresenta redundância (correlação) entre seus termos 

e tem dimensão menor que de  . 

Essa mudança de base se dá pela busca de um novo conjunto de coordenadas, onde   

mantenha as características espaciais e estruturais de  , entretanto em um espaço de menor 

dimensão. 

Se   é uma possível representação sem redundâncias, seus elementos são não-

correlacionados, ou seja, as projeções dos elementos de   nesse novo espaço são não-

correlacionados e ortogonais. Além disso, as variâncias desses elementos são maximizadas uma a 

uma, de forma que a primeira componente principal tenha a máxima variância, a segunda 



 

                                                  

  

8 

 

componente principal tenha a segunda maior variância, assim por diante, até que a m-ésima 

componente principal tenha a menor variância. 

Matematicamente, existem inúmeras formas de se encontrar esse novo sistema de 

coordenadas com os mais variados métodos estatísticos, entre eles, algoritmos diretos 

(Maximização de Variância, Compressão de erro de quadrados médios mínimos) (Jolliffe, 2002) 

e algoritmos de aprendizado (Gradiente Ascendente Estocástico) (Gausch, 1982).  

 

3.1 PCA através da Maximização da Variância 

 Utilizando a notação anterior, uma possível combinação linear dos elementos de        ·, é   

            
  

 

   

 

onde           são coeficientes ou pesos do vetor    com dimensão de tamanho n. Como a 

PCA é um método iterativo, o vetor    é um chute inicial e aleatório, a ser atualizado a cada 

iteração buscando sua convergência final, da onde sairão as coordenadas da nova base. 

 Se a variância de   é máxima, então    é a primeira componente principal de x. Como a 

variância depende da norma e orientação do vetor de pesos      isto é, a variância cresceria sem 

limites, conforme a norma fosse aumentando, é imposto, sem perda de generalidade, que a norma 

de    seja constante e igual a 1. Com o vetor   maximizando o critério de PCA, 

  
             

        
        

             
      

Onde   é a Negentropia envolvida, um termo que será mais bem desenvolvido no Capítulo 

4, E[x] é a esperança sobre a função de densidade de probabilidade do vetor x, a norma de    é a 

norma euclidiana usual  e    é a matriz       de covariâncias do vetor x. 

Nesse novo espaço, os vetores são ortonormais, pois são ortogonais com norma igual a 1, 

então os autovalores         satisfazem a ordem crescente              e a solução 

maximizada do PCA se dá por 

       

 onde    é o vetor canônico da base do sistema de coordenadas sem redundância. 

Consequentemente,    é ortonormal a todos os outros vetores e o autovalor    é máximo 

entre os autovalores que restaram, portanto 

       



 

                                                  

  

9 

 

Recursivamente, segue que 

       

e a k-ésima Componente Principal é dada por 

      
   

 

3.2 PCA através da compressão do erro das médias quadráticas mínimas 

Assim como no exemplo anterior, nesse método de PCA, as componentes principais são 

definidas como somas com pesos onde a variância é máxima. Dessa forma, essas componentes, 

se normalizadas, formam uma base de m vetores ortonormais do subespaço de dimensão m. 

Denotando os vetores bases, mais uma vez como        , a projeção de x no subespaço 

que o abrange é     
  

       . O critério do erro das médias quadráticas (MSE, sigla do inglês 

Mean-square Error), para ser minimizado deve atender a 

    
              

     
 

   

    

Como os vetores são ortonormais, eles também são não correlacionados, e dessa forma, 

podemos reescrever o critério MSE como 

    
                   

   
 

   

    

                
     

 
    

De acordo com (Diamantaras, 1996), de fato a equação anterior tem como resultados os m 

vetores canônicos da base que procuramos. 

 

3.3 PCA através do algoritmo de gradiente ascendente estocástico 

Diferentemente dos dois métodos anteriores para encontrar as componentes principais, esse 

método apresenta uma regra de aprendizado, muito comum em processos que necessitam de 

algum tipo de atualização para cada passo que o método procura as componentes. 

Se a primeira componente principal    atende a hipótese de que         , a regra de 

aprendizado do método é 

                                    
          

onde      é a taxa de aprendizado que controla a velocidade com que o gradiente converge. 



 

                                                  

  

10 

 

Tanto a taxa     ·, quanto à própria convergência do método, tem seus detalhes e 

características garantidas por (Oja, 1995), e após alguma algebrização, pode-se reescrever a regra 

de aprendizado como 

              
     

A nomenclatura do método Gradiente Ascendente Estocástico (SGA, do inglês stochastic 

gradient ascent) é justificada, pois o gradiente não diz respeito à variância     
  , mas sim ao 

valor randômico instantâneo   
 . Matematicamente, isso corresponde a uma aproximação 

estocástica (Kushner, 1978).  

Através da aproximação estocástica com respeito ao vetor   , o gradiente   
  é aproximado 

e o método de algoritmo de gradiente ascendente estocástico, termina como 

                       
   

  

Onde do lado direito da expressão o termo    , é também conhecido como termo Hebbiano. 

O termo    , assim como o termo Hebbiano tem a convergência garantida por (Oja, 1997). Os 

outros termos da equação são restritos devidos as condições da ortonormalidade, logo, o método 

como um todo, apresentada convergência demonstrada por (Oja, 1982). Quando    , o método 

tem regra de aprendizado unidimensional, quando    , a regra tem bidimensionalidade, assim 

por diante, até que se    , a regra de aprendizado é n-dimensional. 

 

 

 

 

 

 

 

 

 

 

 

 



 

                                                  

  

11 

 

 

 

 

Capítulo 4 

 

Análise de Componentes Independentes 

 

A análise de componentes independentes (ICA, do inglês Independent Component 

Analysis) é um método estatístico computacional, que é usado para a separação cega de sinais, 

assim como a análise de componentes principais (PCA, do inglês Principal Component Analysis). 

De fato, seu método é muito parecido com o método PCA, onde alguns autores (Hyvärinen, 

FastICA 2.5) aplicam PCA nos dados antes de aplicar o ICA, como um pré-processamento. 

Apesar dos dois métodos basearem-se em uma estrutura semelhante, sua diferença tem origem na 

natureza dos dados: a correlação e independência das informações. No caso da aplicação da 

análise de componentes principais, é necessário que os dados apresentem algum tipo de 

correlação (redundância), ou seja, dentro dos vetores que compõem a base dos dados, pelo menos 

um deles é uma combinação de outros vetores dessa base.  

Já no caso da análise de componentes independentes, a hipótese levantada é justamente a 

oposta: os dados devem ser mutuamente independentes.  

Apenas com essa mudança de hipótese, caem algumas restrições da Análise de 

Componentes Principais, como a necessidade de dependência inicial dos dados. De fato, como o 

ICA tem menos restrições, um maior número de dados pode receber sua aplicação. A hipótese 

onde as componentes buscadas são independentes abre um grande leque de propriedades 

estatísticas que contribuem para o cálculo das componentes, como por exemplo, a possibilidade 

de se trabalhar com estatísticas de ordem superior a estatísticas de segunda ordem. (Comon, 

1994) 

O largo uso das separações cegas de sinais justifica a quantidade grande de métodos que 

garantem o sucesso, e a escolha para esse método pode associar-se a uma escolha empírica com 

base na observação dos dados. Como, por exemplo, no caso do ?Cocktail Party Problem?, em que 

duas pessoas conversando geram sinais independentes, afinal, uma pessoa, mesmo que esteja 

falando diretamente com outra pessoa, não produz informações sobre como será o sinal emitido 



 

                                                  

  

12 

 

por esta última. Empiricamente ainda, é comum considerar que sinais físicos tenham natureza 

independente, usando a mesma hipótese do exemplo supracitado. 

 

4.1 Conceito Matemático da Análise de Componentes independentes 
 

Assim como a Análise de Componentes Principais (PCA), a análise de componentes 

independentes (ICA), busca o conjunto de   sinais emitidos por fontes independentes   

            que são capturados por   receptadores              , onde todos os elementos 

de   é uma mistura dos   elementos de  . Sem perda de generalidade, assim como feito na 

suposição inicial da separação cega de sinais, caso essa mistura seja linear, os elementos de s e x 

estão interagindo através dos elementos           , de forma que  

     

O objetivo do problema é encontrar os elementos de  , ou seja, encontrar os elementos 

             
   que concebem as componentes independentes em 

     

  Questões de dimensionalidade foram previamente discutidas dentro do Capítulo 1. Apenas 

por simplicidade, nos cálculos para a solução da análise de componentes independentes, será 

considerado o caso em que o número de fontes é o mesmo de receptores, ou seja,     

deixando idênticas as dimensões das matrizes   e  . Como já explanado, apenas nesse caso,   é 

a inversa, propriamente dita, da matriz de mistura  .  

Em uma visualização matricial, se o problema inicial está parametrizado por uma variável 

de tempo  , tem-se 

 
     

 
     

   

       
   

       
   

     
 

     
  

 

cuja respectiva solução é   

 
     

 
     

   

       
   

       
   

     
 

      
  

 

Dessa configuração, percebe-se que o problema está baseado na resolução do sistema 

linear 



 

                                                  

  

13 

 

                                     

                                     

  

                                     

Sem informações prévias sobre a natureza dos dados, traduzidos no formato da matriz de 

mistura, muitas vezes, a única solução plausível para esse tipo de problema, seria uma infinita 

bateria de testes com as mais variadas configurações buscando a solução do sistema de equações. 

Definitivamente, esse não é um processamento de dados interessante, e uma solução possível 

para esse problema, pode ser calculada através da análise de componentes independentes se 

considerarmos que os sinais    são não-gaussianos. Isso basta para encontrar os elementos     

da matriz   e a solução do sistema linear, conhecendo então as soluções              . Em 

alguns casos, faz-se necessário a multiplicação de    por um escalar       para encontrar o 

valor exato da fonte original, já que na solução levantada pelo ICA, não são feitas suposições 

quanto à amplitude do sinal.  

Caso não fosse feita a suposição das componentes serem não-gaussianas, surgem 

problemas espaciais e de distribuição (Hyvärinen, 2001). Por exemplo, caso duas componentes 

independentes    e    tenham uma função densidade de probabilidade (fdp) conjunta gaussiana.  

As funções densidade de probabilidade marginais de    e de    são denotadas por 

                     

                     

como    e    são independentes por hipótese, a fdp conjunta dessas duas componentes é dada por 

                       

Logo, se a fdp conjunta de    e   é gaussiana,  

         
 

  
      

  
     

 

 
   

 

  
      

     

 
  

como a matriz   é ortogonal, devido ao branqueamento dos dados (pré-processamento), ela 

atende a seguinte propriedade 

       

e a fdp conjunta das misturas    e    é dada por (Papoulis, 1991) 



 

                                                  

  

14 

 

          
 

  
      

       

 
         

Mas como   é ortogonal              e                  , a fdp conjunta das 

misturas pode ser resumida em  

          
 

  
      

     

 
  

Fica claro então que as misturas    e    e as componentes independentes    e    têm a 

mesma estrutura espacial, chegando à conclusão que a matriz de mistura   não tem efeito sobre 

os dados, não podendo ser possível encontrar componentes independentes que componham uma 

nova base de vetores com dimensão reduzida. 

As condições abordadas até agora são primárias para o funcionamento do método da 

análise de componentes independentes. Com tais condições consolidadas, o desafio do método 

está em encontrar as componentes independentes menos-gaussianas possíveis.  Já foram 

estudados vários processos matemáticos e estatísticos para o cálculo dessas Componentes 

Independentes: através da curtose e negentropia; através do Estimador de Máxima Probabilidade 

e através da Minimização de Informação Mútua (Hyvärinen, 2001). Nas próximas seções 

abordaremos tais métodos, entretanto para os testes serão utilizados os métodos de FastICA 

através de negentropia. 

 

4.2 Convergência através de curtose e negentropia 
 

Quando a convergência é feita através de curtose ou negentropia, podem-se dividir os 

métodos em dois grandes grupos: os que calculam uma Componente Independente por vez e 

aqueles que calculam várias componentes independentes simultaneamente.  

Por sua vez, para o cálculo de uma componente por vez, existem três alternativas mais 

corriqueiras: Cálculo de Não-Gaussianidade, Algoritmo de Gradiente e o Algoritmo Rápido de 

Ponto - fixo. Cada um destes três métodos pode ter sua convergência calculada através de duas 

medidas estatísticas: via curtose ou via negentropia. 

Por outro lado, quando o cálculo baseia-se na escolha de várias componentes 

independentes simultaneamente, os métodos mais conhecidos são a Ortogonalização 

deflacionária e Ortogonalização simétrica (Hyvärinen, 1997). 



 

                                                  

  

15 

 

4.2.1 Convergência do método de uma componente por vez 
 

Quando o processo estatístico ou matemático encontra uma componente independente 

para cada iteração, a convergência é chamada de unitária ou uma-a-uma, como é informalmente 

conhecida. 

Seu funcionamento está baseado na maximização da não-gaussianidade, e a convergência 

desse processo gera a Primeira Componente Independente. Essa componente é a mais não-

gaussiana possível dentro dos dados.  Depois de encontrada a primeira componente não 

independente, o método procura outra componente, ortogonal a primeira e com a máxima não-

gaussianidade possível, quando encontrada, essa corresponde a Segunda Componente 

Independente. Similarmente, a  -ésima componente encontrada durante o método é ortogonal as 

    componentes anteriores e suas gaussianidades são decrescentes. 

4.2.1.1 Não-gaussianidade através de curtose 
 

A curtose (Apêndice A) ou seu valor absoluto são usados em larga escala como medidas 

de não-gaussianidade na análise de componentes independentes. De fato, como visto acima, uma 

simples análise pode ajudar a descartar possíveis variáveis, já que a curtose nula é geralmente 

associada a distribuições gaussianas. 

Além dessa análise superficial, a curtose pode agir na escolha das componentes. Se o 

modelo de ICA     , admite   componentes independentes   , com variância unitária, a 

primeira componente   deve ter uma resposta que seja compatível com a estrutura      , pois 

está sofrerá uma transformação linear. 

Uma mudança de base plausível pode ser descrita como      , logo, substituindo as 

equações, o problema pode ser proposto como       . 

Devido à linearidade da curtose (Nandi, 1999), tem-se 

                                   
          

 

   

 

 

   

 

e, por simplicidade, tomando-se kurt      , 

           
  

 

   

 



 

                                                  

  

16 

 

e         passa a ser um problema de otimização (Hill ET AL.2007), pois as curvas geradas nas 

  dimensões terão a não-gaussianidade maximizada, quando suas projeções forem vetores 

formadores dessa nova base.  

 

 

 

4.2.1.2 Algoritmo de Gradiente através de curtose 
 

De uma forma geral, para maximizar a curtose, dentro de uma amostra            ,deve 

ser computado o valor da curtose de        em que esta tem maior taxa de crescimento,onde  

  é um vetor de mistura.  Esse é um problema correlato de métodos de Gradiente (Stewart, 2005), 

e pode ser descrito como (Hyvärinen, 2001) 

              

  
                                       

O último termo dentro dos colchetes tem efeito apenas na norma, e não afeta o algoritmo 

de gradiente, logo pode ser desconsiderado. Além disso, como os dados são branqueados, 

               . Então podemos reescrever o algoritmo do gradiente como 

                            

         

agindo como uma versão iterativa e adaptativa.  

4.2.1.3 Algoritmo Rápido de Ponto - Fixo através de curtose 
 

O algoritmo via Gradiente com seu modelo final de aprendizado, gera uma rápida 

adaptação nos ambientes não-estacionários. Entretanto, sua convergência é lenta e depende de 

uma boa escolha da sequência de taxa de aprendizado. É comum que uma escolha ruim da taxa 

de aprendizado, impossibilite o método de convergir em componentes independentes.  Uma 

iteração com ponto fixo pode ajudar com esses problemas. Nesse caso, como o gradiente deve 

estar apontando para  , a reposta esperada deve ser igual a um escalar que multiplique por  . De 

acordo com a técnica de Multiplicadores de Lagrange (Stewart, 2005), a convergência é 

garantida. Derivando a equação de    , temos 

                       



 

                                                  

  

17 

 

Sugerindo um algoritmo de ponto - fixo, onde o lado direito da equação é calculado e 

passa a corresponder ao novo valor de  . Como         

                 

Podemos notar que o valor novo e o valor velho de   apontam para a mesma direção, 

logo não necessariamente ele converge para um único ponto, já que   e –   definem a mesma 

direção. Isso corrobora a afirmação anterior de que os métodos de ICA podem necessitar da 

multiplicação de um escalar para resgatar sinais originais idênticos. 

Esse método é tão eficiente que é chamado de FastICA( Hyvärinen,1997). Suas 

propriedades o tornam claramente superior aos métodos anteriores, pois além de sua 

convergência ser Cúbica (converge muito mais rápido), a desnecessidade de parâmetros para as 

iterações, elimina qualquer erro de convergência, caso fosse necessário escolher uma taxa inicial 

para o método. 

Na verdade, esse método é tão efetivo que é utilizado nas maiorias dos pacotes de análise 

de componentes independentes, devido a sua eficiência e rapidez. Os resultados deste presente 

trabalho também foram computados utilizando-se desse método.  

A negentropia é o método de medição de não-gaussianidade mais completo e confiável, 

sendo definido por (Hyvärinen, 2001) como o estimador ótimo para medição de não-

gaussianidade. Entretanto, seu custo computacional é extremamente elevado e necessita de 

estimadores não-paramétricos para sua utilização. Simplificações do método de medição de não-

gaussianidade através de negentropia excluem as maiores dificuldades do método e são 

amplamente utilizadas.  

4.2. 1.4 Não-gaussianidade através de negentropia 
 

Uma primeira aproximação para a negentropia é a utilização de cumulantes de ordem 

superior, de mesmo aspecto da resolução do método via curtose, 

      
 

  
         

 

  
          

Entretanto, se   tem uma distribuição aproximadamente simétrica, então       será 

aproximadamente zero e a aproximação de negentropia fica dependente apenas da curtose. 

Outra aproximação mais completa do ponto de vista da negentropia é necessária. Outra 

aproximação é substituir termos de ordem superior         por funções não quadráticas   , e 

aproximar a negentropia através de suas esperanças. Sem perda de generalidade, podem-se 



 

                                                  

  

18 

 

escolher duas funções não-quadráticas    e    de forma que a primeira é ímpar e a segunda par. 

A aproximação da negentropia é dada por 

                 
                         

  

onde    e    são constantes positivas e   é uma variável gaussiana normalizada. 

Pode-se utilizar ainda a aproximação com apenas uma função não-quadrática   

                         

desde que a função   não tenha distribuição simétrica, ou a aproximação      dependerá mais 

uma vez apenas da curtose. 

Computacionalmente, funções não-quadráticas com crescimento não muito rápido, 

compõem estimadores mais robustos e com boa taxa de convergência, como por exemplo  

       
 

  
           

              
     

onde        , geralmente escolhida igual a um. 

4.2. 1.5 Algoritmo de Gradiente através de negentropia 
 

Assim como o Algoritmo de Gradiente através de curtose, uma aproximação do modelo 

de gradiente para a negentropia                         , com respeito à   e levando em 

consideração a normalização                 , pode ser descrita como 

                 

         

com                      ,   uma variável aleatória gaussiana padronizada e     . 

A constante  , considerado o algoritmo de auto-adaptação do método, pode ser estimado 

em cada iteração como 

                      

Assim como a medida de não-gaussianidade através da negentropia, diferentes escolhas 

de funções afetam os resultados e principalmente sua convergência. Na aproximação de não-

gaussianidade através do algoritmo de gradiente e negentropia, as funções mais indicadas são 

                

             
  

 
  



 

                                                  

  

19 

 

       
  

onde        , geralmente escolhida igual a um 

A rotina que descreve o método de medição de não-gaussianidade com algoritmo de 

gradiente através da negentropia pode ser resumido como 

Tabela 4.1 Rotina do algoritmo de gradiente através da negentropia - Adaptado de 

(Hyvärinen,2001) 

1. Centralizar os dados. 

2. Aplicar o Branqueamento para obter o vetor  . 

3. Escolher aleatoriamente um vetor   de norma unitária e um valor para  . 

4. Atualizar                  com a escolha de   definida. 

5. Normalizar         . 

6. Caso o sinal de   não seja conhecido, atualizar                    

  . 

7. Se o modelo não apresentar convergência, voltar ao passo 4. 

 

4.2. 1.6 Algoritmo Rápido de Ponto - Fixo através de negentropia 
 

Assim como na convergência através da curtose, o algoritmo rápido de ponto - fixo 

através da negentropia é também um método conhecido de FastICA.  

O modelo de gradiente aponta para uma iteração de ponto fixo onde 

                

seguida da normalização de  . O coeficiente   é eliminado pela normalização, portanto é omitido 

da iteração. Entretanto, o sucesso da convergência não está tão garantido, devido às dificuldades 

algébricas da iteração. 

Uma possibilidade baseia-se em adicionar   multiplicado por uma constante   dos dois 

lados da aproximação anterior, gerando  

 

                          

 



 

                                                  

  

20 

 

e como essa soma não influencia na direção da nossa aproximação, essa soma não interfere nos 

pontos-fixos, sendo estes iguais aos pontos fixos da aproximação                . 

A constante   adequada determina a convergência e o velocidade desse algoritmo 

FastICA, portanto sua escolha deve ser otimizada para evitar iterações desnecessárias.  Um dos 

métodos mais robustos de aproximação é o método de Newton-Raphson, definido por, 

          
     

      
 

Apesar de o método atingir a convergência em poucas iterações, geralmente, para cada 

iteração é necessária computar uma matriz inversa, diminuindo a velocidade do método.  

Como esse algoritmo é computado com o intuito de fornecer os resultados o mais rápido 

possível, algumas variantes do método devem ser propostas e aplicadas. 

O primeiro passo para essa variante, é notar que o máximo da aproximação via 

negentropia de     é obtida com a otimização de          . De acordo com as condições de 

Lagrange (Stewart, 2005), tal otimização é obtida quando 

                  

Denotando a função do lado esquerdo da equação de F, e aplicando o gradiente (segunda 

variante de Lagrange),  

  

  
                   

Uma aproximação razoável é dada por                                   

             . O gradiente torna-se diagonal e pode ser facilmente invertido, e sua aproximação 

no método de Newton, pode ser descrita como 

                                      

que pode ser mais simplificada ainda caso seja multiplicada por               e simplificada 

de forma que 

                               

A rotina que descreve o método de medição de não-gaussianidade com algoritmo rápido 

de ponto - fixo (FastICA) pode ser resumido como 

Tabela 4.2 Rotina do FastICA através da negentropia - Adaptado de (Hyvärinen,2001) 

1. Centralizar os dados. 

2. Aplicar o Branqueamento para obter o vetor  . 



 

                                                  

  

21 

 

3. Escolher aleatoriamente um vetor   de norma unitária. 

4. Aproximar                                

5. Normalizar         . 

6. Se o modelo não apresentar convergência, voltar ao passo 4. 

 

4.2.2 Convergência de Múltiplas Componentes Independentes 
 

Os métodos anteriores para a convergência estimavam uma componente independente, 

por isso elas são chamadas de convergências unitárias. Entretanto, é possível encontrar mais de 

uma componente independente de uma vez ainda mantendo a não-gaussianidade máxima. Uma 

opção para essa estimativa pode partir do fato que as componentes independentes devem ser não -

correlacionadas no espaço branqueado, logo      
      

       
   , onde essa não correlação 

indica ortogonalidade. Logo, para a convergência de várias Componentes Independentes é 

necessário rodar os métodos demonstrados anteriormente, para vários valores de vetores    e 

ortogonalizar os resultados após cada iteração evitando que duas componentes independentes 

tenham convergência em um mesmo vetor. 

Dois métodos comuns utilizados para a convergência de várias componentes 

independentes são os métodos de Ortogonalização Deflacionária e Ortogonalização Simétrica. 

4.2.2.1 Ortogonalização Deflacionária 
 

A Ortogonalização Deflacionária é derivada do método de Gram-Shmidt (Farebrother, 

1974) que consiste em calcular      componentes independentes unitárias de interesse, e depois 

de cada iteração é subtraída da componente independente     , as projeções de todos as 

componentes      
      ,         anteriores. 

A rotina que descreve o método de medição de não-gaussianidade com Ortogonalização 

Deflacionária (FastICA) pode ser resumida como 

Tabela 4.3 Rotina do FastICA através da Ortogonalização Deflacionária- Adaptado de 

(Hyvärinen,2001) 

1. Centralizar os dados. 

2. Aplicar o Branqueamento para obter o vetor  . 



 

                                                  

  

22 

 

3. Escolher o número    de Componentes Independentes. Iniciar contador   em 

1. 

4. Escolher aleatoriamente um vetor    de forma randômica. 

5. Aproximar                                

6. Ortogonalizar o resultado com  

              
      

   

   

 

7. Normalizar            . 

8. Se    não apresentar convergência, voltar ao passo 5. 

9. Com a convergência de   ,         Se     , voltar ao passo quatro. 

 

4.2.2.2 Ortogonalização Simétrica 
 

Enquanto a ortogonalização deflacionária encontra uma Componente Independente por 

vez, e confere se cada iteração apresenta vetores não-correlacionados, a ortogonalização simétrica 

encontra as componentes independentes em paralelo, todas de uma vez. Uma motivação para a 

escolha desse método, é que os erros de estimativa nos primeiros vetores do método deflacionário 

acumulam-se nos vetores seguintes à ortogonalização.   

A ortogonalização simétrica é feita fazendo a convergência unitária para   vetores    em 

paralelo, e depois ortogonalizar todos os vetores    através dos métodos simétricos (Chen, 2009)  

        
 

 
   

Onde a raiz quadrada inversa      
 

 

  é obtida pela decomposição de autovalores       

                
 , de forma que 

                   
    

           
  

A rotina que descreve o método de medição de não-gaussianidade com Ortogonalização 

Simétrica (FastICA) pode ser resumida como 

Tabela 4.4 Rotina do FastICA através da Ortogonalização Simétrica - Adaptado de 

(Hyvärinen,2001) 

1. Centralizar os dados. 



 

                                                  

  

23 

 

2. Aplicar o Branqueamento para obter o vetor  . 

3. Escolher o número    de Componentes Independentes. 

4. Escolher os valores iniciais para os vetores   .  

5. Ortogonalize a matriz  , de acordo com o passo 6. 

6. Para todo         aproximar                                

7. Ortogonalizar simetricamente a matiz            
  calculando  

        
 

 
   

8. Se   não apresentar convergência, voltar ao passo 5. 

 

4.3 Convergência através do Estimador de Máxima Probabilidade 
 

O método de máxima probabilidade é um método baseado no ajuste de uma distribuição 

ou de modelo estatístico para os dados, provendo estimativas para os parâmetros deste modelo ou 

distribuição (Lucien, 1990). O método de Probabilidade Máxima, tradução do Inglês ?Maximum 

Likelihood (ML)?, é um aperfeiçoamento das Estatísticas Bayesianas, que descrevem as 

incertezas sobre quantidades invisíveis nos dados de forma probabilística (Howson, 2005). 

Seja   um vetor, a Estimativa de máxima Probabilidade,      maximiza a função de 

probabilidade de distribuição conjunta 

                               

Convenientemente, são escolhidos os cálculos sobre o logaritmo da função de 

probabilidade, pois muitas densidades de probabilidades contêm termos exponenciais. Em todo o 

caso, o resultado não é afetado, pois o estimador      que máxima a função        , também 

maximiza a função              (Hyvärinen, 2001). O Estimador de máxima probabilidade é 

geralmente encontrado através das soluções da equação de probabilidade 

 
 

  
               

       
   

Por hipótese, na Análise de componentes independentes, os vetores são independentes, 

então a função de probabilidade de distribuição conjunta pode ser reescrita como 

                  

 

   

 



 

                                                  

  

24 

 

Um artifício muito utilizado para o cálculo desse estimador está associado à relação de 

mapeamento de dois ou mais vetores no espaço. Se dois vetores aleatórios   e  , estão 

relacionados através de um mapeamento único 

       

         

Então, a densidade de distribuição      pode ser obtida através da densidade      (Dantas, 

2004), de forma que 

       
 

        
       

    
       

onde    é a matriz Jacobiana 

      

 
 
 
 
 
 
 
 
      

   

      

   
  

      

   
      

   

      

   
 

      

   
    

      

   

      

   
 

      

    
 
 
 
 
 
 
 

 

 onde       é a i-ésima componente do vetor função     .  

Através desse artifício, o vetor densidade    do vetor de mistura      pode ser dado 

calculado através de 

                                 

 

 

onde       . Essa identidade pode ser expressa através da função             
  

                   
   

 

 

Utilizando-se da propriedade de calcular o Estimador de Máxima Probabilidade com 

vetores independentes em formato de produtório, o Estimador  , em relação à   é 

             
             

 

   

 

   

 

Como dito acima, é comum utilizar-se do logaritmo do Estimador para o cálculo, sendo 

assim 



 

                                                  

  

25 

 

                   
                  

 

   

 

   

 

por simplicidade de notação, substitui-se a somatória sobre o índice t pela esperança do operador, 

e dividi-se o estimador por T, obtendo 

 

 
                    

                  

 

   

 

Na prática, são necessários algoritmos que computem a Estimativa de Máxima 

Probabilidade, entre eles: Algoritmos de Gradiente, Algoritmo rápido de ponto - fixo e o 

princípio da máxima informação mútua, Infomax (Linsker, 1988). 

 

4.3.1 Algoritmos de Gradiente para a Estimativa da Máxima Probabilidade 
 

Assim como as convergências dos Componentes Independentes da ICA, utilizando-se da 

maximização de não-gaussianidade, os métodos mais simples para o cálculo da Máxima 

Probabilidade são os algoritmos de gradiente. De fato, eles têm uma sequência de computações 

bem similar. Dois métodos conhecidos para maximizar a probabilidade são o algoritmo de Bell-

Sejnowsi e o algoritmo de gradiente natural (Hyvärinen, 2001). 

 

4.3.1.1 Algoritmo de Bell-Sejnowski 
 

O algoritmo de Bell-Sejnowski (Bell, 1995) é descrito como uma versão estocástica do 

gradiente do algoritmo de Máxima probabilidade. 

O primeiro passo desse método consiste em calcular o gradiente do logaritmo do Estimador 

de Máxima Probabilidade  

 

 

     

  
                     

onde                         é um vetor de funções onde os elementos são chamados de 

funções escore e são definidas como 

          
  

  
 

  
 



 

                                                  

  

26 

 

A estimativa de Máxima Probabilidade é proporcional ao gradiente da função de 

probabilidade 

                         

E uma versão estocástica dessa proporcionalidade é dada pela omissão da esperança 

                     

Apesar de convergir, a inversão da matriz   deixa o processo extremamente lento. A 

velocidade do método pode ser aumentada, tratando os dados com pré-processamento 

(branqueamento), ou utilizando-se do Gradiente Natural. 

 

4.3.1.2 Algoritmo de Gradiente Natural 
 

O gradiente natural é baseado da estrutura geométrica do espaço paramétrico onde estão os 

dados e está relacionado com os Grupos de Lie e sua estrutura (San Martin, 1999). 

O início do método de gradiente natural está baseado em multiplicar a estimativa da 

máxima probabilidade anterior, em sua forma não-estocástica, por    , obtendo 

                    

Esse algoritmo pode ser interpretado como uma decorrelação não-linear (Li ET AL, 2010). 

A convergência acontecerá quando            , o que significa que    e        são não 

correlacionados para todo índices diferentes    . 

As três funções não-lineares mais utilizadas como     , são 

                 

                 

          

Os índices   e – nas duas primeiras funções não-lineares estão relacionadas com uma estimativa 

de qual função é mais adequada aos dados. 

Essa escolha pode ser feita computando o momento não-polinomial (Leite, 2005), onde 

                           
   

onde    são algumas estimativas de componentes independentes. Caso o momento não-polinomial 

tenha resposta positiva, a função de não-linearidade a ser usada deve ser      , caso o momento 

não-polinomial tenha resposta negativa, a função escolhida deve ser a      , justificando a 

relação de seus índices. 



 

                                                  

  

27 

 

4.3.2 Algoritmo Rápido de Ponto - Fixo 
 

O algoritmo rápido de ponto - fixo (FastICA) para a estimativa de máxima probabilidade 

é quase idêntico ao algoritmo empregado para a maximização da não-gaussianidade, se as 

estimativas das componentes independentes compuserem um espaço branqueado. 

 Considerando a linearidade da esperança no logaritmo da probabilidade, com uma 

densidade assumida de    , seu cálculo é 

 

 
           

 

   

         
                    

 O primeiro termo do lado direito da igualdade é a soma de termos       
    , 

maximizado quando      geram componentes independentes, de acordo com o Teorema 8.1 

de (Hyvärinen, 2001). A não-correlação e a variância unitária de    significam que     
   

           , que pode ser reescrito como 

                                              

logo      deve ser constante. 

Em particular, o termo            deve ser constante e a probabilidade basicamente 

consiste na soma de   termos da forma otimizada do FastICA, deixando o método de algoritmo 

rápido de ponto-fixo para a estimativa de máxima probabilidade, quase idêntico ao algoritmo 

usado para a maximização da não-gaussianidade. 

O algoritmo para a não-gaussianidade é calculado através de  

                                      

onde               . Reescrevendo esse algoritmo na forma de matriz, é obtido 

                               
        

Onde            
            e     . 

Para expressar a convergência usando informação não branqueada, basta multiplicar os dois 

lados pela matriz de branqueamento. Em suma, essa multiplicação apenas substitui   por  : 

                               
        

onde                       e            
           . Essa última é a iteração básica 

do FastICA com estimativa de máxima probabilidade e gradiente natural. 

A cada passo do FastICA, a matriz   deve ser projetada sobre as matrizes branqueadas, e 

tal projeção pode ser feita através do método clássico das raízes quadradas de matrizes 



 

                                                  

  

28 

 

         
 

 
   

onde C é a matriz de correlação dos dados. 

A rotina que descreve o método de estimativa de máxima probabilidade com Algoritmo 

Rápido de Ponto - Fixo (FastICA) pode ser resumida como 

 

Tabela 4.5 Rotina do FastICA através da Estimativa de Máxima Probabilidade - Adaptado de 

(Hyvärinen, 2001) 

1. Centralizar os dados. 

2. Computar a matriz de correlação           

3. Escolher a matriz inicial de separação   

4. Computar  

     

                         

     
 

             
         

5. Atualizar a matriz de separação de acordo com 

                               
        

6. Descorrelacionar e normalizar  , através de  

         
 

 
   

 

7. Se   não apresentar convergência, voltar ao passo 3. 

 

4.3.3 Princípio da Maximização da Informação Mútua (Infomax) 
 

O princípio da Infomax é baseado na maximização da entropia como um vetor-saída, ou seja, do 

fluxo de informação dentro de redes-neurais (Wasserman, 1989) 

Se   é um vetor-entrada, onde os vetores-saídas, após um processo de redes-neurais, são da 

forma 

          
        



 

                                                  

  

29 

 

onde    é alguma função não-linear e    é o vetor-peso dos neurônios(Anderson, 1995),   é um 

vetor gaussiano com componentes de ruído. A entropia dos vetores-saída (Gokhale, 1989) é 

             
           

     

   Utilizando-se da fórmula de entropia para uma transformação (Cover, 1991) 

       
           

                     
  

  
      

onde         
           

   , é a função definida pela rede neural. A derivada do segundo 

termo do lado direito da igualdade da entropia resulta em 

         
  

  
               

    
              

 

 

O vetor-saída da entropia tem o mesmo formato do cálculo da maximização de máxima 

probabilidade, com a diferença de que as funções densidades   utilizadas anteriormente são 

substituídas pela utilização das derivadas   
  de funções não-lineares. A escolha das funções não-

lineares    recai sobre as opções anteriores dos outros algoritmos, ou seja,              

pode ser denotada por k. 

4.4 Convergência através da Minimização de Informação Mútua 
 

No tópico 2.3.3 foi apresentado o conceito de maximização da informação mútua, também 

conhecido em sua literatura como Infomax. Mas assim como seus algoritmos antecedentes, os 

dados envolvidos foram considerados como seguindo um modelo compatível com o da Analise 

de Componentes Independentes.  Caso a hipótese do modelo não seja levada em consideração, 

um novo tipo de medida para a convergência das Componentes Independentes deve ser usada. 

Em todo o caso, o método ICA deverá ser composto por componentes independentes, logo 

uma medida que calcula a dependência de duas possíveis componentes pode ser utilizado para o 

cálculo da convergência. Dessa forma, a análise de componentes independentes pode ser vista 

como uma decomposição linear que minimiza a dependência (Hyvärinen, 2001) de vetores dentro 

de um espaço. 

Uma aproximação interessante para essa tentativa de convergência é utilizando-se da 

Minimização da Informação Mútua. 

A Informação Mútua   entre    variáveis aleatórias            é definida como 



 

                                                  

  

30 

 

                       

 

   

     

onde      é o diferencial de entropia, previamente apresentado. 

 

4.4.1 A não-gaussianidade dentro da Minimização da informação mútua  
 

A informação mútua pode ser obtida através do diferencial de entropia, assim como feito no 

tópico 2.3.3. Para uma transformação linear do tipo      

                       

 

   

                

Considerando os vetores   , por hipótese, não-correlacionados e variância unitária, como 

visto no algoritmo FastICA para maximização de máxima probabilidade, podemos concluir que  

                                              

portanto ,   deve ser constante e           também deve ser constante.  

A negentropia     , assim como descrita anteriormente  

                      

Devido à variância unitária dos vetores   , pode ser considerada como  

                  

Dessas duas igualdades, após alguma algebrização, a medida de informação mútua pode ser 

escrita como 

                             

 

   

 

Nessa configuração, a minimização da informação mútua é equivalente a encontrar as 

direções onde a negentropia é maximizada. Como visto anteriormente, a negentropia pode ser 

utilizada como uma medida de não-gaussianidade. Entretanto, ao contrário da convergência via 

negentropia, onde era possível encontrar várias componentes, através da ortogonalização 

deflacionária, com o método de minimização de informação mútua, esse tipo de convergência 

unitária para várias componentes independentes não é possível. 

Caso seja necessário, pode-se excluir a hipótese de não-correlação dos vetores   , e 

trabalhar-se apenas com a aproximação anterior. Tal resultado, apesar de não ótimo, aumenta o 

alcance do método, e demonstrar robustez na técnica. 



 

                                                  

  

31 

 

 

4.4.2 Máxima Probabilidade na minimização de informação mútua  
 

A relação desses dois algoritmos já foi percebida na introdução do principio de 

Maximização de Informação Mútua (Infomax) como uma alternativa para a estimativa de 

máxima probabilidade. 

Relembrando que a expectativa do logaritmo da máxima probabilidade,   

 

 
           

 

   

         
                    

Como visto na estimativa de máxima probabilidade, as densidades    são iguais as 

densidades de   
  . Denotando as fdp de                    , a informação mútua pode ser 

aproximada para 

                  

   

                        

Essa configuração de Informação mútua é uma aproximação da estimativa de máxima 

probabilidade, a não ser pelo sinal e pela constante     . 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 



 

                                                  

  

33 

 

 

Capítulo 5 

 

Reconhecimento de Padrões 

 

Reconhecer padrões é uma das vertentes da Aprendizagem de Máquina (Duda, 1973), 

cujo objetivo é classificar informações, baseado em algum conhecimento prévio da natureza 

dessas informações ou em informações estatísticas extraídas dos padrões existentes nos dados. 

Por sua vez, a aprendizagem da máquina é um campo da Inteligência Artificial e seu estudo é 

voltado ao desenvolvimento e descoberta de algoritmos que auxiliam a máquina em uma 

determinada tarefa através de aperfeiçoamentos em cada iteração de seu programa (Michie ET 

AL 1994). Em suma, um algoritmo pode aprender ou aperfeiçoar a realização de alguma tarefa 

específica através de um banco de dados ou através de uma análise instantânea.  

Os estudos de Inteligência Artificial (IA) começaram logo após a Segunda Guerra 

Mundial (Turing, 1950) e contemplam pesquisas sobre dispositivos computacionais que simulam 

uma capacidade racional na resolução de um problema ou na execução de uma tarefa.  

Apesar da área de pesquisa em Inteligência Artificial ser tão grande e abranger inúmeros 

métodos, o Reconhecimento de padrões é uma técnica extremamente utilizada (van der Walt ET 

AL, 2006) e geralmente associada a redes neurais. 

O termo padrão pode ter diversas acepções, de acordo com a área em que é utilizado. No 

presente caso, pode-se associar-se padrão a um comportamento de um perfil para uma 

determinada fácie. Se esses elementos repetem-se de maneira previsível, diz-se que existe um 

padrão desses elementos. Um modelo que pode ser usado para gerar algum tipo de informação, a 

priori, não disponível através da análise do comportamento do modelo, também é um 

reconhecimento de padrão desse modelo (Fuller, 1975). 

 

 5.1 Sistema de Reconhecimento de padrões 

Em geral, pode-se dividir o Reconhecimento de Padrões, em etapas subseqüentes que 

contemplam a aplicação desse método. São elas: aquisição de dados, localização de segmentação 

de padrões, extração de características, classificação e pós-processamento, descritos a seguir. 

5.1.1 Aquisição dos Dados 



 

                                                  

  

34 

 

 

No subtítulo, o termo ?Aquisição?, geralmente associado à ação de captar dados, tem uma 

maior abrangência do que esta costumeira. Nessa etapa, não é necessário que os dados coletados 

sejam amostras recolhidas pelo próprio elemento que aplicará esse método. O fato de se emular 

ou simular dados pode ser considerado uma forma de aquisição de informações. Ou ainda, 

podem-se coletar informações de um banco de dados conhecidos para o uso do método. 

Nessa etapa são escolhidas as características que serão treinadas e testadas. A natureza 

física dos dados pode indicar qual tipo de característica terá maior influência no treino e na 

classificação de um conjunto de informações, como pode ser notado pelos exemplos de 

aplicações mostrados na Tabela 3.1, baseada nas notas de aula do Prof. João Ascenso (Ascenso, 

2003) 

 

 Tabela 5. 1 Aplicações do Reconhecimento de Padrões 

Análise Entradas Saídas 

Identificação de Recursos 

Naturais 

Imagens Espectrais, Dados 

de amostras geológicas 

Litologia, presença de minérios, formas 

de terrenos 

Reconhecimento de Voz Sinais de Voz Palavras/ Identidade do locator 

Testes Destrutivos Ultra-sons/ Imagens Presença/Ausênsia de anomalias 

Detecção de falhas  

( Circuitos integrados, Texturas)  
Imagens Acietação Rejeição 

Robótica 
Imagens 3D, Laser, Luz 

estruturada 

Identificação de Objetos, tarefas 

industriais 

Identificação e contagem de 

células 

Tecidos selecionados, 

amostras de sangue 
Tipo de célula 

Detecção/ Diagnóstico de 

doenças 
ECG,EEG 

Condições cardíacas, cerebrais. 

Patologias 

      

 

5.1.2 Localização de segmentação de padrões 

 

A busca e reconhecimento dos padrões existentes nos dados podem ser feitos por vários 

métodos, onde os de maior uso podem ser divididos em algoritmos baseados em fatores 



 

                                                  

  

35 

 

estatísticos (teoria da decisão) ou algoritmos baseado em fatores sintáticos (estrutural). Através 

de análises estatísticas, o reconhecimento de padrões baseia-se na caracterização de padrões 

estatísticos, os quais, por hipótese, são gerados por distribuições probabilísticas. Já as análises 

sintáticas de reconhecimento de padrões atuam baseadas na inter-relação estrutural e espacial 

(geometria) de recursos. (Koutroumbas, 2008).  

 Uma grande variedade de algoritmos pode ser aplicada para reconhecimento de padrões, 

entre os mais conhecidos estão os classificadores mais simples do teorema de Bayes, os 

algoritmos k-vizinhos, redes neurais, entre outros. 

 

5.1.3 Extração de Características 

 

Os padrões encontrados em um conjunto de dados são identificados e diferenciado s 

formando, para cada qual, uma característica associada (Kulikowski ET AL, 1991). Por exemplo, 

caso seja utilizado um algoritmo Sintático, a estrutura dos dados é o fator decisivo para separar e 

identificar esses padrões.  

Elementos dentro do conjunto de informações que apresentam semelhanças geométricas 

estão associados ao mesmo padrão, e assim, associados a uma mesma característica estrutural. 

Essa característica deve ser invariante a transformações espaciais, ou a mesma não pode ser 

chamada de padrão. Essas transformações podem ser rotações, translações, coordenadas polares, 

entre outras, como indica a Figura 5.1.  

 

Figura 5. 1 Exemplos de Translação e Rotação de um Objeto-Padrão Inicial 

 



 

                                                  

  

36 

 

Quando essas características são extraídas intrinsecamente dos dados, o reconhecimento 

de padrões é chamado de aprendizado não-Supervisionado. Por outro lado, quando são inseridas 

informações sobre as características, tais informações contêm rótulos (títulos) que são utilizados 

como características, esse reconhecimento de padrões é chamado de aprendizado supervisionado. 

Em suma, se um conjunto de padrões que já foi classificado ou descrito, este conjunto de padrões 

é denominado o conjunto de treinamento e a estratégia de aprendizagem é caracterizada como um 

aprendizado supervisionado. Se o sistema de padrões não é fornecido, a priori, a estratégia de 

aprendizagem estabelece classes de características com base nas regularidades intrínsecas dos 

padrões, e é conhecida por aprendizado não-supervisonado. 

 

5.1.4 Classificação 

 

Como visto no subtópico 3.1.3, o aprendizado do reconhecimento de padrões pode ser 

supervisionado ou não-supervisionado, e a classificação de informações depende desse modelo 

de aprendizagem. 

O modelo supervisionado, como descrito, precisa de um conjunto de informações prévias, 

anexadas de rótulos que servirão de base para as características. A essas informações prévias é 

dado o nome de conjunto de treino (Schuermann ET AL, 1996). O conjunto de treino de uma 

aprendizagem não supervisionada funciona como um banco de dados, guardando as informações 

sobre determinadas estruturas e associando-as aos rótulos anexados a cada característica. Esse 

conjunto é utilizado com a finalidade associar rótulos a padrões presentes em outro conjunto de 

dados, conhecido como conjunto de teste. 

Basicamente, o algoritmo encontra padrões no conjunto de teste, associa-os a algum 

padrão semelhante existente no conjunto de treino. Dada essa associação, os padrões encontrados 

no conjunto de teste têm anexados em si o rótulo do padrão de treino que lhe é semelhante. 

Se o modelo de aprendizagem for do tipo não-supervisionada, ele não necessita do 

conjunto de treino. O algoritmo procura padrões intrínsecos dentro das informações e os separa 

em classes.  

Cada classe será um subconjunto do conjunto total de informações, e dentro de cada 

classe, todos os elementos serão apenas informações que tem o mesmo padrão, ou seja, elementos 

que se assemelham estarão juntos em alguma classe.  



 

                                                  

  

37 

 

 

5.1.5 Pós-Processamento 

 

Essa fase está associada a um aperfeiçoamento da técnica como um passo iterativo de 

reconhecimento de padrões. Averiguar quais características compõem melhores escolhas, ou 

estão associadas mais fortemente aos padrões dos dados. 

Quanto à aprendizagem não-supervisionada, caso exista algum conhecimento sobre 

rótulos, esses podem ser empregados nas classes que apresentam altas concentrações de 

elementos com esses rótulos. De fato, esse tipo de rotulação não implica na mudança do termo da 

aprendizagem para Supervisionada, já que toda separação dessas classes foi feita sem essas 

informações. Como tais informações são aferidas após toda a separação desses padrões, esta é 

considerada um pós-processamento do algoritmo. 

 

5.2 Aprendizagem Supervisionada  

 

No caso da aprendizagem supervisionada, um conjunto de treino serve de guia para que o 

algoritmo encontre os padrões existentes nos dados. As alterações dos pesos desses padrões são 

calculadas de forma a que a resposta do algoritmo se assemelhe a alguma resposta que se 

encontra no conjunto de treino.  

Os dados do conjunto de treino servem de exemplos de treinamento. Na aprendizagem 

supervisionada, cada exemplo é um par constituído por um objeto de entrada (normalmente um 

vetor) e um valor de saída desejado (rótulo). Um algoritmo de aprendizado supervisio nado 

analisa os dados de treinamento e produz uma função que é chamado de classificadora (se a saída 

é discreta) ou uma função de regressão (se a saída é contínua). A função inferida deve prever o 

valor de saída correta para qualquer objeto de entrada válida. Isto exige que o algoritmo de 

aprendizagem generalize de forma satisfatória, a partir de situações invisíveis dos dados de 

treinamento, os rótulos desses valores de entrada.  

A aprendizagem supervisionada pode ser também subdividida em métodos paramétricos e 

métodos não paramétricos. 

 



 

                                                  

  

38 

 

5.2.1 Métodos Paramétricos 

 

Métodos Paramétricos são aplicados quando a distribuição que gerou o conjunto de dados 

é conhecida, ou se pode avaliar qual a possível distribuição, dentre as várias distribuições 

plausíveis, que gerou as informações (Cardoso, 2001). Admite-se ainda que a distribuição possa 

ser expressa analiticamente, sendo o objetivo determinar os parâmetros da mesma. 

Matematicamente, o modelo paramétrico   é uma coleção de distribuições de 

probabilidade, onde cada membro dessa coleção    é descrito por um parâmetro de dimensão 

finita  . Se   é a dimensão total do modelo, para    , o conjunto de valores possíveis para os 

parâmetros do método são denotados por        e o modelo é descrito como 

            

Se o método paramétrico pode ser descrito apenas com funções contínuas, pode-se 

descrevê-lo através de suas funções de densidade de probabilidade 

            

Como exemplos de métodos paramétricos podem-se destacar: 

  A família de Poisson (Good, 1986) 

          
  

  
                       

onde   é a função densidade de probabilidade. Neste caso        e         

A família Normal (Marsaglia, 2004), parametrizada por        , é dada por 

          
 

     
     

      

      
 
             

.  A Família de Translação de Weibull (Weibull, 1951), parametrizada por          , é 

dada por 

          
 

 

        

      
     

      

    
                 

Apesar de notadamente conhecidos e de teoricamente bem estruturados, esses métodos 

necessitam de informações geralmente não disponíveis pra sua utilização. O mais usual método 

paramétrico para obter os parâmetros da distribuição é o método de estimativa de máxima 

verossimilhança, o que pode ser geralmente feito de modo analítico. 

 



 

                                                  

  

39 

 

5.2.1.1 Estimativa de Máxima Verossimilhança 

 

Assim como na Análise de Componentes Independentes, a estimativa de máxima 

verossimilhança para métodos paramétricos de reconhecimento de padrões é um método 

estatístico utilizado para ajustar um modelo estatístico para dados e fornecer estimativas para os 

parâmetros do modelo. 

Para um conjunto fixo de dados e modelo de probabilidades subjacentes conhecidos, o 

método de máxima verossimilhança seleciona os valores dos parâmetros do modelo que 

maximizam a função de verossimilhança. A estimativa apresenta uma abordagem unificada para 

avaliação, que é bem definida, no caso da distribuição normal e em quase todas as outras 

distribuições. 

Seja   um vetor de n-ésima dimensão, onde seus elementos            são 

observações de uma distribuição desconhecida      . Essa distribuição desconhecida       

pertence a uma família de distribuições definida por               . Então      pode ser 

definida como 

                  

O valor desconhecido   é conhecido por valor verdadeiro do parâmetro. O método de 

estimativa máxima de verossimilhança busca um estimador    que se aproxime do valor 

verdadeiro 

Sem perda de generalidade, considerando que as amostras são independentes, a função 

de densidade conjunta de   é definida por 

                                                

Estendendo o domínio da fdp, pode-se definir a densidade através do parâmetro  . Para o 

mesmo vetor  , essa aproximação é conhecida por função de verossimilhança, e é definida por 

                                          

 

   

 



 

                                                  

  

40 

 

Há de se notar que essa função de verossimilhança não é uma densidade de probabilidade, 

pois ela não precisa ser uma função aditiva, portanto não pode ser considerada uma medida de 

probabilidade. 

Como visto no capítulo 3, é comum e mais conveniente trabalhar com o logaritmo da 

função de probabilidade. É costumeiro, ainda, dividir a função de verossimilhança pela sua 

média. Essa aproximação pode ser definida por 

    
 

 
                   

 

 
        

  

   

 

O método de máxima verossimilhança estima   , através da maximização da função 

        .Logo, o estimador de máxima verossimilhança    é definido por  

      
    

                 

 

5.2.2 Métodos Não-Paramétricos 

 

Em um método não-paramétrico, não é conhecida a distribuição que gerou os dados nem 

se admite que essa distribuição possa ser expressa analiticamente, sendo necessário exprimir a 

densidade de probabilidade de forma numérica. 

O primeiro passo para a aplicação desse método é calcular uma estimativa para a função 

densidade de probabilidade a partir dos dados de treino. Em geral, se   é a probabilidade de um 

vetor  , no conjunto de dados     pode ser expressa por 

             

 

 

Se   é um espaço com   amostras, e dessas, estão disponíveis   amostras nos dados, 

através da probabilidade binomial, têm-se 

     
 
 
            

e sua esperança é        . 



 

                                                  

  

41 

 

Como a distribuição binomial tem um pico muito alto na sua esperança, o número de 

amostras    deve ser aproximadamente igual ao valor esperado, ou seja,      , ou seja, para 

um modelo discreto o suficiente,        . 

Se   é uma região pequena de forma que      não tenha grandes variações, é possível 

aproximar-se a Probabilidade P por (Box ET AL, 1978): 

             

 

         

 onde      e   é o volume da região  . 

Como              e       , a combinação dessas duas equações resulta em 

       
  
  

 

Com o intuito de estimar a densidade em   , define-se   subconjuntos de  , de forma que 

  esteja contido em     com         . Logo podemos definir as probabilidades marginais 

como 

       
  
   

 

e            quando 

   
   

     

   
   

     

   
   

  
 

   

Os limites acima, para a convergência        
  

   
, não são numericamente computados, 

pois dentro da possibilidade desses limites, é mais interessante calcular a probabilidade p(x) e não 

sua aproximação para essas   amostras. 

Dada as condições acima para métodos não-paramétricos, é comum o uso do algoritmo de 

Janelas de Parzen, quando    
  

  
 e do algoritmo dos K vizinhos mais próximos, também 

conhecido por K-NN do inglês ?K-nearest neighbour?, quando      . 

 

5.2.2.1 Método das Janelas de Parzen 

 



 

                                                  

  

42 

 

O método das janelas de Parzen'' (Parzen, 1962), funciona como um histograma contínuo 

onde A região    é centralizada no ponto  , onde se é desejada a estimativa da densidade 

(Bishop, 1995).  

Espacialmente, assume-se que a região    é um cubo de   dimensões e com aresta de 

tamanho   . O número de amostras em cada região é obtido da função de janela 

                    
 

 
                     

  

As estimativas são dadas por  

       
    

  
 

 

   

 

    
 

 
 

 

  
  

    
  

 

 

   

   

O tamanho da janela tem grande influência na estimativa, pois se a janela é muito grande, 

ela acaba repetindo informações já observadas em outras janelas. Se a janela é pequena, uma 

quantidade de informações será descartada para os cálculos da estimativa. 

A convergência do método é aferida se 

   
   

                         
   

  
       

onde        é a média e   
     é a variância da variável aleatória      . Pelas seguintes 

premissas, 

   
   

                  
   

      

a convergência é garantida. 

O método de janela de Parzen apresenta certos problemas, como a escolha ótima do 

tamanho da janela, ou a seleção inicial do volume de  .   

 

5.3.2.2 K Vizinhos mais Próximos 

 

O método dos k vizinhos mais próximos, cuja sigla é K-NN do inglês K-nearest 

Neighbour, é um algoritmo classificador baseado na distância estrutural de um conjunto de testes 

do conjunto de treino (Cover, 1967). Dado um conjunto de testes com várias amostras, cada 



 

                                                  

  

43 

 

amostra será analisada espacialmente e seu rótulo (classificação) será igual a do maior número de 

vizinhos na vizinhança escolhida. 

O método K-NN é considerado um tipo de aprendizado preguiçoso, pois dado um 

conjunto de treino, ele só atua localmente e seus cálculos só são efetuados caso seja também 

inserido um conjunto de testes. Apesar disso, o K-NN está entre os algoritmos mais simples da 

Aprendizagem da Máquina, o que torna sua utilização recomendada para casos onde 

computações probabilísticas ou escolhas iniciais podem complicar a solução. De fato, a única 

entrada necessária, além dos conjuntos de Teste e Treino, é o número de vizinhos   da vizinhança 

(Toussaint, 2005) 

 

 

                                                                 Figura 5. 2  Vizinhos Utilizados para Diferentes Números de K 

 

Seja              um conjunto de treino com   amostras rotuladas, ou seja, com uma 

identificação relacionada a cada amostrada e   um ponto que necessita ser classificado. 

O ponto mais próximo       , matematicamente, é o ponto onde 



 

                                                  

  

44 

 

       
     

 
        

com         é a distância entre os pontos        . 

Se     e        é o ponto mais próximo de  , o rótulo atribuído ao ponto  , será o 

mesmo rótulo do ponto     

Analiticamente, Se       e                      são os   pontos mais próximos de  , 

o rótulo atribuído ao ponto x, será o rótulo de maior freqüência entre os elementos de  . 

Costuma-se escolher o número de vizinhos   como sendo ímpar, evitando possíveis 

empates entre os números de classes para uma classificação. Na necessidade de um desempate, os 

pontos de   que tem maior peso nos rótulos, são exatamente os mais próximos de    

Não existem regras que limitam o número de vizinhos inferiormente ou superiormente, 

mas algumas observações devem ser levadas em conta, considerando-se que existem   amostras 

de treino: 

Se   é muito grande, dentro do espaço de   amostras, o rótulo atribuído ao ponto de teste 

 , pode conter um erro, devido ao grande número de outras classificações, que não as corretas 

espacialmente. 

Se   tem um determinado rótulo, mas existem poucas amostras      
  com o mesmo 

rótulo, escolher um número   de vizinhos muito grande, pode também induzir a classificação 

errônea, pelo mesmo motivo da afirmação anterior. 

 

5.3 Aprendizagem Não-Supervisionada 

 

Os algoritmos que se enquadram na categoria de aprendizagem não-supervisionada, 

assumem que os rótulos de seus elementos não são conhecidos. Seu problema está relacionado 

em determinar como os dados estão organizados, ou seja, separar os dados em classes, entretanto 

sem dar um rótulo a essa classe. 

O método consiste em encontrar padrões nos dados de entrada e então, arbitrariamente, 

organiza os padrões em categorias. Se dois elementos têm padrões semelhantes, ambos terão a 

mesma classe dentro da aprendizagem não-supervisionada. Se algum valor de entrada apresenta 

um padrão que não se assemelha a de nenhuma classe, o algoritmo cria uma nova classe para 

abrigar esse valor de entrada. 



 

                                                  

  

45 

 

Iterativamente, seja um conjunto de dados             e o primeiro valor de entrada 

seja             . O algoritmo encontra um padrão para    e cria uma Classe A, tal que     . 

Para o segundo valor de entrada               , se o algoritmo encontra o mesmo padrão 

encontrado para   , a Classe   incorpora também a entrada    , se não, é criado uma Classe B tal 

que     . E assim sucessivamente para todos os elementos de  .  

As aprendizagens não-supervisionadas também podem ser divididas em dois grupos, 

métodos paramétricos e métodos não-paramétricos, assim como eram divididas as aprendizagens 

supervisionadas. Entretanto, as aprendizagens não supervisionadas, por caráter de aplicação estão 

associadas a problemas onde não existem informações prévias sobre os dados. Dessa foram, os 

métodos paramétricos são raramente usados, pois sua aplicação implicaria em uma contradição 

no contexto do próprio algoritmo. Com essa premissa, serão apresentados apenas os métodos 

não-paramétricos, que abrigam a grande maioria dos algoritmos utilizados 

Como exemplos de métodos não-paramétricos de aprendizagem não supervisionada, 

podem-se citar: Mapas Auto Organizáveis, Análise de Agrupamento e a própria Análise de 

Componentes Independentes. 

 

5.3.1 Mapas Auto-Organizáveis 

 

Os mapas auto-organizáveis, com sigla SOM do inglês Self-Organizing Maps, é um tipo 

de rede neural artificial (Michie, 1994) que discretiza o espaço de entrada das amostras de 

treinamento produzindo uma representação de baixa dimensão, normalmente bidimensional, 

chamado de mapa. Dentro da categoria das redes neurais artificiais, o SOM tem um 

comportamente ímpar,  no sentido de que o seu algortimo contempla uma função de vizinhança 

para preservar as propriedades topológicas (Lima,1993) do espaço de entrada. 

O modelo de SOM opera em dois modos: treinamento e mapeamento (Kohonen,1982).A 

parte do treinamento constrói o mapa usando exemplos de entrada. Este processo é do tipo 

competitivo, também chamado de quantização vetorial. Quanto ao mapeamento,este classifica 

automaticamente um novo vetor de entrada. 

Os mapas auto-organizáveis consistem em componentes estruturais chamadas de 

neurônios. Associado a cada neurônio,existe um vetor-peso de mesma dimensão dos vetores dos 

dados de entrada e uma posição espacial do mapa. O arranjo usual de neurônios é um espaço 



 

                                                  

  

46 

 

regular em uma grade hexagonal ou retangular, de forma que o mapeamento de um espaço de 

entrada seja maior que o espaço do mapa. Um elemento de entrada será associado a um 

determinado neurônio quando o vetor-peso relacionado a este neurônio é o mais próximo possível 

do elemento de entrada.  

Seja    um vetor de entrada k-dimensional, tal que                
  e           

seja o número de vetores de entrada do problema. Se   é o espaço de saídas do SOM, cada 

neurônio  , através de um vetor peso  , está associado às entradas   , de forma que 

 

                     
 
  

 

5.3.2 Análise de Agrupamento 

 

A análise de agrupamento,tradução do inglês clustering, é a separação de um conjunto de 

observações em subconjuntos (clusters) de modo que as observações no mesmo cluster são 

similares em algum sentido estrutural (Aldenderfer, 1984). Os algoritmos de análise de 

agrupamento podem ser divididos em métodos hierárquicos ou particionais. Os algoritmos 

hierárquicos encontram subgrupos sucessivos de neurônios, usando clusters previamente 

estabelecidas, enquanto que os algoritmos particioanis determinam todos os clusters de uma vez.  

Há a possibilidade desses grupos reterem não apenas os elementos de cada cluster, mas 

como a característica em comum entre estes elementos. Esse tipo de algoritmo é conhecido como 

bi-clustering e sua resultante é uma matriz de dados, onde as amostras e colunas são agrupadas 

simultaneamente. 

A análise de agrupamentos, utiliza-se de conceitos métricos para determinar os clusters e 

os elementos que compõe estes clusters (Romesburg,2004). Nesse aspecto, este método 

assemelha-se ao método K-NN, baseado nas mesmas métricas supracitadas (Euclidiana, 

Mahalanobis, Manhattan). De fato, as escolhas estruturais dos algoritmos e os cáclculos que 

governam ambos os métodos são similares, a não ser quanto a necessidade de um grupo de 

treinamento, que o K-NN precisa, enquanto que a análise de agrupamento não. 

 



 

 

  

47 

 

Capítulo 6 

 

Métodos 

 

Objetiva-se a construção de um procedimento para o reconhecimento e classificação de 

fácies litológicas e de suas qualidades quanto às características à possibilidade da rocha ser 

Reservatório, ou seja, abrigar óleo ou gás. Para esse novo procedimento, trabalhou-se com um 

método de Separação Cega de Sinais Não-Supervisionado, a Análise de Componentes 

Independentes, e um método de Reconhecimento de Padrões Supervisionado, o K-Vizinhos mais 

Próximos. Uma aplicação correlata foi feita anteriormente (Sancevero, 2008) com êxito. 

Quanto à abordagem, de acordo com (Gil, 1991), essa dissertação e sua metodologia, 

podem ser descritas como: 

Pesquisa aplicada: conhecimentos práticos são dirigidos à solução de problemas específicos; 

Abordagem Quantitativa e Qualitativa: as informações têm valores quantificáveis e dinâmicos 

para serem analisados e classificados.  

Pesquisa Exploratória: avalia o problema com maior familiaridade visando torná-lo explícito e 

a construir hipóteses sobre o mesmo. 

Pesquisa Explicativa: identifica os fatores que compõe ou contribuem para a ocorrência de 

observações físicas. 

Para os testes foi utilizado como compilador o MATLAB R2008a, e as toolbox FastICA 

2.5(Hyvärinen) e knnclassification, disponível no PUDN(Programmers United Develop Net). 

6.1 Dados 
 

Neste presente trabalho, dois conjuntos de dados foram escolhidos para servirem como 

base para os métodos: Dados de Perfil do Campo de Namorado e dados sísmicos obtidos de 

relações não-lineares com esses dados, localizado na Bacia de Campos. A escolha do Campo de 

Namorado é motivada pela abundante quantidade de informações sobre esse campo, sendo 

considerado um campo escola (Barboza, 2005). 

 

A Bacia de Campos, localizada na porção sudeste do Brasil, ao longo da costa norte do 

Estado do Rio de Janeiro, possui uma área de 100 mil Km2, até a lâmina d‘água de 3.000 m 

(Sacco ET AL, 2007). O Campo de Namorado encontra-se na parte centro-norte da zona de 



 

 

  

48 

 

acumulações de hidrocarbonetos da Bacia de Campos, a 80 km da costa, em profundidade d'água 

entre 140 m e 250 m e foi descoberto em 1975(Vidal ET AL, 2007). Foram utilizados sete poços 

desse campo: NA01, NA02, NA04, NA07, NA011A, RJS234, RJS42.   

Tabela 6. 1 Fácies Litológicas 

Fácies   Descrição 

1 INLD Interlaminado Lamoso Deformado 

2 CBC Conglomerados e Brechas Carbonáticas 

3 DAL Diamictito Arenoso Lamoso 

4 CR Conglomerados Residuais 

6 AGA Arenito Grosso, Amalgamado 

7 AMFL Arenito Médio Fino Laminado 

8 AMGM Arenito Médio Gradado ou Maciço 

9 AMC Arenito Médio Cimentado 

10 AFI Arenito/Folhelho Interestratificado 

11 AFFI Arenito/Folhelho Finamente Interestratificado 

12 SAE Siltito Argiloso Estratificado 

13 ISAM Interlaminado Siltito Argiloso e Marga 

14 FR Folhelho Radioativo 

15 IAB Interlaminado Arenoso Bioturbado 

16 ISFD Interlaminado de Siltito e Folhelho, Deformado, Bioturbado 

17 MB Marga Bioturbada 

18 R Ritmito 

19 AG Arenito Glauconítico 

20 FSMN Folhelho Siltico com Níveis de Marga Bioturbada 

21 ACFE Arenito Cimentado, com Feições de Escorregamento 

22 SAAD Siltito Argiloso/Arenito Deformado 

23 AMFLC Arenito Médio/Fino Laminado Cimentado 

24 ISFI Interestratificado Siltito/Folhelho Intensamente Bioturbados 

25 MBO Marga Bioturbada Outra 

26 FC Folhelho Carbonoso 

27 AMMF Arenito Maciço Muito Fino 

28 SAA Siltito Areno-Argiloso 

29 ISF Interlaminado Siltito/Folhelho 

       

 

 

6.1.1 Dados de Perfil 
 



 

 

  

49 

 

Os dados de Perfil utilizados neste trabalho são: Perfil Sônico (DT), Raio Gama (GR), 

Resistividade (ILD), Densidade (RHOB), Porosidade Neutrônica (NPHI). Ainda de Acordo com 

(Sacco ET AL, 2007), esses perfis são explicados na Tabela 6.2 

Tabela 6. 2 Perfis Geológicos 

DT Medição do tempo que um pulso sonoro leva para atravessar determinado 

intervalo deformação geológica. Relaciona inversamente o tempo de trânsito e a 

porosidade da formação rochosa. Exemplo: Quanto maior o tempo de trânsito, 

menor a densidade da formação. 

GR Medição da emissão radioativa natural de rochas que contém potássio 40 ou 

elementos da série urânio-tório. Usada para o cálculo volumétrico de argila e 

cálculo volumétrico de rocha no reservatório que contém argila 

ILD Medição da propriedade da formação geológica em resistir à passagem de uma 

corrente elétrica, identificando os tipos de fluidos que preenchem os poros do 

reservatório, fornecendo informações para o cálculo de saturação de água. Caso 

seja conhecido o valor de NPHI é possível estimar quantidade de 

hidrocarbonetos presentes nos poros 

RHOB Medição da densidade média de uma unidade litológica da formação, baseando-

se na emissão de raios-gama (provenientes do césio 137), que colidem com os 

elétrons presentes na rocha e após essas colisões, os raios gama que retornam são 

contabilizados. Exemplo: Quanto menor a contagem de emissões gama que 

retornaram, maior a densidade da rocha. 

NPHI Medição do índice de hidrogênio na formação litológica através da emissão de 

nêutrons. Seu princípio baseia-se na inexistência de carga elétrica do nêutron que 

tem massa relativa ao hidrogênio. Os nêutrons penetram a formação geológica, e 

colidem com os átomos dos diferentes elementos. Essa colisão desacelera os 

nêutrons reduzindo-os até níveis termais quando retornam aos sensores 

A freqüência de amostragem dos dados de perfil é 0.2m, ou seja, os perfis apresentam 

valores a cada 20 cm de profundidade. Devido a seu alto custo, não existe testemunho probatório 

para todas as profundidades, sendo utilizados para aferição do resultado apenas a parte 

testemunhada dos poços. Das 4732 amostras nos dados, existem 1950 amostras disponíveis com 

testemunho. São essas amostras testemunhadas que são utilizadas nos testes desta pesquisa. 

A variação de valores de cada perfil está atrelada à natureza física que esta se propõe a 

medir. Dentro das amostras utilizadas essa variação é de: 



 

 

  

50 

 

 

52,0080 &amp;lt;DT &amp;lt;  120,9727 

21,1875  &amp;lt;GR &amp;lt;   109,6797 

-1,0420   &amp;lt;  ILD &amp;lt; 3229,000 

0,578    &amp;lt;NPHI  &amp;lt;37,9408 

1,7336&amp;lt;RHOB&amp;lt;2.7410 

Um exemplo, retirado dos dados, de como as informações estão dispostas está na Tabela 

6.3 

 

Tabela 6. 3 exemplo de amostra de Dado de Perfil 

POÇO PROF DT GR ILD NPHI RHOB TEST 

NA01 3004.4 86.2667 58.9102 15.0469 22.0139 2.195 AMGM 

 

Tal configuração é entendida por uma amostra dos dados. 

Na primeira célula dessa tabela, encontra-se identificado o poço de onde foi retirada essa 

amostra de perfil; na segunda célula está a qual profundidade, em relação à superfície, esta 

amostra foi captada; na terceira, o perfil Sônico; na quarta, o Raio Gama; na quinta, a 

Resistividade; na sexta, a Porosidade Neutrônica; na sétima, a Densidade e na oitava, o 

testemunho relativo a essa amostra.  

6.1.2 Dados Sísmicos 

 

A base dos atributos sísmicos o traço sísmico complexo, que pode ser descrito como 

                 , onde       é o traço sísmico, e      é a transformada de Hilbert do traço 

sísmico. Em coordenadas polares, essa equação é escita como                      onde 

                    é a amplitude instântanea (Ampli) e                       a fase 

instantânea (Fase). 

Da derivada da fase instantânea, consegue-se a frequencia instantânea (Freq), ou seja, 

              . A derivada de uma traço sísmico(Deri), de acordo com (Russel, 2004), é um 

atributo recursivo e é calculado através da aplicação de um operador ao longo do conjunto de 

traços. A aplicação de um operador diferença recursiva resulta na primeira derivada do traço 

sísmico. Isto é feito tomando a diferença entre amostras adjacentes, onde            . 



 

 

  

51 

 

A impedância acústica Determinística (DDI) é baseada na minimização do erro entre a 

Convolução Forward da refletividade do perfil da impedância estimada e das amplitudes sísmicas 

de cada traço (Francis, 2005),            
                     

       , 

onde S = WR é a convolução escrita em sua forma multiplicativa e                     com 

   sendo a Impedância Acústica. A impedância acústica Estocástica (DSI) é a média de 50 

realizações, isto é, 50 simulações estocásticas e ela gera um conjunto de representações 

alternativas das impedâncias heterogêneas de acordo com o volume de sísmica 3D. A tabela 6.4 

resume as equações e dá um exemplo pata a impedancia acústica retirada da inversão estocástica 

pois as equações envolvidas  não estão associadas diretadmente a esse atributo. 

 

    Tabela 6. 4 Dados Sísmicos 

DDI            
                     

        

DSI A impedância Acústica retirada da Inversão Sísmica Estocástica, ajuda na 

solução de questões interessantes (Dubrule ET AL, 1997), como construção de 

representações geológico-realistas 3D e quantificação da Incerteza sobre a geração 

de modelos ou ?realizações?. 

 

Fase                       

Freq                

Deri             

Ampli                     

                                                         

 

 

A variação dos valores dos dados sísmicos é de: 

 

5848,82 &amp;lt;DDI &amp;lt;  7875,3 

5880,89  &amp;lt;DSI &amp;lt;   7786,4 

-3,1406   &amp;lt;  Fase &amp;lt; 3,137,8 

-2854,6    &amp;lt;Freq  &amp;lt;1694,91 

-1683620&lt;Deri &amp;lt;1436740 



 

 

  

52 

 

87,9947 &amp;lt;Ampli &amp;lt;28000,2 

 

Tabela 6. 5 Exemplo de Amostra de Dado Sísmico 

POCO DDI DSI Fase Freq Deri Ampli 

NA01A 6361.16 6344.68 -178.299 681.609 94755.7 7199.26 

 

O intervalo de amostragem dos dados sísmicos gira em torno de 10 a 20 metros, ou seja, 

tem frequência muito maior que o intervalo de amostragem de dados de poço. Como o 

testemunho está relacionado aos dados de perfil, é necessária uma mudança de escala para 

integrar os dados sísmicos a um rótulo de testemunho.  

Esses dados sísmicos, em sua configuração original, representam informações não de uma 

amostra, mas de uma região que abrange várias amostras, logo, para os dados de perfil foi 

necessária, a construção de médias que se referem às amostras que tem participação na resposta 

sísmica.   

Uma amostra utilizada nesse teste pode conter um número variável de amostras originais 

de dados de poço, dependendo do número de amostras que estão participando da influência do 

dado sísmico, ou seja, para alguma das amostras utilizadas nesse teste, podem estar sendo usadas 

vinte amostras de dados de perfil, enquanto que para outra amostra, podem estar sendo usadas 

apenas cinco. 

Nessa configuração, têm-se várias respostas de uma mesma informação para apenas uma 

amostra e uma alternativa para a utilização dessas, sem trabalhar com dados errôneos, foi o 

cálculo das médias dessas informações para cada dado sísmico. Uma informação sísmica tem sua 

resposta ligada a dez amostras dos dados de perfil, portanto, para essa informação sísmica, 

existem dez respostas para o perfil DT, dez respostas para o perfil GR, e dez repostas também 

para os perfis RHOB, NPHI e ILD. Como as técnicas Geoestatísticas funcionam com 

informações de uma amostra, essas múltiplas respostas devem ser aglutinadas em apenas uma 

amostra, e para isso foi escolhida a média entre elas. 

Essas médias são calculadas para os cinco tipos de dados de perfil, assim como são 

calculados os desvios-padrão dos dados, mas esses não têm uma utilização comprovada para os 

dados quando trabalhados com a Análise de Componente Independente e, portanto não são 

utilizados como variáveis para o ICA. 



 

 

  

53 

 

Os dados de testemunho também devem ser trabalhados para que representem apenas uma 

amostra, pois assim como os dados de Perfil, várias respostas estão ligadas à apenas uma amostra 

sísmica. Mas, diferente das respostas nos dados de perfil, as repostas encontradas no testemunho 

são pouco variáveis e, geralmente, tem predominância de uma resposta. As amostras, como 

testemunho, podem ser classificadas como Indefinidas, Não-Reservatório, Possível Reservatório 

ou Reservatório. E a escolha para a classificação da amostra é em cima daquela que tem maior 

freqüência dentro das amostras originais dos dados. Se uma amostra de dado sísmico tem vinte e 

sete amostras de dados de poço, de forma que elas estejam distribuídas como na identificação 6.3, 

a classificação para essa amostra é de Reservatório. 

Tabela 6. 6 Separação de Testemunho nos Dados Sísmicos com predominância 

Indefinida Reser 

Possivel  

Reser Não Reser 

0 20 1 6 

Para esse caso, a escolha da classificação não gera nenhuma dúvida, devido à 

predominância de uma resposta em relação às outras. Entretanto, algumas amostras podem conter 

respostas em que a escolha não é unânime, pois não existe uma larga predominância de uma 

resposta, como na Tabela 6.7.  

Tabela 6. 7  Separação de Testemunho nos Dados Sísmicos sem predominância  

Indefinida Reser 

Possivel  

Reser Não Reser 

0 13 0 13 

A escolha para esse caso só não é arbitrária dada à natureza do programa criado para 

analisar os dados, que escolherá a classificação de Não Reservatório, pois será o último dado ser 

analisado.   

Essa perda de informação dos dados de reservatório é relevada e aceitada a primeira 

instância, pois a grande maioria das amostras não tem dados relacionados dessa forma e 

apresentam respostas com classificações predominantes em relação às outras, sobrando poucas 

amostras que possam encontrar problemas como observados na Identificação 6.4. 

6.2 Aplicação dos métodos 
 



 

 

  

54 

 

Com a base de dados consolidada, a próxima etapa da pesquisa é a aplicação dos métodos 

de Análise de Componentes Independentes e K-vizinhos mais próximos. Como as saídas de 

métodos supervisionados são os rótulos para cada entrada, a sequência de aplicação dos métodos 

começa com a Análise de Componentes Independentes, que gera uma saída com as características 

supracitada no capítulo 2 (sem redundância, redução de dimensionalidade). 

Esse novo espaço gerado pelo ICA será divido em duas partes, onde uma dessas será 

utilizada como treino e a outra será utilizada como teste de classificação do K-NN. Os 

testemunhos ligados a parte de treino são os rótulos conhecidos utilizados no K-NN. O 

testemunho referente à parte de classificação não é utilizada durante essa aplicação, afinal o K-

NN devolverá um rótulo provável para cada amostra de entrada (teste de classificação). Esse 

rótulo provável é comparado ao testemunho conhecido para averiguar o número de predições 

corretas do método. O número de predições corretas dividido pelo número total de entradas 

(tentativas) é a taxa de acerto do método. O Fluxograma abaixo ilustra essas aplicações. 

 

 

                                                            Figura 6. 1 Fluxograma dos métodos 

 

A taxa de acerto configura uma expectativa quantitativa do sucesso do método entre todas 

as tentativas do teste, ou seja, se a taxa de acerto é de 80%, isso significa que 80% dos possíveis 

rótulos encontrados ao final dos métodos eram idênticos ao testemunho comprovado do dado. 

6.3 Treinos e Testes 
 

Com a aplicação da Análise de Componentes Independentes nos dados, obtém-se um 

novo conjunto de dados, por escolha da mesma dimensão. Dessa forma, quanto aos dados de 

perfil, esses dados compõem uma matriz 1950x5, ou seja, 1950 amostras com cinco colunas, 

Pré-
Processamento 

dos Dados

Análise de 
Componentes 
Independentes

Escolha de 
componentes, 

treino e 
classificação 

K-Vizinhos Mais 
Próximos

Cálculo da Taxa 
de Acerto



 

 

  

55 

 

onde essas colunas são os perfis disponíveis. Os dados sísmicos, similarmente, têm dimensão 

1950x6, onde as colunas são os dados sísmicos disponíveis. 

Com essa nova base de dados, pode-se escolher com quantos componentes independentes 

serão feitos os testes no KNN. No caso dos dados de perfil, se forem utilizadas todas as 

componentes independentes disponíveis, cada amostra de treino e teste terá cinco dimensões. Se 

for utilizada somente uma componente independente, cada amostra de treino e teste terá apenas 

uma dimensão. Assim como se forem escolhidas três componentes independentes, cada amostra 

de treino e classificação terá três dimensões. 

Com a base de dados escolhida e os métodos já estabelecidos, o próximo passo é escolher 

possíveis conjuntos de Treino e Testes dos métodos. A priori, foram testados dois arranjos de 

Treinos: Par/Impar e Todos/Um, assim como foram utilizados dois tipos de classificação: 

Litofácies e Reservatório/Não Reservatório. 

6.3.1 Treinos  

 

O treino Par/Impar é identificado com esse nome, pois seu treino baseia-se nas amostras 

pares do dado, enquanto as amostras ímpares são classificadas e recebem o possível rótulo do 

método KNN. Dessa forma o conjunto de treino contém 975 amostras e o conjunto de 

classificação contém também 975 amostras.  

O treino Todos/Um consiste em escolher uma das amostras disponíveis, considerar como 

conjunto de treino todas as outras amostras do dado e considerar como conjunto de teste, essa 

amostra retirada. Esse processo é feito para todas as amostras. Dessa forma, o conjunto de treino 

é composto por 1949 amostras e o conjunto de teste por uma amostra, onde esse processo é 

repetido 1950 vezes, contabilizando um processo por amostra. 

6.3.2 Classificação 
A classificação de Litofácies tem disponível para rótulo os 29 tipos de rochas disponíveis 

na tabela 6.1, ou seja, cada amostra tem como rótulo a rocha referente ao testemunho. Mas 

apenas 21 destas rochas aparecem nos testemunhos. 

A classificação Reservatório/Não reservatório tem disponível para rótulo a possibilidade 

de a amostra ser de uma rocha Reservatório, Não Reservatório ou Possível Reservatório. Esta 

classificação está relacionada às rochas, através da Figura 6.2. 

 



 

 

  

56 

 

Fácies   Descrição 

1 INLD Interlaminado Lamoso Deformado 

2 CBC Conglomerados e Brechas Carbonáticas 

3 DAL Diamictito Arenoso Lamoso 

4 CR Conglomerados Residuais 

6 AGA Arenito Grosso, Amalgamado 

7 AMFL Arenito Médio Fino Laminado 

8 AMGM Arenito Médio Gradado ou Maciço 

9 AMC Arenito Médio Cimentado 

10 AFI Arenito/Folhelho Interestratificado 

11 AFFI Arenito/Folhelho Finamente Interestratificado 

12 SAE Siltito Argiloso Estratificado 

13 ISAM Interlaminado Siltito Argiloso e Marga 

14 FR Folhelho Radioativo 

15 IAB Interlaminado Arenoso Bioturbado 

16 ISFD Interlaminado de Siltito e Folhelho, Deformado, Bioturbado 

17 MB Marga Bioturbada 

18 R Ritmito 

19 AG Arenito Glauconítico 

20 FSMN Folhelho Siltico com Níveis de Marga Bioturbada 

21 ACFE Arenito Cimentado, com Feições de Escorregamento 

22 SAAD Siltito Argiloso/Arenito Deformado 

23 AMFLC Arenito Médio/Fino Laminado Cimentado 

24 ISFI Interestratificado Siltito/Folhelho Intensamente Bioturbados 

25 MBO Marga Bioturbada Outra 

26 FC Folhelho Carbonoso 

27 AMMF Arenito Maciço Muito Fino 

28 SAA Siltito Areno-Argiloso 

29 ISF Interlaminado Siltito/Folhelho 

   

 

   Reservatório 

   

 

  Possível Reservatório 

   

 

  Não Reservatório 

Figura 6. 2 Classificação Reservatório/Não Reservatório 



 

 

  

57 

 

Capítulo 7 

 

Resultados e Discussões 

 

 

7.1 Treino Par/Ímpar / Classificação de Litofácies 
 

Essa primeira bateria de resultados utiliza os parâmetros padrão do FastICA (Aproximação 

função-objetiva cúbica e Ortogonalização deflacionária). Quanto ao K-NN, foi utilizado um teste 

que consiste em treinar os dados ímpares e classificar os dados pares (as matrizes de dados têm 

4732 amostras referentes a diferentes profundidades, retiradas de sete poços) e a classificação 

refere-se a todas as vinte e uma litologias disponíveis no testemunho dos poços.  

No primeiro teste desta bateria, foi computada apenas uma componente, que é a primeira 

das componentes independentes encontradas pelo método FastICA, ou seja, a componente menos 

gaussiana possível. Como a ICA tem melhores resultados para as componentes menos gaussianas 

possíveis, foi levantada a hipótese de que a resposta para esse teste seria melhor do que os testes 

que se aproximavam mais da distribuição normal. No geral, o primeiro teste apresenta um 

comportamento esperado para esse tipo de busca de classificação via KNN,uma vez que o gráfico 

apresenta um aumento na taxa de acerto conforme o número de vizinhos cresce, estabilizando-se 

a partir de um número de vizinhos.  

 O número mínimo de vizinhos (NMV) deste e de todas as baterias de resultados foi 

de três vizinhos. O teste inicia-se com uma taxa de acerto perto dos 23%, um número pouco 

expressivo, mesmo para as complexas relações envolvidas com dados geológicos. Esse número 

tem um comportamento crescente, até o teste com o vizinho com o máximo acerto (VMA), que 

neste caso é o teste com 47 vizinhos, onde o teste encontra o auge do seu acerto, 35%. A 

modificação do número de vizinhos para números maiores do que quinze não gera maiores taxas 

de acerto, mas a taxa de acerto encontrada já é razoável. 

O segundo teste dessa bateria de resultados considera somente a segunda componente 

encontrada pelo método FastICA. Nesse teste, com o NMV, obtém-se uma taxa de acerto um 

pouco maior que 23%, mas em compensação, o VMA é o teste com 41 vizinhos, onde este já 

atinge seu auge e estabiliza-se, no mesmo molde da primeira tentativa desta bateria de testes.  



 

 

  

58 

 

Outros testes envolvendo apenas uma componente, a exemplo dos testes anteriores, 

apresentaram resultados similares a dos dois primeiros. 

O terceiro teste dessa bateria de resultados utilizou-se das duas primeiras 

componentes. Em geral a construção da análise de dados via KNN, tem como caráter comum 

aumentar sua taxa de acerto, a partir do aumento de informações espaciais sobre os dados, logo, 

os testes com mais componentes, como este, geram resultados com uma porcentagem de 

resultado maior. Entretanto resultado deste teste é inferior aos dois primeiros, atingindo um ápice 

de sucesso em torno de 30% com quinze vizinhos, apesar de o seu NMV ter taxa superior ao dos 

outros dois testes, com 26% de acerto. Este teste foi marcante para a pesquisa, pois apresentou 

certa inconsistência entre seu resultado e o comportamento esperado pelo método KNN. Essa 

inconsistência é notada quando são feitos testes com números de vizinhos superiores a quinze, 

que é o VMA deste teste, com taxa de acerto de 30%. Por exemplo, o teste com vinte e um 

vizinhos, tem porcentagem de acerto de 28%. Essa pequena diminuição não gera uma 

preocupação individual com esse teste, entretanto, uma hipótese quanto à inclusão de múltiplas 

dimensões na busca pela classificação via KNN foi construída para explicar esse comportamento 

anômalo do método. Essa hipótese é esclarecida e debatida na parte de Conclusões.   

O quarto teste com 3 componentes apresentou uma melhora significativa na 

porcentagem de acerto . Seu NMV tem 35% de acerto, igualando-se ao VMA dos testes 

anteriores. Seu comportamento não é tão diferenciado quanto o teste feito com duas componentes 

e seu VMA é atingido rapidamente com nove vizinhos, ao valor de 43% de acerto. 

Após analisar a inconsistência no comportamento da classificação com duas 

componentes e a estável classificação com três componentes, o próximo passo foi analisar se o 

emprego de todas as componentes teria algum impacto parecido com o visto no quarto teste dessa 

bateria. Sendo assim, o quinto teste é feito com as cinco componentes independentes encontradas 

pelo método FastICA e o resultado é mais anômalo do que o teste com três componentes, 

reforçando a hipótese de que esse comportamento é derivado da construção espacial do problema. 

O NMV é também o VMA com taxa de acerto superior a 65%. A taxa de acerto deste teste é 

caracterizada por um comportamento decrescente, isto é, quanto maior o número de vizinhos, 

menor a taxa de acerto. E essa taxa tem uma variação substancial, pois com cinquenta e um 

vizinhos, sua taxa de acerto é de 55%, sendo que começou com quase 66%. As principais razões 

para esse comportamento tão dissonante, como dito acima, estão na parte das conclusões gerais, 



 

 

  

59 

 

entretanto, é interessante verificar que mesmo com esse inesperado resultado, o método atingiu 

sua maior taxa de acerto com um número mínimo de vizinhos, ou seja, com uma computação 

barata, o teste tem uma taxa de acerto expressiva e consideravelmente superior ao ápice de acerto 

dos outros métodos.    

 

          (a)  Primeiro Teste                (b)  Segundo Teste 



 

 

  

60 

 

 

       (c)  Terceiro Teste                     (d)  Quarto Teste 

 

(e) Quinto Teste 

 

Figura 7.1 - Primeira Bateria de Resultados 

 

 

 

 

 

 



 

 

  

61 

 

7.2 Treino Par/Ímpar / Classificação de Reservatórios 
 

Apesar de uma alta porcentagem de acerto, a busca pela sua melhoria, apontou para a 

classificação via K-NN, uma vez que esta é bem extensa e detalhada. Para essa bateria de testes, 

foi escolhida uma classificação mais simples, porém não menos importante: a de cada amostra 

testemunhada quanto a sua característica como Reservatório, Não-Reservatório, ou ainda, 

Possível Reservatório. Os outros parâmetros continuam iguais ao do primeiro teste. 

No primeiro teste desta bateria, com uma componente, já é possível ver aumento na 

porcentagem de acerto, com NMV apresentando uma porcentagem de acerto maior que a 66%, 

um número superior a todos os testes da primeira bateria. O VMA foi com 47 vizinhos e chega a 

quase 72%.  

O segundo teste dessa bateria envolve três componentes. Como esperado, o 

comportamento do gráfico difere-se do comum. O NMV aumenta a porcentagem de acerto para 

pouco mais que 70%. O VMA ocorre no teste com nove vizinhos e tem aproximadamente 74% 

de acerto.  

Pela primeira vez observam-se duas partes bem distintas dentro desse 

comportamento, do NMV até o VMA, o gráfico é apenas crescente, depois do VMA o gráfico 

torna-se decrescente. Tal efeito é de certa forma, benéfico à interpretação do gráfico, pois 

restringe possíveis testes posteriores, dentro dessa configuração de parâmetros há um pequeno 

número de vizinhos testados.  

O terceiro teste desta bateria foi com quatro componentes, e seu NMV é superior a 

78% e seu VMA é de aproximadamente 79%, mas a variação desse acerto para um número maior 

de vizinhos é menor que os outros testes, sendo que o teste com quarenta e cinco vizinhos tem 

76% de acerto. 

 O quarto, e último teste, desta bateria são com as cinco componentes independentes 

disponíveis nos dados. Assim como no teste de cinco componentes da primeira bateria de testes, 

o NMV coincide com o VMA e é de quase 84% de acerto, entretanto seu comportamento 

decrescente é menos acentuado que o do teste com cinco componentes anterior, diminuindo a 

taxa de acerto em 4%, resultando em 80% de acerto com o teste é feito com cinquenta e um 

vizinhos. Assim como notado no primeiro teste, o comportamento quando o teste é feito  com 



 

 

  

62 

 

cinco componentes é o mais anômalo dentro da bateria de testes, entretanto, é o que apresenta 

maior acerto entre todos. 

  

 

          (a)  Primeiro Teste             (b)  Segundo Teste 

 

           (c)  Terceiro Teste                 (d)  Quarto Teste 

 

Figura 7.2 - Segunda Bateria de Resultados 

 



 

 

  

63 

 

 

 

7.3 Treino Menos-um / Ambas Classificações 

 

Após modificar o tipo de classificação, e conseguir bons resultados quanto à taxa de acerto, 

outra mudança quanto aos parâmetros foi levada em consideração, o tipo de treino. Enquanto as 

duas primeiras baterias foram treinadas com metade das profundidades possíveis (ímpares) e 

tentava-se classificar a outra metade (pares), foi construído um treino onde se treina com todas as 

amostras menos uma delas e classifica-se essa amostra retirada. Por exemplo, excluía-se a 

primeira amostra do treino, treinava com todos os restantes, e classifica a amostra que foi 

retirada. Repetindo esse processo para cada amostra, tem-se a porcentagem de acerto desse 

treino. 

Essa bateria de resultados foi dividia em duas partes, a primeira é baseada na classificação 

de todas as rochas (Figura 7.3), assim como a primeira bateria e a segunda parte é baseada na 

classificação pelas características quanto a reservatório, assim como a segunda bateria. 

Na primeira parte, o primeiro teste é de uma componente independente e o segundo é de 

três componentes. O primeiro não difere muito dos resultados da primeira bateria de testes e seu 

comportamento é similar, mas com um aumento na taxa de acerto. Seu NMV acerta pouco mais 

que 20%, enquanto seu VMA é de 38% com cinqüenta e um vizinhos. O segundo apresenta uma 

coerência de classificação muito grande, ficando em torno de 47% a maior parte do tempo. 

Quanto ao terceiro teste, com cinco componentes, seu NMV é seu VMA e tem porcentagem de 

acerto de quase 69%, e o efeito anômalo é amenizado, atingindo 60% de acerto quando o teste é 

feito com cinqüenta e um vizinhos. 



 

 

  

64 

 

 

        (a)  Primeiro Teste             (b)  Segundo Teste 

 

(c) Terceiro Teste 

 

 Figura 7. 3 - Terceira Bateria de Resultados - Classificação Fácies  

 

Na segunda parte da terceira bateria de resultados (Figura 7.4), o primeiro teste feito com 

uma componente, o segundo com três componentes e o terceiro com cinco componentes. O 

primeiro teste tem um comportamento parecido com o do teste de uma componente da segunda 



 

 

  

65 

 

bateria, mas seu VMA sobe, atingindo 73% de acerto. Nos testes com três e cinco componentes, 

o efeito anômalo é amenizado, em relação aos testes da segunda bateria e o VMA têm valores 

aumentados. Para três componentes o VMA passa de 77% enquanto que para cinco componentes 

esse aumento chega a 85%. Essas informações mostram um parâmetro interessante, o efeito de 

um treino na classificação. Apesar dos valores superiores de acertos encontrados, o tempo 

computacional gasto é muito superior ao tempo computacional gasto com o teste Par/Ímpar.  

A razão do tempo computacional do treino Menos-Um      e o tempo computacional do 

treino Par/Ímpar      é aproximadamente 
    

         
  e devido a essa diferença, outros 

testes nesse trabalho optaram pelo treino Par/Ímpar, sem perda de aplicabilidade, pois um bom 

treino é suficiente e poupam tempo computacional do problema. 



 

 

  

66 

 

 

       (a)  Primeiro Teste                 (b)  Segundo Teste 

 

(c) Terceiro Teste 

 

 

                         Figura 7. 4 - Terceira Bateria de Resultados - Classificação de Reservatórios 

  
 

 

 



 

 

  

67 

 

 

7.4 Resultados Comparativos 
 

Após mudanças em aspectos de treino e classificação, mudanças para com os diversos 

parâmetros do FastICA fazem-se necessários. Para a quarta bateria de resultados, foi utilizado o 

mesmo treino e classificação da Primeira Bateria de Testes e os testes foram feitos com cinco 

parâmetros. Os parâmetros modificados foram quanto à função-objetivo inicial e quanto ao tipo 

de ortogonalização. As funções-objetivo testadas foram as da Tabela 7.1, onde a1 e a2 são 

constantes arbitrárias. Quanto à ortogonalização, ela foi testada na sua característica 

deflacionária, que procura uma componente de cada vez, e na simétrica que procura todas as 

componentes de uma vez.  

Tabela 7. 1 – Funções-Objetivo 

Função Sigla Fórmula 

Cúbica Cubi g(u) = u
3
 

Tangente 

Hiperbólica Tanh g(u)=tanh(a1.u) 

Gaussiana Gauss g(u)=u.exp(-a2(u
2
/2))  

Quadrada Skew g(u)=u
2
 

 

O teste foi rodado na sequência da Tabela 7.2 

 

Tabela7. 2 - Sequência Quarta Bateria de Resultados 

Teste Função  Ortogonalização 

Primeiro Cubi Deflacionária 

Segundo Tanh Deflacionária 

Terceiro Gauss Deflacionária 

Quarto Skew Deflacionária 

Quinto Cubi Simétrica 

Sexto Tanh Simétrica 

Sétimo Gauss Simétrica 

Oitavo Skew Simétrica 

Todos os testes apresentaram resultados idênticos, apenas com diferença no número de 

iterações necessárias para sua convergência. Isso denota que a estrutura dos dados em litofácies 

não é afetada por mudanças de parâmetros de FastICA, devido a simplicidade espacial da sua 



 

 

  

68 

 

estrutura. Nas mudanças efetuadas nos testes, o caso que necessitou de mais iterações foi o 

quarto. O quarto teste tem aproximação quadrática de erro e por isso tem clara desvantagem 

perante os outros testes. Algumas tentativas de convergência para o quarto testes não alcançaram 

sucesso devido a esses problemas de convergência. A resposta de todos está mostrada na Figura 

7.5 

 

 

 

 

Figura7. 5 - Quarta Bateria de Resultados 

 

 

 

7.5 Comparativo ICA e FastICA 

 

Depois de testes com mudanças de parâmetros, esta quinta bateria de testes compara 

algoritmo usado o FastICA 2.5, programado por Hyvärinen, em 2001,com um típico de ICA de 

Projection-Pursuit disponível em toolbox do MATLAB. 

Esse teste é feito nos moldes da Segunda Bateria de Testes, com treino Par/Ímpar e 

classificação de Litofácies e pelo grupo da rocha quanto à possibilidade desta ser reservatório. 



 

 

  

69 

 

Para poucas dimensões, como o problema de perfis de poço, ambas as classificações de 

ICA e de FastICA têm os mesmo resultados para uma aproximação na casa de     . A Figura 

7.6, abaixo representa a classificação idêntica para todas as Litofácies. 

 

 

 
  Figura 7. 6 -  Teste entre ICA e FastICA – Classificação de Fácies 

 

A Figura 7.7 representa a classificação idêntica para a característica de Reservatório 

 

Figura 7.7 - Teste entre ICA e FastICA – Classificação de Reservatório 



 

 

  

70 

 

 

Como a resposta nos dois casos é a mesma, passa-se para outro tipo de comparação entre 

os testes: o tempo que cada método gasta para separar as componentes independentes. Foram 

construídas matrizes quadradas aleatórias, com tamanho variando de duas a trinta dimensões tal 

teste. Foi calculado o tempo computacional que cada programa utiliza para identificar as 

componentes independentes: 

O programa FastICA 2.5 é extremamente mais rápido que o programa convencional, 

como mostra a Figura 7.8 é o resultado para esse teste, onde o eixo x denota as dimensões das 

matrizes e o eixo y o tempo gasto para cada uma dessas dimensões.  

 

 

Figura 7. 8 – Comparação entre velocidades de ICA e FastICA 

 

7.6 Comparativo ICA e PCA 

 

As baterias de resultados anteriores mostram que a Análise de Componentes 

Independentes apresenta uma eficácia satisfatória no reconhecimento e classificação de fácies 

litológicas e da classificação das rochas em grupos de reservatório, possível reservatório e não -

reservatório. De fato, a metodologia assemelha-se muito à classificação através da Análise de 

Componentes Principais, com vasta bibliografia referente ao tema (Sancevero, 2008), (Doveton, 



 

 

  

71 

 

1994), (Talaat, 1989). Dada essa bibliografia estruturada da PCA, uma análise interessante recai 

sobre a comparação das eficiências entre a PCA e a ICA.  

Esta comparação ajuda a ilustrar a diferença final nas porcentagens de acerto, 

enquanto que uma comparação mais detalhada entre os métodos já foi feita por (Wong, 2002), 

mostrando vantagem do método ICA sobre o método PCA. Para esta comparação, apenas 

ilustrativa, primeiro escolheu-se o teste de K-NN Par/Ímpar, a classificação de todas as Litofácies 

presentes no testemunho, e a utilização cinco componentes da ICA. Os resultados encontram-se 

na Figura 7.9 abaixo 

Figura 7. 9 – Comparação ICA e PCA – Classificação Fácies 

 

 A próxima etapa desta bateria de resultados consiste na utilização de K-NN com treino 

Par/Ímpar, a classificação de grupos de reservatório, e a utilização cinco componentes da ICA. Os 

resultados encontram-se na Figura 7.10  



 

 

  

72 

 

 
Figura 7. 10 – Comparação ICA e PCA – Classificação de Reservatórios 

 

 Em qualquer situação a Análise de Componentes Independentes mostrou-se mais eficiente 

que a Análise de Componentes Principais. 

 

 

7.7 Resultados da predição de um poço 

 

O método aplicado FastICA é eficaz e mais eficiente que a PCA. Além disso, o método tem 

a mesma eficiência que a aplicação do método ICA, com a vantagem de ter a velocidade 

computacional bem maior. Uma prova ao método é a tentativa de predição de um poço inteiro, 

utilizando-se apenas os testemunhos conhecidos de outros poços, ou seja, classificar todas as 

amostras disponíveis de um determinado poço, sem usar nenhuma amostra com testemunho 

conhecido dele.  

Para tal teste foi escolhida a classificação do Poço NA01 do Campo de Namorado. Este 

poço tem 799 amostras de dados de perfil, as primeiras amostras do dado utilizado nos outros 

testes. E como conjunto de treino utilizou-se todas as outras amostras que dispunham de 

testemunho e não se encontravam no poço NA01. As amostras foram classificadas em 

reservatório, possível reservatório e não-reservatório e a Figura 7.11 mostra a comparação entra a 

predição e o testemunho conhecido. 

Outra perspectiva interessante deste teste é identificar com quanto de porcentagem o 

método foi capaz de classificar corretamente cada uma das possíveis classes, ou seja, identificar, 



 

 

  

73 

 

separadamente, as taxas de acerto do método. A Tabela 7.3 contém essas informações baseadas 

no presente teste do Poço NA01.  

 

Tabela7. 3 – Predição individual da parte testemunhada 

      Predição     

   
  

   
  T

e
st

e
m

u
n

h
o

   Reservatório 
Possível 
Reservatório 

Não 
Reservatório   

Reservatório 97% 3% 0%   

Possível 
Reservatório 

8% 80% 12% 
  

Não Reservatório 21% 0% 79%   

          
 

De acordo com os dados presentes na Tabela 7.3, quando a amostra em teste era 

reservatório, a predição acertou 97% dos casos. Em 3% dos casos, a predição apontou uma 

amostra de possível reservatório, mas nunca classificou estas amostras como não-reservatório. 

Essas porcentagens demonstram a consistência da classificação quando a amostra trata-se de uma 

amostra de reservatório, pois apesar de ocorrerem classificações de possíveis reservatórios, esta é 

uma escolha próxima da real condição da amostra, em contrapartida, a classificação de não 

reservatório, não apresenta a mesma qualidade de predição. 

As outras porcentagens mantêm-se na média esperada pela classificação e são justificadas 

pelos desvios e aproximações da ICA. 

 

 

 

 

 

 

 

 

 

 

 



 

 

  

74 

 

 

Profundidade (m)                      Testemunho                  Predição via ICA 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

                Figura 7.11- Predição de Poço NA01 

 

7.8 Testes Dados Sísmicos 

Os testes têm o mesmo padrão utilizado quando foram trabalhados os dados de poço, 

inclusive com as mesmas plataformas. Os testes foram feitos usando apenas os dados de Sísmica 

2.988,

4 

 3.148 



 

 

  

75 

 

(DDI, DSI, Fase, Freq,Deri, Ampli), a Análise de Componentes Independentes padrão do, os 

treinos foram o Par-Ímpar ( que treina com as amostras ímpares e classifica as pares) e o Menos-

Um(que retira uma amostra dos dados, treina com todas as outras e tenta classificar a amostra 

retirada) e a classificação é Indefinida, Não-Reservatório, possível reservatório e Reservatório, 

dentro do método KNN.  

 

 7.8.1 Teste Par- ímpar 

Foram feitos testes, utilizando uma componente (primeira e a segunda), duas, quatro e as 

seis componentes disponíveis. A porcentagem de acerto é perfeita em quatro desses cinco treinos, 

e uma delas tem acerto de 99%. 

 

Figura 7.12 - Teste Par-Ímpar – Classificação de Fácies 

 

 

7.8.2 Teste Menos-Um 

 

Foram feitos testes, utilizando uma componente, três e seis componentes disponíveis. A 

porcentagem de acerto do teste com apenas uma componente tem uma taxa de acerto para três 

vizinhos de 98,98% enquanto que para todos os outros é de 99,01%. No segundo teste, com um 

comportamento mais esperado temos o número mínimo de vizinhos (NMV) com um acerto em 



 

 

  

76 

 

torno de 98,96% e depois sobre para 99% de acerto, onde ocorre o valor de máximo acerto 

(VMA). Já o teste com todas as componentes tem para o NMV, acerto de 98,90%, o VMA é de 

99,05% e ocorre para quatro vizinhos, depois se estabiliza em pouco mais de 90% para os outros 

números de vizinhos. 

 

 

Figura 7.13 - Teste Menos-Um – Classificação de Reservatórios 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

  

 

 



 

 

  

77 

 

 

 

 

Capítulo 8 

 

Conclusões 

 

8.1 Conclusões Dados de Poço  

 

Qualquer análise mais aprofundada requer toda a atenção para diversos aspectos dialéticos de 

objetos de estudo que tem sua validade testada. E para tanto, situações distintas levam a 

diferentes maneiras de se observar um determinado resultado. Ao desconsiderarem-se os 

elementos parciais de uma observação de qualquer resultado, o que sobra é uma análise vazia e 

estagnada, meramente ilustrativa de uma situação controlada. Objetivos devem ser alcançados e a 

metodologia e os testes feitos estão intimamente ligados às escolhas unilaterais de como, quando 

e onde,tais testes são propostos e finalmente executados. 

Os testes rodados, apesar de não abrangerem todo e qualquer teste possível da Análise de 

Componentes Independentes, conjecturam um padrão de seu funcionamento claro e cria um 

corpo sólido de informação, essa a qual é a base para toda a discussão sobre o método. Não que o 

algoritmo ICA precise de respaldo ou prova, as inúmeras aplicações, em áreas distintas do 

mesmo já provém o mérito do seu uso.  Todo esse suporte e a bibliografia de crescimento 

exponencial tornam a amostra dos testes mais quantitativa que qualitativa, focando os testes para 

um caminho mais experimental, juntamente com a análise de resultados. Essa investida 

experimental pode, e deve ser prolongada, visto que apesar da efervescência na produção de 

artigos sobre o ICA, ainda é escasso o número de papers na área de interesse desta pesquisa. 

O número de dimensões de uma amostra está diretamente ligado ao número de componentes 

independentes que podem ser encontradas e os resultados também conferem um padrão 

interessante sobre o uso das componentes dentro do treino do KNN. Um comportamento, 

esperado no caso de qualquer análise via KNN, seria um gráfico crescente, que aos poucos tende 

a estabilizar-se em um número não muito grande de vizinhos. Esse comportamento é verificado 

ao utilizar-se somente uma, duas, ou até três componentes. Nos testes em que foram usadas 



 

 

  

78 

 

quatro e cinco componentes, nota-se um comportamento anômalo, onde o gráfico tende a 

decrescer conforme aumenta o número de vizinhos. Esse tipo comportamento provavelmente é 

deriva do de uma reposição espacial dos pontos quando a dimensão cresce. Ou seja, pontos que 

estavam agrupados em uma porção do espaço, podem ser separados quando é colocada uma nova 

dimensão. Dessa forma, quanto menos vizinhos são considerados, melhor é a taxa de acerto, já 

que ao escolher-se um número grande de vizinhos, o método pode acabar escolhendo vizinhos 

que não fazem parte da mesma classificação, pois os pontos com mesma classificação est ão 

espalhados no espaço. De qualquer forma, as taxas de sucesso aumentam conforme se usam mais 

componentes, pois com um treino mais elaborado, as chances de acerto sobem, mesmo com esse 

inconveniente desvio de padrão. A natureza dessa anomalia deve-se também ao fato de que as 

componentes independentes encontradas aproximam-se consideravelmente da Distribuição 

Normal, diferentemente do ideal que seria uma distribuição menos gaussiana possível.  

Por diversas vezes os testes foram refeitos, e alguns resultados são médias dos resultados 

obtidos em todos os testes, pois como o método é iniciado a partir de um ponto aleatório, e seus 

passos seguintes dependem deste, os resultados podem ser diferentes, apresentando pequenos 

desvios-padrão, mas sempre correspondem a um mesmo padrão. 

Se fosse o caso de apenas escolher um rótulo de qualidade para a Análise de Componentes 

independentes via FastICA na classificação de Fácies, de acordo com os resultados dos testes, o 

termo seria eficiente. A eficácia do método é óbvia, pois seus resultados chegam a taxas de 

sucesso superiores a 80%, em uma área de estudo que contém grandes erros e desvios, sem 

mencionar as incertezas geofísicas. Mas, mais do que isso, o teste é extremamente eficiente, 

podendo resolver grandes blocos de informação em poucos segundos. Sua eficiência permite que 

sejam rodados vários testes em cima das amostras, sem um comprometimento de prazo, já que o 

custo computacional é baixo.  

Como já explanado, uma abrangente série de testes, leva a múltiplos resultados, os quais 

indicam o melhor caminho a se tratar uma determinada situação-padrão. No caso dos dados de 

poço, nota-se que mudanças na função-objetivo ou de ortogonalização não afetam o resultado do 

teste, e como a natureza física desse tipo de dado é sempre a mesma, o método, ao ser empregado 

futuramente, não precisa desse tipo de aporte. Porém outros tipos de dados geram amostras 

diferentes, onde a influência dessas mudanças pode promover mudanças estruturais nas 

Componentes Independentes e apresentar um resultado mais interessante. 



 

 

  

79 

 

 

8.2 Conclusões Dados Sísmicos 

 

Os dados sísmicos mostram-se muito eficientes para a classificação das rochas quanto 

suas características de Reservatório. Mais eficientes que os dados de poço, e isso provêm da 

estrutura dos dados. Os Dados sísmicos são gerados a partir de cálculos não-lineares, ou seja, tem 

menor correlação entre si, comparados aos dados de perfil. Essa menor correlação colabora para 

distanciar os padrões entre as amostras, deixando os padrões mais robustos e diferenciados, 

permitindo ao método da Análise de Componentes Independentes iterações mais coerentes, 

chegando a um resultado mais correto para o Método K-NN gerar a classificação dos dados. 

Com essa propriedade de acerto elevada, uma tentativa interessante de testes, é dificultar o 

tipo de classificação, como por exemplo, a classificação completa de fácies ou de características 

mais difíceis de serem calculadas, como Porosidade. 

O comportamento dos dados, como levantado na Análise de Componentes Independentes 

em cima de dados de poço, não é largamente debatido aqui, pois a taxa de acerto entre os 

vizinhos tem uma diferença mínima, mostrando que os dados estão postados de forma que para 

poucos vizinhos, já se atinge uma porcentagem de acerto satisfatória. 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 



 

 

  

81 

 

Bibliografia 
 

 

 

ANDERSON, J.A. An Introduction To Neural Networks- Bradford Book, 1995 

 

ASCENSO, J. Reconhecimento de Padrões. Notas de Aula, 2003 

  

BALLANDA, K. P.; MACGILLIVRAY, H. L. Kurtosis: A Critical Review. The American 

Statistician (American Statistical Association) 42 (2): 111–119, 1988 

 

BARZOZA, E.G. Análise Estratigráfica do Campo de Namorado com base na interpretação 

Sísmica Tridimensional, Tese de Doutorado – UFRGS, 2005 

 

BELL A.J.; SEJNOWSKI T.J. An information maximization approach to blind separation and 

blind deconvolution, Neural Computation, 7, 6, 1129-1159, 1995 

 

BISHOP, C. M. Neural Networks for Pattern Recognition. Oxford University Press, 1995  

 

BLACK, PAUL E. Manhattan distance,  ed., U.S. National Institute of Standards and 

Technology.,2006 

 

BOX, G.E.P.; HUNTER, J.S.; HUNTER, W.G. Statistics for Experimenters: An Introduction to 

Design, Data Analysis, and Model Building. 1978 

 

CARDOSO, J.-F. and Souloumiac, A., Blind beamforming for non Gaussian signals. IEE Proc. F. 

v140 i6. 362-370, 2002 

 

CASEY.M.A. Method for extracting features from a mixture of signals, United States, Mitsubish 

Electric Research Laboratories, INC (Cambridge, MA), 2001  

 

CHEN, X.; WANG, L.; XU,Y. A Symmetric Orthogonal FastICA Algorithm and Applications in 

EEG, vol. 2, pp.504-508, 2009 Fifth International Conference on Natural Computation, 2009 

 

COMON.P. Independent Component Analysis: a new concept?, Signal Processing, Elsevier, 

36(3):287—314,1994 

 

COVER, T.M.; HART P.E. Nearest neighbor pattern classification. IEEE Transactions on 

Information Theory 13 (1): 21–27, 1967 

 

COVER, T.M.; THOMAS, J.A. Elements of Information Theory. Wiley, 1991. 

 

DANTAS, C. A. B. Probabilidade: Um curso introdutório – 2. ed.1 – São Paulo: Editora da 

Universidade de São Paulo, 2004 

 



 

 

  

82 

 

DE MAESSCHALCK, R.; JOUAN-RIMBAUD, D.; MASSART.D.L. The Mahalanobis distance. 

Chemometrics and Intelligent Laboratory Systems 50:1–18. 2008 

 

DIAMANTARAS, K.I.; KUNG, S.Y. Principal Component Neural Networks: Theory and 

Applications. Willey, 1996 

 

DUBRULE, O.; THIBAU, M.; LAMY, P.; HAAS,A.  Geostatistical reservoir characterization 

constrained by 3D seismic data,1997 

 

FAREBROTHER, R. W. Algorithm AS 79: Gram-Schmidt Regression. Journal of the Royal 

Statistical Society. Series C (Applied Statistics) Vol. 23, No. 3 pp. 470-476, 1974 

 

FRANCIS, A. Limitations of Deterministic and Advantages of Stochastic Seismic Inversion, 

2005 

 

FULLER, R.B. ET AL. Synergetics: Explorations in the Geometry of Thinking, published by 

Macmillan , Vol. 1, 1975 

 

GAUCH, H. G. , JR. Multivariate Analysis in Community Structure. Cambridge University 

Press, Cambridge, 1992 

 

GIL, A.C. Como elaborar projetos de pesquisa. São Paulo: Atlas, 1991. 

 

GOKHALE, D.V.; AHMED, N.A.; RES, B.C.; PISCATAWAY, N.D. Entropy Expressions and 

Their Estimators for Multivariate Distributions. Information Theory, IEEE Transactions on 35 

(3): 688–692, 1989 

GOOD, I. J. Some statistical applications of Poisson's work. Statistical Science 1 (2): 157–180, 

1986 

 

HILL, T.; LEWICKI, P.  Statistics Methods and Applications. StatSoft, Tulsa, OK, 2007 

 

HOWSON, C.; URBACH, P. Scientific Reasoning: the Bayesian Approach (3rd ed.). Open Court 

Publishing Company,2005 

 

HYVÄRINEN, A.; OJA, E. A fast fixed-point algorithm for independent component analysis. 

Neural Computation, 9(7):1483-1492, 1997 

 

 HYVÄRINEN, A. Fast and Robust Fixed-Point Algorithms for Independent Component 

Analysis. IEEE Transactions on Neural Networks, 1999 

 

HYVÄRINEN, A. New approximations of differential entropy for independent component 

analysis and projection pursuit. In Advances in Neural Information Processing Systems, volume 

10, pages 273-279. MIT Press, 1998. 

 

HYVÄRINEN, A.; KARHUNEN, J.; OJA,E. Independent Component Analysis, New York: 

Wiley, 2001 



 

 

  

83 

 

 

JOANES, D.N.; GILL, C.A. Comparing measures of sample skewness and kurtosis. Journal of 

the Royal Statistical Society (Series D): The Statistician 47 (1), 1998 

 

JOLLIFFE, I.T. Principal Component Analysis, Series: Springer Series in Statistics, 2nd ed., 

Springer, NY, 2002, XXIX, 487 p. 28 illus.2002 

 

JUTTEN, C.; HERAULT, J. Blind separation of sources, part I: An adaptive algorithm based on 

neuromimetic architecture. Signal Processing, 24:1-10, 1991. 

 

KOHONEN, T. Self-Organizing Maps, Springer Series in Information Science no. 30. Springer, 

Berlin Heidelberg, 

 

KOUTROUMBAS, K.; THEODORIDIS, S. Pattern Recognition (4th ed.). Boston: Academic 

Press, 2008 

 

KULIKOWSKI, C. A.; WEISS, S. M. Computer Systems That Learn: Classification and 

Prediction Methods from Statistics, Neural Nets, Machine Learning, and Expert Systems, 1991 

 

KUSHNER, H.J.; CLARK. D.S. Stochastic approximation methods for constrained and 

unconstrained systems. Springer – Verlag, 1978. 

 

LE CAM, L. Maximum likelihood — an introduction. ISI Review 58 (2): 153–171, 1990 

 

LEITE, L. Análise de Componentes Independentes Aplicada à Identificação de Regiões 

Lesionadas Em Mamogramas. COPPE/UFRJ,M.Sc.,Engenharia Elétrica, 2005 

 

LI, Z.; AN, J.; SUN, L.; YANG, M. A Blind Source Separation Algorithm Based on Whitening 

and Non-linear Decorrelation.  vol. 1, pp.443-447, 2010 Second International Conference on 

Computer Modeling and Simulation, 2010 

 

LIMA, E. L. Espaços Métricos. Projeto Euclides, Rio de Janeiro, 1993. 

 

LINSKER, R. Self-organization in a perceptual network. Computer 21: 105-117, 1988 

 

MARCHINI,J.L.; HEATON, C.; RIPLEY B. D. FastICA Algorithms to perform ICA and 

Projection Pursuit. R package version 1.1-11, 2009 

MARSAGLIA, G. Evaluating the normal distribution. Journal of Statistical Software 11 (4), 2004  

 

MICHIE, D.; SPIEGELHALTER, D.J.; TAYLOR, C.C. ML, neural and statistical classification, 

New York. cap.1,2,11, 1994 

 

MURATA, M.; IKEDA, S.; ZIEHE A. An approach to blind source separation based on temporal 

structure of speech signals, in IEEE Trans. Signal Processing, 2001.. 

 

NANDI, A.  Blind Estimation Using Higher-Order Statistics. Kluwer, 1999 



 

 

  

84 

 

 

OJA, E. Subspace Methods of Pattern Recognition. Research Studies Press, England, and Wiley, 

USA, 1983 

 

OJA, E.; KARHUNEN, J. Stochastic approximation of the eigenvectors and eigenvalues of the 

expectation of a random matrix. Journal of Math. Analysis and Applications, 106:69-84, 1985 

 

OJA , E.; OGAWA, H.; WANGVIWATTANA, J. Learning in nonlinear constrained Hebbian 

networks. In Proc. Int. Conf. on Artificial Neural Networks (ICANN‘91), páginas 385-390, 

Espoo, Finlândia, 1991 

 

PAPOULIS, A. Probability Random Variables, and Stochastic processes. McGraw – Hill, 3
rd

 

Edition, 1991 

 

PARZEN, E. On estimation of a probability and mode. Ann. Math. States, 33:1065-1076, 1962 

 

ROSA, H., ET AL. Caracterização de eletrofácies por meio de ferramentas estatísticas 

multivariadas. Rem: Rev. Esc. Minas,  Ouro Preto,  v. 61,  n. 4, Dec.  2008 . 

 

RUSSEL, B. H. The application of multivariate statistics and neural networks to the prediction of 

reservoir parameters using seismic attributes, Tese de Doutorado defendida na Faculdade de 

Calgary, Alberta, 2004. 

 

SACCO, T. SUSLICK, S.B. VIDAL, A.C. Modelagem Geológica 3D do Campo de Namorado 

Utilizando Dados de Perfilagem de Poços Verticais, 2007 

SACEVERO, S.S, REMACRE A.Z , VIDAL, A.C , PORTUGAL R.S. Aplicação de técnicas de 

estatística multivariada na definição da litologia a partir de perfis geofísicos de poços, RBGf 

38(1) 61-74,2008, 2008 

SAN MARTIN, L. Álgebras de Lie, Editora UNICAMP, 1999 

 

SANCEVERO, S., REMACRE, A., VIDAL, A., PORTUGAL, R.. Aplicação de técnicas de 

estatística multivariada na definição da litologia a partir de perfis geofísicos de poços. Revista 

Brasileira de Geociências, América do Norte, 38, dez. 2008 

 

SCHUERMAN, J. Pattern Classification: A Unified View of Statistical and Neural Approaches.  

Wiley&amp;amp;Sons, 1996 

 

STEWART, J. Calculus .Pioneira Thomson Learning, 2ª ed, 2005 

 

STONE, J. V. A Brief Introduction to Independent Component Analysis in Encyclopedia of 

Statistics in Behavioral Science, Volume 2, pp. 907–912, Ed. Brian S. Everitt &amp;amp; David C. 

Howell, John Wiley &amp;amp; Sons, Ltd, Chichester, 2005 

 



 

 

  

85 

 

TOUSSAINT, G.T. Geometric proximity graphs for improving nearest neighbor methods in 

instance-based learning and data mining. International Journal of Computational Geometry and 

Applications 15 (2): 101–150,  2005 

 

VIDAL, A.C, SANCEVERO, S.S. REMACRE A.Z, COSTANZO, C.P., Modelagem 

Geoestatística 3d Da Impedância Acústica Para a Caracterização Do Campo De Namorado, 

RBGf  25(3): 295-305, 2007 

 

WASSERMAN, P. Neural Computing Theory and Practice. Van Nostrand Rheinhold, New York, 

1989. 

 

WEIBULL, W. A statistical distribution function of wide applicability, J. Appl. Mech.-Trans. 

ASME 18 (3): 293–297, 1951 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 



 

 

  

87 

 

 

 

 

Apêndice A – Termos Estatísticos 

 

- Centralizar um vetor em torno de Zero 

Centralizar um dado em torno de zero, consiste subtrair do vetor x sua esperança estatística: 

          

A esperança de uma determinada função, distribuição ou amostra      é dada por 

               
 

  

 

Em alguns casos, é comum dividir o vetor aleatório x centralizado pelo seu desvio padrão 

 , branqueando os seus dados. 

O desvio padrão   é dado por         x), e  

                    

 Um dado branqueado é quando a sua matriz de covariância é igual à identidade. 

 

- Dependência 

Seja T um subconjunto de um espaço S, e            são os elementos de T. Diz-se que T 

é linearmente dependente se existem escalares             , não todos nulos, tais que 

                      

 

- Independência 

A definição de uma base de vetores independentes é que não existe combinação possível 

entre quaisquer vetores que sejam iguais a um vetor dessa base. Estatisticamente, a independência 

é descrita quando a ocorrência de um evento não interfere na ocorrência de outro evento, e estes 

são chamados de independentes entre si. Algebricamente, para que dois eventos, X1 e Y1, sejam 

não-correlacionados, basta que 

              

Se dois eventos    e    são independentes então a densidade de probabilidade conjunta é 

igual o produto das densidades marginais 



 

 

  

88 

 

 

                         

 

- Curtose 

A curtose é uma medida de dispersão que se caracteriza por ser um cumulante de quarta 

ordem de uma variável aleatória (Ballanda, 1988). curtose é a tradução do inglês ?Kurtosis‘, e por 

isso é denotada de kurt ( ). Seu modelo clássico tem relação com o quarto momento padronizado 

de uma distribuição (   
    (Joanes, 1998), entretanto, para os cálculos envolvidos na análise de 

componentes independentes, é comum utilizar-se da notação com esperança estatística, isto é, 

                          

onde      é a esperança. Como   é assumida como normalizada, sua variância é igual a um logo 

        e a função da curtose pode ser simplificada para 

                  

Para variáveis gaussianas a curtose é zero, enquanto para a maioria das distribuições não -

gaussianas ela é não nula (Joanes, 1998), sugerindo que pode servir como medida de não-

gaussianidade de uma variável aleatória. 

A curtose ainda atende a propriedades de linearidade, isto é, dadas    e    variáveis 

aleatórias independentes, são válidas as seguintes relações 

                                

           
          

onde   é uma constante. 

 

 

 

- Negentropia 

 



 

 

  

89 

 

A negentropia, também conhecida como entropia negativa ou sintropia, é baseada no 

diferencial de entropia sobre uma quantidade de informação. Para uma definição mais 

quantitativa da negentropia é necessária, porém, uma definição mais precisa da entropia. A 

entropia é uma grandeza associada à imprevisibilidade de uma variável. Quanto mais 

imprevisível for o resultado de uma ação  , maior será a entropia associada à  . 

Matematicamente, a entropia   de um vetor aleatório com densidade       pode ser definida 

como 

                          

  Uma variável gaussiana maximiza um conjunto de variáveis aleatórias de mesma variância 

(Gokhale, 1989), pois sua distribuição é a mais aleatória possível. A entropia tem valores 

menores para distribuições que se encontram concentradas em certos valores, portanto, pode ser 

usada como medida de não-gaussianidade, uma vez que nesse caso a entropia é máxima.  

Uma maneira de se obter esse resultado é utilizar-se de alguma medida que tenha limite 

igual a zero quando uma distribuição tenha uma distribuição que se distancia da distribuição 

normal. A forma mais utilizada (Hyvärinen, 1998) é uma versão normalizada (diferencial de 

entropia), conhecida por negentropia   e definida como 

                      

onde         é uma variável aleatória gaussiana com mesma correlação e covariância de  .  

Dessa forma, a negentropia     , será sempre não-negativa, pois           tem o maior 

valor possível entre as variáveis randômicas de mesma variância de  , isto é                     

Por outro lado, a negentropia      só será zero quando       é uma distribuição correlata 

da distribuição normal, logo,      é uma medida de não gaussianidade. 

 

 

 

 

 

- Matriz de Covariância 

 



 

 

  

90 

 

Se   é um vetor tal que                  a matriz de covariância   de   é denotada por 
   e é calculada através de 

 

     
                     

   
                     

  

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 



 

 

  

91 

 

 

 

Apêndice B – Métrica 
 

Métrica é um conceito que generaliza a idéia geométrica de distância. Um conjunto em que 

há uma métrica definida recebe o nome de espaço métrico (Lima, 1993). 

Se   é um conjunto que admite uma métrica  , então a função distância            , 

associa dois elementos de um conjunto a um número real e deve obedecer aos seguintes axiomas: 

 

1)Ser sempre positiva                    

2)Ser simétrica                         

3)Obedecer a desigualdade triangular                                   

4)Ter resposta nula apenas para pontos coincidentes                

 

O conceito empregado anteriormente para a definição de ponto mais próximo pode pairar 

sobre vários tipos de distância, tais como a distância euclidiana, distância de Manhattam, e a 

distância de Mahalanobis, descritos a seguir 

A distância usual entre dois pontos, que pode ser definida pela aplicação repetida do 

teorema de Pitágoras. É o conceito de distância mais comumente utilizado. 

Mais especificamente, se                               são pontos em algum espaço de 

  dimenões, então a distância euclidiana    entre estes pontos é definida como 

           
           

            
              

 

 

   

 

A distância de Manhattan, considerada por Hermann Minkowski no século XIX, é uma 

forma de geometria em que a usual distância é substituída por uma nova métrica, onde esta é 

dada pela soma das diferenças absolutas das coordenadas de dois pontos (Black, 2006). Tal 

distância também é conhecida como Geometria do táxi, ou distância   . Tal métrica faz alusão à 

distância percorrida por táxis nas ruas de Manhattan, que são dispostas em formato quadriculado. 

Se                               são pontos em algum espaço de   dimensões, então a 

distância de Manhattan      entre estes pontos é definida como 



 

 

  

92 

 

                                           

 

   

 

A título de curiosidade, é interessante notar que uma circunferência
i
·, na métrica de 

Manhattan, geometricamente é dada por um quadrado cujos lados compõem ângulos de     com 

os eixos coordenados. 

A distância de Mahalanobis é baseada nas correlações entre variáveis com as quais 

distintos padrões podem ser identificados e analisados (Maesschalck, 2000). Essa métrica foi 

introduzida na década de 1930 pelo matemático indiano Prasanta Chandra Mahalanobis. É uma 

estatística útil para determinar a similaridade entre uma amostra desconhecida e uma conhecida. 

Distingue-se da distância euclidiana já que leva em conta as correlações do conjunto de dados e é 

invariante à escala, ou seja, não depende da escala das medições. 

Se                é um ponto em algum espaço de   dimensões,               , a 

média de   e   a matriz de covariância de  , então a distância de Mahalanobis      desse ponto 

é definida como 

              
           

 

Se                               são pontos de mesma distribuição em algum espaço de 

  dimensões e   é a matriz de covariância entre esses pontos, então a distância de Mahalanobis é 

definida como 

                
           

Em particular, se a matriz de covariância é a matriz de identidade, a distância de 

Mahalanobis é reduzida a distância euclidiana. Se a matriz de covariância é diagonal, a distância 

de Mahalanobis é definida como uma distância euclidiana normalizada  

      
       

 

  
   

       
 

  
     

       
 

  
 

     
       

 

  
 

 

   

 

onde    é o desvio padrão entre       . 

 

 

                                                
i
 Por definição, circunferência é o conjunto de pontos com distância fixa, chamada raio, até algum ponto chamado de 

centro 


</field>
	</doc>
</add>