<?xml version="1.0" encoding="utf-8"?>
<add>
	<doc>
		<field name="docid">BR-TU.11709</field>
		<field name="filename">17110_Disserta%c3%a7%c3%a3o%20J%c3%balio%20Hoffimann%20Mendes.pdf</field>
		<field name="filetype">PDF</field>
		<field name="text">
T H E I N V E R S E P R O B L E M O F H I S T O R Y M A T C H I N G

A Probabilistic Framework for
Reservoir Characterization and Real Time Updating

BY

j ú l i o h o f f i m a n n m e n d e s

A dissertation submitted in partial fulfillment of

the requirements for the degree of

Master of Science

in

Civil Engineering

in the

Department of Civil Engineering

Federal University of Pernambuco

Recife, PE 50670-901

May 28, 2014



a d v i s o r s

Ramiro Brito Willmersdorf

Ézio da Rocha Araújo

c o m m i t t e e

Alexandre Anozé Emerick

Bernardo Horowitz

Júlio Hoffimann Mendes: The Inverse Problem of History Matching, A Probabilistic

Framework for Reservoir Characterization and Real Time Updating

c© May 28, 2014



Catalogação na fonte
Bibliotecária Margareth Malta, CRB-4 / 1198

M538i Mendes, Júlio Hoffimann.
The inverse problem of history matching, a probabilistic framework for 

reservoir characterization and real time updating / Júlio Hoffimann Mendes. 
- Recife: O Autor, 2014.

xv, 121 folhas, il., gráfs., tabs.

Orientador: Prof. Dr. Ramiro Brito Willmersdorf.
Cooreintador: Prof. Dr. Ézio da Rocha Araújo.
Dissertação (Mestrado) – Universidade Federal de Pernambuco. CTG. 

Programa de Pós-Graduação em Engenharia Civil, 2014.
Inclui Referências.

1.   Engenharia  civil.   2.   Simulação  de  Reservatórios.  3.  Ajuste  ao 
Histórico I.  Willmersdorf,  Ramiro Brito. (Orientador).  II. Araújo,  Ézio da 
Rocha. (Coorientador). III. Título.

                   UFPE

624 CDD (22. ed.)       BCTG/2014-164



?????????	??
?????	?
??

???	?????

????	?	
??

?????	??	???
??
??????	??	
?????

	
????????
?? ??! "?# 
" 
??$?? 
"?
?????#% &amp;amp;??
"?
???%# "?

?????????	?
????
?????	??
???	???

???
??????	???????
????	?
??
?
????	?	??	?	????
?	?????????
???	???
?
????

???????????	??	?
?	????????
?	????	?
?????	???

"?$?!"?" 
'?#

()*??
??$$?? !!
??!"??

??!??"?# 
?
? !"?" %?
	
???	??

????$?+
,-
"?
? ??
"?
,./0

?#??!% "?#??1

2222222222222222222222222222222222222222222

#?$3
?#3
? ??#?
?#?%?
4?**??#?"?#$
?
??
?


5?#??!% "?#6

2222222222222222222222222222222222222222222

#?$3
?#3
78??
" 
???9 
	# ):?
?
??
?


5????#??!% "?#6

? !? 
?? ??! "?# 1

2222222222222222222222222222222222222222222

#?$3
?#3
? ??#?
?#?%?
4?**??#?"?#$
?
??
?


5?#??!% "?#6

222222222222222222222222222222222222222222
?#3
	*?? !"#?
	!?8;
???#??&lt;
=

?%#?&gt;#??


5?? ??! "?#
??%?#!?6

222222222222222222222222222222222222222222

#?$3
?#3
??#! #"?
??#?@?%8
=
??
?


5?? ??! "?#
?!%?#!?6



Dedicated to my family and friends for all their love.

To one of the pioneers of this theory, Albert Tarantola.

1949 – 2009



R E S U M O

Em Engenharia de Petróleo e outras áreas da ciência, Mitigação de Incertezas baseada

em Histórico (MIH) é o termo moderno usado por especialistas ao se referirem

a ajustes contínuos de um modelo matemático dadas observações. Tais ajustes

tem maior valor quando acompanhados de diagnósticos que incluem intervalos

de confiança, momentos estatísticos, e idealmente caracterização completa das dis-

tribuições de probabilidade associadas.

Neste trabalho, o bastante conhecido problema de ajuste ao histórico em campos

de petróleo é revisado sob uma perspectiva Bayesiana que leva em consideração

toda possível fonte de incerteza teórica ou experimental. É uma aplicação direta

da metodologia geral desenvolvida por Albert Tarantola no seu livro intitulado

‘’Inverse Problem Theory and Methods for Model Parameter Estimation”.

Nosso objetivo é fornecer a pesquisadores da área de Óleo &amp;amp; Gás um software

escrito em uma linguagem de programação moderna (i. e. Python) que possa ser

facilmente modificado para outras aplicações; realizar a inversão probabilística

com dezenas de milhares de células como uma prova de conceito; e desenvolver

casos de estudo reproduzíveis para que outros interessados neste tema possam

realizar “benchmarks” e sugerir melhoramentos.

Diferentemente de outros métodos de sucesso para MIH como Ensemble Kalman

Filters (EnKF), o método proposto, denomidado Ensemble MCMC (EnMCMC),

não assume distribuições a priori Gaussianas. Pode ser entendido como uma cadeia

de Markov de ensembles e teoricamente é capaz de lidar com qualquer distribuição

de probabilidade multimodal.

Dois casos de estudo sintéticos são implementados em um cluster de computação

de alto desempenho usando o modelo MPI de execução paralela para distribuir as

diversas simulações de reservatório em diferentes nós computacionais. Resultados

mostram que a implementação falha em amostrar a distribuição a posteriori, mas

que ainda pode ser utilizada na obtenção de estimativas maximum a posteriori

(MAP) sem fortes hipóteses a respeito dos dados (e. g. a priori Gaussianas).

Palavras-chave: simulação de reservatórios, ajuste ao histórico.

vi



A B S T R A C T

In Petroleum Engineering and other fields, History-based Uncertainty Mitigation

(HUM) is the modern generic term used by experts when referring to continuous

adjustments of a mathematical model given observations. Such adjustments have

greater value when accompanied by uncertainty diagnostics including statistical

bounds, moments, and ideally full characterization of underlying distributions.

In this work, the well-known/ill-posed history matching problem is reviewed

under a Bayesian perspective which takes into account every possible source of

experimental and theoretical uncertainty. It is the direct application of the general

framework developed by Albert Tarantola on his textbook entitled ‘’Inverse Problem

Theory and Methods for Model Parameter Estimation”.

Our aim is to provide other researchers in the field with a software package writ-

ten in a modern programming language (i. e. Python) that can be easily adapted to

their needs; to perform probabilistic inversion with tens of thousands of cells as a

proof of concept; and to develop reproducible case studies for others to benchmark

and propose improvements.

Unlike other successfull HUM methods such as Ensemble Kalman Filters (EnKF),

the proposed method here called Ensemble MCMC (EnMCMC) does not assume

Gaussian priors. It can be thought as a Markov chain of ensembles and theoretically

can deal with any (multimodal) probability distribution.

Two synthetic case studies are implemented in a time-shared high-performance

computer cluster using the MPI parallel execution model to distribute reservoir

simulations among various computational nodes. Results show the implementa-

tion fails to sample the posterior distribution but still can be used to obtain clas-

sical maximum a posteriori (MAP) estimates without strong assumptions on the

data (e. g. Gaussian priors).

Keywords: reservoir simulation, history matching.

vii



P U B L I C A T I O N S

Some ideas and figures have appeared in:

Mendes, Willmersdorf, and Araújo [2013a]

Mendes, Willmersdorf, and Araújo [2013b]

Mendes, Willmersdorf, and Araújo [2013c]

Software implementations of the methods presented in this thesis are publicly

available at https://github.com/juliohm/HUM. Refer to Appendix B for additional

code snippets discussed throughout the text.

Please cite this work if you plan to use the software.

viii

https://github.com/juliohm/HUM


Old wood best to burn, old wine to drink, old friends to trust, and old authors to read.

Francis Bacon

A C K N O W L E D G E M E N T S

My family and friends are quite important to me. I won’t list all but my parents for

educating me with precious principles. Rigoberto Mendes da Silva &amp;amp; Lindevany

Hoffimann de Lima Mendes, I love you. A special thanks goes to my father for his

unconditional support and presence.

I would like to thank my advisor, Ramiro Brito Willmersdorf, for pushing me

towards hard problems, for his harsh sincerity againist my mistakes and for his

friendship along these years. He is certaintly one of the greatest finds of my career

and definitely has shaped who I am today.

Although rare, the long philosophical discussions at the campus with my co-

advisor, Ézio da Rocha Araújo, were inspiring. His vast knowledge on various

subjects and clear understanding of the fundamentals contributed to many of the

insights I had.

Thanks to Alexandre Emerick, Flávia Pacheco and Régis Romeu from c e n p e s for

their accreditation in our research; to p e t ro b r a s for the financial support medi-

ated by the p f r h - 2 6 program; and to c e na pa d - p e for providing computational

resources.

ix



C O N T E N T S

I i n v e r s e p ro b l e m t h e o r y 1

1 b a s i c c o n c e p t s 2

1.1 What is an inverse problem? . . . . . . . . . . . . . . . . . . . . . . . 3

1.2 Why inverse problems are hard? . . . . . . . . . . . . . . . . . . . . . 6

1.3 The maximum likelihood principle . . . . . . . . . . . . . . . . . . . . 8

1.4 Tarantola’s postulate . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9

1.5 Classical vs. probabilistic framework . . . . . . . . . . . . . . . . . . . 10

2 c l a s s i c a l f r a m e w o r k 12

2.1 Basic taxonomy for inverse problems . . . . . . . . . . . . . . . . . . 13

2.2 Linear regression and the least-squares estimate . . . . . . . . . . . . 16

2.3 Tikhonov regularization . . . . . . . . . . . . . . . . . . . . . . . . . . 19

2.4 Levenberg-Marquardt solution to nonlinear regression . . . . . . . . 24

3 p ro b a b i l i s t i c f r a m e w o r k 28

3.1 Definition of probability . . . . . . . . . . . . . . . . . . . . . . . . . . 29

3.2 States of information . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36

3.3 Bayesian inversion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37

3.4 Ensemble Markov chain Monte Carlo . . . . . . . . . . . . . . . . . . 42

II h i s t o r y m at c h i n g 55

4 p r e l u d e 56

4.1 Problem description . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57

4.2 Case studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58

4.3 Comments on reproducibility . . . . . . . . . . . . . . . . . . . . . . . 63

5 c h a n n e l i z e d r e s e rv o i r 64

5.1 Setting priors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65

5.2 Probabilistic inversion . . . . . . . . . . . . . . . . . . . . . . . . . . . 69

5.3 Analysis of the results . . . . . . . . . . . . . . . . . . . . . . . . . . . 70

6 b ru g g e f i e l d 80

6.1 Setting priors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81

6.2 Probabilistic inversion . . . . . . . . . . . . . . . . . . . . . . . . . . . 82

x



c o n t e n t s xi

6.3 Analysis of the results . . . . . . . . . . . . . . . . . . . . . . . . . . . 83

7 c o n c l u s i o n 86

7.1 General comments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86

7.2 Technical difficulties . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88

7.3 Suggested improvements . . . . . . . . . . . . . . . . . . . . . . . . . 89

III a p p e n d i x 91

a o m i t t e d p ro o f s 92

a.1 The majority of inverse problems is ill-posed . . . . . . . . . . . . . . 92

a.2 Maximum likelihood estimation for i.i.d. Gaussians . . . . . . . . . . 92

a.3 System of equations for discrete linear inverse problems . . . . . . . 93

a.4 Maximum likelihood and least-squares . . . . . . . . . . . . . . . . . 93

a.5 Weighted linear least-squares estimate . . . . . . . . . . . . . . . . . . 94

a.6 Levenberg-Marquardt gradient and Hessian . . . . . . . . . . . . . . 94

a.7 Conditional probability by conjunction of states . . . . . . . . . . . . 95

a.8 Kernel density estimation as a convolution . . . . . . . . . . . . . . . 95

b c o d e s n i p p e t s 97

b.1 Iteratively reweighted least-squares . . . . . . . . . . . . . . . . . . . 97

b.2 Least absolute shrinkage and selection operator . . . . . . . . . . . . 98

b.3 Metropolis algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99

b.4 Online Bayesian inversion . . . . . . . . . . . . . . . . . . . . . . . . . 100

b.5 Histogram fitting with kernel density estimation . . . . . . . . . . . . 101

c k e r n e l p c a 103

c.1 Kernel Gramian matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . 104

c.2 Centering in the feature space . . . . . . . . . . . . . . . . . . . . . . . 105

c.3 Eigenproblem and normalization . . . . . . . . . . . . . . . . . . . . . 106

c.4 Preimage problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107

b i b l i o g r a p h y 110



L I S T O F F I G U R E S

Figure 1.1 Geostatistical inference . . . . . . . . . . . . . . . . . . . . . . 4

Figure 1.2 Non-bijective map G : M 7?? D . . . . . . . . . . . . . . . . . 6
Figure 1.3 3D non-convex surfaces . . . . . . . . . . . . . . . . . . . . . 10

Figure 2.1 Univariate Gaussians . . . . . . . . . . . . . . . . . . . . . . . 14

Figure 2.2 Polynomial regression models . . . . . . . . . . . . . . . . . . 16

Figure 2.3 Bullet trajectory prediction . . . . . . . . . . . . . . . . . . . . 18

Figure 2.4 Ridge regression . . . . . . . . . . . . . . . . . . . . . . . . . . 20

Figure 2.5 L-curve criterion . . . . . . . . . . . . . . . . . . . . . . . . . . 22

Figure 2.6 L1-norm regression . . . . . . . . . . . . . . . . . . . . . . . . 23

Figure 2.7 Numerical simulator as a black box . . . . . . . . . . . . . . 26

Figure 3.1 Homogeneous distribution for Jeffreys parameters . . . . . . 32

Figure 3.2 Disjunction for producing histograms . . . . . . . . . . . . . 32

Figure 3.3 Impact clouds on a cathodic screen . . . . . . . . . . . . . . . 33

Figure 3.4 The p-event of an interval for a Jeffreys parameter. . . . . . . 34

Figure 3.5 Conditioning probability densities . . . . . . . . . . . . . . . 35

Figure 3.6 Dirac delta function ?(x; x0) . . . . . . . . . . . . . . . . . . . 37

Figure 3.7 Nescience towards Omniscience . . . . . . . . . . . . . . . . . 37

Figure 3.8 2D Gaussian N
(
mprior, Cm

)
. . . . . . . . . . . . . . . . . . . 38

Figure 3.9 Uncertainty in the forward operator . . . . . . . . . . . . . . 39

Figure 3.10 Graph for a 3-state machine . . . . . . . . . . . . . . . . . . . 44

Figure 3.11 Limiting behavior for a 3-state Markov chain . . . . . . . . . 44

Figure 3.12 Histogram for a Gaussian mixture . . . . . . . . . . . . . . . 47

Figure 3.13 Trace and autocorrelation for a Gaussian mixture . . . . . . 48

Figure 3.14 Probabilistic inversion of y = x2 . . . . . . . . . . . . . . . . . 50

Figure 3.15 KDE for a 2D Gaussian . . . . . . . . . . . . . . . . . . . . . . 51

Figure 3.16 Anisotropic joint distribution . . . . . . . . . . . . . . . . . . 52

Figure 3.17 Stretch move ?(t)
k
? ?(t+1)

k
. . . . . . . . . . . . . . . . . . . 52

Figure 4.1 Mind map of algorithms and concepts . . . . . . . . . . . . . 56

Figure 4.2 Training image of size 250x250 pixels . . . . . . . . . . . . . 58

Figure 4.3 Ten-spot well configuration . . . . . . . . . . . . . . . . . . . 59

xii



List of Figures xiii

Figure 4.4 Oil/water saturation within channelized reservoir . . . . . . 59

Figure 4.5 Production history for channelized reservoir . . . . . . . . . 59

Figure 4.6 Brugge field . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60

Figure 4.7 Top view of Brugge field realization . . . . . . . . . . . . . . 61

Figure 4.8 Brugge field permeability curves . . . . . . . . . . . . . . . . 62

Figure 4.9 Brugge field oil production history . . . . . . . . . . . . . . . 62

Figure 5.1 Filtersim realizations . . . . . . . . . . . . . . . . . . . . . . . 66

Figure 5.2 kPCA for increasing polynomial kernel degrees . . . . . . . 68

Figure 5.3 Stretch move: bean plot of prior and posterior log-probabilities 71

Figure 5.4 KDE move: bean plot of prior and posterior log-probabilities 71

Figure 5.5 KDE move: production history for the prior ensemble . . . . 72

Figure 5.6 KDE move: production history for the posterior ensemble . 73

Figure 5.7 KDE move: acceptance fraction for each walker . . . . . . . . 74

Figure 5.8 KDE move: 25 most probable images in prior ensemble . . . 75

Figure 5.9 KDE move: 25 most probable images in posterior ensemble 76

Figure 5.10 KDE move: maximum a posteriori estimate . . . . . . . . . . 77

Figure 5.11 Filtersim: bean plot of prior and posterior log-probabilities . 77

Figure 5.12 Filtersim: acceptance fraction for each walker . . . . . . . . . 77

Figure 5.13 Filtersim: 25 most probable images in posterior ensemble . . 78

Figure 5.14 Filtersim: production history for the posterior ensemble . . 78

Figure 5.15 Filtersim: maximum a posteriori estimate . . . . . . . . . . . 79

Figure 5.16 kPCA reconstruction . . . . . . . . . . . . . . . . . . . . . . . 79

Figure 6.1 Prior on observations changing over time . . . . . . . . . . . 82

Figure 6.2 KDE move: bean plot of prior and posterior log-probabilities 83

Figure 6.3 KDE move: acceptance fraction for each walker . . . . . . . . 84

Figure 6.4 KDE move: production history for the prior ensemble . . . . 84

Figure 6.5 KDE move: production history for the posterior ensemble . 85



L I S T O F T A B L E S

Table 4.1 Channelized reservoir summary table . . . . . . . . . . . . . 58

Table 4.2 Brugge rock formations . . . . . . . . . . . . . . . . . . . . . 61

Table 5.1 SSIM statistics for Filtersim and KDE-based proposals . . . 75

xiv



L I S T O F A L G O R I T H M S

2.1 Coordinate descent for sparse regularization . . . . . . . . . . . . . . . 23

3.1 Metropolis-Hastings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46

3.2 Stretch move in Rn . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53

xv



L I S T I N G S

Listing 5.1 Filtersim parameters file . . . . . . . . . . . . . . . . . . . . . 65

Listing B.1 Iteratively reweighted least-squares . . . . . . . . . . . . . . . 97

Listing B.2 Least absolute shrinkage and selection operator . . . . . . . 98

Listing B.3 Metropolis algorithm . . . . . . . . . . . . . . . . . . . . . . . 99

Listing B.4 Online Bayesian inversion . . . . . . . . . . . . . . . . . . . . 100

Listing B.5 Histogram fitting with kernel density estimation . . . . . . . 101

xvi



A C R O N Y M S

SVM Support Vector Machine

MPS Multiple-Point Statistics

SVD Singular Value Decomposition

LASSO Least Absolute Shrinkage and Selection Operator

KKT Karush-Kuhn-Tucker

DFP Davidon-Fletcher-Powell

BFGS Broyden-Fletcher-Goldfarb-Shanno

MCMC Markov chain Monte Carlo

EnKF Ensemble Kalman Filter

MAP Maximum a Posteriori

KDE Kernel Density Estimation

RML Randomized Maximum Likelihood

K-L Karhunen-Loève

kPCA Kernel Principal Component Analysis

kMAF Kernel Maximum Autocorrelation Factor

MPI Message Passing Interface

GPGPU General-Purpose Computing on Graphics Processing Units

HUM History-based Uncertainty Mitigation

TPFA Two-Point Flux Approximation

EnMCMC Ensemble Markov chain Monte Carlo

SSIM Structural Similarity

xvii



Part I

I N V E R S E P R O B L E M T H E O R Y

In which the general problem of estimating parameters of a (physical)

system based on real evidence (i. e. observation) is set up. The classi-

cal and probabilistic framework for the solution are briefly reviewed

emphasizing the very different questions they target.



1
B A S I C C O N C E P T S

For certainly it is excellent discipline for an author to feel that he must say all he has to

say in the fewest possible words, or his reader is sure to skip them; and in the plainest

possible words, or his reader will certainly misunderstand them.

John Ruskin

1.1 What is an inverse problem? . . . . . . . . . . . . . . . . . . . . . 3

1.2 Why inverse problems are hard? . . . . . . . . . . . . . . . . . . . 6

1.3 The maximum likelihood principle . . . . . . . . . . . . . . . . . 8

1.4 Tarantola’s postulate . . . . . . . . . . . . . . . . . . . . . . . . . . 9

1.5 Classical vs. probabilistic framework . . . . . . . . . . . . . . . . 10

What is an inverse problem? Why is it so hard to solve? What to expect as a reasonable

solution? All these questions should be addressed in the following sections.

Inverse problems are quite general and don’t require concrete examples to be

understood. Like the elementary concept of an inverse image (a. k. a. preimage) of

a function, these problems are nothing but abstraction. Indeed, the solution of an

inverse problem is an inverse image1.

Without even knowing what an inverse problem is, the smart reader is probably

speculating about ill-posedness and how to deal with it. There isn’t magic and the

most natural question2 has to be adapted for approximate answers to be inferred,

in a least-squares sense.

However, redesigning the questions can be surprisingly innovative and reward

one’s mind with new points of view.

1 Or a collection of images with known distribution.
2 What is x = f?1(y)?

2



1.1 w h at i s a n i n v e r s e p ro b l e m ? 3

1.1 w h at i s a n i n v e r s e p ro b l e m ?

Consider a function G : M 7?? D that for each parameter m ? M associates a
response d ? D. It’s possible to conceptualize exactly three types of inference [4]:

F O R WA R D P R O B L E M Given m and G; Find d = G(m).

I N V E R S E P R O B L E M Given d and G; Find m
?
= G?1(d).

S U P E R V I S E D L E A R N I N G Given m and d; Find G.

Regardless of the meaning of M and D, hereafter called model space and data space,

or the complexity of G, the transfer function3; these problems have rather distinct

tractability. The f o r wa r d p ro b l e m is the easiest as its solution is obtained by

direct function evaluation. Techniques for solving the i n v e r s e p ro b l e m will be

discussed throughout this thesis because clearly the inverse G?1 may not exist

or be available. Finally, s u p e rv i s e d l e a r n i n g4 has being extensively studied

as a Machine Learning subtopic and various successful classification/regression

techniques such as SVM were developed for solving it [5].

i m p o r ta n t n o t e : In the literature, the term f o r wa r d p ro b l e m is also

used to designate the inductive process of deriving physical laws from experiments.

This is essentially a human being quality, and it’s much harder, if not impossible,

for a computer to reproduce. In this text, the transfer function G is given.

For better understanding, the following concrete examples attach meaning to M,

D and G. They’re all practical applications.

Example 1.1 (Geostatistics)

A random field Z(x; ?) is a stochastic process on spatial coordinates x ? Rn [6].
For the oil industry it characterizes the uncertainty on petrophysical properties in

a reservoir model [7, 8, 9]. A very common task a professional in this area has to

accomplish is conditional sampling:

3 Often representing an expensive simulation code.
4 It isn’t the more intuitive name for the third type of inference, s y s t e m i d e n t i f i c at i o n p ro b l e m

is a possible alternative [4].



1.1 w h at i s a n i n v e r s e p ro b l e m ? 4

A spatial property is measured at few locations within an acceptable deviation

from the true (unknown) value. This hard data comes from boreholes as the prod-

uct of laboratory experiments with cores, well tests, or sometimes are indirectly

obtained from well logs, see Figure 1.1a. The task is to fill in the 3D reservoir

model with the mentioned property honoring the hard data and prior probability

distribution of the field. A complementary step not considered in this example is

soft data integration [10, 11].

(a) Rock porosity at well locations. (b) Ordinary Kriging estimation.

Figure 1.1: Geostatistical inference on a 3D reservoir model.

The result of applying Ordinary Kriging to the Stanford VI data set [11] is shown

in Figure 1.1b. Similar to Stanford V [12], this stratigraphic model was synthesized

as a fluvial channel system with the purpose of extensively testing (MPS) algo-

rithms for reservoir characterization.

In spite of a neat deterministic solution, it presents non-physical smoothness [13].

This inverse problem is better solved by sequential simulation which is a technique

for drawing random values from univariate distributions sequentially built with

the hard data [14]. In fact, this subject has been widely discussed [15, 16, 17]. Se-

quential simulation allows multiple realizations to be drawn to characterize the ran-

dom field.

In mathematical language, m is defined as the flattened array of length nx×ny×
nz containing all porosity values of the 3D grid; d is of reduced length, the number

of cells with hard data; and G is the “selection” operation, it simply discards grid

locations that aren’t in d, and is represented by a matrix whose entries are either 1

or 0.



1.1 w h at i s a n i n v e r s e p ro b l e m ? 5

Thus, a linear inverse problem of the form:

?
???

???

d

?
???

???

=

G
? ?? ??
???

1 0 · · · 0 0
0 0

. . . 1 0
...

...
...

...
...

?
???

?
????????

????????

m

?
????????

????????

(1.1)

?

Example 1.2 (Well Testing)

For assessing a newly discovered reservoir under dynamic conditions, the just

drilled pilot well is provisionally completed for controlled production during a

short period of time, and closed until hydrostatic equilibrium is reestablished.

The well pressure is registered along with the imposed production rate. The

phenomenon can be analytically modelled by the diffusion equation with the ap-

propriate boundary conditions for the transient regime5. The analytical solution

for vertical wells under constant production rate is [18, 19]:

pw = pi ?
qµ

2??h

(
1

2
Ei

[
?µctr

2
w

4?t

])
(1.2)

with Ei(x)
def
=

??
x

e??

?
d?, the exponential integral function. The ultimate goal of

the test is to estimate the productivity index for the well in the long term by

first estimating the rock permeability—the only unknown in Equation 1.2. Similar

estimation is desired when in situ permanent sensors are installed [20]. ?

Example 1.3 (Physical Measurements)

A collection of instruments is used for measuring parameters of a physical system.

That may involve human interaction and/or impossible to control environmental

conditions. As a result, instrument readings may be (and generally are) not precise,

even though accurate6.

Suppliers of these instruments then provide a statistical analysis of the uncer-

tainties involved in the measurement process that should be used to describe

the output. For instance, if parameters m are to be measured by an instrument

I : M 7?? M, the supplier provides the conditional probability Pr(mout | m) or the
covariance CM.

5 The reservoir has hypothetical infinite extension.
6 Precision is about statistical variance, whereas accuracy is the absence of bias in the measurements.



1.2 w h y i n v e r s e p ro b l e m s a r e h a r d ? 6

Ideally, the instrument is the identity function I ? IdM, the forward and inverse
problem share the same solution. In the real world, is common practice to assume

the (Gaussian) error ? is independent of the input:

mout = I(m) = m + ? (1.3)

The inverse problem is to find the true values for the physical parameters m

given the instrument measurements mout, or as for the later assumption, to sub-

tract m = mout ? ?. Note the error is a random variable. ?

This work is concerned with the inverse problem of history matching, a much

more computational expensive problem that the oil industry is encouraging re-

searchers to devote time thinking. It’ll be introduced in the following chapters

along with its theoretical and practical difficulties.

1.2 w h y i n v e r s e p ro b l e m s a r e h a r d ?

m1

m2

m3

M

M

d1

d2

d3

D

D

G

?
?

G?1

Figure 1.2: Non-bijective map between model and data space.

Back in 1902, the French mathematician Jacques Hadamard (1865 – 1963) con-

ceived the term well-posed problem to designate a very important concept that can

be adapted for use in inverse problem theory. A boundary value problem in math-

ematical physics is said to be well-posed in the sense of Hadamard if it satisfies all

of the following criteria [21]:

• There exists a solution

• The solution is unique

• The solution depends continuously on the data7

7 Meaning slight changes in the boundary conditions cause little, if any, impact on the solution.



1.2 w h y i n v e r s e p ro b l e m s a r e h a r d ? 7

A problem that isn’t well-posed in this sense is termed ill-posed problem, see

Figure 1.2 for an illustration.

Theorem 1. The majority of inverse problems is ill-posed.

Proof. Given two equinumerous sets |A| = |B|, the number of bijections from A to

B never exceeds the number of non-bijections, except when both sets are empty or

singleton.

i n f i n i t e c a s e Denote fA7?B, bA 7?B and nA7?B the number of functions, bi-

jections and non-bijections from A to B, respectively. It follows that:

bA7?B 6 fA7?B = fA\{a} 7?B 6 nA7?B

fA7?B = fA\{a} 7?B because A and A \ {a} have the same cardinality; fA\{a} 7?B 6

nA7?B because a function f : A \ {a} 7?? B can be extended by mapping f(a) ?
f(A \ {a}), which is non-injective.

f i n i t e c a s e |A| = |B| = n, fA7?B = nn, bA7?B = n!, nA7?B = nn ? n!

Apart from the empty and singleton sets for which only one (bijective) function is

defined, the result (left for the reader) is proved by induction:

nn ? n! &gt; n! (?n &gt; 2)

Put differently, well-posed inverse problems are accidental. For instance, assume

the model and data space are finite; if |M| 6= |D|, none of the inverse problems is
well-posed, else |M| = |D| = n and the percentage8 of n!

nn
? 0% vanishes very

rapidly with n.

Not only ill-posed, inverse problems are sometimes computationally expensive

within current solving strategies. If a more detailed characterization of the model

space is desired and the transfer function is a demanding physical simulation,

the wall time required for the solution increases considerably. Up to date, two

frameworks coexist for the solution of inverse problems, the probabilistic being in

general more expensive than the classical, for reasons that will be clear in next

sections.

8 The third criterion of Hadamard is not taken into account, the exact percentage is smaller.



1.3 t h e m a x i m u m l i k e l i h o o d p r i n c i p l e 8

1.3 t h e m a x i m u m l i k e l i h o o d p r i n c i p l e

Given a statistical model for experiment outcomes x1, x2, . . . , xm parameterized by

?, the likelihood function is a reinterpretation of the joint probability density f as

if the observations were “fixed parameters”:

L(? | x1, x2, . . . , xm)
def
= f(x1, x2, . . . , xm | ?) (1.4)

The maximum likelihood principle states the model parameters are to be set to

maximize the experiment probability (i. e. an extremum estimator):

?? = arg max
???

L(? | x1, x2, . . . , xm) (1.5)

This is to say over all plausible parameters ? ? ?, only ?? is of interest. For
instance, if the univariate Gaussian model is assumed and the random variables

are i.i.d., the parameters can be shown to match the sample mean and standard

deviation (see Appendix A.2):

?? =

{
µ?

??2

}

=

{
1
m

?
i xi

1
m

?
i(xi ? µ?)

2

}

(1.6)

For inverse problems, the principle is interpreted likewise: find the parameters

m? ? M that best honor the observed data d ? D through the forward operator G
according to some loss function (e. g. ?d ? G(m)?L2 ):

m? = arg min
m?M

loss (d, G(m)) (1.7)

It can be in general a non-convex optimization problem over a high-dimensional

space. In Example 1.1, the Kriging model has parameters with 6 million entries

(i. e. cells) and the loss function is the mean square estimation error or estimation

variance ?2 = E
[
(Z?(x; ?) ? Z(x; ?))2

]
with the estimator Z?(x; ?)

def
=

?
i ?iZ(xi; ?)

a linear combination of the hard data.

The maximum likelihood principle is widely applied in statistics and real life

engineering applications.



1.4 ta r a n t o l a’ s p o s t u l at e 9

1.4 ta r a n t o l a’ s p o s t u l at e

“The most general solution of an inverse problem provides a probability distri-

bution over the model space.” – Albert Tarantola

On his book Inverse Problem Theory and Methods for Model Parameter Estimation

[22], Tarantola argued that solutions to inverse problems are of greater value if

they come attached to probability distributions. He developed a very clever theory

in which all information available about the problem is modeled within a richer

probabilistic framework. This dissertation is an attempt to apply this framework

to the problem of history matching and, at worst, will serve to clarify what parts

of it requires further work for practical use by the oil industry.

A careful review of the theory is given in Chapter 3. For now, it’s sufficient

to know that once a coordinate system is fixed for the data and model spaces,

probability densities can be defined, and the Bayesian updating performed:

?M(m) = k ?M(m) L(m) (1.8)

The posterior distribution over the model space ?M(m) is obtained from the

prior ?M(m) by incorporating the likelihood L(m) which expresses how good the

parameters m are in explaining the data. In Equation 1.8, k is a normalization

constant.

Having a continually updated probability distribution for the parameters allows

answering questions such as What is the most probable parameter?9 What is the proba-

bility of a parameter being in a certain range? and many others. Tarantola’s framework

is therefore, as previously mentioned, “richer” at providing the analyst a variety

of insights.

However, it requires higher level of abstraction from the implementer as to iden-

tify and model the various states of information. Not to mention, information isn’t

always available, specially if it costs billions of dollars to be acquired.

The probabilistic framework is relatively recent, few researchers have employed

it in large scale problems for assessing its efficiency.

9 Maximum likelihood estimation.



1.5 c l a s s i c a l v s . p ro b a b i l i s t i c f r a m e w o r k 10

1.5 c l a s s i c a l v s . p ro b a b i l i s t i c f r a m e w o r k

It’s clear that the classical framework, as the direct application of the maximum

likelihood principle, does not address many important questions. As Tarantola

once said: “This is not the solution; it is, rather, the mean of all possible solutions. Looking

at this mean provides much less information than looking at a movie of realizations.”.

Referring to the least-squares solution of an inverse problem [22].

It’s also clear that, by what was exposed in Section 1.4, the probabilistic frame-

work is more general as it considers the maximum likelihood estimation as one of

many possible post-processing steps. All the embedded information is preserved

and accumulates as new data becomes available.

Following this reasoning, the comparison c l a s s i c a l v s . p ro b a b i l i s t i c is

unfair, nevertheless it is important to be raised. The point is to make it clear when

it is worth applying one framework over another.

This dissertation endorses the Tarantola’s postulate with the justification being

multimodal hypersurfaces in high-dimensional spaces. Figure 1.3 is a collection of

3D synthetic surfaces for which the maximum likelihood estimation isn’t helpful.

Figure 1.3: 3D non-convex surfaces.



1.5 c l a s s i c a l v s . p ro b a b i l i s t i c f r a m e w o r k 11

The classical framework is the option for rough estimates, if no data is available

to assemble representative distributions or if the problem is known to be unimodal

beforehand.

Concerning ill-posedness handling, maximum likelihood estimation does it ex-

plicitly with the help of regularization techniques, whereas Bayesian updating

guarantees the existence of an unique well-defined posterior. Alternatively, the

probabilistic framework always gives an unique answer: the posterior distribution

over the model space. This e x p l i c i t v s . i m p l i c i t ill-posedness treatment will

be revisited in more depth when it’s appropriate.

Algorithmically, Tarantola’s framework relies on randomness for effective explo-

ration of high-dimensional spaces. The classical approach uses, in general, deter-

ministic optimization routines for solving slightly modified (i. e. biased) problems.

In summary, what to expect as a reasonable solution? Notwithstanding a subjec-

tive question, a probability distribution over the model space is almost always a

satisfactory answer.



2
C L A S S I C A L F R A M E W O R K

If you don’t know anything about computers, just remember that they are machines that

do exactly what you tell them but often surprise you in the result.

Richard Dawkins

2.1 Basic taxonomy for inverse problems . . . . . . . . . . . . . . . . 13

2.2 Linear regression and the least-squares estimate . . . . . . . . . 16

2.3 Tikhonov regularization . . . . . . . . . . . . . . . . . . . . . . . . 19

2.4 Levenberg-Marquardt solution to nonlinear regression . . . . . . 24

Likely to be employed by frequentists1, the classical framework aims at finding the

most probable parameters for the (physical) system that honors the observations.

It consists of maximizing the likelihood function, no matter the prior state of in-

formation on the system2. For the practical experimentalist, the framework often

translates into minimizing the misfit on the training data, disregarding previous

attempts or externally acquired knowledge.

As optimization is omnipresent, it makes sense to approach the framework by

informally classifying which inverse problems reflect “good” objective functions

and which don’t. A basic taxonomy is presented that groups inverse problems as

discrete, continuous, linear and nonlinear.

It occurs that adding engineer-designed terms to the objective can improve the

solution process and the estimation itself—a technique generically referred to as

regularization. The most commonly used regularization schemes are presented for

reference, elaborating on this topic is out of scope of the present work.

1 Followers of frequentist statistics.
2 A virtual prior is often introduced for stability.

12



2.1 b a s i c ta x o n o m y f o r i n v e r s e p ro b l e m s 13

Finally, the problem of nonlinear inversion is investigated with a specialized

version of the Gauss-Newton method for unconstrained optimization.

This chapter is heavily influenced by Aster, Borchers, and Thurber’s “Parameter

Estimation and Inverse Problems” [4].

2.1 b a s i c ta x o n o m y f o r i n v e r s e p ro b l e m s

2.1.1 Discrete vs. continuous

Inverse problems for which the model and data space have continuous functions

as elements of study are termed continuous. These problems are often expressed as

integral operators:

?b

a

g(s, x)m(x) dx = d(s) (2.1)

where m(x) is the unknown, d(s) the observation, and g(s, x) the kernel function.

Equation 2.1 occurs so often in mathematical models, it has a name—Fredholm

integral equation of the first kind. For instance, in electrical engineering, the kernel

depends explicitly on s ? x; convolution/deconvolution arises as one of the most

important forward/inverse problems:
??

??

g(s ? x)m(x) dx = d(s) (2.2)

For further clarification, consider g(s, x) = 1 over [a, b] ? R, the inverse problem
has no solution unless d(s) = C is a constant. Moreover, there are multiple func-

tions m(x) for which the definite integral evaluates to C. The problem is ill-posed.

Since continuous functions cannot be represented by digital computers, these

problems must be discretized first. The parameters m = (m1, m2, . . . , mn)? and

observations d = (d1, d2, . . . , dm)? are both represented by a finite3 number of

coordinates and the inversion is said to be discrete.

Parameter estimation is an alternative name for discrete inverse problems, it is

originated from the fact that discretization is generally achieved by series expan-

sion and parametrization. For example, the univariate Gaussian distribution is en-

tirely determined by two parameters N
(
µ, ?2

)
, no matter the complex approxima-

tions the computer does for reproducing it:

3 More restrictive than just discrete.



2.1 b a s i c ta x o n o m y f o r i n v e r s e p ro b l e m s 14

?6 ?5 ?4 ?3 ?2 ?1 0 1 2 3 4 5 6

µ = 0 ?2 = 0.6

µ = 0 ?2 = 1.0

µ = 0 ?2 = 3.0

µ = ?2 ?2 = 0.8

Figure 2.1: Univariate Gaussians parameterized by
(
µ, ?2

)?
.

Given that large-scale inverse problems are solved by computers, discretization

is of major importance. Few continuous inverse problems have an elegant solution

like in Example 2.1.

Example 2.1 (Lagrange Interpolation)

Find a polynomial p(X) ? R[X] of degree n with given zeros x1, x2, . . . , xn ?
R. This problem is inverse to the direct problem of finding the roots of a given

polynomial p(X) ? R[X]. In this case, the inverse problem is conceptually easier to
solve: p(X) = c(X ? x1)(X ? x2) · · ·(X ? xn) for c ? R.

More generally, find a polynomial p(X) ? R[X] of degree n that assumes given
values y1, y2, . . . , yn ? R at given distinct points x1, x2, . . . , xn ? R. The solution
is given by the Langrange interpolation theorem.

Whether this example is an i n v e r s e p ro b l e m or s u p e rv i s e d l e a r n i n g, it

is open to philosophical discussion. ?

2.1.2 Linear vs. nonlinear

An inverse problem is linear if its forward operator G satisfies superposition and

scaling:

G(m1 + m2) = G(m1) + G(m2) (2.3)

G(?m) = ? G(m) (2.4)

It’s nonlinear otherwise, and also harder to solve. The model and data spaces of

discrete inverse problems are assumed to be linear manifolds so that it makes sense

to add and scale coordinates (a. k. a. components). This is a reasonable assumption

with no penalties to most real applications.



2.1 b a s i c ta x o n o m y f o r i n v e r s e p ro b l e m s 15

Every discrete linear inverse problem can be trivially written as a system of

linear equations by looking up the basis for the associated model and data spaces

(see Appendix A.3):

Gm = d (2.5)

with G the matrix for the linear transformation. The problem then reduces to

“solving” the system for m even when an inverse G?1 does not exist. Various

well-established results from linear algebra have been applied for the solution of

these problems, they are reviewed in Section 2.2.

Fredholm integral equations of the first kind are a good example of continuous

linear inverse problems, Equation 2.1 is an integral and as such:
?b

a

g(s, x)
[
? m1(x) + m2(x)

]
dx = ?

?b

a

g(s, x)m1(x) dx +
?b

a

g(s, x)m2(x) dx (2.6)

Since coupled multiphysics and complex models are considered in realistic En-

gineering simulations, nonlinear outcomes are obtained in general. Inversion is

much difficult and requires iterative optimization for an appropriate solution. The

Levenberg-Marquardt algorithm is discussed in Section 2.4.

2.1.3 Well-posed vs. ill-posed

The Hadamard definition for well-posedness is revisited for completeness with

normed spaces [23].

Definition 2.1 (Well-posedness). Let X and Y be normed spaces, T : X 7?? Y a
(linear or nonlinear) mapping. The equation T(x) = y is called properly-posed or

well-posed if the following holds:

• Existence and uniqueness: ?y ? Y, ?!x ? X, y = T(x).

• Stability: for every sequence (xn)n?N with T(xn) ? T(x), it follows that
xn ? x.

Equations for which (at least) one of these properties does not hold are called

improperly-posed or ill-posed.

i m p o r ta n t n o t e : Discrete linear inverse problems can be further classified

as purely underdetermined, purely overdetermined or mixed determined depending on

the range and null space of G [24].



2.2 l i n e a r r e g r e s s i o n a n d t h e l e a s t - s q ua r e s e s t i m at e 16

2.2 l i n e a r r e g r e s s i o n a n d t h e l e a s t - s q ua r e s e s t i m at e

Since no exact preimage exists for noisy observations, parameterized mathematical

models are adjusted to fit data with respect to some misfit measure, as illustrated

in Figure 2.2. Linear regression is the term used when the model being fit depends

linearly on the parameters d? = Gm.

Figure 2.2: Polynomial regression models.

The widely employed procedure is to minimize the sum of squared errors be-

tween model and observation dobs
4:

mL2 = arg min
m?M

(dobs ? Gm)
?(dobs ? Gm) (2.7)

It has a closed form solution mL2 = (G
?G)?1G?dobs when G has full column

rank and probabilistic meaning5 when the noise is Gaussian (see Appendix A.4).

When the null space of G denoted N(G) isn’t trivial, the pseudo-inverse defined

through SVD is used to compute a least-squares and minimum-length solution:

G†
def
= VpS

?1
p U

?
p (2.8)

with rank(G) = p in the singular value decomposition for G:

G = USV? =
[
Up U0

][Sp 0
0 0

][
Vp V0

]?
(2.9)

The pseudo-inverse solution m† = G†dobs always exist and, as a least-squares

solution, satisfies the normal equations:

(G?G)m† = G
?dobs (2.10)

4 dobs and d are used indistinguishably.
5 It’s the maximum likelihood estimate.



2.2 l i n e a r r e g r e s s i o n a n d t h e l e a s t - s q ua r e s e s t i m at e 17

Most importantly, it can be shown by pure algebraic manipulation that m† min-

imizes the length ?m?L2 over all residuals ?d ? Gm?L2 (refer to Aster et al.).
The major problem with pseudo-inverses is that they introduce non-negligible bias

to the solution, whereas for instance, the least-squares estimate mL2 is unbiased

under Gaussian assumption, E [mL2] = mtrue. Bounds are derived for the intro-

duced bias with the concept of model resolution: for the pseudo-inverse G† of the

forward operator G define the resolution matrix RG
def
= G†G and notice

RGm = G
†Gm = G†(Gm) ? m (2.11)

is a defeatured version of m that will be exact approximation RGm = m if nothing

is missed in the null space N(G). A simple measure for the bias is the trace Tr(RG);

the closer to that of the identity matrix, the lower the bias. An exact (but not very

useful) quantification is obtained comparing the expected value for m† and the

true (unknown) parameters mtrue:

E
[
m†
]
= E

[
G†d

]
= G†E [d] = G†Gmtrue = RGmtrue (2.12)

? BIAS = E
[
m†
]
? mtrue = (RG ? I)mtrue (2.13)

Equation 2.13 gives a theoretical bound ?BIAS? 6 ?RG ? I??mtrue? since no prior
knowledge exists for ?mtrue?. It can be further manipulated to incorporate SVD
factors RG ? I = VpV?p ? VV

? = ?V0V
?
0 .

Another important issue is the instability of the generalized inverse solution.

Small singular values cause it to be extremely sensitive to noise in the data as

translated by the condition number of G. In practice, all the solutions discussed—

least-squares and generalized inverse—aren’t implemented directly; techniques for

stabilizing the inverse problem produce considerably better results, the so called

regularization or damped estimation is revised in the next section.

The regression model can also be very sensitive to outliers, as is the case for

least-squares. The usual misfit measure for inconsistent data is the L1-norm of

the residual ?d ? Gm?L1 , its resistance to outliers is often explained by arguing
the inconsistency isn’t magnified by taking squares. L1 regression is robust, the

main reason it’s the second choice is non-differentiability. The minimization is

performed by iteratively reweighted least-squares [25] which is a sequence of least-

squares solutions converging to the L1 estimate mL1 . A possible implementation

in the GNU Octave programming language is presented in Appendix B.1.

Therein, the weighted system G?RGm = G?Rd with R
def
= diag

(
|d ? Gm|

p?2
)

diminishes the influence of large residuals (e. g. outliers) when 1 6 p &amp;lt;2. It’s

http://www.gnu.org/software/octave/


2.2 l i n e a r r e g r e s s i o n a n d t h e l e a s t - s q ua r e s e s t i m at e 18

extremely stable for p = 1, specially if a cut-off value is used for rounding up

residuals that are close to zero.

Example 2.2 (Bullet Trajectory)

This is the classical problem of fitting a parabola to consecutive snapshots of a

bullet trajectory with the discrete linear model y(t) = m1 + m2t ?
1
2
m3t

2 for the

elevation at instant t &gt; 0. The parameters to be estimated (m1, m2, m3)? have

well-known physical meaning.

Observations are made and the linear system Gm = d built:
?
???

1 t1 ?
1
2
t21

1 t2 ?
1
2
t22

...
...

...

?
???

?
???

???

m1

m2

m3

?
???

???

=

?
???

???

y1

y2
...

?
???

???

In the presence of an outlier, the L2 regression underestimates the initial bullet

velocity and the gravitational field giving a poor trajectory prediction. The L1 is

less sensitive to the inconsistent observation, as shown in Figure 2.3.

Time [t]

E
le

va
ti

o
n

[y
(t
)]

Observed data
L1 regression
L2 regression

outlier

Figure 2.3: Bullet trajectory prediction with L1 and L2 regression.

This plot was reproduced from the already mentioned textbook, example 2.4, as

it illustrates the danger in applying pure least-squares for inversion on arbitrary,

realistic data sets. ?

Weighted systems like those for the Lp solution naturally arise when uncertainty

in the measurements is stochastically modeled. The Euclidean distance is replaced

by a scale-invariant metric that is better adapted to settings involving non spheri-

cally symmetric distributions.



2.3 t i k h o n ov r e g u l a r i z at i o n 19

Definition 2.2 (Mahalanobis distance). The Mahalanobis distance of a multivariate

vector x ? Rn from a group of values with mean µ ? Rn and covariance ? ? Rn×n

is defined as:

DM(x) =

?
(x ? µ)???1(x ? µ)

The metric in Definition 2.2 is used to formulate a weighted linear least-squares

problem with Cd the covariance matrix for the measurements:

arg min
m?M

(dobs ? Gm)
?C?1d (dobs ? Gm) (2.14)

where the best estimate6 might have closed form mM = (G?C
?1
d

G)?1G?C?1
d

dobs

(see Appendix A.5). In practice, the noise is assumed to be separate from the input

as in Example 1.3 [26, 27] and the covariance to be diagonal Cd = diag
(
?21, ?

2
2, . . . , ?

2
m

)
.

The least-squares solution is recovered for Cd = I.

To end this section, it is important to know that uncertainty in the data can be

propagated through all linear estimators m(·) = G
(·)dobs because of a basic result

from multivariate statistics.

Lemma 1. The covariance of a linear mapping y = Ax + b is Cy = ACxA? with Cx the

covariance for x [6].

The covariance for the least-squares estimate is, for instance, derived by setting

A = (G?G)?1G? in Lemma 1:

CmL2
= (G?G)?1G?CdG(G

?G)?1 (2.15)

and if the measurements are uncorrelated under the same degree of uncertainty

Cd = ?
2I, Equation 2.15 simplifies to:

CmL2
= ?2(G?G)?1 (2.16)

Remark 1. The classical framework for discrete linear inverse problems gives gen-

eralized linear estimates for the parameters and models uncertainty through direct

covariance propagation.

2.3 t i k h o n ov r e g u l a r i z at i o n

In the previous section, the general solution to discrete linear inverse problems was

reviewed and, at that time, it was already mentioned that using those estimates

6 Linear minimum-variance unbiased estimate.



2.3 t i k h o n ov r e g u l a r i z at i o n 20

could lead to erroneous predictions. This is mainly caused by ill-conditioning as

briefly explained with SVD on rank-deficient matrices, or by the presence of out-

liers as illustrated by Example 2.2.

Regularization consists of solving the trade-off between resolution and stability

of the estimate. It can be thought as the process of penalizing terms in the SVD of

G that are highly sensitive to noise—terms associated to small singular values.

Recapping Section 2.2, it was mentioned that the pseudo-inverse estimate m†

is also minimum-length, meaning it’s the solution to the following optimization

problem:

minimize ?m?L2
s.t. ?d ? Gm?L2 6 t(?)

(2.17)

where t(?) is an increasing function of ?. The constraint is usually incorporated

into the objective in a damped minimization and the resulting problem is known

in statistics as Ridge regression.

minimize (d ? Gm)?(d ? Gm)
? ?? ?

least-squares

+ ?2m?m
? ?? ?
regularizer

(2.18)

The regularizer contributes to pushing the solution towards the origin—as ?2

increases, resolution is lost—and in the case of Ridge regression is represented by

concentric circles in a 2D model space, see Figure 2.4. For any ?2 ? [0, ?), the
Ridge estimate m? lies on a point of tangency between ellipses (i. e. least-squares)

and circles; in the lower extreme lim
? 7?0

m? = mL2 .

m1

m2 least-squares
regularizer

+ mL2

?
2 ?

0

?
?

?
2

m?

Figure 2.4: Ridge estimate m? as a function of ?
2.



2.3 t i k h o n ov r e g u l a r i z at i o n 21

The effect of the regularizer on the normal equations is numerically very intu-

itive, the damped objective is rewritten as augmented norm:

minimize

?????

[
G

?I

]
m ?

[
d

0

]?????

2

L2

(2.19)

and the normal projection gives:

[
G? ?I

][G
?I

]
m =

[
G? ?I

][d
0

]
(2.20)

Equation 2.20 simplifies to
(
G?G + ?2I

)
m = G?d, it’s very clear ?2 is being

added to the diagonal of G?G to fix its condition number, in which case the esti-

mate can be safely resolved:

m? =
(

G?G + ?2I
)?1

G?d (2.21)

What is a reasonable value for ?? Once again, it’s a trade-off between resolution

and stability. If the SVD of G ? Rm×n is substituted in Equation 2.21, a damped
decomposition is produced where the damping factors (a. k. a. filter factors) are

well-determined in terms of the singular values s1, s2, . . . , smin(m,n):

fi
def
=

s2i
s2
i
+ ?2

(2.22)

The smaller they are s2i ? ?2, the higher the penalty fi ? 0, whereas bigger
values s2i ? ?2 aren’t discarded, fi ? 1. Equation 2.21 can be rewritten in terms of
the filter matrix F

def
= diag(f1, f2, . . . , fmin(m,n)):

m? =
(

G?G + ?2I
)?1

G?d

= G?d

= VFS†U?d

(2.23)

and the resolution RG,?
def
= G?G = VFV? is clearly a function of ?2, the regularizer

damping constant.

A reasonable candidate for ? can be obtained by the L-curve criterion: the log-

log cross-plot of ?m??L2 and ?d ? Gm??L2 parameterized by ? has an L-shape.
The damping constant should be selected near the corner to minimize both terms

simultaneously, see Figure 2.57.

7 In Economics, the curve is called the Pareto optimal frontier for the multi-objective minimization.



2.3 t i k h o n ov r e g u l a r i z at i o n 22

?m??L2

?d
?

G
m

?
? L

2 good candidates

high instability

high bias

Figure 2.5: L-curve criterion for choosing reasonable damping constants ?2.

Another very common technique for this selection, mostly applied to s u p e r-

v i s e d l e a r n i n g problems, is k-fold cross-validation: the original data set is ran-

domly partitioned into k equal size subsets, one of which is retained for validation,

the other k ? 1 are used for training the model. The process is repeated k times and

the final estimator is taken as the average of all.

i m p o r ta n t n o t e : Apart from the mentioned techniques—L-curve and k-fold

cross-validation—damping constant selection for Tikhonov regularization can be

made implicit within an iterative optimization approach [28].

Ridge regression (a. k. a. zeroth-order Tikhonov regularization) damps the objec-

tive with the L2-norm of the model parameters. It’s also interesting consider other

norms on different arguments. The first well-known modification is the use of a L1

regularizer, producing the LASSO:

minimize (d ? Gm)?(d ? Gm) + ?2?m?L1 (2.24)

It has many advantages over the L2 regularizer, especially when it comes to fea-

ture selection (i. e. discarding parameters). The contour lines for the L1-norm have

?-shape and because the minimum occurs at the intersection with least-squares
ellipses, it almost always happens at the diamond corners, see Figure 2.6. The cor-

ners are on the axis meaning many parameters in the solution are exactly zero,

thus the name Sparse Regularization.



2.3 t i k h o n ov r e g u l a r i z at i o n 23

m1

m
2

least-squares
regularizer

0

m?

Figure 2.6: LASSO ?-shape regularizer.

The augmented objective with L1 regularization is no longer differentiable, the

optimum is derived using the concept of subderivative from convex optimization,

directly embedded in Algorithm 2.1. A possible implementation of this shrinkage

operator [29] is presented in Appendix B.2.

In Tibshirani’s “The lasso problem and uniqueness” [30], the LASSO solution is

investigated from the KKT conditions using the term “well-defined” in favour of

well-posed.

Algorithm 2.1: Coordinate descent for sparse regularization
Input: G ? Rm×n, d ? Rm, ? ? R
Output: m? = arg min m?M?d ? Gm?

2
L2

+ ?2?m?L1
// initialize m randomly or use the Ridge estimate

m ? (G?G + ?2I)?1G?d
repeat

foreach column G?j? do update mj:

aj ? 2G?j?
?

G?j?

cj ? 2G?j?
?
(d ? Gm + mjG

?j?)

if cj &amp;lt;??
2 then mj ?

cj + ?
2

aj

else if cj &gt; ?
2 then mj ?

cj ? ?
2

aj

else mj ? 0
end

until converge

return m



2.4 l e v e n b e r g - m a rq ua r d t s o l u t i o n t o n o n l i n e a r r e g r e s s i o n 24

When combined these two regularizers form what is known in the literature as

Elastic net regularization. This is mainly done to overcome LASSO limitations for

“small m, large n” problems.

minimize (d ? Gm)?(d ? Gm) + ?1?m?L1 + ?2?m?
2
L2

(2.25)

Ridge regression and the LASSO are recovered for ?1 = 0 and ?2 = 0 respectively.

Note that their exponents differ in the objective function.

Bias towards the origin isn’t the only choice. If guesses m0 are allowed, they are

incorporated by translation ?m ? m0?L2 . Furthermore, higher-order regularizers
are produced with the introduction of finite-difference operators.

The first-order Tikhonov regularization takes the first derivative of the parame-

ters into account using the forward difference matrix D1. Similarly, second-order

Tikhonov regularization uses central differences D2 to count for second derivatives:

D1
def
=

?
????????

?1 1

?1 1

· · ·
?1 1

?1 1

?
????????

D2
def
=

?
????????

1 ?2 1

1 ?2 1

· · ·
1 ?2 1

1 ?2 1

?
????????

(2.26)

The most general regularizer here discussed ?D(·)m?L(·) serves for a variety of
purposes. For instance, Ridge regression is equivalent to D = I and L2-norm.

There are no limits to creativity. . .

minimize (d ? Gm)?C?1d (d ? Gm)
? ?? ?

weighted least-squares

+ ?1?m ? m0?L1
? ?? ?

regularized towards m0

+ ?2?D1m?2L2
? ?? ?
first derivative

(2.27)

and a quote by Alan Kay is appropriate to end this section: “The best way to predict

the future is to invent it.”

2.4 l e v e n b e r g - m a rq ua r d t s o l u t i o n t o n o n l i n e a r r e g r e s s i o n

Very often the forward operator G : M 7?? D is much more complex than a matrix
multiplication d = Gm. It might represent an entire engineering system with a

broad variety of nonlinear interactions. For such systems, linear algebra can’t be

applied directly as in the previous derivations for generalized linear estimators.



2.4 l e v e n b e r g - m a rq ua r d t s o l u t i o n t o n o n l i n e a r r e g r e s s i o n 25

One possible way to tackle a general operator d = G(m) is by iterative optimiza-

tion. First, recall the Newton-Raphson method for finding the roots of a continu-

ously differentiable nonlinear square system of equations F(m) = 0:

J(mk)
(
mk+1 ? mk

)
= ?F(mk) (2.28)

with J(m) ? ?Fi(m)
?mj

the Jacobian matrix. It can be employed to find the local min-

ima m? = arg min m?M f(m) of a twice continuously differentiable function f(m)

through the necessary condition ?f(m?) = 0:

H(mk)
(
mk+1 ? mk

)
= ??f(mk) (2.29)

with H(m) ? ?
2f(m)

?mi?mj
the Hessian. Second, note the Newton-Raphson update can-

not be performed for the system G(m) ? d = 0 even though an analytical expres-

sion may exist. This is because the inverse problem is not guaranteed to have a

solution G(m?) = d and the system to be square (i. e. m ? n).
The Levenberg-Marquardt solution to nonlinear inverse problems consist of ap-

plying Newton-Raphson to a damped least-squares objective:

f(m)
def
=

m?

i=1

(
G(m)i ? di

?i

)2
(2.30)

As with linear regression, if the observations are assumed to be Gaussian, then to

maximize the likelihood is equivalent to minimize the objective in Equation 2.308.

By introducing the notation f(m) =
?m

i=1 fi(m)
2, and the misfit vector F(m) =

(f1(m), f2(m), . . . , fm(m))?, the gradient and the Hessian needed for the Newton-

Raphson update are given by (see Appendix A.6):

?f(m) = 2J(m)?F(m) (2.31)

H(m) = 2J(m)?J(m) + Q(m) (2.32)

with J(m) ??F(m) being the Jacobian and Q(m) def= 2
?m

i=1 fi(m)?2fi(m). A good
approximation to the Hessian is obtained ignoring the Q(m) term in Equation 2.32,

in this case the update ?m = mk+1 ? mk is such that:

J(mk)?J(mk)?m = ?J(mk)?F(mk) (2.33)

Equation 2.33 is sometimes referred to as the Gauss-Newton method. The Levenberg-

Marquardt algorithm is introduced with a slight modification:
(

J(mk)?J(mk) + ?2I
)
?m = ?J(mk)?F(mk) (2.34)

8 The proof in Appendix A.4 is still valid for nonlinear operators G.



2.4 l e v e n b e r g - m a rq ua r d t s o l u t i o n t o n o n l i n e a r r e g r e s s i o n 26

where ?2 is adjusted during optimization. Larger values leads to steepest decent,

whereas steps with small penalization ?2I mimic the Gauss-Newton method. The

final effect of this penalizer is very much that of the Tikhonov regularization, but

with a different dynamic interpretation. Refer to Aster et al. for comparing the two.

A specialized derivation for linear operators d = Gm and Gaussian priors can

be found in Oliver, Reynolds, and Liu’s “Inverse Theory for Petroleum Reservoir Char-

acterization and History Matching” [24].

i m p o r ta n t n o t e : More advanced and well-established strategies with simi-

lar adaptive behavior for solving unconstrained optimization exist as part of the

Quasi-Newton family of methods (e. g. DFP, BFGS). They use a numerical approxi-

mation of the Hessian based on successive gradient evaluations.

The most critical issue with these algorithms is the need for derivatives. The nu-

merical simulator G is in general a black box either on purpose to avoid complexity

or because its source code isn’t available. Moreover, commercial software doesn’t

necessarily implement adjoint code [31, 32, 33].

m

G d

u = ??K?p

?·u = qw
?w

+
qo
?o

Figure 2.7: Numerical simulator as a black box.

Finite differences are very sensitive to the stencil and also very costly. The ap-

proach is unfeasible for demanding simulators in high-dimensions unless a robust

framework such as DAKOTA is used for computing them in parallel, or a low

fidelity proxy is iteratively fitted (e. g. Polynomial, Kriging, Particle filters).

Unlike discrete linear inverse problems—for which inversion is performed by

SVD, Cholesky factorization or conjugate gradients9—nonlinear inverse problems

are challenging due to the lack of characterization of nonlinearities in black box

9 Preferred for large sparse matrices [34].

http://dakota.sandia.gov/


2.4 l e v e n b e r g - m a rq ua r d t s o l u t i o n t o n o n l i n e a r r e g r e s s i o n 27

solvers. Introspection and dedicated analysis of the source code for G can over-

come this actual limitation and further improve performance of existent software.

Other methods such as Kalman filters developed for linear dynamical systems

perform very well in practice even in the presence of nonlinearities. These filters

that linearize about the current mean and covariance are sometimes referred to as

extended Kalman filters and detailed explanation is out of the scope of this work.

More recent and powerful variations are obtained with the use of an ensemble,

producing the EnKF [35, 36, 37] or with probabilistic collocation techniques [38, 39].



3
P R O B A B I L I S T I C F R A M E W O R K

Information can tell us everything. It has all the answers. But they are answers to

questions we have not asked, and which doubtless don’t even arise.

Jean Baudrillard

3.1 Definition of probability . . . . . . . . . . . . . . . . . . . . . . . . 29

3.2 States of information . . . . . . . . . . . . . . . . . . . . . . . . . . 36

3.3 Bayesian inversion . . . . . . . . . . . . . . . . . . . . . . . . . . . 37

3.4 Ensemble Markov chain Monte Carlo . . . . . . . . . . . . . . . . 42

Responding to the statement that one estimate alone isn’t informative, specially

on the richness of high-dimensional spaces, the probabilistic framework relies on

a subjective degree of belief to model the plausibility1 of any given preimage. This

means that each estimate m ? M is assigned a measure of consistency with previ-
ous observations and expert knowledge (e. g. probability).

The first important distinction that has to be made against the classical frame-

work is the retification2 of plausibility distributions. In this chapter, these are the

main objects of attention and carry all the information about the inverse problem

to be solved—the state of information. Two extreme states are identified, represent-

ing maximum uncertainty (i. e. flat shape) and total confidence (i. e. peak shape);

and a Bayesian rule derived from Kolmogorov axioms, suitable for distributions

that aren’t necessarily normalizable, is used to navigate from one extreme to the

other in the learning direction.

As in Chapter 2, the specificities of the forward operator G : M 7?? D and the
associated spaces are completely hidden so to highlight the core assumptions of

1 The terms plausibility, probability and belief are used interchangeably in this document.
2 In Programming Languages, retification is the process of creating first-class objects.

28



3.1 d e f i n i t i o n o f p ro b a b i l i t y 29

the theory and to make the text accessible to readers coming from different fields.

They will only be introduced in Part II of the dissertation for the general history

matching problem or within small simple examples.

At a higher abstraction level, the solution is naturally formulated as an integral

that represents the marginalization of the posterior with respect to the noisy output

measurements d ? D. It is in general more computationally demanding than the
mathematical optimization techniques previously presented.

The MCMC strategy for exact sampling of the posterior is briefly reviewed with

a “hands on” approach. Among the many variants of the algorithm, only those

with support for distributed parallel execution should be considered. This is a

strict and important requirement if the forward operator is expensive.

This chapter is heavily influenced by Tarantola’s “Inverse Problem Theory and

Methods for Model Parameter Estimation” [22].

3.1 d e f i n i t i o n o f p ro b a b i l i t y

Plausibility measures follow a multitude of similar axioms depending on the goal

of the theorist [40, 41]. Herein, the term “probability” is formally redefined and

all other names such as plausibility, belief, etc. assigned the same meaning. This

linguistic abuse is to avoid confusion with subtle conceptual differences and make

the reading more pleasant.

Definition 3.1 (Probability). For a probability space (X, F, P) with X a finite-

dimensional universe, and F the associated ?-algebra, the probability measure of

an event A ? X, denoted P(A) ? R, satisfies the Kolmogorov axioms:

• P(A?B) = P(A) + P(B) for disjoint events A?B = ?.

• There is continuity at zero, i. e. , if a non-increasing chain A1 ? A2 ? ···
tends to the empty set, then P(Ai) ? 0.

The function itself P : F 7?? R with no reference to a particular event is called the
probability distribution and is written P(·) for short.

It follows from Definition 3.1 that the empty set has zero probability P(?) = 0.
The universe X is generic notation for either M or D, but it can also represent the

Cartesian product X = M×D. In all cases, there is no guarantee of finite probability3.

3 The distribution may not be normalizable.



3.1 d e f i n i t i o n o f p ro b a b i l i t y 30

Non-normalizable distributions evaluate to probabilities that can’t be interpreted

intuitively, but are still useful as a relative measure: given two events A, B ? F, it’s
still possible to compare P(A), P(B) ? R. This is a crucial point often obfuscated
by overloaded notation.

Definition 3.2 (Density). For any probability distribution P(·) over X and fixed
coordinate system, there exists (Radon-Nikodym theorem) f(x), called probability

density such that ?A ? X, P(A) =
?

A
dx f(x).

Example 3.1 emphasizes that probability densities in Definition 3.2 aren’t nec-

essarily bounded nor intuitive. This exotic behavior will be suppressed with the

notion of p-events in the following paragraphs.

Example 3.1 (Jeffreys Parameters)

In Physics, reciprocal variables are usually defined to provide the scientist with

different arguments about the same phenomenon:

x ?? 1/x
Frequency f ?? Period T = 1/f
Resistivity ? ?? Conductivity ? = 1/?

Compressibility ? ?? Bulk modulus k = 1/?
Consider that x &gt; 0 is strictly positive like in the pairs in the above diagram.

For any two samples xa, xb ? X the absolute differences don’t match |xa ? xb| 6=??? 1xa ?
1
xb

???, and it would be incorrect to arbitrarily select one of them as the distance
between points. A good distance is invariant under change of coordinates, for

instance take the resistivity/conductivity pair:

D(?a, ?b)
def
=

????log
?a

?b

???? =
????log

?a

?b

???? = D(?a, ?b) (3.1)

Equation 3.1 is accounting for “octaves”4 instead of plain differences. In differential

form, the distance element for D(xa, xb) =
???log xaxb

??? is given by dL(x) = 1xdx and
therefore the unbounded density f(x) = 1/x assigns probabilities proportional to

the length of the event. These positive reciprocal variables are here called Jeffreys

parameters as suggested by Tarantola [42]. ?

Outside engineering and other applied fields, the notion of density is sometimes

discarded in favour of volumetric probabilities. Unlike densities that are affected by

change of coordinates x? = x?(x):

f?(x?) = f(x)

????
?x

?x?

???? (3.2)

4 In Music, an octave is the interval between a pitch and another with double its frequency.



3.1 d e f i n i t i o n o f p ro b a b i l i t y 31

volumetric measures have fixed shape independently of the coordinate system.

Assume that the notion of volume is supported by X, that is, for any event A ? X
the volume V(A) is defined. This is true for most practical inverse problems or if X

has a metric (see Example 3.1). The volume element dV(x) = v(x) dx is integrated

with volume density v(x) over the event of interest:

V(A) =

?

A

dx v(x) (3.3)

It’s then possible to represent any probability distribution P(·) over X by either
a volumetric probability F(x) or probability density f(x):

P(A) =

?

A

dV(x) F(x) =

?

A

dx f(x) (3.4)

where the convention is to use upper case letters for volumetric measures. The

relation between volumes and densities f(x) = v(x)F(x) is obtained by substituting

the volume element in Equation 3.4.

Every reasoning that follows requires a good understanding of these concepts,

information is defined in terms of volumes, but implemented in terms of densities to be in

agreement with popular scientific software.

Definition 3.3 (Homogeneous distribution). The homogeneous probability distribution

M(·) over X induced by the volume V(·) is a distribution that assigns for each event
A ? X a probability proportional to its volume V(A). The homogeneous probability
density is taken proportional to the volume density µ(x) ? v(x):

M(A) =

?

A

dx µ(x)

If the volume of the universe V =
?

X
dx v(x) is finite, the homogeneous density

is normalized µ(x)
def
= v(x)/V, otherwise it only reproduces the shape of the volume

density in Definition 3.3, reinforcing the idea that absolute probabilities need not

be propagated until the posterior is fully characterized.

The homogeneous distribution in Example 3.1 is directly obtained by looking

up the volume element dL(x) = 1
x
dx, its density is proportional to the volume

density µ(x) ? 1/x defined over X. Mnemonically, the homogeneous distribution
for a Jeffreys parameter x is represented by the density µ(x) = k/x with k ? R+ an
arbitrary positive constant, see Figure 3.1.

Lemma 2. If the expression of the volume element dV(x) = v(x) dx is known for a

space X, the probability density representing the homogeneous distribution is given by

µ(x) = k v(x), k ? R+.



3.1 d e f i n i t i o n o f p ro b a b i l i t y 32

Lemma 3. If there is a metric g in X, then the volume element is dV(x) =
?
|det g(x)|dx.

By Lemma 2, the homogeneous probability density is given by µ(x) = k
?
|det g(x)|.

xa xb

•

•

x

µ
(x
)

k = 1, 2, 3, . . . ? R+

µ(xa)

µ(xb)
=

xb
xa

Figure 3.1: Homogeneous distribution for Jeffreys parameters.

The dynamics of the system is introduced by conjunction and disjunction of states

(i. e. probability distributions). These two basic operations serve as a mechanism

to manipulate information.

Definition 3.4 (Disjunction). Given two states P1(·) and P2(·) with probability den-
sity f1 and f2, respectively, the disjunction written (P1 ? P2)(·) is represented by
density:

f1 ? f2
def
=

f1 + f2

2

The intuition behind Definition 3.4 is that of a histogram. The mountains and

valleys of the input distributions are added up producing potentially multimodal

states, see Figure 3.2.

? =

Figure 3.2: Disjunction for producing histograms.

Disjunction of states alone can be used to approximate complex unknown dis-

tributions as illustrated in Example 3.2. This operation is usually underestimated

compared to its counterpart—the conjunction.



3.1 d e f i n i t i o n o f p ro b a b i l i t y 33

Example 3.2 (Cathodic screen)

Let r and ? be the polar coordinates on a cathodic screen and consider a special

device emitting electrons onto it continuously. The exact impact location of the

particle is beyond the instrument capabilities, the best a computer can do is provide

a density fi(r, ?) for the coordinates of the i-th impact point.

+ f1(r, ?) + f2(r, ?)

+ f3(r, ?)

+ f4(r, ?)

Figure 3.3: Impact clouds on a cathodic screen.

The clouds fi(r, ?) in Figure 3.3 are combined by disjunction to approximate the

unknown emission distribution g(r, ?), much like an ordinary histogram:

g(r, ?) ? f1(r, ?) + f2(r, ?) + · · ·+ fn(r, ?)
n

This is an illustration for example 1.11 found in Tarantola [22]. ?

Definition 3.5 (Conjunction). Given two states P1(·) and P2(·) with probability den-
sity f1 and f2, respectively, and the homogeneous distribution M(·) over X with
density µ, the conjunction written (P1 ? P2)(·) is represented by density:

f1 ? f2
def
=

1

v

f1 ·f2
µ

with v
def
=

?

X
dx

f1(x)f2(x)
µ(x)

the normalization “constant”.

The expression in Definition 3.5 is insensitive to the homogeneous distribution

by design, i. e. , for any distribution P(·), the conjunction (P ? M)(·) = P(·) doesn’t
add new information. This is the rationale for defining states of information in the

next section.



3.1 d e f i n i t i o n o f p ro b a b i l i t y 34

Both binary operations can be easily extended to n arguments5:

f1 ? f2 ? · · ·? fn
def
=

f1 + f2 + · · ·+ fn
n

(3.5)

f1 ? f2 ? · · ·? fn
µ

def
=

1

v

f1

µ

f2

µ
· · · fn

µ
(3.6)

with v
def
=

?

X
dx µ(x)

f1(x)
µ(x)

f2(x)
µ(x)

· · · fn(x)
µ(x)

. However, this extra typing isn’t necessary in

any of the succeeding derivations.

Definition 3.6 (p-event). For any event B ? X it’s possible to attach a probability
distribution MB(·), called the p-event of B, through the density:

µB(x)
def
=

?
?

?

k µ(x), if x ? B

0, otherwise

with k ? R+ an arbitrary constant and µ(x) the density for the homogeneous
distribution over X.

The branched expression in Definition 3.6 copies the shape of the homogeneous

density inside a particular event (i. e. domain), see Figure 3.4. When evaluated

at an event A ? X, it’s proportional to the volume of the intersection A ? B. In
mathematical notation this is written MB(A) ? V(A?B).

0.5 1 1.5 2 2.5 3 3.5 4 4.5 5

1

2

3

4

0.5 1 1.5 2 2.5 3 3.5 4 4.5 5

0.2

0.4

0.6

0.8

µ(x) = 1/x µB(x), B = [1, 3]

Figure 3.4: The p-event of the interval [1, 3] for a Jeffreys parameter x &gt; 0.

Remark 2. Depending on the choice of the event B ? X, the associated p-event
MB(·) is a tool for normalizing distributions within a subdomain of interest.

Conditional probabilities are defined in terms of p-events as a special case of

conjunction. The conditioning isn’t done with events, but probability distributions

directly. In practical words, information is combined in a fuzzy fashion.

Definition 3.7 (Conditional probability). Given P(·) a probability distribution over
X and an arbitrary event B ? X with associated p-event MB(·), the conditional
probability of P(·) given B is the conjunction (P ? MB)(·).

5 The textbook has an error, see list of errata [43].



3.1 d e f i n i t i o n o f p ro b a b i l i t y 35

It can be shown that for any event A ? X the expression in Definition 3.7 is
proportional to the volume of the intersection (see Appendix A.7):

(P ? MB)(A) =
P(A?B)
P(B)

(3.7)

and the left-hand side of Equation 3.7 is identified with the usual notation P(A | B).

Moreover, by writing P(A ? B) = P(A | B) P(B) = P(B | A) P(A), the Bayes rule
relating conditionals is obtained:

P(A | B) =
P(B | A) P(A)

P(B)
(3.8)

i m p o r ta n t n o t e : The Bayes theorem—probability of the causes—showed in

Equation 3.8 is not used in any way by the framework. The update to be defined

in Section 3.3 is performed using simple conjunction of states, and the naming

Bayesian inversion remounts only to the fact that probabilities are interpreted as

“degrees of belief”.

Conditioning probability densities is a more complicated topic and some as-

sumptions are needed for the standard formulas to be safely applied. Consider

the joint density f(m, d) with (m, d) ? M×D and an application m 7? d(m) from
M into D. The general idea is to retain only the information of f(m, d) for which

d = d(m), and forget all values for which d 6= d(m), see Figure 3.5. This is done by
taking the “orthogonal” limit towards d = d(m) since it’s invariant under change

of coordinates [44]. In practical terms, the conditioning of probability densities is

safe if the forward operator G : M 7?? D isn’t highly nonlinear and the coordinates
close to that of a Cartesian system.

-4

-2

 0

 2

 4

-4 -2  0  2  4

d
=

d
(m

)

?
?

?
?

?
?

m

d

f(m, d)

orthogonal

limit

Figure 3.5: Conditioning the joint density f(m, d) towards the curve d = d(m).



3.2 s tat e s o f i n f o r m at i o n 36

3.2 s tat e s o f i n f o r m at i o n

Probability distributions are interpreted as states of information to be combined

by means of conjunction/disjunction with the aim of improving the degree of un-

derstanding about a given system. It’s appropriate to formally define two extreme

opposite states for future reference and illustration purposes.

The first state of information is that of nescience or total ignorance. In Bayesian

jargon it would be referred to as noninformative6 and is considered for that school

the very last option during modeling of random variables. It’s arguably the last

option since priors better be consistent with the current conception of the system

which is rarely void.

Definition 3.8 (Nescience). The state of nescience against the space X is represented

by the homogeneous distribution in Definition 3.3. The notation µ(x) for the prob-

ability density is preserved unless a more verbose form like µX(x) is needed for

disambiguation.

It’s important to highlight once more, nescience as homogeneous distribution

doesn’t imply flat shape for the probability density µ(x). In fact, its shape might

radically change after a change of coordinates x? = x?(x). Scientists that are not

familiar with this notion should be careful with their intuition.

The second extreme state to be defined invokes divinity through the capacity of

full comprehension of reality or omniscience. It is the choice for deterministic variables

or if uncertainties in the system are to be systematically neglected. Like showed in

the textbook [22], the solution is here derived for the most general inverse problem,

and only in Part II that uncertainties will possibly be neglected for producing

feasible case studies.

Definition 3.9 (Omniscience). The state of omniscience against the space X at point

x0 ? X is represented by the Dirac delta function ?(x; x0).

The Dirac delta function ?(x; x0) is illustrated in Figure 3.6. It’s such that all the

probability mass is concentrated at point x0 ? X. For linear spaces, the notation
?(x ? x0) is commonly used, but since it isn’t particularly advantageous, there is

no need for distinction in this document.

It was mentioned in the beginning of this chapter that states of information

are first class objects. The general picture to retain in mind when solving inverse

6 Some statisticians disagree with the nomenclature arguing it’s as informative as any other state.



3.3 b ay e s i a n i n v e r s i o n 37

?

x0

Figure 3.6: Dirac delta function ?(x; x0).

problems is one in which probability distributions are morphing towards a more

peaked shape, see Figure 3.7. It is noteworthy that because most inverse problems

are ill-posed, there may be various peaks instead of a unique mass concentration

point. Formally, it is said that states of information converge in distribution.

µ(x)
D?? ?(x; x0)

Figure 3.7: Nescience towards Omniscience.

The rules for navigating in the learning direction—for which uncertainty is con-

sistently reduced—are presented in the next section, and even though multimodal

distributions are the most common answer, they progressively present small vari-

ance near the peaks as new evidence is incorporated.

Remark 3. The classical framework seeks the best match to the data, whereas the

probabilistic framework is only concerned with uncertainty mitigation.

3.3 b ay e s i a n i n v e r s i o n

Bayesian inversion translates into uncertainty modeling and mitigation (i. e. setting

priors and incorporating data) in that order. In this section, all sources of uncer-

tainty regarding the general inverse problem G : M 7?? D are identified using a
consistent and mnemonic notation; and only after that, information is combined.

Uncertainties are either experimental or theoretical. Inversion is precisely the conjunc-

tion of these two kinds together with posterior marginalization, as will be shown

in the following paragraphs. Moreover, a multi-stage update can be performed in

an outer loop using disjunction of states if the time interval at which (reliable) data

is observed is considerably greater than the time spent with marginalization.



3.3 b ay e s i a n i n v e r s i o n 38

3.3.1 Uncertainty in the model space

The first prior that can be extremely hard to set is the initial belief about the real

model mtrue ? M. This state of information should be supported by experts in
the field and extensively proven to be plausible according to all available evidence.

This a priori state is represented by probability density ?M(m) which can be equal

to nescience µM(m) in absence of good characterization.

Example 3.3 (Gaussian prior)

Consider models m ? Rn, the Gaussian centered at mprior = (m1, m2, . . . , mn)?

with covariance Cm has the form:

?M(m) = ((2?)
n det Cm)

?1/2 exp
(
?
1

2
(m ? mprior)

?C?1m (m ? mprior)

)

and is illustrated for the 2D case in Figure 3.8.

?M(m2) ?M(m1)

m1 m2

?
M
(
m

1
,m

2
)

Figure 3.8: 2D Gaussian N
(
mprior, Cm

)
.

From the modeler’s perspective, the guess that best supports the available data

before inversion is the center mprior of the Gaussian, it’s subject to precision C?1m .

?

3.3.2 Uncertainty in the data space

Similarly to the model space, the prior state for the observed data dobs ? D is
denoted ?D(d). It is hardly the nescience state or considered free of uncertainty—

omniscience ?(d; dobs). It can take the shape of a multivariate Gaussian N (dobs, Cd)

as in Example 3.3 or any other distribution.



3.3 b ay e s i a n i n v e r s i o n 39

3.3.3 Uncertainty in the forward operator

If the theory behind the forward operator G : M 7?? D was very solid, free of
rounding errors and consistent for any conceivable input m ? M, the graph of
G itself would be sufficient for modeling uncertainty (or the lack of it). However,

in general, the numerical simulator is only trusted up to a certain degree, and

defining a conditional probability ?(d | m) on the output given the input is more

satisfactory approach, see Figure 3.9.

m

d

?(d | m)

G

Figure 3.9: Graph of G replaced by the conditional distribution ?(d | m).

3.3.4 Combining information

The joint state ?(m, d) over M×D accounts for all experimental uncertainty in the
inverse problem. By design, ?M(m) assembled from static data and ?D(d) guessed

from initial evidence are independent, and thus, it follows ?(m, d) = ?M(m) ?D(d).

The joint state ?(m, d) over M×D accounts for all theoretical uncertainty derived
by conditioning the forward operator to its input ?(m, d) = ?(d | m) µM(m).

Example 3.4 (Perfect Forwarding)

If the scientist neglects uncertainty in the forward operator G on purpose, the

distribution ?(m, d) = const. ?(d; G(m)) concentrates all the mass in the curve

d = G(m). Both marginals ?M(m) =
?

D
dd ?(m, d) and ?D(d) =

?

M
dm ?(m, d)

are constant and don’t carry rich information about neither M nor D separately.

This assumption is designated here perfect forwarding7. ?

7 The name has no relation with the same term in the context of programming languages (e. g. C++).



3.3 b ay e s i a n i n v e r s i o n 40

The conjunction of both theoretical and experimental states is taken with µ(m, d)

the homogeneous density over M×D and k some normalization constant:

?(m, d)
def
= k

?(m, d) ?(m, d)
µ(m, d)

(3.9)

By the same reason ?(m, d) is decoupled as a product of states, it is true that

µ(m, d) = µM(m) µD(d). The desired solution to the general inverse problem is

the result of marginalization:

?M(m) =

?

D

dd ?(m, d) (3.10)

Equation 3.10 defines the posterior density over the model space M. It has all

the information before and during the application of G : M 7?? D, and as Bayesian
inversion is a fixed-point iteration, ?M(m) is the next prior to be used in the right-

hand side of Equation 3.9. Furthermore, it can be rewritten as prior times likeli-

hood (remember Equation 1.8):

?M(m) = k ?M(m)

?

D

dd
?(d | m) ?D(d)

µD(d)
(3.11)

Considering that the forward operator is a fully-featured engineering simulation,

the likelihood in Equation 3.11 is a very computationally expensive integral over

the data space D. There are two relevant scenarios to emphasize at this point de-

pending on how negligible is the theoretical uncertainty for the particular inverse

problem being studied.

p e r f e c t f o r wa r d i n g The more tractable inversion where perfect forward-

ing, defined in Example 3.4, is assumed to hold true. The conditional distribution

over the linear space D is concentrated ?(d | m) = ?(d ? G(m)) with homogeneous

distribution µD(d) = const. The forward operator G is evaluated only once in the

likelihood:

L(m) =

?

D

dd
?(d ? G(m)) ?D(d)

µD(d)
= const. ?D(G(m)) (3.12)

g e n e r a l c a s e Perfect forwarding doesn’t hold. The forward operator is a

black box and no analytical antiderivative is available. Thus, the integral L(m) is

approximated using quadrature rules or Monte Carlo integration. For example:

L(m) ? 1
N

N?

i=1

?(di | m)

µD(di)
, di ? ?D(d) (3.13)



3.3 b ay e s i a n i n v e r s i o n 41

The g e n e r a l c a s e with a Cartesian system is illustrated in Figure 3.9. The

forward operator is still evaluated once as the mean of the conditional ?(d | m) for

a given m ? M, this is made clear by a Gaussian distribution with mean G(m) and
theoretical precision C?1t :

?(d | m) = ((2?)m det Ct)
?1/2 exp

(
?
1

2
(d ? G(m))?C?1t (d ? G(m))

)
(3.14)

i m p o r ta n t n o t e : The prior ?D(d) might also be Gaussian with probability

density N(dobs, Cd), however it does not involve any call to the forward operator G.

According to this exposition, the probabilistic framework seeks the posterior

distribution ?M(m) ? ?M(m) L(m) as the solution to the general inverse problem
introduced in Chapter 1. After data d ? D is observed and compared to the output
of the theoretical transfer function G : M 7?? D, the posterior gives a rich descrip-
tion on the model parameters m ? M in terms of probabilities. Practical questions
can be answered with such description:

What is the most probable model after observation is made? It’s known as the MAP

estimate, or the sample with greater probability:

mMAP = arg max
m?M

?M(m)

µM(m)
(3.15)

What is the probability an event E has happened? The posterior integrated over all

samples m ? E:

Pr(E) =
?

E

dm ?M(m) (3.16)

What is the expected value for the model space? By definition, it’s an integral or

weighted summation over all samples m ? M:

E[m] =

?

M

dm m ?M(m) (3.17)

Although far more general than the classical framework, this theory faces one

major challenge—computational efficiency. Except for specific problems like those

with Gaussian priors and linear mapping where ?M(m) is analytically obtained

(refer to Tarantola), the full characterization of the posterior can be very hard to

tackle. In high dimensions, the likelihood needs to be evaluated multiple times in

a Markov chain, making the implementation yet more costly.

In the g e n e r a l c a s e, the likelihood L(m) is the integral over D shown in

Equation 3.11. Whenever dim D ? dim M and the prior ?D(d) is addressed by



3.4 e n s e m b l e m a r k ov c h a i n m o n t e c a r l o 42

what is called the Wiener-Askey scheme8, L(m) can be accurately approximated

using advanced sparse grid integration techniques [39, 45, 46, 47, 48]. Otherwise,

either dim D is large or ?D(d) is not handled by the polynomial chaos expansion,

retaining the correctness of transitions in this Markov process can be very difficult.

Ignoring performance issues for a moment, the posterior does always exist and

is uniquely defined. It’s common to say the probabilistic framework enjoys implicit

regularization rather than explicit, see Section 2.3. If ?M(m) ? 0 is identically null,
it means uncertainties were likely underestimated or that the conjunction of states

was unable to deal with information inconsistency.

3.4 e n s e m b l e m a r k ov c h a i n m o n t e c a r l o

The ultimate role of MCMC in this chapter is to sample complex posterior dis-

tributions in high-dimensional spaces assuming candidates can be easily drawn

from the correspondent prior or proposal distribution. In traditional Monte Carlo,

space exploration is quite ineffective in capturing the distribution in its entirety,

and some of the probability modes might never be visited or be poorly sampled

due to the “totally random” strategy of the sampler (e. g. direct sampling, rejection

sampling, importance sampling).

Random exploration in high dimensions can sometimes be improved9 by simply

recording the very last candidate drawn, the Markov chain assumption states in

simple words future is independent of the past given the present and leads to one

of the most influential family of algorithms developed in this century (e. g. Gibbs

sampling, Metropolis-Hastings, etc.). For the sake of clarity, the definitions are here

limited to discrete time stochastic processes with discrete state space. The reader is

pointed to Gamerman and Lopes’s “Markov Chain Monte Carlo: Stochastic Simulation

for Bayesian Inference” [49] for a more general presentation of the subject.

Following the notation in that textbook, a discrete time stochastic process is a

family of random quantities {?(t) : t ? T} over a state space S with countable index
set T. Without loss of generality, indices are taken from the set of natural numbers

N and generally represent iterations of a simulation scheme. The states ?(t) ? S
are usually vectors in Rn, but can also be matrices or any other multidimensional

abstraction. Special interest is paid to the dynamic and limiting behavior of the

8 Most usual parametric distributions: uniform, triangular, gamma, etc.
9 In the sense of visiting and sampling high probability regions.



3.4 e n s e m b l e m a r k ov c h a i n m o n t e c a r l o 43

sequence (?(1), ?(2), ?(3), . . .) as this is what the computer will be sampling for

long runs.

Definition 3.10 (Markov chain). The discrete time stochastic process {?(n) : n ? N}
over S is a Markov chain if it satisfies

Pr
(
?(n+1) ? A | ?(n) = x, ?(n?1) ? An?1, . . . , ?(0) ? A0

)

= Pr
(
?(n+1) ? A | ?(n) = x

)

for all A0, . . . , An?1, A ? S and x ? S.

If the state space S is further assumed to be discrete, Definition 3.10 is usually

rewritten on a point basis x0, . . . , xn?1, x, y ? S:

Pr
(
?(n+1) = y | ?(n) = x, ?(n?1) = xn?1, . . . , ?(0) = x0

)

= Pr
(
?(n+1) = y | ?(n) = x

) (3.18)

The notion of state transition x ? y is evident in Equation 3.18. If it doesn’t
depend on the iteration counter n = 0, 1, 2, . . ., the chain is said to be homogeneous.

Definition 3.11 (Transitional probability). For a homogeneous Markov chain, the

transitional probability P(·, ·) is a function such that:

• ?x ? S, P(x, ·) is a probability distribution over S

• ?A ? S the function x 7? P(x, A) can be evaluated

P(·, ·) is also named the transition or kernel function. If S = {x1, x2, . . . , xr} is
finite, the transition matrix defined by Pij

def
= P(xi, xj) summarizes the behavior of

the chain in jumping between states xi ? xj:

P =

?
???

P(x1, x1) · · · P(x1, xr)
...

. . .
...

P(xr, x1) · · · P(xr, xr)

?
??? (3.19)

and since Definition 3.11 was established here for homogeneous chains, the matrix

P is fixed during simulation. Its rows add up to unit
?

j Pij = 1, or equivalently,

1
def
= (1, 1, . . . , 1)? is an eigenvector of P with eigenvalue 1.

Probabilities are assigned to each point10 x1, x2, . . . , xr ? S per iteration, the
initial distribution ?(0) =

(
?(0)(x1), ?(0)(x2), . . . , ?(0)(xr)

)
is multiplied on the

right by the transition matrix P to get the next state ?(1) = ?(0)P and the process

is repeated ?(n) = ?(0)Pn towards an equilibrium distribution that is guaranteed to

exist if the chain is irreducible and aperiodic [50].

10 In history matching, each point is a property map such as a permeability field.



3.4 e n s e m b l e m a r k ov c h a i n m o n t e c a r l o 44

Example 3.5 (Finite State Machine)

Consider a non-deterministic 3-state machine with transitional probabilities illus-

trated in Figure 3.10. This automata is such that at any given time n = 1, 2, . . .

there is a chance ?(n) =
(
?(n)(x1), ?(n)(x2), ?(n)(x3)

)
of being somewhere in

{x1, x2, x3}.

x1

x2

x3

1.0 0.9

0.6

0.4

0.1

Figure 3.10: Graph for a 3-state {x1, x2, x3} machine.

The transition matrix P =
[

0 1 0
0 0.1 0.9
0.6 0.4 0

]
for the graph admits an equilibrium dis-

tribution ?(?) = ( 27
122

, 50
122

, 45
122

) no matter the initial guess ?(0). Hence, the graph

is connected (i. e. irreducible) and aperiodic. The ternary plot in Figure 3.11 illus-

trates the limiting behavior of the chain limn?? ?(n) starting at ?(0) = (1, 0, 0).

0

0.2

0.4

0.6

0.8

1

0 0.2 0.4 0.6 0.8 1
0

0.2

0.4

0.6

0.8

1

?(n)(x1)?
(n)(x2)

?(n)(x3)

Figure 3.11: limn?? ?(n) starting at ?(0) = (1, 0, 0).

In Linear Dynamical Systems, this convergence is fully understood. The station-

ary or invariant distribution ?(?) is the normalized non-trivial solution to ?P = ?

or equivalent homogeneous system ?(P ? I) = 0. ?



3.4 e n s e m b l e m a r k ov c h a i n m o n t e c a r l o 45

Example 3.5 has its value in clarifying notation and helps introducing some

important concepts in MCMC simulation. First, the hitting time for a particular

event A ? S defined as TA
def
= min{n &gt; 0 : ?(n) ? A} is not easy to predict in

general. Most available implementations of MCMC allow for a burn-in period which

means to throw away the initial steps of the chain afterwards avoiding samples

from distributions other than that of equilibrium. This policy is criticized by some

researchers in the field who prefer to put effort designing representative priors and

never resort to burn-in, anyways it is still useful feature to have in software. Second,

the limiting or equilibrium distribution ?(?) is what the chain aims to mimic.

After various iterations (see Figure 3.11), this target distribution is sampled almost

perfectly in a convergence rate guided by the transitional probability P(x, y) in

Equation 3.20 for discrete state spaces or transition kernel p(x, y) in Equation 3.21

for continuous state spaces.

?(n+1)(y) =
?

x?S
P(x, y)?(n)(x) (3.20)

?(n+1)(y) =

?

S

p(x, y) ?(n)(x) dx (3.21)

Irreducibility and aperiodicity, and therefore uniqueness of the stationary distri-

bution ?, are not always possible to assert in practice. A sufficient condition for the

existence of ? = ?P known as detailed balance is often used to design transitions.

Definition 3.12 (Reversible chain). A Markov chain is said to be reversible if its

transitional probability satisfies detailed balance:

?x, y ? S, ?(x)P(x, y) = ?(y)P(y, x)
Intuitively, Definition 3.12 is saying that the rate at which the system moves from

x to y when in equilibrium, ?(y)P(x, y), is the same as the rate at which it moves

from y to x, ?(x)P(y, x). Such reversible chains can be built using Hastings idea of

acceptance probability [51].

The transition kernel p(x, y) is factored into p(x, y) = q(x, y)?(x, y), x 6= y,
that is, a proposal q(x, y) times an acceptance ?(x, y)

def
= min

{

1, ?(y)q(y,x)
?(x)q(x,y)

}

. The

proposed move x ? y is quite general as no strong requirements are imposed on
q(x, y), it’s accepted or not depending on the probability ratio with the current

position, thus a Markov chain.

Remark 4. The proposal q(x, y) is “any” (often random) procedure to generate y

given the current position is x (e. g. y ? N(x, ?2)).



3.4 e n s e m b l e m a r k ov c h a i n m o n t e c a r l o 46

Algorithm 3.1: Metropolis-Hastings
Input: posterior ?(·), proposal q(·,·), N ? N
Output: samples xn ? ?, n = 1, 2, . . . , N

// initialize the chain intelligently

x0 ? xguess
foreach n = 0, 1, 2, . . . , N ? 1 do sample xn+1:

draw u ? U[0,1]
draw x? ? q(xn, x?)

?(xn, x?) ? min
{

1,
?(x?)q(x?, xn)
?(xn)q(xn, x?)

}

if u &amp;lt;?(xn, x?) then xn+1 ? x?

else xn+1 ? xn
end

return x1, x2, . . . , xN

Algorithm 3.1 is one of the most popular implementations of the last decade

in statistical-related fields (e. g. Data Analysis, Machine Learning). The posterior

density ?(·) is only evaluated in a ratio ?(x?)
?(xn)

which has the advantage that no full

characterization (i. e. normalizing constants) is needed to generate samples.

The Metropolis-Hastings algorithm solves the difficult problem of sampling a

complex distribution represented by ?(·) with the introduction of an auxiliary pro-
posal q(·, ·). In the algorithm, acceptance probabilities can be rewritten:

?(xn, x
?) = min

{

1,
?(x?)/q(xn, x?)
?(xn)/q(x

?, xn)

}

(3.22)

so to notice the ratio gets close to unit when the proposal is a good approximation

of the target q ? ?. In Example 3.6, different proposals are employed to show the
impact on MCMC sampling.

Example 3.6 (Gaussian mixture)

Consider the target distribution is proportional to the conjunction of two Gaussian

modes ?(x) ? 0.3e?x2 + 0.7e?(x?10)2 and that the move x ? y is also Gaussian
centered at the current position of the chain q(x, y) = 1

?
?
2?

e
? 1

2
(y?x)2

?2 . For such

cases where the proposal is symmetric q(x, y) = q(y, x), the acceptance takes the

simpler form ?(x, y) = min
{

1, ?(y)
?(x)

}

as originally introduced by Metropolis in a

chemical physics problem [52]. A possible implementation in the R programming

language is found in Appendix B.3, it should not be compared to/preferred over

widely tested packages available on CRAN.

http://www.r-project.org/
http://cran.r-project.org/


3.4 e n s e m b l e m a r k ov c h a i n m o n t e c a r l o 47

The standard deviation ? in the proposal q(x, y) is the tuning parameter used

to produce different histograms in Figure 3.12. In all three settings, the chain is

initialized in the left mode x = 0 and run for N = 5000 iterations.

0.0

0.2

0.4

0.6

?5 0 5 10 15
x

?(
x)

? = 1

0.0

0.2

0.4

0.6

?5 0 5 10 15
x

?(
x)

? = 10

0.0

0.2

0.4

0.6

?5 0 5 10 15
x

?(
x)

? = 100

Figure 3.12: Metropolis sampling with different proposals ? = 1, 10, 100.

For small perturbations ? = 1, the chain is unable to get out of the left mode

and the bimodal characteristic of the target is totally lost. For a wide proposal ? =

100, both modes are captured, but the rejection is too high (? 98%). Satisfactory
histograms are produced in between these extremes with acceptances as good as

50% and honoring the Gaussian mixture model. Since the target distribution isn’t

expensive, automatic tuning can be easily achieved with Bayesian optimization.

?

The most natural-to-ask challenging-to-answer question regarding MCMC is

when to stop the chain? Convergence diagnostics exist, but are sometimes misleading

and require careful inspection to avoid fallacy. Widely used visual diagnostics are

that of trace and autocorrelation plots. The former is the history of samples while the

later measures the dependence across successive MCMC iterations as separated by

lag distances. Both illustrated in Figure 3.13 for the three chains in Example 3.6.

Definition 3.13 (Autocorrelation). The autocorrelation of a chain {?(t) : t ? T} for
a lag distance k ? N is the correlation coefficient between ?(t) and ?(t+k):

?(k)
def
=

cov
(
?(t), ?(t+k)

)

?t ?t+k



3.4 e n s e m b l e m a r k ov c h a i n m o n t e c a r l o 48

0 1000 2000 3000 4000 5000

?
3

?
2

?
1

0
1

2

? = 1

Iterations

x

0 5 10 15 20 25 30 35

?
1

.0
?

0
.5

0
.0

0
.5

1
.0

? = 1

Lag

A
u

to
co

rr
e

la
tio

n

0 1000 2000 3000 4000 5000

?
2

0
2

4
6

8
1

0
1

2

? = 10

Iterations

x

0 5 10 15 20 25 30 35

?
1

.0
?

0
.5

0
.0

0
.5

1
.0

? = 10

Lag

A
u

to
co

rr
e

la
tio

n

0 1000 2000 3000 4000 5000

?
2

0
2

4
6

8
1

0
1

2

? = 100

Iterations

x

0 5 10 15 20 25 30 35

?
1

.0
?

0
.5

0
.0

0
.5

1
.0

? = 100

Lag

A
u

to
co

rr
e

la
tio

n

Figure 3.13: Trace and autocorrelation for the chains in Example 3.6.

The samples for ? = 100 are highly correlated even for large lags as indicated

by the plots. This chain is therefore inferior in quality compared to the other two.

However, autocorrelation alone doesn’t tell much about the mixing of the chain (i. e.

“closeness” to steady state) as the trace for ? = 1 is trapped in the left mode.



3.4 e n s e m b l e m a r k ov c h a i n m o n t e c a r l o 49

More recently, sophisticated visualization techniques were introduced for diag-

nosing MCMC convergence11 in high-dimensional spaces [53]. They complement

the commonly used potential scale reduction factor diagnostic developed by Gelman

and Rubin [54] and as such, also rely on the assessment of multiple chains. Vari-

ous other visual and nonvisual diagnostics exist in the literature [55], all with some

imperfection. Nonetheless, they provide great insight about the process dynamics.

The idea of multiple chains in MCMC is inspired by general optimization of

multimodal objectives. Different starting points are fed in iterative procedures that

are only guaranteed to find local minima. These locals are then considered together

to reveal the global minimum, or a very good candidate for it. In general Bayesian

inference, multiple chains are started from an overdispersed distribution that is an

initial approximation of the target, they might overlap in the long run or capture

different modes of the stationary distribution if lucky.

In inverse problem theory, the target distribution to be sampled with MCMC is

the posterior ?M(m) over the model space M. As already explained in Section 3.3.4,

each sample involves a call to the forward operator G : M 7?? D on a point m? ? M
that now can be understood as a proposal m? ? ?M(m).

Example 3.7 (Root-finding)

Consider the forward operator f : R 7?? R+
0

is a parabola f(x) = x2 that is being

repeatedly evaluated on a point x? ? R. The point is unknown, but the output is
very poorly monitored—modeled by a Gaussian centered at y = 4. Probabilistic

inversion for x consists in the following settings:

• x ? U[?10,10] (a prior state)

• y = x2 (a forward operator)

• y ? N(4, 1) (observations)

The solution in Figure 3.14 was implemented in the Python programming lan-

guage using the development version of the PyMC12 module which contains the

function stochastic_from_data() contributed by myself. The code in Appendix B.4

is an infinite loop where, at each iteration, y is observed and state of information

about x is updated.

11 Or better, the lack of it.
12 PyMC version 2.

http://www.python.org/
https://github.com/pymc-devs/pymc


3.4 e n s e m b l e m a r k ov c h a i n m o n t e c a r l o 50

It can be seen the analytical roots to x2 = 4 are recovered in the histogram as

modes ±2 even for noisy output measurements. More generally, given a function
x 7? f(x) and a target y?, the solution space f(x) = y? can be characterized by a
probability distribution.

Figure 3.14: Finding the roots of x2 = 4.

Even though f(x) = x2 is not invertible, all the roots were recovered. In the clas-

sical framework, only one of {?2, 2} would be found depending on the algorithm

(e. g. Bisection, Newton, Secant) and its starting point.

The reader is encouraged to experiment with the code, in particular try differ-

ent priors (e. g. rnormal(0,1,10000)) to see the learning process in action. For

convenience, the function stochastic_from_data() was copied from the PyMC

repository to Appendix B.5. Although root-finding was previously formulated as

a forward problem (see Example 2.1), it is solved here, on a different perspective,

by inverse problem theory. ?

In Example 3.7, arbitrary histograms (a. k. a. non-parametric distributions) are

fitted with kernel density estimation [56, 57], a linear combination of symmetric

basis functions Kh centered at each sample xi ? R:

f?h(x) =
1

n

n?

i=1

Kh(x ? xi) (3.23)

with h the kernel bandwidth. It can be thought as a convolution (see Appendix A.8)

and generalized to multiple dimensions as depicted in Figure 3.15.



3.4 e n s e m b l e m a r k ov c h a i n m o n t e c a r l o 51

Figure 3.15: Kernel density estimation for a 2D Gaussian.

All single-chain samplers in the previous examples share a serious limitation in

that they cannot be parallelized. The likelihood function needs to be evaluated in

sequence as, for any Markov process, the future depends on the present. It was

this simple claim that caused researchers to develop multi-chain samplers for taking

advantage of multicore processors and computer clusters.

Consider a Markov chain of ensembles {?(t) : t ? T}, that is, each point ?(t) ? Snw

is a collection of nw points (or walkers) in S, written ?(t) =
{

?
(t)
1

, ?(t)
2

, . . . , ?(t)nw
}

.

At first, it might seem a bad idea replace the sampling in S by the one in Snw ,

specially if S is high-dimensional (e. g. permeability maps). However, the ensemble

carries rich information about the target distribution at each iteration and more

importantly, some of the walkers can advance in parallel without violating detailed

balance. This is the rationale behind Markov chain ensemble samplers proposed here

for the history matching problem.

In such samplers, the jumps for a given walker are dictated by the ensemble of

the corresponding iteration. In other words, the proposal distribution takes into

consideration all the chains being run in parallel to decide where to move next.

Moves ?(t)
k
? ?(t+1)

k
are proposed for each chain ?(t)

k
? ?(t) based on what is

called the complementary ensemble:

?
(t)

[k]

def
=

{

?
(t+1)
1

, . . . , ?(t+1)
k?1

, ?(t)
k+1

, . . . , ?(t)nw
}

(3.24)

which can be represented by a matrix with dim S rows and nw ? 1 columns. The

notation indicates all chains up to k ? 1 were moved before iteration k and that

these new positions are used to update ?(t)
k

, similar to Gibbs sampling [49].



3.4 e n s e m b l e m a r k ov c h a i n m o n t e c a r l o 52

For problems where no physical interpretation is available to design reasonable

proposals, the stretch move works incredibly well [58]. It was designed to be affine

invariant meaning its performance is unaffected by highly anisotropic distributions

(e. g. highly correlated variables) as the one illustrated in Figure 3.16.

x1

x
2

Figure 3.16: Anisotropic joint distribution for x1 and x2.

The stretch is made along a line connecting two distinct walkers in the ensemble

using a scaling random variable Z with density g(z) such that g(1
z
) = z g(z). This

constraint ensures the proposal is symmetric [59]. Researchers in the field suggest,

for instance:

g(z) ?

?
?

?

1?
z

, if z ?
[
1
a

, a
]

0, otherwise
(3.25)

with a &gt; 1 a tuning parameter. Algorithm 3.2 gives a better description of the

move happening in Figure 3.17.

?

?
(t)
k

??

Figure 3.17: Stretch move ?(t)
k
? ?(t+1)

k
for the ensemble sampler.



3.4 e n s e m b l e m a r k ov c h a i n m o n t e c a r l o 53

Algorithm 3.2: Stretch move in Rn

Input: posterior ?(·), ensemble ?(t) =
{

?
(t)
1

, ?(t)
2

, . . . , ?(t)nw
}

Output: updated ensemble ?(t+1)

foreach k = 1, 2, . . . , nw do move ?
(t)
k
? ?(t+1)

k
:

draw u ? U[0,1]
choose ? ? ?(t)

[k]
at random

propose ?? = ? + Z(?(t)
k

? ?)

?(?
(t)
k

, ??) ? min
{

1, Zn?1
?(??)

?(?
(t)
k

)

}

if u &amp;lt;?(?
(t)
k

, ??) then ?(t+1)
k

? ??

else ?
(t+1)
k

? ?(t)
k

end

return ?(t+1)

The complementary ensemble is represented by gray dots, one of which is cho-

sen at random. It’s denoted ? in the sketch and determines the direction ?(t)
k

? ?

for the line search with random step size Z. Similar picture and a more detailed

explanation of the algorithm can be found in the literature, as well as convergence

tests showing the superior performance of this MCMC method [58].

It’s tempting to parallelize the loop k = 1, 2, . . . , nw by simultaneously advanc-

ing each walker in the ensemble. This practice violates detailed balance and there-

fore the correctness of the sampling for the stretch move. To overcome such barrier,

a partition trick was proposed that presents an ideal speed-up of nw
2

[60].

The ensemble is partitioned into two halves ?(t) = ?l ??r, all walkers in the
left half ?l are updated in parallel based on walkers in the right half only, then

the update is performed for the other half ?r the same way using the just updated

ensemble ?l. These consecutive updates ?l ? ?r and ?r ? ?l each involve nw2
walkers, hence the speed-up in evaluating the likelihood.

The parallel stretch move outperforms the classical Metropolis-Hastings algo-

rithm considerably, either with respect to autocorrelation or acceptance fraction

of the moves. The tuning with a &gt; 1 in Equation 3.25 is analogous to the usual

tuning of the covariance matrix in a Gaussian proposal. If the acceptance fraction

is too low, it can be raised by decreasing a; if it’s too high, the parameter better be

increased. In the paper, the authors suggest to fix a = 2 inspired by the excellent

results with various target distributions modeled in Astrophysics [60].

Beyond trial and error with the parameter a, the parallel-tempered version of

the sampler proves to be very effective strategy in the presence of many discon-



3.4 e n s e m b l e m a r k ov c h a i n m o n t e c a r l o 54

nected modes [61]. Such target distributions are difficult to sample from because

in general likely zones are separated by extremely low probability regions.

A parallel-tempered ensemble sampler uses the same physical analogy as does sim-

ulated annealing [62] but with multiple replicas running at different temperatures

in parallel. The mixing is improved by exchanging “energy” between hot and

cold configurations. The higher is the temperature, the better is the exploration,

whereas lower temperatures are tied to local investigation of the state space. Paral-

lel tempering can tremendously improve the performance of the sampler for multi-

modal distributions [63]; temperature scheduling and other important features of

the method [64] aren’t discussed here to avoid deviating too much from inverse

problem theory.

As a matter of fact, the most successful methods for history matching today (e. g.

EnKF, RML) are ensemble samplers developed under the assumption of Gaussian

distributions. Although nonlinearity of the forward operator is sometimes hidden

using a trick with extended state vectors, it is still an open issue for these types

of filters [35, 36, 65], specially smoothers for which data is assimilated all at once

instead of sequentially in time [66].

In Part II, ensemble samplers will be used to approximate the evolving state of

information on model parameters present in petroleum reservoir history matching.

The chapters therein seek a probability distribution over the reservoir description

that is consistent with production levels and other relevant performance indicators.

Implementation difficulties regarding computational performance will surely

arise—that is the main point of this work.



Part II

H I S T O R Y M A T C H I N G

In which the state of information about the petroleum reservoir (e. g.

porosity/permeability maps, net-to-gross, depth of oil–water contact)

is updated using inverse problem theory. The discussion is restricted to

specific case studies designed to mimic important features of realistic

petroleum fields.



4
P R E L U D E

No problem is too small or too trivial if we really do something about it.

Richard Feynman

4.1 Problem description . . . . . . . . . . . . . . . . . . . . . . . . . . 57

4.2 Case studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58

4.3 Comments on reproducibility . . . . . . . . . . . . . . . . . . . . 63

History

Matching

High

Performance

Computing Multicore

CPU

MPI

GPGPU

Inverse

Problem

Theory

Parallel

Tempering

Ensemble

MCMC
EnKF

Machine

Learning

Kernel

MAF

Kernel

PCA

Geostatisticsfiltersim

snesim

Figure 4.1: Mind map of algorithms and concepts.

The mind map in Figure 4.1 gives an overview of some of the algorithms that will

be implemented or used in the following chapters. Not all leaves will necessarily

be utilized, and the picture is better seen as a non-exhaustive visual organization

of today’s history matching technology.

56



4.1 p ro b l e m d e s c r i p t i o n 57

Leaves in red color represent the computational expensive core of the solution,

these algorithms require massive evaluation of the reservoir simulator (i. e. forward

operator) and therefore determine the feasibility of the framework. In green color

are the leaves that makes this discussion possible: distributed-memory execution

using the Message Passing Interface (MPI); and correct usage of modern, shared-

memory, multicore CPUs.

Leaves in orange color are either auxiliary algorithms for parametrization and

dimensionality reduction (e. g. kPCA, kMAF), advanced sampling of the proposal

distribution (e. g. filtersim, snesim); or represent the usage of graphics cards to

fasten geostatistical simulation (i. e. GPGPU).

All these acronyms will be revisited right before their application within case

studies in order to clarify their exact contribution. The text is written in the form

of a tutorial to ease understanding of the material and enlight cause/effect of the

taken decisions.

4.1 p ro b l e m d e s c r i p t i o n

The modern formulation of the history matching problem is exactly that of the

probabilistic framework introduced in Part I. It consists of three elements:

M O D E L S PA C E The parameters m ? M include permeability/porosity
maps, depth of oil–water contact, relative permeability curves, or any other

uncertain physical property of the petroleum reservoir.

D A T A S PA C E The measurements d ? D include well production rates
(oil, gas, water), bottom-hole pressure (BHP), recovery factors, or any other

dynamic output available during production.

F O R WA R D O P E R A T O R The forward operator G : M 7?? D is a fully-
featured numerical reservoir simulator.

in the quest for information [67] about the reservoir description. Uncertainties in

the petrophysical properties of the reservoir are mitigated based on the production

history of the petroleum field, and there is no direct attempt to find the “best” model

for the data1.

1 Perhaps History-based Uncertainty Mitigation (HUM) is a better term for modern history matching.



4.2 c a s e s t u d i e s 58

4.2 c a s e s t u d i e s

4.2.1 Channelized reservoir

A 2D training image extensively used at Stanford university for testing MPS sim-

ulation algorithms [11, 68, 69, 70, 71] is interpreted as the permeability field of a

synthetic fluvial reservoir. The channels in Figure 4.2 present high permeability

and determine the pattern of the flow in this black-oil model.

Figure 4.2: Training image of size 250x250 pixels.

Grid specifications and petrophysical properties are summarized in Table 4.1.

The OPM based reservoir simulator written for this case study is free software,

its source code is the best resource for exact reproduction of additional settings

such as relative permeability function, convergence criteria, etc. These won’t be

replicated here. At a higher abstraction level, the incompressible flow is modelled

with a simple TPFA pressure solver and implicit Euler transport.

Table 4.1: Channelized reservoir summary table.

Description Value Description Value

Number of cells 250×250×1 Porosity (?e) 30%
Cell size 10 m ×10 m ×1 m Low permeability (?l) 10 mD

Fluid phases water + oil High permeability (?h) 10 D

Water density (?w) 1000 kg/m3 Oil density (?o) 890 kg/m3

Water viscosity (µw) 0.89 cP Oil viscosity (µo) 1 cP

There are two groups of wells, 8 producers in the first and 2 injectors in the

second, all illustrated in Figure 4.3. The total liquid rate for each group is controlled

http://www.opm-project.org/
https://github.com/juliohm/HUM/blob/master/case_studies/channelized/simulator.cpp


4.2 c a s e s t u d i e s 59

to never exceed 2000 m3/day at reservoir conditions. The simulation runs for 20

time steps of 90 days each, totaling 5 years of production. Refer to Figure 4.4 for a

static movie of oil/water saturation and to Figure 4.5 for the production history.

?
Prod1

?
Prod2

?
Prod3

?
Prod4

?
Prod5

?
Prod6

?
Prod7

?
Prod8

?
Inj1

?
Inj2

Figure 4.3: Ten-spot well configuration.

Figure 4.4: Oil/water saturation for various time steps within channelized reservoir.

0 5 10 15 20

242

244

246

248

250

252

254

Prod1 Prod2 Prod3 Prod4

0 5 10 15 20

245

250

255

Prod5 Prod6 Prod7 Prod8

0 5 10 15 20

?1,020

?1,010

?1,000

?990

?980

?970

Inj1 Inj2

Time step

P
ro

d
u

ct
io

n
[m

3
/

d
ay

]

Figure 4.5: Production history for channelized reservoir.

i m p o r ta n t n o t e : The simulator can write VTK files if requested by passing

the --vtk option. For further functionality, --help.

The training image in Figure 4.2 is the true (unknown) model mtrue, only used

to generate the production history dobs in Figure 4.5 as if measured on a real

platform. Any permeability value which varies between [?l, ?h] is scaled to the

unit interval [0, 1] for keeping numbers small in numerical computations. In terms

http://www.vtk.org/


4.2 c a s e s t u d i e s 60

of implementation, the reservoir simulator expects input data to be a whitespace

(or end of line) separated list of values in the interval [0, 1].

Hence, the model space is the unit hypercube M ? [0, 1]62500; the data space has
dimension dim D = 20 time steps ×8 producers = 160; and the forward operator
G takes m ? M, scales it to the range [?l, ?h], and perform the flow simulation
returning d ? D, the production history.

4.2.2 Brugge field

The Brugge field is a benchmark case developed and made available by TNO for

assessing closed-loop control strategies and methods for history matching [72].

Various researchers in the field participated in this comparative study and shared

their results in the form of journal publications [73, 74].

Figure 4.6: Brugge field.

The field in Figure 4.6 consists of an E–W elongated half-dome with a large

boundary fault at its northern edge, and an internal fault with a modest throw

at an angle of 20? to the former. Its extension is roughly 10 km ×3 km including
four rock formations (a. k. a. reservoir zones). Formation properties are averaged

in Table 4.2 along with some remarks.

There are 60048 cells (44550 active). Prior realizations for porosity, horizontal

and vertical permeability, net-to-gross and connate water saturation are given. Poro-

perm regression curves for each rock formation and entire reservoir are provided

and can be used to correlate these petrophysical properties during generation of

new realizations [72]:

Kx = 0.01 e
45.633?mD (4.1)

https://www.tno.nl


4.2 c a s e s t u d i e s 61

Table 4.2: Brugge averaged properties per rock formation.

Formation Thickness Porosity Permeability Net-to-gross Remarks

Schelde 10 m 20.7% 1105 mD 60% Discrete sand

bodies in shale

Waal 26 m 19.0% 90 mD 88% Contains loggers:

carbonate

concretions

Maas 20 m 24.1% 814 mD 97% —

Schie 5 m 19.4% 36 mD 77% Irregular carbonate

patches

The 20 producer and 10 injector wells in Figure 4.7 are all concentrated near

the top of the dome. The free water level is at 1678 m and the model is initialized

with reference pressure of 2466 psi at 1700 m. Verbose settings such as PVT table

and well scheduling are found in the source code and won’t be replicated here.

Relative permeability curves shown in Figure 4.8 for reference are considered free

of uncertainty.

Figure 4.7: Top view of Brugge field porosity realization.

The true reservoir model is kept in secret by TNO on purpose and only its

history is made available for 10 years of production—BHP, oil/water production

rate per well and production/injection for the entire field. Oil production rates

for all producer wells are collected at each time step t of CMGTM IMEX R© as the
observation vector dtobs. This means dim D = 20 oil rates and that the Bayesian

inference is performed multiple times (e. g. number of time steps).

https://github.com/juliohm/HUM/blob/master/case_studies/brugge/brugge.tmpl


4.2 c a s e s t u d i e s 62

0.2 0.4 0.6 0.8 1

0

0.2

0.4

0.6

0.8

1

k r
w

k
ro
w

Sw
k
r

Figure 4.8: Brugge field relative permeability curves.

Oil production is shown in Figure 4.9 for all 20 producers (BR-P-1 to BR-P-20)

and 122 time steps.

0 1,000 2,000 3,000 4,000

0

100

200

300

BR-P-1 BR-P-2 BR-P-3
BR-P-4 BR-P-5

0 1,000 2,000 3,000 4,000

0

100

200

300

BR-P-6 BR-P-7 BR-P-8
BR-P-9 BR-P-10

0 1,000 2,000 3,000 4,000

0

100

200

300

BR-P-11 BR-P-12 BR-P-13
BR-P-14 BR-P-15

0 1,000 2,000 3,000 4,000

0

100

200

300

BR-P-16 BR-P-17 BR-P-18
BR-P-19 BR-P-20

Time step

P
ro

d
u

ct
io

n
[m

3
/

d
ay

]

Figure 4.9: Brugge field oil production history.

In summary, this inverse problem consists of porosity values m ? M for all
44550 active cells in the grid, IMEX as the forward operator G and observations

dtobs collected in an online fashion (i. e. for each time step).



4.3 c o m m e n t s o n r e p ro du c i b i l i t y 63

4.3 c o m m e n t s o n r e p ro du c i b i l i t y

Most of the scientific work produced today isn’t reproducible. If reproducibility is

just a matter of data and software availability, then various tools and services exist

to make results reachable by other researchers and interested readers.

The chapters herein serve as documentation for the case studies made available

on the web2. Although all studies were designed with reproducibility in mind,

some of them require commercial software for evaluation. Namely, the CMGTM

IMEX R© reservoir simulator and Results Report R© tool are both required by the
Brugge field case.

2 Check https://github.com/juliohm/HUM for software.

https://github.com/juliohm/HUM


5
C H A N N E L I Z E D R E S E R V O I R

Imagination is more important than knowledge. For knowledge is limited, whereas

imagination embraces the entire world, stimulating progress, giving birth to evolution. It

is, strictly speaking, a real factor in scientific research.

Albert Einstein

5.1 Setting priors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65

5.2 Probabilistic inversion . . . . . . . . . . . . . . . . . . . . . . . . . 69

5.3 Analysis of the results . . . . . . . . . . . . . . . . . . . . . . . . . 70

We’re given observations dobs and a conceptual model mguess in the form of a

training image, and are asked for a probability distribution over M. Although, for

this case study, the given model equals the true model mtrue, this information is

not used anywhere in this chapter because the true model is unknown in practice.

The Bayesian approach starts regularizing the inverse problem by setting priors

over both the spaces, M and D. It’s the most critical component towards a good

approximation of the posterior, and is here attempted with MPS simulation, kPCA

parametrization and KDE.

After priors have been set, inversion is performed with the further assumption of

perfect forwarding (see Example 3.4). Updated ensembles and a few convergence

indicators such as the ensemble sampler mean acceptance fraction are analyzed to

assess the effectiveness of the method.

The software exploits multiple levels of parallelism, including distributed-memory

execution with MPI on a time-shared computer cluster and multicore computing

by spawning processes.

64



5.1 s e t t i n g p r i o r s 65

5.1 s e t t i n g p r i o r s

5.1.1 Prior on model parameters

The conceptual model mguess in Figure 4.2 is simulated with Filtersim [71, 75] for

taking high-order statistics into consideration. Elaborating on this MPS simulation

algorithm is considered out of scope, but guidelines for tuning its parameters are

available on the web [76]. The initial ensemble of 200 realizations is generated with

the following XML parameter file in SGeMS:

Listing 5.1: filtersim.xml

1&amp;lt;parameters&gt;&amp;lt;algorithm name=" filtersim_cont " /&gt;

2&amp;lt;GridSelector_Sim value=" filtersimGrid " region=" " /&gt;

3&amp;lt;Property_Name_Sim value="perm" /&gt;

4&amp;lt;Nb_Realizations value=" 200 " /&gt;

5&amp;lt;Seed value=" 211175 " /&gt;

6&amp;lt;PropertySelector_Training grid="mguess" property=" permeability " region=" " /&gt;

7&amp;lt;Patch_Template_ADVANCED value="7 7 1" /&gt;

8&amp;lt;Scan_Template value=" 11 11 1" /&gt;

9&amp;lt;Trans_Result value="0" /&gt;

10&amp;lt;Hard_Data grid=" " property=" " region=" " /&gt;

11&amp;lt;Use_SoftField value="0" /&gt;

12&amp;lt;Region_Indicator_Prop value=" " /&gt;

13&amp;lt;Active_Region_Code value=" " /&gt;

14&amp;lt;Use_Previous_Simulation value="0" /&gt;

15&amp;lt;Previous_Simulation_Prop value=" " /&gt;

16&amp;lt;Use_Region value="0" /&gt;

17&amp;lt;Nb_Multigrids_ADVANCED value="3" /&gt;

18&amp;lt;Debug_Level value="0" /&gt;

19&amp;lt;Cmin_Replicates value=" 10 10 10 " /&gt;

20&amp;lt;Data_Weights value=" 0.5 0.3 0.2 " /&gt;

21&amp;lt;CrossPartition value="1" /&gt;

22&amp;lt;KMeanPartition value="0" /&gt;

23&amp;lt;Nb_Bins_ADVANCED value="5" /&gt;

24&amp;lt;Nb_Bins_ADVANCED2 value="2" /&gt;

25&amp;lt;Use_Normal_Dist value="0" /&gt;

26&amp;lt;Use_Score_Dist value="1" /&gt;

27&amp;lt;Filter_Default value="1" /&gt;

28&amp;lt;Filter_User_Define value="0" /&gt;

29&amp;lt;/parameters&gt;

Randomly selected realizations are shown in Figure 5.1 for reference. They all

present channels similar to that of the training image as intended. Only the first

100 out of 200 realizations are used in the study.



5.1 s e t t i n g p r i o r s 66

Figure 5.1: Filtersim realizations for channelized training image.

Remark 5. The prior state of information about the petroleum reservoir mtrue is

introduced with (MPS) simulation.

i m p o r ta n t n o t e : I ported the SGeMS build maintained by AR2Tech R© to
the GNU/Linux operating system. Please refer to https://github.com/juliohm/

ar2tech-SGeMS-public for installation instructions.

The prior density ?M(m) needs to be evaluated anywhere in the model space M,

not just at the ensemble members
{

m(k)
}100
k=1

. This is a requirement imposed by

the MCMC algorithm with arbitrary proposals explained in Section 3.4. Moreover,

dim M = 62500 is far too high for KDE. A simple attempt can show that all RAM

of a modern computer (e. g. 8GB Linux 64bits) is exhausted by the current SciPy

Gaussian KDE implementation.

Parametrization with dimensionality reduction is employed to circumvent both

of these issues. Among the available methods, PCA [77, 78] and MAF [79, 80]

were investigated in their kernel-based versions [81, 82, 83]. kPCA performed very

well for the ensemble under study, whereas kMAF, on the other hand, ended up

producing numerically unstable generalized eigenproblems1, therefore, the later

was not used with this dataset.

In a different context, MAF (without kernels) has been successfully applied in

mining [84, 85] to avoid the modelling of cross-covariance in the Linear Model of

Coregionalization [13]. This technique is particularly powerful when more than two

random fields are to be cosimulated (e. g. grade estimation in multivariate deposits).

1 Of course, the code might have bugs.

https://github.com/juliohm/ar2tech-SGeMS-public
https://github.com/juliohm/ar2tech-SGeMS-public
http://scipy.org/


5.1 s e t t i n g p r i o r s 67

Regardless of the parametrization technique adopted, the idea is the same: write

the permeability map m ? M that has dimension dim M = 62500 in terms of a
reduced number (e. g. 50) of uncorrelated coordinates ? = (?1, ?2, . . . , ?50)?:

m = m(?) (5.1)

In linear PCA (a. k. a. K-L decomposition [86, 87, 88]), the parametrization is a

product m = E?1/2? that maximizes the variance of the ensemble. Here, E and ?

are truncated matrices of eigenvectors and eigenvalues of the ensemble covariance.

As a side note, the covariance is never computed directly because of its size, for

instance 62500×62500. A dual formulation with kernels is implemented instead,
that solves eigenproblems of size 100×100 (i. e. number of realizations squared).

For nonlinear parametrization m = m(?), vectors are mapped2 onto an abstract

very high-dimensional (possibly infinite) feature space F where the ensemble is in

some sense “linearly separable” [81]. This mapping ? : M 7?? F introduces a new
preimage problem [89, 90, 91] that is attacked with the classical framework, either by

transport of metrics or fixed-point iteration. Previous attempts have shown little

difference between these two variants [92], and the later is used in this work.

The minor details of kPCA parametrization and associated preimage problem

are treated in Appendix C. A specialized matricial formulation is developed for

the polynomial kernel purposed in Sarma’s “Efficient Closed-loop Optimal Control of

Petroleum Reservoirs under Uncertainty” Ph.D. thesis [70] and derived publications

[88]. It’s a sum up to degree d for preserving up to 2d-th order statistics:

k(x, y)
def
= ?x, y?+ ?x, y?2 + · · ·+ ?x, y?d (5.2)

Figure 5.2 shows how Equation 5.1 can be used to reconstruct 250×250 pixels,
channelized images m from 50 uncorrelated coordinates ?. The channel patterns

are consistently recovered as the degree is increased, and this is made clear by

denoised versions shown for each reconstruction. More importantly, it proves that

linear PCA (i. e. d = 1) isn’t able to capture channel patterns in the ensemble.

Remark 6. MPS simulation and nonlinear kPCA parametrization are responsible

together for generating and preserving high-order statistics.

After parametrization with dimensionality reduction is performed, the initial

ensemble
{

m(k)
}100
k=1

is mapped to the feature space
{

?
(k)

}100

k=1
and the forward

operator is replaced by the function composition:

G?
def
= G ? m (5.3)

2 The mapping is never actually evaluated.



5.1 s e t t i n g p r i o r s 68

d = 1 d = 2 d = 3 d = 4

Kernel PCA for increasing degrees: reconstruction above and denoised version below

Figure 5.2: kPCA for increasing polynomial kernel degrees.

Thus, EnMCMC will be updating lower-dimensional ensembles in a reduced

space ? through the application of G?(?) = G(m(?)). This inverse problem is

represented by the triple (?, D, G?) which is not equivalent to (M, D, G) due to

parametrization losses, but can be implemented with today’s computers.

The prior state of information on model parameters is obtained by Gaussian

KDE on the initial ensemble of features
{

?
(k)

}100

k=1
using a bandwidth computed

by 10-fold cross-validation:

?M(m(?)) = KDE(?) (5.4)

This data-driven approach with cross-validation is preferred over Scott’s rule of

thumb [56] implemented in SciPy by default as it does not impose any assumption

on the ensemble. Among the available implementations (e. g. SciPy, scikit-learn,

Statsmodels), only scikit-learn’s KernelDensity correctly exploits log-scale to avoid

the “vanishing problem”3, and is therefore used.

5.1.2 Prior on observations

The history for each of the 8 producer wells in Figure 4.5 is concatenated into a

vector dobs of size 8 wells ×20 time-steps = 160. The prior state of information on
observations is assumed to be multivariate Gaussian centered at dobs:

?D(d) = N(dobs, I) (5.5)

3 Probabilities vanish in high dimensions (e. g. p = 0.1 =? p50 ? 0).

http://scipy.org
http://scikit-learn.org
http://statsmodels.sourceforge.net


5.2 p ro b a b i l i s t i c i n v e r s i o n 69

5.2 p ro b a b i l i s t i c i n v e r s i o n

Perfect forwarding is assumed to hold true which means the posterior probability

is simply the product of shapes in Equation 5.4 and Equation 5.5:

?M(?) ? ?D(G?(?)) ?M(?) (5.6)

In log-scale, this update rule is written log ?M(?) = log ?D(G?(?)) + log ?M(?)

where the constant term is dropped for simplicity. It will be canceled out in the

MCMC acceptance criterion ultimately.

The number of walkers in EnMCMC is equal to the number of members in the

ensemble, in this case 100 walkers are used to explore R50. It must be an even

number since the ensemble is divided into two halves for parallelization by the

emcee Python package. That is, 50 reservoir simulations are run in parallel and

the ensemble is updated in two consecutive steps with total theoretical wall-time

of twice that of a single reservoir simulation.

Three proposal distributions are investigated: the stretch move in ? explained

in Section 3.4, KDE sampling of the prior, and a Filtersim-based proposal inspired

in Hansen et al.’s work [15, 93, 94, 95, 96]. In the later case, Filtersim is used to

generate 100 new realizations that are mapped to the feature space F.

The Hastings acceptance probability for the (symmetric) stretch move ? ? ?? is
a function of the posterior ?M(?):

?(?, ??) = min
{

1,
?M(?

?
)

?M(?)

}

(5.7)

The KDE and Filtersim based proposals are assumed to sample the true prior

information q(?, ??) ? ?M(??) and therefore their acceptance only depends on the
likelihood:

?(?, ??) = min
{

1,
?M(?

?
) ?M(?)

?M(?) ?M(?
?
)

}

= min
{

1,
L(?

?
) ?M(?

?
) ?M(?)

L(?) ?M(?) ?M(?
?
)

}

= min
{

1,
L(?

?
)

L(?)

}

(5.8)

The prior is canceled out in Equation 5.8 but this does not mean it’s not being

considered by the framework. In fact, it’s being sampled as the ideal proposal.

Furthermore, custom proposals like these present ideal theoretical speed-up of nw

compared to the stretch move which can handle at most nw
2

reservoir simulations

at a time.

http://dan.iel.fm/emcee


5.3 a na ly s i s o f t h e r e s u lt s 70

5.3 a na ly s i s o f t h e r e s u lt s

Results are reported for each of the three proposal distributions mentioned in

the previous section. Possible causes for bad/good performance of the applied

methods are highlighted whenever possible. The intent here is to reason against

the applicability of the framework and illustrate implementation difficulties.

Due to deadline constraints and long job queues in the cluster4, Markov chain

walkers advanced no more than 1000 steps and are quite probably in high autocor-

relation regime. Each step consists of 100 evaluations of the forward operator, that

is 1000×100 = 100000 evaluations in total.

5.3.1 Parallel stretch move

This configuration proved to be ineffective. Stretch moves in ? were such that the

associated preimages did have extremely low probability according to the KDE fit

with the prior ensemble. In other words, doing such moves in the ensemble
{

?
(k)

}

used for KDE training produces candidates that are very unlikely in 50 dimensions.

This is one possible manifestation of the “vanishing problem”.

Remark 7. Generic moves that are independent of the target problem, such as the

parallel stretch move, will surely present inferior performance in high dimensions.

The acceptance fraction is exactly zero for all of the walkers in the ensemble and

therefore the state of information is unchanged. In any case, log-probabilities are

shown in Figure 5.3 for giving the reader a sense of magnitude.

Prior and posterior distribution coincide. Values around ?800 show the real need

for working in log-scale as in finite floating-point precision exp(?800) = 0 and all

ensemble members would be equally implausible; which is clearly not true.

5.3.2 KDE-based proposal

This time candidates are directly proposed by KDE assuming that it’s the prior

state of information on ?. This is reasonable assumption since the training step

was done with a featurized version of the prior ensemble
{

m(k)
} ???

{

?
(k)

}

.

4 c e na pa d - p e: http://www.cenapad-pe.ufpe.br

http://www.cenapad-pe.ufpe.br


5.3 a na ly s i s o f t h e r e s u lt s 71

prior posterior

prior vs. posterior

2000

1800

1600

1400

1200

1000

800

600

400

lo
g
-p

ro
b
a
b
il
it

y

Figure 5.3: Stretch move: bean plot of prior and posterior log-probabilities.

The acceptance criterion depends only on the log-likelihood and no more on

the evaluation of the prior, alleviating slightly the vanishing of probabilities. In

Figure 5.4, the posterior ensemble members are more likely to represent the true

unknown model than their prior versions before history assimilation as expected.

prior posterior

prior vs. posterior

1200

1100

1000

900

800

700

600

lo
g
-p

ro
b
a
b
il
it

y

Figure 5.4: KDE move: bean plot of prior and posterior log-probabilities.

When forwarded to the data space, the initial ensemble presents a bias around

250 m3/day illustrated in Figure 5.5. This is not an issue for the Metropolis-Hastings

algorithm itself, but it seriously compromise the way samples are drawn from the

KDE-based proposal in high-dimensions: in such scenarios, probabilities are quite

low and the only candidates that survive are those in a pretty small vicinity of

the training ensemble (i. e. initial ensemble). The direct consequence is incorrect



5.3 a na ly s i s o f t h e r e s u lt s 72

data fitting5: the ensemble shrinks to the wrong curve in Figure 5.6 after history is

assimilated all at once (i. e. smoother).

0 5 10 15 20
235

240

245

250

255

260

265
well 1

0 5 10 15 20
235

240

245

250

255

260

265
well 2

0 5 10 15 20
235

240

245

250

255

260

265
well 3

0 5 10 15 20
235

240

245

250

255

260

265
well 4

0 5 10 15 20
235

240

245

250

255

260

265
well 5

0 5 10 15 20
235

240

245

250

255

260

265
well 6

0 5 10 15 20
235

240

245

250

255

260

265
well 7

0 5 10 15 20
235

240

245

250

255

260

265
well 8

history for prior ensemble

timestep

p
ro

d
u
c
ti

o
n
 r

a
te

 [
m

³/
d
]

Figure 5.5: KDE move: production history for the prior ensemble.

Remark 8. KDE was not designed for very high-dimensional problems and is in

general quite sensitive to the initial ensemble used for training.

The mean acceptance fraction of 1% is unacceptably low and for sure the pos-

terior distribution is not being sampled given the prior, see Figure 5.7. Increasing

the number of EnMCMC iterations is not helpful at all because of the bias already

illustrated in Figure 5.6.

This configuration was run with 71 (70 slaves + 1 master) processes but could

have been run with up to 101 for optimal speed-up. It took 1 day + 10 hours + 43

minutes to complete. In the Oil &amp;amp; Gas industry, real time large-scale history-based

uncertainty mitigation happens on a daily basis—the usual time scale for a field

model under production in commercial simulators.

Prior and posterior ensemble members are shown in Figure 5.8 and Figure 5.9,

respectively. The MAP estimate is also shown side by side with the true (unknown)

reservoir in Figure 5.10.

These results show how challenging this problem of finding the posterior state of

information in high-dimensional spaces actually is when no assumption is made

5 Assuming the OPM simulator and OPM itself are free of bugs.



5.3 a na ly s i s o f t h e r e s u lt s 73

0 5 10 15 20
235

240

245

250

255

260

265
well 1

MAP

0 5 10 15 20
235

240

245

250

255

260

265
well 2

MAP

0 5 10 15 20
235

240

245

250

255

260

265
well 3

MAP

0 5 10 15 20
235

240

245

250

255

260

265
well 4

MAP

0 5 10 15 20
235

240

245

250

255

260

265
well 5

MAP

0 5 10 15 20
235

240

245

250

255

260

265
well 6

MAP

0 5 10 15 20
235

240

245

250

255

260

265
well 7

MAP

0 5 10 15 20
235

240

245

250

255

260

265
well 8

MAP

history for posterior ensemble

timestep

p
ro

d
u
c
ti

o
n
 r

a
te

 [
m

³/
d
]

Figure 5.6: KDE move: production history for the posterior ensemble.

about the underlying distribution (e. g. Gaussian prior). They also give hints on

the technical difficulties involved in the computer implementation, particularly

the sampling of the proposal.

As can be seen, although the initial ensemble moves towards higher probability

regions, the implementation fail to sample the posterior distribution. It’s important

to stress that the framework and its implementation are different entities and that

the failure of the later doesn’t invalidate the former.

5.3.3 Filtersim proposal

In this attempt, candidates are drawn with the Filtersim geostatistical algorithm.

So far, this algorithm has only been used to generate the initial ensemble in a pre-

processing step, see Section 5.1.1. The difference here is that a modified version

of SGeMS6 is run at every EnMCMC iteration for generating a completely new

ensemble, resulting in a non-trivial move.

Such ensembles proposed by Filtersim are “mapped” to the feature space in the

same fashion as was the initial ensemble. The chain proceeds with its exploration

of ? as before with the only additional effort of calling SGeMS 1000 times.

6 https://github.com/juliohm/ar2tech-SGeMS-public

https://github.com/juliohm/ar2tech-SGeMS-public


5.3 a na ly s i s o f t h e r e s u lt s 74

0 20 40 60 80 100

walker index

0.0

0.2

0.4

0.6

0.8

1.0

a
c
c
e
p
ta

n
c
e
 f

ra
c
ti

o
n

mean acceptance = 1.0 %

Figure 5.7: KDE move: acceptance fraction for each walker.

i m p o r ta n t n o t e : Unlike in the previous attempts, this non-trivial proposal

requires a modified version of SGeMS installed on the cluster. I fixed some minor

bugs in the original software and added a command LoadCartesianGrid so that

it could be fully run in non-interactive mode without its graphical user interface.

The file format the command expects is documented in the source code.

In Figure 5.11, posterior log-probabilities reach higher values compared to the

previous KDE-based attempt. This indicates that space exploration with Filtersim

is more effective, though it still presents unacceptably low acceptance fractions as

shown in Figure 5.12.

The posterior ensemble is illustrated in Figure 5.13 and forwarded in Figure 5.14.

A naive visual comparison with the KDE-based posterior ensemble in Figure 5.9

ratifies Filtersim is doing a slightly better job in exploring the model space. This

statement is made rigorous here with the aid of a measure of structural similarity

between images—the SSIM index [97].

Posterior SSIM statistics are shown in Table 5.1 for both proposals using a scan

window size of 7 in accordance with the Filtersim patch size in Listing 5.1. The

interquartile range for the SSIM index in the Filtersim proposal is slightly greater

than the one for the KDE-based proposal, which means more diversity of patterns.

Remark 9. Filtersim does better exploration of the model space compared to the

KDE-based attempt with the channelized reservoir under study.

https://github.com/juliohm/HUM/blob/master/case_studies/channelized/mtrue.dat


5.3 a na ly s i s o f t h e r e s u lt s 75

prior ensemble

Figure 5.8: KDE move: 25 most probable images in prior ensemble.

Table 5.1: SSIM statistics for Filtersim and KDE-based proposals.

SSIM index

mean std min 25% 50% 75% max interquartile range

KDE-based 0.214 0.021 0.189 0.203 0.203 0.224 0.291 0.02129

Filtersim 0.218 0.021 0.189 0.208 0.216 0.230 0.291 0.02233

The MAP estimate is shown in Figure 5.15 side by side with the true reservoir.

Comparing its SSIM index of 0.212 with the SSIM index of 0.203 for the KDE-based

MAP estimate, Filtersim gets closer to the true reservoir model.

i m p o r ta n t n o t e : Space exploration is also affected by the way images are

reconstructed from the lower-dimensional space ?. Various members in the final

ensemble when reconstructed with kPCA present completely different patterns as

shown in Figure 5.16.



5.3 a na ly s i s o f t h e r e s u lt s 76

posterior ensemble

Figure 5.9: KDE move: 25 most probable images in posterior ensemble.

This configuration with Filtersim as the proposal distribution was run with 101

(100 slaves + 1 master) processes and took 2 days + 18 hours + 33 minutes to

complete. According to experiments, at least 16 hours are spent outside reservoir

simulation. Such considerable waste of time is due to serialization/deserialization

of large Python objects for message passing.

Among the three attempts—stretch move, KDE-based, Filtersim—the Filtersim

algorithm had the best, yet not acceptable, results.



5.3 a na ly s i s o f t h e r e s u lt s 77

true reservoir MAP estimate

Figure 5.10: KDE move: maximum a posteriori estimate.

prior posterior

prior vs. posterior

1100

1000

900

800

700

600

500

lo
g
-p

ro
b
a
b
il
it

y

Figure 5.11: Filtersim: bean plot of prior and posterior log-probabilities.

0 20 40 60 80 100

walker index

0.0

0.2

0.4

0.6

0.8

1.0

a
c
c
e
p
ta

n
c
e
 f

ra
c
ti

o
n

mean acceptance = 0.9 %

Figure 5.12: Filtersim: acceptance fraction for each walker.



5.3 a na ly s i s o f t h e r e s u lt s 78

posterior ensemble

Figure 5.13: Filtersim: 25 most probable images in posterior ensemble.

0 5 10 15 20
235

240

245

250

255

260

265
well 1

MAP

0 5 10 15 20
235

240

245

250

255

260

265
well 2

MAP

0 5 10 15 20
235

240

245

250

255

260

265
well 3

MAP

0 5 10 15 20
235

240

245

250

255

260

265
well 4

MAP

0 5 10 15 20
235

240

245

250

255

260

265
well 5

MAP

0 5 10 15 20
235

240

245

250

255

260

265
well 6

MAP

0 5 10 15 20
235

240

245

250

255

260

265
well 7

MAP

0 5 10 15 20
235

240

245

250

255

260

265
well 8

MAP

history for posterior ensemble

timestep

p
ro

d
u
c
ti

o
n
 r

a
te

 [
m

³/
d
]

Figure 5.14: Filtersim: production history for the posterior ensemble.



5.3 a na ly s i s o f t h e r e s u lt s 79

true reservoir MAP estimate

Figure 5.15: Filtersim: maximum a posteriori estimate.

original reconstruction original reconstruction

Kernel PCA reconstruction

Figure 5.16: kPCA reconstruction.



6
B R U G G E F I E L D

Some people say, “How can you live without knowing?” I do not know what they mean. I

always live without knowing. That is easy. How you get to know is what I want to know.

Richard Feynman

6.1 Setting priors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81

6.2 Probabilistic inversion . . . . . . . . . . . . . . . . . . . . . . . . . 82

6.3 Analysis of the results . . . . . . . . . . . . . . . . . . . . . . . . . 83

This case study is an example of online inversion. We’re given observations dtobs at

various time steps t and are asked for a probability distribution over M along with

the reservoir simulation. That is, the inversion is performed multiple times, each

corresponding to an instantaneous set of measurements from producer wells.

The prior state of information about the petroleum reservoir changes along the

process whenever an observation is assimilated. This is different than what was

done in Chapter 5 in which the prior distribution was fixed and the inversion was

performed only once for the entire history.

3 time steps were chosen for assimilation by visual inspection of the production

curves. The perfect forwarding assumption is again exploited as this problem is

much more expensive than the previous flow in a synthetic channelized reservoir.

Priors are first set with the initial ensemble provided by TNO, and most of what

was said earlier regarding parametrization and dimensionality reduction could be

replicated here. Inversion is then performed in an online fashion producing results

that are analyzed in the last section of the chapter.

Due to time constraints and other licensing issues, code testing and a more

careful analysis of the results were compromised. Nevertheless, the software is

available to anyone who is eager to improve the applicability of the framework.

80



6.1 s e t t i n g p r i o r s 81

6.1 s e t t i n g p r i o r s

6.1.1 Prior on model parameters

TNO provided to all participants of the benchmark an initial ensemble of 104

realizations constructed with various geostatistical algorithms. Some of the layers

were built with MPS simulation whereas others were not. The result is a complex

reservoir that presents all sorts of physical (and non-physical) characteristics.

Were we simulating layers (or rock facies) separately, it would make sense to

parametrize with nonlinear kPCA. However, petrophysical maps are being gener-

ated here for all 44550 active cells without any pre-processing, what makes linear

kPCA a good alternative. Furthermore, the linear parametrization has closed form

which is preferred over the general fixed-point iteration discussed in Appendix C.

The initial ensemble
{

m(k)
}100
k=1

is mapped to the feature space ? exactly as in

Chapter 5. The resulting ensemble
{

?
(k)

}100

k=1
is made of members that have 50

components each, an appreciable dimensionality reduction 44550 ? 50.
KDE is performed to approximate a nonparametric distribution that represents

the prior state of information on the model space:

?M(m(?)) = KDE(?) (6.1)

6.1.2 Prior on observations

3 time steps were chosen for assimilation: 1812, 2421 and 3029. These correspond

to dates in IMEX (i. e. TIME keyword) for which a considerable mismatch was

reported in comparing the true history with predictions from the prior ensemble.

An observation dtobs at time step t is assumed to be the center of a multivariate

Gaussian. This is illustrated in Figure 6.1 and written in mathematical notation as:

?tD(d) = N(d
t
obs , 0.1× I) (6.2)

No attempt is made to model correlation between different time steps. It might

be that in alternative settings, the prior state ?tD(d) is actually affected by previous

measurements dt?1obs or assimilation.



6.2 p ro b a b i l i s t i c i n v e r s i o n 82

dtobs

t
?
t D
(d
)

Figure 6.1: Prior on observations changing over time.

6.2 p ro b a b i l i s t i c i n v e r s i o n

Perfect forwarding is assumed to hold true. At every time step, the posterior is

simply the product of Equation 6.1 and Equation 6.2:

?M(?) ? ?tD(G?(?)) ?M(?) (6.3)

with G?
def
= G?m the composition that maps ? 7? m 7? d. For the next assimilation

at t + 1, the prior ?M is replaced with the posterior ?M just computed. This fixed-

point rule is such that at the end of 3 assimilations:

?M(m) ?
3?

t=1

?tD(G
?(?))?M(?) (6.4)

or in log-scale log ?M(m) =
?3

t=1 log ?
t
D(G

?(?)) + log ?M(?) with the constant

term ultimately dropped in the Metropolis-Hastings algorithm.

i m p o r ta n t n o t e : Although Equation 6.4 or its log-scale version are correct,

it’s safer to implement Equation 6.3 in a loop to avoid extremely low probabilities.

The resulting code is also more flexible to future generalizations.

10 EnMCMC iterations are run for each of the 3 history data. For sure, no MCMC

algorithm can ever converge with such a small number, and unfortunately, no time

is left for longer runs at this moment of writing. These 30 EnMCMC iterations

gives a total of 30×100 = 3000 very expensive reservoir simulations.

https://github.com/juliohm/HUM/blob/master/case_studies/brugge/main.py


6.3 a na ly s i s o f t h e r e s u lt s 83

6.3 a na ly s i s o f t h e r e s u lt s

Although more expensive, this case study was easier to tackle compared to the

channelized reservoir in Chapter 5 in the sense that the presence of patterns (e. g.

channels, vugs) was not considered at all. The only attempt with the KDE-based

proposal presented higher mean acceptance fraction, but due to time constraints

the chains could not be run for longer and escape the burn-in period.

Prior and posterior log-probabilities are shown in Figure 6.2. It can be noticed

that the ensemble members with lower probability were replaced by candidates in

better accordance with the production history and prior state of information.

prior posterior

prior vs. posterior

7500000

7000000

6500000

6000000

5500000

5000000

4500000

4000000

3500000

3000000

lo
g
-p

ro
b
a
b
il
it

y

Figure 6.2: KDE move: bean plot of prior and posterior log-probabilities.

The mean acceptance fraction of 39.8% shows the ensemble is changing at an

acceptable rate in these initial EnMCMC iterations as expected. This performance

is likely to degrade as more iterations are run, or in other words, the rate of change

is likely to decrease after the burn-in period. See Figure 6.3.

The predicted history for prior and posterior ensemble are shown in Figure 6.4

and Figure 6.5 respectively, and for 8 selected wells in the field—BR-P-1 to BR-P-8.

The MAP estimate is also shown, it fits the data reasonably well on that macroscale.

More investigation and careful analysis are needed before concluding anything

about the implementation feasibility in this case. Longer runs and convergence

checks are required before stating it did actually succeed. Other important history

such as BHP and water rates were not considered and must be included for more

realistic scenarios.



6.3 a na ly s i s o f t h e r e s u lt s 84

0 20 40 60 80 100

walker index

0.0

0.2

0.4

0.6

0.8

1.0

a
c
c
e
p
ta

n
c
e
 f

ra
c
ti

o
n

mean acceptance = 39.8 %

Figure 6.3: KDE move: acceptance fraction for each walker.

0 20 40 60 80 100 120
0

50

100

150

200

250

300

350
BR-P-1

0 20 40 60 80 100 120
0

50

100

150

200

250

300

350
BR-P-2

0 20 40 60 80 100 120
0

50

100

150

200

250

300

350
BR-P-3

0 20 40 60 80 100 120
0

50

100

150

200

250

300

350
BR-P-4

0 20 40 60 80 100 120
0

1000

2000

3000

4000

5000

6000
BR-P-5

0 20 40 60 80 100 120
0

50

100

150

200

250

300

350
BR-P-6

0 20 40 60 80 100 120
0

50

100

150

200

250

300

350
BR-P-7

0 20 40 60 80 100 120
0

50

100

150

200

250

300

350
BR-P-8

history for prior ensemble

timestep

p
ro

d
u
c
ti

o
n
 r

a
te

 [
m

³/
d
]

Figure 6.4: KDE move: production history for the prior ensemble.



6.3 a na ly s i s o f t h e r e s u lt s 85

0 20 40 60 80 100 120
0

50

100

150

200

250

300

350
BR-P-1

MAP

0 20 40 60 80 100 120
0

50

100

150

200

250

300

350
BR-P-2

MAP

0 20 40 60 80 100 120
0

50

100

150

200

250

300

350
BR-P-3

MAP

0 20 40 60 80 100 120
0

50

100

150

200

250

300

350
BR-P-4

MAP

0 20 40 60 80 100 120
0

1000

2000

3000

4000

5000

6000
BR-P-5

MAP

0 20 40 60 80 100 120
0

50

100

150

200

250

300

350
BR-P-6

MAP

0 20 40 60 80 100 120
0

50

100

150

200

250

300

350
BR-P-7

MAP

0 20 40 60 80 100 120
0

50

100

150

200

250

300

350
BR-P-8

MAP

history for posterior ensemble

timestep

p
ro

d
u
c
ti

o
n
 r

a
te

 [
m

³/
d
]

Figure 6.5: KDE move: production history for the posterior ensemble.



7
C O N C L U S I O N

What we think or what we know or what we believe is in the end of little consequence.

The only thing of consequence is what we do.

John Ruskin

7.1 General comments . . . . . . . . . . . . . . . . . . . . . . . . . . . 86

7.2 Technical difficulties . . . . . . . . . . . . . . . . . . . . . . . . . . 88

7.3 Suggested improvements . . . . . . . . . . . . . . . . . . . . . . . 89

This masters dissertation and associated software development ended up being a

purely academic exercise, serving to identify some of the technology barriers that

exist in doing probabilistic inversion with realistic petroleum reservoirs.

Various enhancements to the current implementation are needed before it can

possibly be employed by the industry with tens or hundreds of thousands of grid

cells. They are not trivial as Chapter 5 and Chapter 6 have shown, but the source

code made available on the web is a good start point.

7.1 g e n e r a l c o m m e n t s

As previously mentioned, the failure of this particular implementation in sampling

the posterior state of information correctly doesn’t imply that the framework is

infeasible in practice. It just means more tweaks are necessary to overcome current

hardware limitations and to better represent in a computer the prior information

that we have about the true (physical) model. In spite of that, the implementation

might still be useful for obtaining classical MAP estimates without strong, and

sometimes unrealistic assumptions (e. g. Gaussian prior).

86



7.1 g e n e r a l c o m m e n t s 87

The simplicity of the code is an evidence of the framework generality where

states of information are updated without complicated programming paths. In

fact, it’s a simple generic loop with the EnMCMC sampler.

Internals of the target simulator G are totally ignored in each of the case studies.

This can be interpreted as a feature for hiding complexity, but also as an implemen-

tation deficiency; it actually depends on the quality of the obtained results. With

that said, I argue it’s a deficiency in both chapters and that the simulator internals

should have been used to add those very high-dimensional ill-posed problems

with extra rich information.

Dissertation results and the software used to generate them were made available

in such a way other researchers can investigate and contribute new ideas. The

channelized reservoir case study is fully open source whereas the Brugge field

benchmark case relies on proprietary software, and therefore is only reproducible

to those with a software license.

Another major difference between Chapter 5 and Chapter 6 resides in the way

the history was assimilated—offline vs. online inversion. Although the two case

studies have about the same dimensionality dim M, the number of wells in the

Brugge field is considerably greater. The observations were considered per time

step so that dim D was low enough for fitting a multivariate Gaussian.

The channelized reservoir benchmark case was designed for assessing the per-

formance of advanced history-based uncertainty mitigation methods. It has the

interesting appeal of visually comparing patterns before and after inversion, and

can be used to guide the development of new clever heuristics for history-matching

problems with complex geology.

The novelty of this work is in the nonparametric representation of the prior

state of information with nonlinear kPCA and KDE; and introduction of MCMC

ensemble samplers to petroleum engineering history-matching problems.

The posterior probability under perfect forwarding assumption was rewritten in

log-scale and directly used in the EnMCMC algorithm in order to overcome the

vanishing of probabilities on digital computer finite precision—a crucial step for

dealing with high-dimensional model spaces (e. g. R50).

kPCA proved to be an effective method for dimensionality reduction that is

particularly powerful for preserving high-order statistics. Together with KDE and

k-fold cross-validation, the software was able to encode information without strong

probabilistic assumptions on the current ensemble.



7.2 t e c h n i c a l d i f f i c u lt i e s 88

EnMCMC showed itself as a high-performance Markov chain sampling algo-

rithm that can exploit MPI pools to distribute work among various computational

nodes. Its theoretical speed-up of either nw
2

for the parallel stretch move or nw

for custom proposal distributions1 allowed the execution of expensive case studies

within 2–4 days (i. e. wall time).

As measured by the SSIM index, and among the three attempts in Chapter 5, the

Filtersim-based proposal did the most successful exploration of the model space.

The final ensemble however remained trapped at a local minimum and the full

characterization of the posterior distribution was never achieved.

Similar conclusion was made in Chapter 6, in which the expensive probabilistic

inversion of the Brugge field was set up and implemented with the KDE-based

proposal. The posterior state of information was not captured because of the small

number of EnMCMC iterations, but a MAP estimate was succesfully found with-

out any Gaussian assumption on the petrophysical properties of the reservoir.

7.2 t e c h n i c a l d i f f i c u lt i e s

Developing the software in a high-level programming language such as Python

was a quite pleasurable experience; porting the code together with all its depen-

dencies to an old Red Hat R© computer cluster was not. I had to recompile all third-
party libraries (e. g. NumPy/SciPy, mpi4py, VTK, Qt, . . . ) with a custom toolchain

and link against the MPI implementation supported by the c e na pa d - p e cluster

for everything to work as expected. The process involved various obscure error

messages that gave little clue about what the actual misconfiguration was.

The SLURM resource manager installed on the cluster was also misconfigured

causing some symbols to not be resolved during runtime. I solved the issue by

manual preload of the missing libraries right before job scheduling.

Some of the computing nodes in the cluster were damaged causing the crash

of the entire job with non-deterministic errors. They were identified and simply

excluded from subsequent executions.

CMG licensing was another issue I faced while working on the second case study.

The license manager was having trouble with multiple requests in parallel close

to the available limit (e. g. 100). No solution was presented by the support and I

forcibly reduced the number of processes in the MPI pool.

1 Assuming as many walkers as the ensemble size.

https://computing.llnl.gov/linux/slurm/


7.3 s u g g e s t e d i m p rov e m e n t s 89

Not really a technical difficulty and more like a caveat, users must be aware that

the OPM initiative is very recent and that its API is subject to radical modifications.

Finally, minor issues regarding RAM limits and other cluster resources were

solved on demand by implementing alternative memory-friendly code in response

to the large amount of data being transmitted and transformed for each realization.

7.3 s u g g e s t e d i m p rov e m e n t s

Various issues within the current software implementation should be addressed

before it can actually be employed for sampling the posterior state of information

in high-dimensional petroleum history-matching problems.

The major room for improvements that comes to mind lies in the design of

proposal distributions (or moves) that are aware of the current ensemble position.

In all implemented attempts, the current ensemble is discarded and replaced by

a new one as if the current realizations never existed. In mathematical notation,

the move x ? y that should be dependent on the current state x is not using this
information to propose a new state y in a “good direction”: q(x, y) = q(y).

Assuming better proposals are available, other Machine Learning tools could

be investigated for fitting the ensemble just proposed. KDE was not originally

designed for high-dimensional problems and even in log-scale might present poor

approximation quality.

Perfect forwarding is assumed to hold true in both case studies. In terms of com-

putational requirements, it’s extremely difficult to throw this assumption away and

take theoretical uncertainties into account. Nonetheless, it is a possible research

topic to dive in.

Still related to computational performance, online inversion in the Brugge field

benchmark case doesn’t need the reservoir simulation to run all the way to the end.

For the initial time steps the simulation should be interrupted right away as the

remaining history is useless for data assimilation.

Another very important feature that wasn’t implemented is the possibility of

restarting the algorithm anywhere in the chain. Specially if the target simulator

terminates abnormally very often with the proposed realizations as inputs, it’s of

enormous interest to fix the bug and continue from where the chain has stopped.



7.3 s u g g e s t e d i m p rov e m e n t s 90

In Part II, no attempt was made to assess the predictive capability of the posterior

ensemble or how each of its members perform for future timesteps beyond the

assimilated history. This can be done with little programming effort.

Finally, the research community is invited to benchmark other HUM methods

(e. g. EnKF, RML) with the case studies developed here and send me pull requests

to make them available to future readers.



Part III

A P P E N D I X



A
O M I T T E D P R O O F S

a.1 t h e m a j o r i t y o f i n v e r s e p ro b l e m s i s i l l - p o s e d

Following Theorem 1, it remains to proof by induction nn ? n! &gt; n! for all n &gt; 2:

22 ? 2! &gt; 2! X

(n + 1)n+1 ? (n + 1)! = (n + 1)
[
(n + 1)n ? n!

]

= (n + 1)
[((

n

0

)
nn +

(
n

1

)
nn?1 + · · ·+

(
n

n

))
? n!

]

= (n + 1)
[
nn ? n! +

((
n

1

)
nn?1 + · · ·+

(
n

n

))]

&gt; (n + 1)
[
n! +

(
n

1

)
nn?1 + · · ·+

(
n

n

)]

&gt; (n + 1)! X

a.2 m a x i m u m l i k e l i h o o d e s t i m at i o n f o r i . i . d. g au s s i a n s

Assume x1, x2, . . . , xm are independent and identically distributed Gaussian ran-

dom variables with xi ? N
(
µ, ?2

)
. The likelihood is given by the joint probability:

L(µ, ? | x1, x2, . . . , xm) = f(x1, x2, . . . , xm | µ, ?)

=
?

i

1

?
?
2?

exp
[
?
(xi ? µ)

2

2?2

]

=
(2?)?m/2

?m
exp

[
?

?
i(xi ? µ)

2

2?2

]

The log-likelihood is then:

ln L(µ, ? | x1, x2, . . . , xm) = ?
1

2
m ln 2? ? m ln ? ?

?
i(xi ? µ)

2

2?2

92



Deriving the log-likelihood w.r.t. µ and ? individually and setting to zero gives:

µ? =
1

m

?

i

xi and ??
2 =

1

m

?

i

(xi ? µ?)
2

Because the logarithm is monotonically increasing function, this is the maximum

likelihood estimation.

a.3 s y s t e m o f e q uat i o n s f o r d i s c r e t e l i n e a r i n v e r s e p ro b l e m s

Every parameter m = m1e1 + m2e2 + · · ·+ mnen ? M is a linear combination of
vectors in the basis. The forward operator satisfies superposition and scaling:

G(m) = m1G(e1) + m2G(e2) + · · ·+ mnG(en)

Let G be the matrix whose columns are the coordinates of G(e1), G(e2), . . . , G(en)

in the data space.

? d =

?
???G(e1) G(e2) · · · G(en)

?
???

?
??????

??????

m1

m2
...

mn

?
??????

??????

= Gm

a.4 m a x i m u m l i k e l i h o o d a n d l e a s t - s q ua r e s

Assume the observations have independent Gaussian errors di = (Gm)i + ?i with

?i ? N
(
0, ?2i

)
. The likelihood is given by:

L(m | d) =
(2?)?m/2
?

i ?i
exp

[
?
?

i

(di ? (Gm)i)
2

2?2
i

]

and the maximum occurs at:

arg max
m?M

L(m | d) = arg min
m?M

?

i

(di ? (Gm)i)
2

?2
i

It’s the least-squares estimate to Gm = d except for the 1/?2i factors. Scaling

the system of equations with W
def
= diag(1/?1, 1/?2, . . . , 1/?m), Gw = WG and

dw = Wd causes the least-squares to match the maximum likelihood estimate:

?dw ? Gwm?2L2 =
?

i

(dwi ? (Gwm)i)
2
=

?

i

(di ? (Gm)i)
2
/?2i

93



? arg max
m?M

L(m | d) = arg min
m?M

?dw ? Gwm?2L2

This is equivalent to incorporating data uncertainty Cd
def
= diag(?21, ?

2
2, . . . , ?

2
m)

into the objective (d ? Gm)?C?1
d

(d ? Gm).

a.5 w e i g h t e d l i n e a r l e a s t - s q ua r e s e s t i m at e

Let O(m) = (dobs ? Gm)
?C?1

d
(dobs ? Gm) be the objective function, it can be ex-

panded to:

O(m) = d?obsC
?1
d dobs ? d

?
obsC

?1
d Gm ? m

?G?C?1d dobs + m
?G?C?1d Gm

and the gradient w.r.t. m derived:

?m O(m) = ?G?(C?1d )
?dobs ? G

?C?1d dobs + 2G
?C?1d Gm

The inverse of the covariance is symmetric (C?1
d

)? = C?1
d

and then:

?m O(m) = ?2G?C?1d dobs + 2G
?C?1d Gm

By setting ?m O(m) = 0, the result follows if G?C?1d G is invertible:

mCd = (G
?C?1d G)

?1G?C?1d dobs

In particular, a proof for pure least-squares is obtained erasing all occurrences

of C?1
d

.

a.6 l e v e n b e r g - m a rq ua r d t g r a d i e n t a n d h e s s i a n

Let f(m) =
?m

i=1 fi(m)
2 with fi(m) =

G(m)i?di
?i

be the objective for the Newton-

Raphson update and F(m) = (f1(m), f2(m), . . . , fm(m))? the misfit vector. The

components of the gradient are given by:

?f(m)j =
m?

i=1

2?fi(m)jF(m)j

or in matrix notation with J(m) ??F(m) the Jacobian:

?f(m) = 2J(m)?F(m)

Similarly, consider the components of the Hessian:

H(m) = ?2f(m) =
m?

i=1

?2
(
fi(m)

2
)
=

m?

i=1

Hi(m)

94



The j, k entry of Hi(m) has the form:

Hij,k(m) =
?2 (fi(m)

2
)

?mj?mk

=
?

?mj

(
2fi(m)

?fi(m)

?mk

)

= 2

(
?fi(m)

?mj

?fi(m)

?mk
+ fi(m)

?2fi(m)

?mj?mk

)

or in matrix notation with Q(m)
def
= 2

?m
i=1 fi(m)?2fi(m):

H(m) = 2J(m)?J(m) + Q(m)

a.7 c o n d i t i o na l p ro b a b i l i t y b y c o n j u n c t i o n o f s tat e s

Let P(·) be a probability distribution with density f(x), and MB(·) the p-event of B
with density µB(x). Also, let µ(x) be the homogeneous probability density over X.

According to Definition 3.5, the conjunction (P ? MB)(·) is represented by density:

g(x) =
1

v

f(x) µB(x)

µ(x)

From Definition 3.6, the p-event nulls out g(x) outside B:

g(x) =

?
?

?

1
v
f(x), if x ? B

0, otherwise

The normalizing constant is (provided the integral is finite) v =
?

B
dx f(x) =

P(B). Thus, evaluating (P ? MB)(·) on any event A ? X is the classical ratio of
probabilities:

(P ? MB)(A) =

?

A

dx g(x) =
1

v

?

A?B
dx f(x) =

P(A?B)
P(B)

a.8 k e r n e l d e n s i t y e s t i m at i o n a s a c o n v o l u t i o n

For any batch of data X = {x1, x2, . . . , xn}, the empirical density function is defined

as the average of Dirac delta functions:

fX(x)
def
=

1

n

n?

i=1

?(x ? xi)

95



The empirical cumulative distribution, shown here for completeness, is derived by

remembering that
?

R
g(x)?(x)dx = g(0) for any function g:

?x

??

fX(y)dy =

?x

??

1

n

n?

i=1

?(y ? xi)dy

=
1

n

n?

i=1

?x

??

?(y ? xi)dy

=
1

n

n?

i=1

?

R

1(y 6 x) ?(y ? xi)dy

=
1

n

n?

i=1

1(xi 6 x)

= FX(x)

with 1 the indicator function. Given any other function k, the convolution with fX

is written by definition:

(fX ?k)(x)
def
=

?

R

fX(x ? y)k(y)dy

=

?

R

1

n

n?

i=1

?(x ? y ? xi)k(y)dy

=
1

n

n?

i=1

?

R

?(x ? y ? xi)k(y)dy

=
1

n

n?

i=1

k(xi ? x)

Let k(x) = Kh(?x) = Kh(x) be a symmetric kernel and the result follows:

(fX ?Kh)(x) =
1

n

n?

i=1

Kh(x ? xi)

96



B
C O D E S N I P P E T S

b.1 i t e r at i v e ly r e w e i g h t e d l e a s t - s q ua r e s

Listing B.1: lp_solve.m

1 % Copyright (c) 2013 Júlio Hoffimann Mendes

2 %

3 % This program is free software: you can redistribute it and/or modify

4 % it under the terms of the GNU General Public License as published by

5 % the Free Software Foundation, either version 3 of the License, or

6 % (at your option) any later version.

7

8 % usage: x = lp_solve (A,b,tol,maxiter,cutoff,p)

9 %

10 % Find the L_p solution to Ax=b by iterating least-squares.

11 function x = lp_solve (A,b,p,tol,maxiter,cutoff)

12

13 iter = 1; x = A\b; % least-squares estimate as initial guess

14 do

15 r = abs (A*x-b); % current residual

16 r(r &amp;lt;cutoff) = cutoff; % fix small entries

17 R = diag (r.^(p-2));

18

19 newx = (A’*R*A)\(A’*R*b); % solution to the weighted system

20 err = norm (newx-x)/(1+norm (x));

21

22 x = newx;

23 until (err &amp;lt;tol || ++iter &gt; maxiter)

24

25 endfunction

26 % References:

27 % SCALES, J. A.; GERSZTENKORN, A; TREITEL, S., 1988. Fast Lp Solution

28 % of Large, Sparse, Linear Systems: Application to Seismic Travel

29 % Time Tomography.

97



b.2 l e a s t a b s o l u t e s h r i n k ag e a n d s e l e c t i o n o p e r at o r

Listing B.2: lasso.m

1 % Copyright (c) 2013 Júlio Hoffimann Mendes

2 %

3 % This program is free software: you can redistribute it and/or modify

4 % it under the terms of the GNU General Public License as published by

5 % the Free Software Foundation, either version 3 of the License, or

6 % (at your option) any later version.

7

8 % usage: x = lasso (A,b,d,tol,maxiter)

9 %

10 % Find the minimum of (Ax-b)’*(Ax-b) + d^2 ||x||_1.

11 function x = lasso (A,b,d,tol,maxiter)

12

13 iter = 1; n = columns (A); a = 2*sum (A.^2);

14 x = (A’*A + d^2 * eye (n)) \ A’*b; % Ridge estimate as initial guess

15 do

16 oldx = x;

17 for j = 1:n

18 cj = 2*A(:,j)’*(b - A*x + x(j)*A(:,j));

19

20 % subderivative for |xj|

21 if (cj &amp;lt;-d^2)

22 x(j) = (cj + d^2) / a(j);

23 elseif (cj &gt; d^2)

24 x(j) = (cj - d^2) / a(j);

25 else

26 x(j) = 0;

27 endif

28 endfor

29 err = norm (x-oldx) / (1 + norm (oldx));

30 until (err &amp;lt;tol || ++iter &gt; maxiter)

31

32 endfunction

33 % References:

34 % HASTIE, T.; TIBSHIRANI, R.; FRIEDMAN, J., 2009. The Elements of

35 % Statistical Learning: Data Mining, Inference, and Prediction.

98



b.3 m e t ro p o l i s a l g o r i t h m

Listing B.3: metropolis.R

1 # Copyright (c) 2013 Júlio Hoffimann Mendes

2 #

3 # This program is free software: you can redistribute it and/or modify

4 # it under the terms of the GNU General Public License as published by

5 # the Free Software Foundation, either version 3 of the License, or

6 # (at your option) any later version.

7

8 # usage: x = metropolis (pi,N,s)

9 #

10 # Generate N samples from the target density pi using Metropolis-Hastings

11 # with a symmetric proposal q(x,y) = Gaussian(y;x,s).

12 metropolis&amp;lt;- function(pi,N,s) {

13 # initialize the chain

14 x&amp;lt;- numeric(N); x[1]&amp;lt;- 0

15

16 for (n in 1:(N-1)) {

17 # propose a move xn --&gt; y

18 y&amp;lt;- rnorm(1,x[n],s)

19

20 # acceptance probability

21 alpha&amp;lt;- min(1,pi(y)/pi(x[n]))

22

23 if (runif(1) &amp;lt;alpha)

24 x[n+1]&amp;lt;- y

25 else

26 x[n+1]&amp;lt;- x[n]

27 }

28

29 return(x)

30 }

31 # References:

32 # GAMERMAN, D.; LOPES, H. F., 2006. Markov Chain Monte Carlo:

33 # Stochastic Simulation for Bayesian Inference.

99



b.4 o n l i n e b ay e s i a n i n v e r s i o n

Listing B.4: online inversion.py

1 # -*- coding: utf8 -*-

2 # Copyright (c) 2013 Júlio Hoffimann Mendes

3 #

4 # This program is free software: you can redistribute it and/or modify

5 # it under the terms of the GNU General Public License as published by

6 # the Free Software Foundation, either version 3 of the License, or

7 # (at your option) any later version.

8

9 from pymc import *

10 import matplotlib.pyplot as plt

11

12 prior = runiform(-10,10,10000) # initial guess

13

14 while True: # online inversion

15 x = stochastic_from_data( ’x ’, prior)

16 y = x*x

17 obs = Normal( ’ obs ’, y, .1, 4 + rnormal(0,1), observed=True)

18

19 model = Model([x,y,obs])

20 mcmc = MCMC(model)

21 mcmc.sample(10000)

22

23 posterior = mcmc.trace( ’x ’)

24 Matplot.plot(posterior)

25 plt.show()

26

27 prior = posterior[:]

100



b.5 h i s t o g r a m f i t t i n g w i t h k e r n e l d e n s i t y e s t i m at i o n

Listing B.5: pymc patch.py

1 # -*- coding: utf8 -*-

2 # Copyright (c) 2013 Júlio Hoffimann Mendes

3 #

4 # This program is free software: you can redistribute it and/or modify

5 # it under the terms of the GNU General Public License as published by

6 # the Free Software Foundation, either version 3 of the License, or

7 # (at your option) any later version.

8

9 import numpy as np

10 from scipy.stats.kde import gaussian_kde

11 from pymc import Stochastic

12

13 def stochastic_from_data(name, data, lower=-np.inf, upper=np.inf,

14 value=None, observed=False, trace=True,

15 verbose=-1, debug=False):

16 """

17 Return a Stochastic subclass made from arbitrary data.

18

19 The histogram for the data is fitted with Kernel Density Estimation.

20

21 :Parameters:

22 - ‘data‘ : An array with samples (e.g. trace[:])

23 - ‘lower‘ : Lower bound on possible outcomes

24 - ‘upper‘ : Upper bound on possible outcomes

25

26 :Example:

27 &gt;&gt;&gt; from pymc import stochastic_from_data

28 &gt;&gt;&gt; pos = stochastic_from_data(’posterior’, posterior_samples)

29 &gt;&gt;&gt; prior = pos # update the prior with arbitrary distributions

30

31 :Alias:

32 Histogram

33 """

34 pdf = gaussian_kde(data) # automatic bandwidth selection

35

36 # account for tail contribution

37 lower_tail = upper_tail = 0.

38 if lower &gt; -np.inf: lower_tail = pdf.integrate_box(-np.inf, lower)

39 if upper &amp;lt;np.inf: upper_tail = pdf.integrate_box(upper, np.inf)

40 factor = 1./(1. - (lower_tail + upper_tail))

41

42 def logp(value):

43 prob = factor*pdf(value)

44 if value &amp;lt;lower or value &gt; upper:

101



45 return -np.inf

46 elif prob&amp;lt;= 0.:

47 return -np.inf

48 else:

49 return np.log(prob)

50

51 def random():

52 res = pdf.resample(1)[0][0]

53 while res &amp;lt;lower or res &gt; upper:

54 res = pdf.resample(1)[0][0]

55 return res

56

57 if value == None: value = random()

58

59 return Stochastic(logp = logp,

60 doc = ’Non?parametric density with Gaussian Kernels . ’,

61 name = name,

62 parents = {},

63 random = random,

64 trace = trace,

65 value = value,

66 dtype = float,

67 observed = observed,

68 verbose = verbose)

69

70 # Alias following Stochastics naming convention

71 Histogram = stochastic_from_data

102



C
K E R N E L P C A

Consider an ensemble in the form of a matrix X ? RNf×Nr with Nf the number
of features (e. g. number of cells in the reservoir model) and Nr the number of

realizations. Each column xj, j = 1, 2, . . . , Nr is in a very high-dimensional space

Nf ? Nr. Consider also the ensemble is centered (i. e.
?

j xj = 0) for the moment.

Linear PCA in its primal form diagonalizes the covariance matrix C = 1
Nr

?
j xjx

?
j

as an ordinary eigenproblem:

?v = Cv

with eigenvalues ? &gt; 0 and where every solution v ? RNf \ {0} lies in the span of
the ensemble x1, x2, . . . , xNr . This argument is made clear by writing:

Cv =
(?

j

xjx
?
j

)
v =

?

j

xj(x
?
j v)

as a linear combination of realizations. The direct conclusion from this fact is that

?v = Cv is equivalent to:

?x?k v = x
?
k Cv, for all k = 1, 2, . . . , Nr

Repeat the previous steps now in a feature space F reached by some mapping

? : RNf 7?? F. The covariance C? = 1
Nr

?
j ?(xj)?(xj)

? is decomposed with eigen-

values ?? &gt; 0 and eigenvectors v? in the span of ?(x1), ?(x2), . . . , ?(xNr):

???(xk)
?v? = ?(xk)

?C?v? for all k = 1, 2, . . . , Nr

Substitute v? =
?

i ?i?(xi) and C? =
1
Nr

?
j ?(xj)?(xj)

? in the previous equation

to get for all k = 1, 2, . . . , Nr:

??
?

i

?(xk)
??(xi) =

1

Nr

?

i

?i
(
?(xk)

?
?

j

?(xj)
)(
?(xj)

??(xi)
)

103



and define Kij
def
= ?(xi)

??(xj) the kernel Gramian matrix for compact notation:

Nr??K? = K
2
?

As K ? RNr×Nr is symmetric, the coordinates ? def= (?1, ?2, . . . , ?Nr)? of the
eigenvector v? ? F \ {0} in the ensemble basis ?(x1), ?(x2), . . . , ?(xNr) are found
with the now tractable ordinary eigenproblem:

Nr??? = K?

Thus, the intractable primal problem of diagonalizing C ? RNf×Nf is replaced
by the tractable dual problem of diagonalizing K ? RNr×Nr [81].

Note the kernel Gramian matrix only depends on inner products ?(xi)??(xj)

between points in the ensemble. This is crucial because ? : RNf 7?? F itself is
generally expensive to evaluate or is possibly unknown, dismissing the whole pur-

pose of the method. The “kernel trick” replaces inner products by mind-created

functions k(xi, xj) that are valid (i. e. ??, k(xi, xj) = ?(xi)??(xj)) and that can be
easily computed. Such functions can be thought as a measure of similarity between

two images xi, xj ? RNf , for instance the dot product x?i xj (i. e. linear PCA).
The sections below describe all the steps for implementing kPCA. The algorithm

is divided into two (or three) subroutines that correspond to training and actually

doing prediction. The initial assumption that observations should be centered is

dropped with an additional step performed in the feature space.

c.1 k e r n e l g r a m i a n m at r i x

The polynomial kernel k(x, y)
def
= ?x, y?+ ?x, y?2 + · · ·+ ?x, y?d [70, 88] is evaluated

at the columns of X ? RNf×Nr without any assumption about centering. The
resultant Gramian matrix K ? RNr×Nr is symmetric positive semidefinite:

Kij
def
= k(xi, xj)

104



c.2 c e n t e r i n g i n t h e f e at u r e s pac e

Up to this point, mapped observations were assumed to be centered in F, that is,
?

j ?(xj) = 0. Consider the more general case where this assumption does not

hold true and center the data explicitly as in:

?0(xj)
def
= ?(xj) ?

1

Nr

?

j

?(xj)

This new ensemble ?0(x1), ?0(x2), . . . , ?0(xNr) is such that the covariance or

dot product matrix K?ij = ?0(xi)??0(xj) is in accordance with the derivation in

the beginning of this appendix:

???? = K???

with v? =
?

j ??j?0(xj) written in terms of centered observations. Perfect, except

that the function ? : RNf 7?? F isn’t actually available.
The centered Gramian K? is obtained from its non-centered version K using some

algebraic manipulations instead:

K?ij = ?0(xi)
??0(xj)

=

(
?(xi) ?

?

m

?(xm)

)?(
?(xj) ?

?

n

?(xn)

)

= ?(xi)
??(xj) ?

1

Nr

?

m

?(xm)
??(xj) ?

1

Nr

?

n

?(xi)
??(xn) +

1

Nr
2

?

m,n

?(xm)
??(xn)

or in matrix form with 1Nr
def
= 1

Nr
I ? RNr×Nr :

K? = K ? 1Nr K ? K1Nr + 1Nr K1Nr

Thus, after K is computed using the kernel function k(x, y), it’s centered with

the formula derived above. No assumption about centering in the original space

is required and the ensemble X ? RNf×Nr is used as is. The centered K? is still
symmetric positive semidefinite:

K? =
[
?0(x1) ?0(x2) · · · ?0(xNr)

]?[
?0(x1) ?0(x2) · · · ?0(xNr)

]

105



c.3 e i g e n p ro b l e m a n d n o r m a l i z at i o n

The ordinary symmetric eigenproblem ???? = K??? is solved with well-tested linear

algebra software and the solutions ?? are normalized so that the corresponding

eigenvectors v? ? F satisfy v??v? = 1:

1 = v??v?

=

(
?

i

??i?0(xi)

)?(
?

i

??i?0(xi)

)

=
?

i,j

??i??j?0(xi)
??0(xj)

=
?

i,j

= ??i??jK?ij

= ???(K???)

= ???????

If the eigenproblem solver (e. g. LAPACK) returns ?? such that ????? = 1, it

remains to scale the basis as:

A
def
=

[
??1?
??1

??2?
??2
· · · ??Nc?

??Nc

]

with 1 6 Nc 6 min(Nf, Nr) the desired number of components to be retained.

The matrix A ? RNr×Nc is saved as the only information necessary for future
reconstructions.

This first training part of kPCA is independent of the kernel function family

being used (e. g. polynomial, sigmoid), and with A stored in memory, there are at

least two possible goals:

c o m p r e s s i n g Express an image of size Nf in terms of Nc ? Nf components

d e n o i s i n g Purge undesired modes in a valid image of size Nf

Dimensionality reduction is achieved by compressing the data in the feature

space F. The problem of reconstructing a valid preimage x ? RNf in the original
space, given the compression coordinates of ?(x) ? F is known as the preimage
problem. It’s formulated and “solved” next for the polynomial kernel family.

106



c.4 p r e i m ag e p ro b l e m

Consider a point x ? RNf is projected onto a normalized eigenbasis v1, . . . , vNc ? F,
that is, its image ?(x) ? F is approximately written:

?(x) ? Proj(x) def=
Nc?

k=1

?kvk

with ?k = ??(x), vk? the dual coefficients. The eigenbasis of interest is stored in
the columns of A =

[
?1 ?2 · · · ?Nc

]
in terms of the mapped ensemble, and is

never computed explicitly:

vk =

Nr?

i=1

?ki ?(xi) for k = 1, 2, . . . , Nc

Given the projection coordinates ? = (?1, ?2, . . . , ?Nc)
?, how to reconstruct

the corresponding preimage x ? RNf ? This ill-posed inverse problem is usually
“solved” by minimizing the distance to the projection in F:

x? = arg min
z?RNf

?Proj(x) ? ?(z)?2

Such objective function can be rewritten in terms of the kernel k(x, y) by simple

substitution of the expressions for Proj(x) and vk:

?(z)
def
= ?Proj(x) ? ?(z)?2

= k(z, z) ? 2
Nc?

k=1

?k

Nr?

i=1

?ki k(xi, z) + C

with C a term that doesn’t depend on z ? RNf . In matrix form, by dropping the
constant and defining Kz

def
= (k(x1, z), k(x2, z), . . . , k(xNr , z))

?:

?(z) = k(z, z) ? 2??A?Kz

the only term that needs further work is ?. It can also be written in terms of the

(symmetric) kernel:

?k = ??(x), vk? =
Nr?

i=1

?ki k(xi, x) = ?
k?Kx for k = 1, 2, . . . , Nc

with Kx evaluated at x the same way Kz is evaluated at z. Thus, ? = A?Kx and

finally:

?(z) = k(z, z) ? 2K?x AA
?Kz

A row vector b?
def
= ?

?A? = K?x AA
? is defined to simplify expressions in the

following proofs.

107



c.4.1 Linear kernel

The objective function for the linear kernel k(x, y)
def
= x?y is given by:

?(z) = k(z, z) ? 2b?Kz

= z?z ? 2b?X?z

Take the gradient w.r.t. z and equate to zero:

?z?(z) = 2z ? 2Xb = 0

The preimage x? ? RNf is therefore a linear combination of the ensemble X:

x? = arg min
z?RNf

?(z) = Xb

The coefficients b might be normalized before multiplication (i. e.
?

i bi = 1) to

honor hard data (e. g. well logs).

c.4.2 Monomial kernel

The objective for the monomial kernel k(x, y)
def
= (x?y)d is given by:

?(z) = k(z, z) ? 2b?Kz

= (z?z)d ? 2b?(X?z)d

where (X?z)d is entrywise pow(). Take the gradient w.r.t. z and equate to zero:

?z?(z) = 2d(z?z)d?1z ? 2dX diag
(
(X?z)d?1

)
b = 0

There is no closed-form solution for the equation above. It’s approximated with

fixed-point iteration z = f(z):

z = X diag

((
X?z
z?z

)d?1)
b

At each iteration, the preimage is still a linear combination of the ensemble (i. e.

z = Xc) where coefficients are normalized to honor hard data. Note the linear

kernel is recovered for d = 1.

108



c.4.3 Polynomial kernel

The objective for the polynomial kernel k(x, y)
def
= x?y + (x?y)2 + · · ·+ (x?y)d is

given by:

?(z) = k(z, z) ? 2b?Kz

= z?z + (z?z)2 + · · ·+ (z?z)d

? 2b?
(

X?z + (X?z)2 + · · ·+ (X?z)d
)

Take the gradient w.r.t. z and equate to zero:

?z?(z) = 2
(
1 + 2z?z + 3(z?z)2 + · · ·+ d(z?z)d?1

)
z

? 2X

(
I + 2 diag

(
X?z

)
+ 3 diag

(
(X?z)2

)
+ · · ·+ d diag

(
X?z

)d?1)
b = 0

Again, there is no closed-form solution to the equation above. It’s approximated

with fixed-point iteration z = f(z):

z = X diag
(

I + 2X?z + · · ·+ d(X?z)d?1
1 + 2z?z + · · ·+ d(z?z)d?1

)
b

The preimage is a linear combination of the ensemble X where coefficients are

normalized to honor the hard data. Previous solutions for linear and monomial

kernels are trivially recovered.

Regardless of the kernel, the difference between compressing and denoising resides

in the column vector b:

b
def
= A? = AA?Kx

In denoising, a valid image x ? RNf is mapped to the feature space with Kx
and then b = AA?Kx is computed. In compressing, the (uncorrelated) coordinates

? ? RNc are given and b = A? is used instead.
kPCA is used in this dissertation basically for compressing permeability maps

x = x(?), but denoising might also be used as an optional post-processing step.

i m p o r ta n t n o t e : For all kernels here discussed, the preimage is an affine

combination of the ensemble (i. e. a linear combination with coefficients adding up

to unit), but it isn’t convex. This means realizations might have values outside the

original range of the data (e. g. negative permeabilities) that should be truncated if

necessary.

109



B I B L I O G R A P H Y

[1] Júlio H. Mendes, Ramiro B. Willmersdorf, and Ézio R. Araújo. Ajuste de

histórico como problema inverso e conexão com geostatística. In I Workshop

dos Programas de Formação de Recursos Humanos nas Áreas de Petróleo, Gás, Bio-

combustíveis, Naval e Offshore da UFPE, Av. Prof. Moraes Rego, 1235, Março

2013a. UFPE. (Cited on page viii.)

[2] Júlio H. Mendes, Ramiro B. Willmersdorf, and Ézio R. Araújo. Multiple-point

statistics and kpca parametrization for reservoir characterization. Seminários

Periódicos PRH-26/ANP/UFPE, May 2013b. (Cited on page viii.)

[3] Júlio H. Mendes, Ramiro B. Willmersdorf, and Ézio R. Araújo. Ajuste de

histórico sob ótica bayesiana - caracterização geostatística de múltiplos pontos

e inversão quasi-contínua via markov chain monte carlo. Proposta de tese

apresentada no CENPES, Junho 2013c. (Cited on page viii.)

[4] Richard C. Aster, Brian Borchers, and Clifford H. Thurber. Parameter Estima-

tion and Inverse Problems (International Geophysics). Academic Press, 2005. ISBN

0120656043. URL http://www.ees.nmt.edu/outside/courses/GEOP529_book.

html. (Cited on pages 3, 13, 17, and 26.)

[5] Vladimir N. Vapnik. Statistical Learning Theory. Wiley-Interscience, 1998. ISBN

0471030031. (Cited on page 3.)

[6] José Paulo de Almeida e Albuquerque, José Mauro Pedro Fortes, and

Weiler Alves Finamore. Probabilidade, Variáveis Aleatórias e Processos Estocás-

ticos. Técnico-científica n.8. PUC-Rio, Rua Marquêz de São Vicente, 225 - Casa

Editora Gávea, RJ, 22.451-900, 2008. URL http://www.editora.vrc.puc-rio.

br/probabilidade.html. (Cited on pages 3 and 19.)

[7] P. K. Kitanidis. Introduction to Geostatistics: Applications in Hydrogeology

(Stanford-Cambridge Program). Cambridge University Press, 1997. ISBN

0521587476. (Cited on page 3.)

110

http://www.ees.nmt.edu/outside/courses/GEOP529_book.html
http://www.ees.nmt.edu/outside/courses/GEOP529_book.html
http://www.editora.vrc.puc-rio.br/probabilidade.html
http://www.editora.vrc.puc-rio.br/probabilidade.html


b i b l i o g r a p h y 111

[8] Ricardo A. Olea. Geostatistics for Engineers and Earth Scientists. Springer, 1999.

ISBN 0792385233. (Cited on page 3.)

[9] Nicolas Remy, Alexandre Boucher, and Jianbing Wu. Applied Geostatistics with

SGeMS: A User’s Guide. Cambridge University Press, 2011. ISBN 1107403243.

(Cited on page 3.)

[10] Yongshe Liu. Geostatistical Integration of Linear Coarse Scale and Fine Scale Data.

PhD thesis, Department of Petroleum Engineering - Stanford University, 2007.

(Cited on page 4.)

[11] Scarlet A. Castro. A Probabilistic Approach to Jointly Integrate 3D/4D Seismic, Pro-

duction Data and Geological Information for Building Reservoir Models. PhD thesis,

Department of Energy Resources - Stanford University, June 2007. DOWN-

LOAD. (Cited on pages 4 and 58.)

[12] Shuguang Mao and André G. Journel. Generation of a reference petrophysi-

cal/seismic data set: the stanford v reservoir. Technical report, Stanford Uni-

versity, 1999. (Cited on page 4.)

[13] Jean-Paul Chilès and Pierre Delfiner. Geostatistics: Modeling Spatial Uncer-

tainty (Wiley Series in Probability and Statistics). Wiley, 2012. ISBN 0470183152.

URL http://onlinelibrary.wiley.com/doi/10.1002/9780470316993.refs/

summary. (Cited on pages 4 and 66.)

[14] Nicolas Remy. Gstl: The geostatistical template library in c++. Master’s thesis,

Stanford University, March 2001. (Cited on page 4.)

[15] Thomas M. Hansen, Klaus Mosegaard, and Knud S. Cordua. Using geostastis-

tics to describe complex a priori information for inverse problems. In Julián M.

Ortiz and Xavier Emery, editors, Proceedings of the Eighth International Geo-

statistics Congress, pages 329–338. University of Copenhagen, 2008. (Cited on

pages 4 and 69.)

[16] Thomas M. Hansen and Klaus Mosegaard. Visim: Sequential simulation for

linear inverse problems. Computers &amp;amp; Geosciences, (34):53–76, March 2008.

(Cited on page 4.)

[17] Thomas M. Hansen, André G. Journel, Albert Tarantola, and Klaus

Mosegaard. Linear inverse gaussian theory and geostatistics. Geophysics, 71

(6), December 2006. (Cited on page 4.)

http://pangea.stanford.edu/departments/ere/dropbox/scrf/documents/Theses/SCRF-Theses/2000-2009/2007_PhD_Castro.pdf
http://pangea.stanford.edu/departments/ere/dropbox/scrf/documents/Theses/SCRF-Theses/2000-2009/2007_PhD_Castro.pdf
http://onlinelibrary.wiley.com/doi/10.1002/9780470316993.refs/summary
http://onlinelibrary.wiley.com/doi/10.1002/9780470316993.refs/summary


b i b l i o g r a p h y 112

[18] Adalberto José Rosa, Renato de Souza Carvalho, and José Augusto Daniel

Xavier. Engenharia de Reservatórios de Petróleo. Interciência Ltda., Rua

Verna Magalhães, 66 – Engenho Novo – RJ, 2006. URL http://www.

editorainterciencia.com.br/index.asp?pg=prodDetalhado.asp&amp;amp;idprod=82.

(Cited on page 5.)

[19] Tiago Almeida Costa. Aplicação do método dos elementos finitos (mef) para

modelos de testes de formação em poços de petróleo. Master’s thesis, UNI-

CAMP, Março 2013. (Cited on page 5.)

[20] Faruk O. Alpack, Carlos Torres-Verdín, and Kamy Sepehrnoori. Estima-

tion of axisymmetric spatial distributions of permeability and porosity from

pressure-transient data acquired with in situ permanent sensors. Journal of

Petroleum Science and Engineering, 44:231–267, March 2004. (Cited on page 5.)

[21] Jacques Hadamard. La Theorie Des Equations Aux Derivees Partielles. Imprime

En Chine, 1964. (Cited on page 6.)

[22] Albert Tarantola. Inverse Problem Theory and Methods for Model Parameter Esti-

mation. SIAM: Society for Industrial and Applied Mathematics, 2004. ISBN

0898715725. URL http://www.ec-securehost.com/SIAM/ot89.html. (Cited

on pages 9, 10, 29, 33, 36, and 41.)

[23] Andreas Kirsch. An Introduction to the Mathematical Theory of In-

verse Problems (Applied Mathematical Sciences, Vol. 120). Springer, 2011.

ISBN 1441984739. URL http://www.springer.com/mathematics/dynamical+

systems/book/978-1-4419-8473-9. (Cited on page 15.)

[24] Dean S. Oliver, Albert C. Reynolds, and Ning Liu. Inverse Theory for Petroleum

Reservoir Characterization and History Matching. Cambridge University Press,

2008. ISBN 052188151X. URL http://www.cambridge.org/gb/knowledge/

isbn/item1174799. (Cited on pages 15 and 26.)

[25] John A. Scales, Adam Gersztenkorn, and Sven Treitel. Fast lp solution of large,

sparse, linear systems: Application to seismic travel time tomography. Journal

of Computational Physics, 75:314–333, 1988. (Cited on page 17.)

[26] Justyna K. Przybysz-Jarnut, Remus G. Hanea, Jan-Dirk Jansen, and Arnold W.

Heemink. Application of the representer method for parameter estimation in

numerical reservoir models. Computers &amp;amp; Geosciences, 2007. (Cited on page 19.)

http://www.editorainterciencia.com.br/index.asp?pg=prodDetalhado.asp&amp;amp;idprod=82
http://www.editorainterciencia.com.br/index.asp?pg=prodDetalhado.asp&amp;amp;idprod=82
http://www.ec-securehost.com/SIAM/ot89.html
http://www.springer.com/mathematics/dynamical+systems/book/978-1-4419-8473-9
http://www.springer.com/mathematics/dynamical+systems/book/978-1-4419-8473-9
http://www.cambridge.org/gb/knowledge/isbn/item1174799
http://www.cambridge.org/gb/knowledge/isbn/item1174799


b i b l i o g r a p h y 113

[27] Zhao Hui, Li Yang, Yao Jun, and Zhang Kai. Theoretical research on reser-

voir closed-loop production management. SCIENCE CHINA - Technological

Sciences, 54(10):2815–2824, 2011. (Cited on page 19.)

[28] Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge

University Press, 2004. ISBN 9780521833783. URL http://www.stanford.edu/

~boyd/cvxbook. (Cited on page 22.)

[29] Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Sta-

tistical Learning: Data Mining, Inference, and Prediction, Second Edition (Springer

Series in Statistics). Springer, 2009. ISBN 0387848576. URL http://www-stat.

stanford.edu/~tibs/ElemStatLearn. (Cited on page 23.)

[30] Ryan J. Tibshirani. The lasso problem and uniqueness. Eletronic Journal of

Statistics, 7:1456–1490, May 2013. (Cited on page 23.)

[31] Andrew M. Bradley. Pde-constrained optimization and the adjoint method,

June 2013. DOWNLOAD. (Cited on page 26.)

[32] Ruijian Li, A. C. Reynolds, and D. S. Oliver. History matching of three-phase

flow production data. SPE Journal, December 2003. (Cited on page 26.)

[33] Zhan Wu, A. C. Reynolds, and D. S. Oliver. Conditioning geostatistical models

to two-phase production data. SPE Journal, 4, June 1999. (Cited on page 26.)

[34] Jonathan Richard Shewchuk. An introduction to the conjugate gradient

method without the agonizing pain. Carnegie Mellon University - Pittsburg,

PA 15213, August 1994. DOWNLOAD. (Cited on page 26.)

[35] Alexandre Anozé Emerick. History Matching and Uncertainty Characteriza-

tion using Ensemble-based Methods. PhD thesis, The university of Tulsa, 2012.

DOWNLOAD. (Cited on pages 27 and 54.)

[36] Alexandre A. Emerick and Albert C. Reynolds. Investigation of the sampling

performance of ensemble-based methods with a simple reservoir model. Com-

putational Geosciences, November 2012. (Cited on pages 27 and 54.)

[37] P. Sarma and W. H. Chen. Generalization of the ensemble kalman filter using

kernels for non-gaussian random fields. SPE Journal, (119177), February 2009.

(Cited on page 27.)

http://www.stanford.edu/~boyd/cvxbook
http://www.stanford.edu/~boyd/cvxbook
http://www-stat.stanford.edu/~tibs/ElemStatLearn
http://www-stat.stanford.edu/~tibs/ElemStatLearn
http://www.stanford.edu/~ambrad/adjoint_tutorial.pdf
http://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf
http://www.tuprep.utulsa.edu/comparative_study/Emerick_PhD.pdf


b i b l i o g r a p h y 114

[38] Dongxiao Zhang, Heng Li, and Haibin Chang. History matching for non-

gaussian random fields using the probabilistic collocation based kalman filter.

SPE Journal, 2011. (Cited on page 27.)

[39] Júlio Hoffimann Mendes and Ramiro Brito Willmersdorf. Sparse probabilis-

tic collocation for uncertainty quantification in reservoir engineering. 10-th

World Congress on Computational Mechanics, July 2012. DOWNLOAD. (Cited

on pages 27 and 42.)

[40] Nir Friedman and Joseph Y. Halpern. Plausibility measures: A user’s guide.

In Eleventh Conference on Uncertainty in Artificial Intelligence, volume 95, pages

175–184, 1995. DOWNLOAD. (Cited on page 29.)

[41] Nir Friedman and Joseph Y. Halpern. Plausibility measures and default rea-

soning. ACM Journal, 48(4):648–685, 2001. DOWNLOAD. (Cited on page 29.)

[42] Albert Tarantola. Logarithmic parameters, November 2001. DOWNLOAD.

(Cited on page 30.)

[43] Albert Tarantola. Inverse Problem Theory, chapter List of Errata. SIAM: Society

for Industrial and Applied Mathematics, July 2005. DOWNLOAD. (Cited on

page 34.)

[44] Klaus Mosegaard and Albert Tarantola. Probabilistic Approach to Inverse Prob-

lems, chapter Part A, pages 237–265. Academic Press, November 2002. DOWN-

LOAD. (Cited on page 35.)

[45] Nitin Agarwal and N. R. Aluru. A domain adaptive stochastic collocation

approach for analysis of mems under uncertaintes. Journal of Computational

Physics, 2009. (Cited on page 42.)

[46] M. S. Eldred, C. G. Webster, and P. G. Constantine. Evaluation of non-intrusive

approaches for wiener-askey generalized polynomial chaos. American Institute

of Aeronautics and Astronautics, 2008. (Cited on page 42.)

[47] Thomas Gerstner and Michael Griebel. Numerical integration using sparse

grids. (Cited on page 42.)

[48] Sergey Smolyak. Quadrature and interpolation formulas for tensor products

of certain classes of functions. Doklady Akademii Nauk SSSR, 1963. (Cited on

page 42.)

http://www.academia.edu/1956459/Sparse_Probabilistic_Collocation_for_Uncertainty_Quantification_in_Reservoir_Engineering
http://robotics.stanford.edu/people/nir/Papers/uai95.pdf
http://robotics.stanford.edu/people/nir/Papers/FrH5Full.pdf
http://www.ipgp.fr/~tarantola/Files/Professional/Papers_PDF/Music.pdf
http://www.ipgp.fr/~tarantola/Files/Professional/Books/ErrataIP/Errata.pdf
http://www.ipgp.fr/~tarantola/Files/Professional/Papers_PDF/InverseProblemHandbk.pdf
http://www.ipgp.fr/~tarantola/Files/Professional/Papers_PDF/InverseProblemHandbk.pdf


b i b l i o g r a p h y 115

[49] Dani Gamerman and Hedibert F. Lopes. Markov Chain Monte Carlo: Stochastic

Simulation for Bayesian Inference, Second Edition (Chapman &amp;amp; Hall/CRC Texts in

Statistical Science). Chapman and Hall/CRC, 2006. ISBN 1584885874. URL

http://www.crcpress.com/product/isbn/9781584885870. (Cited on pages 42

and 51.)

[50] Christophe Andrieu, Nando de Freitas, Arnaud Doucet, and Michael I. Jordan.

An introduction to mcmc for machine learning. Machine Learning, (50):5–43,

2003. DOWNLOAD. (Cited on page 43.)

[51] W. K. Hastings. Monte carlo sampling methods using markov chains and

their applications. Biometrika, 57(1):97–109, April 1970. DOWNLOAD. (Cited

on page 45.)

[52] Nicholas Metropolis, Arianna W. Rosenbluth, Marshall N. Rosenbluth, Au-

gusta H. Teller, and Edward Teller. Equation of state calculations by fast com-

puting machines. The Journal of Chemical Physics, 21(6):1087–1092, June 1953.

(Cited on page 46.)

[53] Jaakoo Peltonen, Jarkoo Venna, and Samuel Kaski. Visualization for assessing

convergence and mixing of markov chain monte carlo simulations. Compu-

tational Statistics and Data Analysis, 53(12):4453–4470, October 2009. DOWN-

LOAD. (Cited on page 49.)

[54] Andrew Gelman and Donald B. Rubin. Inference from iterative simulation

using multiple sequences. Statistical Science, 7(4):457–511, 1992. DOWNLOAD.

(Cited on page 49.)

[55] Mary Kathryn Cowles and Bradley P. Carlin. Markov chain monte carlo con-

vergence diagnostics: A comparative review. Journal of the American Statistical

Association, 91(434):883–904, June 1996. (Cited on page 49.)

[56] Emanuel Parzen. On estimation of a probability density function and mode.

The Annals of Mathematical Statistics, 33(3):1065–1076, 1962. DOWNLOAD.

(Cited on pages 50 and 68.)

[57] George R. Terrell and David W. Scott. Variable kernel density estimation. The

Annals of Statistics, 20(3):1236–1265, 1992. DOWNLOAD. (Cited on page 50.)

http://www.crcpress.com/product/isbn/9781584885870
http://www.cs.ubc.ca/~nando/papers/mlintro.pdf
http://www.jstor.org/stable/2334940
http://www.cis.hut.fi/projects/mi/papers/csda09_preprint.pdf
http://www.cis.hut.fi/projects/mi/papers/csda09_preprint.pdf
http://www.stat.columbia.edu/~gelman/research/published/itsim.pdf
http://projecteuclid.org/euclid.aoms/1177704472
http://projecteuclid.org/euclid.aos/1176348768


b i b l i o g r a p h y 116

[58] Jonathan Goodman and Jonathan Weare. Ensemble samplers with affine in-

variance. Communications in Applied Mathematics and Computational Science, 5

(1):65–80, 2010. DOWNLOAD. (Cited on pages 52 and 53.)

[59] J. Andrés Christen and Colin Fox. A general purpose sampling algorithm for

continuous distributions (the t-walk). Bayesian Analysis, 5(2):263–282, Decem-

ber 2010. DOWNLOAD. (Cited on page 52.)

[60] D. Foreman-Mackey, D. W. Hogg, D. Lang, and J. Goodman. emcee: The

MCMC Hammer. 125:306–312, March 2013. doi: 10.1086/670067. DOWN-

LOAD. (Cited on page 53.)

[61] D. J. Earl and M. W. Deem. Parallel tempering: Theory, applications, and new

perspectives. Physical Chemistry Chemical Physics (Incorporating Faraday Trans-

actions), 7, 2008. doi: 10.1039/b509983h. DOWNLOAD. (Cited on page 54.)

[62] Radford M. Neal. Annealed importance sampling. Technical Report 9805,

University of Toronto, Ontario, CA, September 1998. DOWNLOAD. (Cited

on page 54.)

[63] Thomas Romary. Integrating production data under uncertainty by parallel

interacting markov chains on a reduced dimensional space. Journal of Compu-

tational Geosciences, 13:103–122, 2009. (Cited on page 54.)

[64] A. Malakis and T. Papakonstantinou. Comparative study of selected parallel

tempering methods. 88(1), 2013. doi: 10.1103/PhysRevE.88.013312. DOWN-

LOAD. (Cited on page 54.)

[65] Alexandre A. Emerick and Albert C. Reynolds. History matching time-lapse

seismic data using the ensemble kalman filter with multiple data assimi-

lations. Computational Geostatistics, 16(3), 2012. DOWNLOAD. (Cited on

page 54.)

[66] Alexandre A. Emerick and Albert C. Reynolds. Ensemble smoother with mul-

tiple data assimilation. Computers &amp;amp; Geosciences, 2012. DOWNLOAD. (Cited

on page 54.)

[67] Albert Tarantola and Bernard Valette. Inverse problems = quest for informa-

tion. Journal of Geophysics, 50:159–170, 1982. DOWNLOAD. (Cited on page 57.)

http://msp.org/camcos/2010/5-1/camcos-v5-n1-p04-p.pdf
http://ba.stat.cmu.edu/journal/2010/vol05/issue02/christen.pdf
http://arxiv.org/pdf/1202.3665v4
http://arxiv.org/pdf/1202.3665v4
http://arxiv.org/pdf/physics/0508111v2
http://arxiv.org/pdf/physics/9803008v2
http://arxiv.org/pdf/1305.4907v3
http://arxiv.org/pdf/1305.4907v3
http://enkf.nersc.no/Publications/eme12a.pdf
http://www.academia.edu/4799481/Ensemble_Smoother_with_Multiple_Data_Assimilation
http://www.ipgp.fr/~tarantola/Files/Professional/Papers_PDF/IP_QI_original.pdf


b i b l i o g r a p h y 117

[68] Guven Burc Arpat. Sequential Simulation with Patterns. PhD thesis, Department

of Petroleum Engineering - Stanford University, January 2005. DOWNLOAD.

(Cited on page 58.)

[69] Mehrdad Honarkhah. Stochastic Simulation of Patterns using Distance-based Pat-

tern Modeling. PhD thesis, Department of Energy Resources Engineering -

Stanford University, April 2011. DOWNLOAD. (Cited on page 58.)

[70] Pallav Sarma. Efficient Closed-loop Optimal Control of Petroleum Reservoirs under

Uncertainty. PhD thesis, Department of Petroleum Engineering - Stanford

University, September 2006. DOWNLOAD. (Cited on pages 58, 67, and 104.)

[71] Tuanfeng Zhang. Filter-based Training Pattern Classification for Spatial Pattern

Simulation. PhD thesis, Department of Geological and Environmental Sciences

- Stanford University, March 2006. DOWNLOAD. (Cited on pages 58 and 65.)

[72] Kees Geel. Description of the Brugge Field and Property Realizations. TNO, Febru-

ary 2008. (Cited on page 60.)

[73] Yan Chen and Dean S. Oliver. Ensemble-based closed-loop optimization ap-

plied to brugge field. SPE Reservoir Evaluation &amp;amp; Engineering, pages 56–71,

February 2010. (Cited on page 60.)

[74] E. Peters, R. J. Arts, G. K. Brouwer, C. R. Geel, S. Cullick, R. J. Lorentzen,

Y. Chen, K. N. B. Dunlop, F. C. Vossepoel, R. Xu, P. Sarma, A. H. Alhutali,

and A. C. Reynolds. Results of the brugge benchmark study for flooding

optimization and history matching. SPE Reservoir Evaluation &amp;amp; Engineering,

pages 391–405, June 2010. (Cited on page 60.)

[75] Sebastien Strebelle. Multiple-point geostatistics: from theory to practice. In

Expanded Abstract Collection from Ninth International Geostatistics Congress. Nor-

wegian Computing Center, 2012. DOWNLOAD. (Cited on page 65.)

[76] Bruno Dujardin. Sensitivity analysis of filtersim and histogram reproduction.

Master’s thesis, Department of Energy Resources Engineering - Stanford Uni-

versity, June 2007. DOWNLOAD. (Cited on page 65.)

[77] Jon Shlens. A tutorial on principal component analysis - derivation, discussion

and singular value decomposition. Technical report, University of California,

San Diego, March 2003. DOWNLOAD. (Cited on page 66.)

http://pangea.stanford.edu/departments/ere/dropbox/scrf/documents/Theses/SCRF-Theses/2000-2009/2005_PhD_Arpat.pdf
http://pangea.stanford.edu/departments/ere/dropbox/scrf/documents/Theses/SCRF-Theses/2010-2019/2011_PhD_Mehrdad_Honarkhah.pdf
https://pangea.stanford.edu/ERE/db/pereports/record_detail.php?filename=Sarma06.pdf
http://pangea.stanford.edu/departments/ere/dropbox/scrf/documents/Theses/SCRF-Theses/2000-2009/2006_PhD_Zhang.pdf
http://geostats2012.nr.no/pdfs/1744859.pdf
https://pangea.stanford.edu/departments/ere/dropbox/scrf/documents/Theses/SCRF-Theses/2000-2009/2007_MS_Dujardin.pdf
http://www.snl.salk.edu/~shlens/pca.pdf


b i b l i o g r a p h y 118

[78] C. G. Wu, Y. C. Liang, W. Z. Lin, H. P. Lee, and S. P. Lim. A note on equivalence

of proper orthogonal decomposition methods. Journal of Sound and Vibration,

265:1103–1110, January 2003. (Cited on page 66.)

[79] Allan Aasbjerg Nielsen. Orthogonal transformations. Technical report, De-

partment of Mathematical Modelling - Techinical University of Denmark,

2000. (Cited on page 66.)

[80] J. Antonio Vargas-Guzmán and Roussos Dimitrakopoulos. Computational

properties of min/max autocorrelation factors. Computers &amp;amp; Geosciences, 29:

715–723, January 2003. DOWNLOAD. (Cited on page 66.)

[81] Bernhard Schölkopf, Alexander Smola, and Klaus-Robert Müller. Nonlinear

component analysis as a kernel eigenvalue problem. Technical Report 44,

December 1996. (Cited on pages 66, 67, and 104.)

[82] Allan Aasbjerg Nielsen. Kernel maximum autocorrelation factor and mini-

mum noise fraction transformations. In IEEE Transactions on Image Processing,

volume 20, pages 612–624, March 2011. DOWNLOAD. (Cited on page 66.)

[83] Allan A. Nielsen and Morton J. Canty. Kernel principal component and maxi-

mum autocorrelation factor analyses for change detection. In Image and Signal

Processing for Remote Sensing XV, 2009. DOWNLOAD. (Cited on page 66.)

[84] Camilla Zacché da Silva. A descorrelação de variáveis com fatorização maf

em estimativa de teores. Master’s thesis, Department of Mining - Federal

University of Rio Grande do Sul, 2013. DOWNLOAD. (Cited on page 66.)

[85] María Noel Morales Boezio. Estudo das Metodologias Alternativas da Geostatís-

tica Multivariada Aplicadas a Estimativa de Teores de Depósitos de Ferro. PhD

thesis, Department of Mining - Federal University of Rio Grande do Sul, 2010.

DOWNLOAD. (Cited on page 66.)

[86] Kwangwon Park. Seeing invisible properties of subsurface oil and gas reser-

voir through extensive uses of machine learning algorithms. Technical report,

Department of Energy Resources Engineering - Stanford University, Septem-

ber 2007. DOWNLOAD. (Cited on page 67.)

[87] Pallav Sarma, Louis J. Durlofsky, Khalid Aziz, and When H. Chen. A new ap-

proach to automatic history matching using kernel pca. SPE Journal, (106176),

February 2007. (Cited on page 67.)

https://www.researchgate.net/publication/222899505_Computational_properties_of_minmax_autocorrelation_factors
http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/5925/pdf/imm5925.pdf
http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/5757/pdf/imm5757.pdf
http://www.lume.ufrgs.br/bitstream/handle/10183/75914/000891754.pdf
http://www.lume.ufrgs.br/bitstream/handle/10183/33664/000761391.pdf
http://cs229.stanford.edu/proj2007/Park-SeeingInvisiblePropertiesofSubsurfaceOilandGasReservoir.pdf


b i b l i o g r a p h y 119

[88] Pallav Sarma, Louis J. Durlofsky, and Khalid Aziz. Kernel principal compo-

nent analysis for efficient, differentiable parametrization of multipoint geo-

statistics. Math Geosciences, 40:3–32, December 2007. (Cited on pages 67

and 104.)

[89] Bernhard Schölkopf, Sebastian Mika, Alex Smola, Gunnar Rätsch, and Klaus-

Robert Müller. Kernel pca pattern reconstruction via approximate pre-images,

1998. DOWNLOAD. (Cited on page 67.)

[90] Sebastian Mika, Bernhard Schölkopf, Alex Smola, Klaus-Robert Müller,

Matthias Scholz, and Gunnar Rätsch. Kernel pca and de-noising in feature

spaces, 1999. DOWNLOAD. (Cited on page 67.)

[91] James T. Kwok and Ivor W. Tsang. The pre-image problem in kernel methods.

In Twentieth International Conference on Machine Learning, 2003. DOWNLOAD.

(Cited on page 67.)

[92] Kwangwon Park. Modeling Uncertainty in Metric Space. PhD thesis, Depart-

ment of Energy Resources Engineering - Stanford University, January 2011.

DOWNLOAD. (Cited on page 67.)

[93] Thomas Mejer Hansen, Knud Skou Cordua, and Klaus Mosegaard. Inverse

problems with non-trivial priors: efficient solution through sequential gibbs

sampling. Computers &amp;amp; Geosciences, (16):593–611, January 2012. DOWNLOAD.

(Cited on page 69.)

[94] Thomas Mejer Hansen, Knud Skou Cordua, Majken Caroline Looms, and

Klaus Mosegaard. Sippi: A matlab toolbox for sampling the solution to in-

verse problems with complex prior information part 1: Methodology. Com-

puters &amp;amp; Geosciences, 52:470–480, September 2012. DOWNLOAD. (Cited on

page 69.)

[95] Thomas Mejer Hansen, Knud Skou Cordua, Majken Caroline Looms, and

Klaus Mosegaard. Sippi: A matlab toolbox for sampling the solution to in-

verse problems with complex prior information part 2: Application to cross-

hole gpr tomography. Computers &amp;amp; Geosciences, 52:481–492, 2012. DOWN-

LOAD. (Cited on page 69.)

[96] Knud Skou Cordua, Thomas Mejer Hansen, and Klaus Mosegaard. Monte

carlo full-waveform inversion of crosshole gpr data using multiple-point geo-

http://www.kernel-machines.org/papers/denoising.ps
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.88.5268&amp;amp;rep=rep1&amp;amp;type=pdf
http://citeseer.ist.psu.edu/viewdoc/download?doi=10.1.1.330.8185&amp;amp;rep=rep1&amp;amp;type=pdf
http://pangea.stanford.edu/~jcaers/Thesis_PhD_KPark.pdf
https://www.researchgate.net/publication/257550612_Inverse_problems_with_non-trivial_priors_efficient_solution_through_sequential_Gibbs_sampling
https://www.researchgate.net/publication/256505077_SIPPI_A_Matlab_toolbox_for_Sampling_the_solution_to_Inverse_Problems_with_complex_Prior_Information_Part_1__Methodology
https://www.researchgate.net/publication/256505491_SIPPI_A_Matlab_toolbox_for_sampling_the_solution_to_inverse_problems_with_complex_prior_information_Part_2Application_to_crosshole_GPR_tomography
https://www.researchgate.net/publication/256505491_SIPPI_A_Matlab_toolbox_for_sampling_the_solution_to_inverse_problems_with_complex_prior_information_Part_2Application_to_crosshole_GPR_tomography


b i b l i o g r a p h y 120

statistical a priori information. Journal of Geophysics, 77(2):H19–H31, February

2012. DOWNLOAD. (Cited on page 69.)

[97] Zhou Wang, Alan Conrad Bovik, Hamid Rahim Sheikh, and Eero P. Simon-

celli. Image quality assessment: From error visibility to structural similarity.

IEEE Transactions on Image Processing, 13(4):600–612, April 2004. DOWNLOAD.

(Cited on page 74.)

http://www.researchgate.net/publication/258647313_Monte_Carlo_full-waveform_inversion_of_crosshole_GPR_data_using_multiple-point_geostatistical_a_priori_information
http://www.cns.nyu.edu/pub/eero/wang03-reprint.pdf


c o l o p h o n

This document was typeset using the typographical look-and-feel classicthesis

developed by André Miede. The style was inspired by Robert Bringhurst’s seminal

book on typography “The Elements of Typographic Style”. classicthesis is available

for both LATEX and LYX:

http://code.google.com/p/classicthesis/

Final Version as of June 3, 2014 (classicthesis version 1.0).

http://code.google.com/p/classicthesis/


D E C L A R A T I O N

I undertake that all the material presented for examination is my own work and

has not been written for me, in whole or in part, by any other person. I also

undertake that any quotation or paraphrase from the published or unpublished

work of another person has been duly acknowledged in the work which I present

for examination.

Recife, PE 50670-901, May 28, 2014

Júlio Hoffimann Mendes,

June 3, 2014


	Dedication
	Abstract
	Publications
	Acknowledgements
	Contents
	List of Figures
	List of Tables
	List of Algorithms
	Listings
	Acronyms
	Inverse Problem Theory
	1 Basic Concepts
	1.1 What is an inverse problem?
	1.2 Why inverse problems are hard?
	1.3 The maximum likelihood principle
	1.4 Tarantola's postulate
	1.5 Classical vs. probabilistic framework

	2 Classical Framework
	2.1 Basic taxonomy for inverse problems
	2.2 Linear regression and the least-squares estimate
	2.3 Tikhonov regularization
	2.4 Levenberg-Marquardt solution to nonlinear regression

	3 Probabilistic Framework
	3.1 Definition of probability
	3.2 States of information
	3.3 Bayesian inversion
	3.4 Ensemble Markov chain Monte Carlo


	History Matching
	4 Prelude
	4.1 Problem description
	4.2 Case studies
	4.3 Comments on reproducibility

	5 Channelized Reservoir
	5.1 Setting priors
	5.2 Probabilistic inversion
	5.3 Analysis of the results

	6 Brugge Field
	6.1 Setting priors
	6.2 Probabilistic inversion
	6.3 Analysis of the results

	7 Conclusion
	7.1 General comments
	7.2 Technical difficulties
	7.3 Suggested improvements


	Appendix
	A Omitted Proofs
	A.1 The majority of inverse problems is ill-posed
	A.2 Maximum likelihood estimation for i.i.d. Gaussians
	A.3 System of equations for discrete linear inverse problems
	A.4 Maximum likelihood and least-squares
	A.5 Weighted linear least-squares estimate
	A.6 Levenberg-Marquardt gradient and Hessian
	A.7 Conditional probability by conjunction of states
	A.8 Kernel density estimation as a convolution

	B Code Snippets
	B.1 Iteratively reweighted least-squares
	B.2 Least absolute shrinkage and selection operator
	B.3 Metropolis algorithm
	B.4 Online Bayesian inversion
	B.5 Histogram fitting with kernel density estimation

	C Kernel PCA
	C.1 Kernel Gramian matrix
	C.2 Centering in the feature space
	C.3 Eigenproblem and normalization
	C.4 Preimage problem

	Bibliography
	Colophon
	Declaration


</field>
	</doc>
</add>