<?xml version="1.0" encoding="utf-8"?>
<add>
	<doc>
		<field name="docid">BR-TU.10782</field>
		<field name="filename">15944_000594748.pdf</field>
		<field name="filetype">PDF</field>
		<field name="text">
UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL
INSTITUTO DE INFORMA?TICA

PROGRAMA DE PO?S-GRADUAC?A?O EM COMPUTAC?A?O

CLARISSA CASSALES MARQUEZAN

Integrated Cluster Environmnet

(ICE) - Plataforma de Gerenciamento
e de Acesso a Mu?ltiplos Clusters

Dissertac?a?o apresentada como requisito parcial
para a obtenc?a?o do grau de
Mestre em Cie?ncia da Computac?a?o

Prof. Dr. Philippe O. A. Navaux
Orientador

Prof. Dr. Alexandre da Silva Carissimi
Co-orientador

Porto Alegre, agosto de 2006



CIP – CATALOGAC?A?O NA PUBLICAC?A?O

Marquezan, Clarissa Cassales

Integrated Cluster Environmnet (ICE) - Plataforma de
Gerenciamento e de Acesso a Mu?ltiplos Clusters / Clarissa
Cassales Marquezan. – Porto Alegre: PPGC da UFRGS, 2006.

97 f.: il.

Dissertac?a?o (mestrado) – Universidade Federal do Rio
Grande do Sul. Programa de Po?s-Graduac?a?o em Com-
putac?a?o, Porto Alegre, BR–RS, 2006. Orientador: Philippe
O. A. Navaux; Co-orientador: Alexandre da Silva Carissimi.

1. Mu?ltiplos clusters. 2. Gerenciamento. 3. Extensibili-
dade. 4. Transpare?ncia. 5. Interoperabilidade. 6. Web Servi-
ces. I. Navaux, Philippe O. A.. II. Carissimi, Alexandre da
Silva. III. T??tulo.

UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL
Reitor: Prof. Jose? Carlos Ferraz Hennemann
Pro?-Reitor de Coordenac?a?o Acade?mica: Prof. Pedro Cezar Dutra Fonseca
Pro?-Reitora de Po?s-Graduac?a?o: Profa. Valqu??ria Linck Bassani
Diretor do Instituto de Informa?tica: Prof. Fla?vio Rech Wagner
Coordenador do PPGC: Prof. Carlos Alberto Heuser
Biblioteca?ria-chefe do Instituto de Informa?tica: Beatriz Regina Bastos Haro



”Um amigo e? algue?m que sabe tudo a seu respeito e gosta de voce? assim

mesmo.”Elbert Hubbard



AGRADECIMENTOS

Em primeiro lugar quero agradecer a? minha fam??lia pelo apoio irrestrito, incen-
tivo e vibrac?a?o com as conquistas que tive ate? hoje e tenho certeza que ela vai estar
sempre comigo. Tambe?m agradec?o pela pacie?ncia e compreensa?o que tiveram nos
momentos em que na?o pude estar junto. Em especial agradec?o a?s minhas irma?s,
Luciana, Anna Lucia e Gabriela, ao meu pai e a? minha ma?e, voce?s sa?o realmente
muito importantes pra mim.

Agradec?o ao meu orientador, prof. Philippe Olivier Alexandre Navaux, com
quem trabalho em pesquisa desde a iniciac?a?o cient??fica. Quero agradecer pelo in-
vestimento que fez em mim, por ter sempre me incentivado, mostrado os melhores
caminhos a seguir para atingi os objetivos. Agradec?o todo o conhecimento que o prof.
Navaux me ajudou a formar, na?o somente conhecimento acade?mico, mas tambe?m
o conhecimento de vida, de como interagir com as pessoas, de como conduzir a
pesquisa, de como motivar e incentivar as pessoas a evolu??rem.

Tambe?m agradec?o ao meu co-orientador, prof. Alexandre Carissimi da Silva,
com quem comecei a trabalhar em pesquisa no mestrado. Um agradecimento por ter
sempre me lembrado de manter o foco no meu trabalho de mestrado, de na?o dispersar
minha atenc?a?o com trabalhos paralelos. Essas recomendac?o?es foram realmente muito
importantes para mim. Agradec?o tambe?m pelas discusso?es te?cnicas, pelas correc?o?es
de rumo do trabalho e pelo incentivo.

Existem pessoas que participaram ativamente do meu trabalho de mestrado,
as quais na?o posso deixar de agradecer: Lucas Schnorr, Rodrigo da Rosa Righi,
Nicolas Maillard e Alexandre Ilha. As discusso?es que tivemos, as trocas de ide?ias
foram muito importantes para o amadurecimento e para a conclusa?o deste trabalho.
Ainda quando agiram como ’advogados do diabo’ foram perfeitos, pois me fizeram
correr atra?s de respostas e de soluc?o?es.

Agradec?o tambe?m a uma pessoa que na?o contribuiu diretamente no meu tra-
balho de mestrado, mas as oportunidades que ele me proporcionou contribu??ram a
aquisic?a?o e utilizac?a?o de conhecimento nesta dissertac?ao?. Muito obrigada ao prof.
Lisandro Zanbenedetti Granville.

Na?o posso deixar de agradecer a compreensa?o do meu chefe na Divisa?o de Redes e
Suporte (DRS) do CPD da UFRGS, Leandro Rey, possibilitando-me uma grande fle-
xibilidade de hora?rio para que eu pudesse terminar a dissertac?a?o. Agradec?o tambe?m,
aos colegas de trabalho na DRS, pelo incentivo.

Aos amigos Rafael Ennes Silva e Priscilla Kurtz um agradecimento pelo compa-
nheirismo durante os va?rios finais de semana, noites e madrugadas que passamos na
sala 209 terminando nossas dissertac?o?es e trabalho de conclusa?o, no caso da Priscilla.
Foram dias e noites de grande trabalho, mas tambe?m de muita diversa?o.



Agradec?o aos amigos 99, principalmente o Diego Contessa (Cac?ula) e o Daniel
Gaspary (Chewie), por sempre me motivarem, seja ao vivo ou pelo ICQ ou MSN.
Aos amigos que esta?o longe, muitos em outros continentes (Karina Roggia e Tiago
Fiorezi), agradec?o pelo carinho e tambe?m pelo incentivo remoto. Aos amigos das
a?ureas e?pocas do Labcom pelas conversas, momentos divertidos e pela amizade.

Aos meus amigos, ou melhor, a? minha fam??lia do Coral Procergs, pela pacie?ncia
e compreensa?o nos momentos que tive de me afastar dos ensaios para terminar a
dissertac?a?o. Quero que voce?s saibam que participar deste grupo me da? motivac?a?o
para a vida e para o trabalho. Voce?s sa?o muito importantes para mim.

Agradec?o o incentivo, algumas vezes na?o ortodoxo, dos companheiros e amigos
da sala 209, os churrascos, os momentos de descontrac?a?o em bares conhecidos da
cidade de Porto Alegre, e no famoso bar perto do Campus do Vale. E Hermann,
eu terminei a dissertac?a?o antes que tu! (Existiam apostas de que eu na?o terminaria
antes dele, hehehe.)

Tambe?m agradec?o aos amigos de pedaladas por me proporcionarem passeios para
distrair um pouco a mente dos percalc?os do trabalho.

Na?o poderia deixar de agradecer a compreensa?o, amizade e incentivo dos meus
amigos de Rosa?rio do Sul. Em especial a?s minhas amigas Ana Paula e Maria Jose?
que, por muitas vezes, na?o pude estar junto delas, mas que sabem que a nossa
amizade esta? acima da presenc?a f??sica.

Enfim, existem muitas pessoas importantes na minha vida e que contribu??ram
para a minha formac?a?o e vida pessoal. A todas elas, na?o citadas nominalmente,
agradec?o com a mesma intensidade e vibrac?a?o.

Muito obrigada!!!



SUMA?RIO

LISTA DE ABREVIATURAS E SIGLAS . . . . . . . . . . . . . . . . . . . 8

LISTA DE FIGURAS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10

LISTA DE TABELAS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12

RESUMO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13

ABSTRACT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14

1 INTRODUC?A?O . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
1.1 Objetivos . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
1.2 Organizac?a?o . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16

2 ESTADO DA ARTE EM GERENCIAMENTO DE SISTEMAS DE

ALTO DESEMPENHO . . . . . . . . . . . . . . . . . . . . . . . . . . 17
2.1 Sistemas Distribu??dos . . . . . . . . . . . . . . . . . . . . . . . . . . 17
2.2 Sistemas de Alto Desempenho . . . . . . . . . . . . . . . . . . . . . 19
2.2.1 Clusters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
2.2.2 Grids . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
2.2.3 Intersecc?a?o entre clusters e grids: sistemas Multicluster . . . . . . . . 26
2.3 Resumo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28

3 PROPOSTA DE AMBIENTE INTEGRADO PARA CLUSTERS: ICE 29
3.1 ICE como um Sistema Distribu??do . . . . . . . . . . . . . . . . . . 31
3.2 Arquitetura ICE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
3.2.1 Middleware de Servic?os . . . . . . . . . . . . . . . . . . . . . . . . . . 34
3.2.2 Portal Web . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
3.2.3 Cena?rio de Aplicac?a?o da Arquitetura ICE . . . . . . . . . . . . . . . . 37
3.3 Resumo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38

4 DESENVOLVIMENTO DO PROTO?TIPO DO AMBIENTE ICE . . . 39
4.1 Deciso?es de Projeto . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
4.1.1 Definic?a?o do Middleware Empregado . . . . . . . . . . . . . . . . . . 39
4.1.2 Ana?lise dos Padro?es e Especificac?o?es . . . . . . . . . . . . . . . . . . . 41
4.2 Descric?a?o do Proto?tipo Implementado . . . . . . . . . . . . . . . . 43
4.2.1 Tecnologias Empregadas . . . . . . . . . . . . . . . . . . . . . . . . . 44
4.2.2 Middleware de Servic?o para Gerenciamento de Aplicac?o?es . . . . . . . 45
4.2.3 Portal Web . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60



4.3 Resumo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69

5 AVALIAC?A?O DO PROTO?TIPO ICE . . . . . . . . . . . . . . . . . . . 71
5.1 Avaliac?a?o Qualitativa . . . . . . . . . . . . . . . . . . . . . . . . . . 71
5.1.1 Metodologia . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
5.1.2 Comparac?a?o entre HPC2N, M3C e ICE . . . . . . . . . . . . . . . . . 72
5.2 Avaliac?a?o Quantitativa . . . . . . . . . . . . . . . . . . . . . . . . . 74
5.2.1 Definic?a?o do escopo das medic?o?es . . . . . . . . . . . . . . . . . . . . 74
5.2.2 Metodologia . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
5.2.3 Plataforma . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76
5.2.4 Experimentos com a operac?a?o de submissa?o de aplicac?o?es . . . . . . . 77
5.2.5 Experimentos com a operac?a?o de visualizac?a?o de scripts de submissa?o 80
5.2.6 Experimentos com a operac?a?o de finalizac?a?o de aplicac?o?es . . . . . . . 82
5.2.7 Experimentos com a operac?a?o de visualizac?a?o de status . . . . . . . . 83
5.2.8 Experimentos com a operac?a?o de recuperac?a?o das sa??das padra?o . . . 83
5.3 Resumo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84

6 CONCLUSA?O . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
6.1 Trabalhos em Paralelo . . . . . . . . . . . . . . . . . . . . . . . . . . 87
6.2 Trabalhos Futuros . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88

REFERE?NCIAS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89



LISTA DE ABREVIATURAS E SIGLAS

C3 Cornell Checkpoint (pre)Compiler

C3 Cluster Command and Control

CORBA Commom Object Request Broker Architecture

DCOM Distributed Component Object Model

ER Entidade Relacionamento

GGF Global Grid Forum

GMA Grid Monitoring Architecture

GPPD Grupo de Processamento Paralelo e Distribu??do

GRAM Grid Resource Allocation and Management

GSI Grid Security Infrastructure

GT Globus Toolkit

HPC High Performance Computing

HPC2N High Performance Computing Center North

HTTP Hypertext Transfer Protocol

ICE Integrated Cluster Environment

IOR Internet-wide Object Reference

JSP Java Server Pages

M3C Managing and Monitoring Multiple Clusters

MPP Massive Parallel Processing

MTBF MeanTime Between Failures

MVC Model View Controller

OASIS Organization for the Advancement of Structured Information Standards

OGSA Open Grid Service Architecture

OGSI Open Grid Service Infrastructure

OpenPBS Open Portable Batch System

OpenSCE Open Scalable Cluster Environment



ORB Object Request Broker

OSCAR Open Source Cluster Application Resource

P2P Peer-to-Peer

PDA Personal Digital Assistants

PUNCH Purdue University Network Computing Hubs

QAME QoS-Aware Management Environment

RMI Remote Method Invocation

RPC Remote Procedure Call

SCMS Smile Cluster Management System

SGBD Sistema de Gerenciamento de Base de dados

SGE Sun Grid Engine

SI Service Implementation

SM Service Module

SMM System Management Module

SNMP Simple Network Management Protocol

SOA Service Oriented Architecture

SOAP Simple Object Access Protocol

UDDI Universal Description Discovery and Integration

URI Identificador Universal de Recursos

USI Unified Service Interface

VPN Virtual Private Network

W3C World Wide Web Consortium

WS Web Services

WSDL Web Services Description Language

WSGRAM Web Services Grid Resource Allocation and Management



LISTA DE FIGURAS

Figura 2.1: Lista de aplicac?o?es existentes nas ma?quinas que fazem parte do
Top 500 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20

Figura 3.1: Visa?o geral do cena?rio de utilizac?a?o do ICE . . . . . . . . . . . . 32
Figura 3.2: Arquitetura ICE . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
Figura 3.3: Instanciac?a?o da Arquitetura ICE . . . . . . . . . . . . . . . . . . 37

Figura 4.1: Arquitetura do Globus Toolkit v4 - Figura retirada do draft (GLO-
BUS Toolkit 4.0 RELEASE MANUALS, 2005) . . . . . . . . . . 42

Figura 4.2: Proto?tipo ICE Implementado . . . . . . . . . . . . . . . . . . . . 44
Figura 4.3: JMF - Job Management Framework . . . . . . . . . . . . . . . . . 51
Figura 4.4: Diagrama de classes para o pacote de classes auxiliares . . . . . . 52
Figura 4.5: Diagrama de classes para as implementac?o?es do OpenPBS-SI e

OAR-SI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
Figura 4.6: Elementos XML da requisic?a?o das operac?o?es de submissa?o e vi-

sualizac?a?o de scripts . . . . . . . . . . . . . . . . . . . . . . . . . 55
Figura 4.7: Exemplo de retorno da operac?a?o de submissa?o. (a) Submissa?o

com OpenPBS (b) Submissa?o com OAR . . . . . . . . . . . . . . 55
Figura 4.8: Exemplo de retorno da operac?a?o de submissa?o contendo erro. . . 56
Figura 4.9: Elementos XML de retorno da operac?a?o de visualizac?a?o de scripts. 56
Figura 4.10: Exemplo de retorno da operac?a?o de visualizac?a?o em um cluster

com OpenPBS. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
Figura 4.11: Exemplo de retorno da operac?a?o de visualizac?a?o em um cluster

com OAR. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
Figura 4.12: Elementos XML de requisic?a?o e retorno da operac?a?o de finalizac?a?o. 58
Figura 4.13: Exemplo de retorno da operac?a?o de verificac?a?o de estado das filas

do gerenciador OAR. . . . . . . . . . . . . . . . . . . . . . . . . . 59
Figura 4.14: Elementos XML da requisic?a?o e resposta da operac?a?o de recu-

perac?a?o de arquivos de sa??da. . . . . . . . . . . . . . . . . . . . . 60
Figura 4.15: Diagrama de classes do JM-SM . . . . . . . . . . . . . . . . . . . 61
Figura 4.16: Diagrama Entidade Relacionamento (ER) do ambiente ICE . . . 62
Figura 4.17: Snapshot Portal Web - Configurac?a?o de operac?o?es . . . . . . . . . 63
Figura 4.18: Snapshot Portal Web - Pa?gina principal da configurac?a?o de grupos 64
Figura 4.19: Snapshot Portal Web - Associac?a?o de clusters a grupos . . . . . . 64
Figura 4.20: Snapshot Portal Web - Associac?a?o de funcionalidades a grupos . . 65
Figura 4.21: Snapshot Portal Web - Associac?a?o de funcionalidades a clusters . 66
Figura 4.22: Snapshot Portal Web - Pa?gina principal do menu Job Management 67
Figura 4.23: Snapshot Portal Web - Pa?gina de submissa?o de aplicac?o?es . . . . 67



Figura 4.24: ER da funcionalidade de Job Management . . . . . . . . . . . . . 68
Figura 4.25: Exemplo 1 de visualizac?a?o de scripts de submissa?o . . . . . . . . 68
Figura 4.26: Exemplo 2 de visualizac?a?o de scripts de submissa?o . . . . . . . . 69

Figura 5.1: Cena?rio de aquisic?a?o das medidas de overhead . . . . . . . . . . . 75
Figura 5.2: Gra?fico Operac?a?o de Submissa?o - OpenPBS . . . . . . . . . . . . 77
Figura 5.3: Gra?fico Operac?a?o de Submissa?o - OAR . . . . . . . . . . . . . . . 78
Figura 5.4: Gra?fico Operac?a?o de Visualizac?a?o de Scripts - OpenPBS . . . . . 80
Figura 5.5: Gra?fico Operac?a?o de Visualizac?a?o de Scripts - OAR . . . . . . . . 81



LISTA DE TABELAS

Tabela 3.1: Descric?a?o dos perfis atualmente definidos no ambiente ICE . . . . 36

Tabela 4.1: Comparac?a?o entre CORBA e Web Services . . . . . . . . . . . . . 40
Tabela 4.2: Operac?o?es da JM-USI . . . . . . . . . . . . . . . . . . . . . . . . 46

Tabela 5.1: Comparac?a?o entre o ambiente ICE e algumas ferramentas relaci-
onadas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73

Tabela 5.2: Descric?a?o das variac?o?es de para?metros para submissa?o de aplicac?o?es. 76
Tabela 5.3: Descric?a?o do cluster utilizado para os experimentos . . . . . . . . 77
Tabela 5.4: Desvio padra?o para operac?a?o de submissa?o . . . . . . . . . . . . . 79
Tabela 5.5: Desvio padra?o para operac?a?o de verificac?a?o de scripts . . . . . . . 81
Tabela 5.6: Desempenho da operac?a?o de finalizac?a?o de aplicac?o?es . . . . . . . 82
Tabela 5.7: Desempenho da operac?a?o de verificac?a?o do status das aplicac?o?es . 83
Tabela 5.8: Desempenho da operac?a?o de recuperac?a?o das sa??das padra?o . . . . 84



RESUMO

Facilitar o gerenciamento e o acesso a sistemas de alto desempenho vem sendo
uma a?rea de pesquisa explorada nos u?ltimos anos. Isto acontece uma vez que se
verifica o aumento do nu?mero de usua?rios, muitos pertencentes a outras a?reas, como
por exemplo, biologia, geologia, hidrologia, etc e, desta forma, e? preciso facilitar
os meios de interac?a?o destes usua?rios com tais sistemas, assim como melhorar as
te?cnicas de gerenciamento dos mesmos. Ale?m do aumento do nu?mero e da mul-
tidisciplinaridade desses usua?rios, existe tambe?m o fato de que grande parte deles
tem acesso a diferentes tipos de sistemas de alto desempenho. Em geral, estes sis-
temas possuem ferramentas na?o padronizadas, sendo que cada uma apresenta uma
interface e um conjunto de ac?o?es e comandos a serem realizados para que possam
dispor de suas funcionalidades. Este trabalho propo?e a definic?a?o de uma arquite-
tura de gerenciamento e acesso a mu?ltiplos clusters, que seja capaz de ser facilmente
extens??vel, transparente, interopera?vel e de fa?cil utilizac?a?o, configurac?a?o e manu-
tenc?a?o. Como resultado da arquitetura proposta, foi desenvolvido um proto?tipo,
denominado ICE - Integrated Cluster Environment. Os principais objetivos da ar-
quitetura e do ambiente ICE sa?o: (i) capacidade de uniformizac?a?o do modo como as
ferramentas de clusters sa?o utilizadas e, tambe?m, uniformizac?a?o na maneira como
os clusters sa?o acessados; (ii) transpare?ncia na forma de acesso e uso dos clusters; e
(iii) capacidade de extensibilidade em dois n??veis: o primeiro refere-se a? extensibili-
dade do nu?mero de funcionalidades (servic?os) providos pelo sistema e o segundo esta?
relacionado a? capacidade do sistema lidar com o uso de diferentes ferramentas que
possuem a mesma funcionalidade. Ale?m da descric?a?o da arquitetura e do proto?tipo,
neste trabalho, tambe?m e? realizada uma avaliac?a?o do ambiente ICE. Essa avaliac?a?o
foi dividida em dois momentos. O primeiro traz a comparac?a?o das caracter??sticas
do ambiente proposto com algumas ferramentas relacionadas. No segundo momento
sa?o apresentados alguns experimentos que visam identificar o overhead inserido pelo
ambiente ICE na execuc?a?o das operac?o?es do mo?dulo de gerenciamento de aplicac?o?es,
desenvolvido e descrito neste trabalho.

Palavras-chave: Mu?ltiplos clusters, gerenciamento, extensibilidade, transpare?ncia,
interoperabilidade, Web Services.



ABSTRACT

Integrated Cluster Environmnet (ICE): Management and Access
Platform for Multiple Clusters

Some researches have been done over the last years in order to improve the man-
agement and access of high performance systems. One of the motivations of these
researches is the continuously increase in the number of users who, not rarely, belong
to other areas, such as biology, geology, hydrology, etc; so it is necessary to provide
some access and also management facilities in these kinds of systems. Further-
more, the users also have access to different kinds of high performance systems, and
they have to deal with distinct tools of their underlying, which are not standardize.
Therefore, the users need to learn the specificities of each tool in each high perfor-
mace system that they have permission to access. Aiming to solve this problem,
this work proposes an architecture to provide access and management of multiple
clusters with extensibility, transparance, interoperability, user-friendly, manageabil-
ity and maintainability. As a result of the proposed architecture, a prototype called
ICE - Integrated Cluster Environment - was developed. The main goals of the archi-
tecture and the ICE environment are: (i) the capability of uniforming the manner
the cluster tools are used and accessed; (ii) cluster access and transparence use; and
(iii) two extensibility levels: the first one refers to capability of extending the number
of functionalities/services provided by the environment and the second one concerns
to the capability of extending the number of tools, with the same functionality, the
system is able to manage. Beyond the architecture and prototype description, this
research presents the evaluation of ICE environment, which is divided in two parts.
The first one brings the comparison of the features between ICE and some related
work. The second part shows some experiments that intend to verify the overhead
inserted by ICE environment when performing the tasks of the job management
module, also developed and explained in this work.

Keywords: multiple clusters, management, extensibility, transparence, interopera-
bility, Web Services.



15

1 INTRODUC?A?O

Os sistemas distribu??dos esta?o presentes em quase todas as ac?o?es realizadas nos
tempos atuais. Desde a operac?a?o de verificar o saldo banca?rio, o pagamento de uma
conta atrave?s do carta?o de cre?dito, ate? a verificac?a?o da previsa?o do tempo (a qual
e? atingida atrave?s do uso de sistemas de alto desempenho). Sistemas distribu??dos
compreendem uma gama muito grande de cena?rios, como por exemplo: a Internet,
intranets, redes locais de universidades e empresas, redes sem fio etc. No escopo
deste trabalho sera?o considerados os sistemas distribu??dos cujo foco e? a obtenc?a?o
de melhor desempenho das aplicac?o?es. Tais sistemas sa?o chamados de Sistemas de
Alto Desempenho (HPC - High Performance Computing). Em geral sa?o formados
por ma?quinas de redes locais, que podem ou na?o estar ligadas a outras redes.

Cada vez mais, o gerenciamento desse tipo de sistema torna-se uma tarefa com-
plexa, que requer um nu?mero maior de varia?veis e ferramentas. Pode-se dizer que
dentro da a?rea de HPC existem tre?s grandes a?reas de utilizac?a?o: clusters, grids
e mu?ltiplos clusters. Para cada uma destas a?reas existem diversas ferramentas
para gerenciamento. Algumas delas sa?o: OSCAR (LIGNERIS; et al., 2003; OS-
CAR - OPEN SOURCE CLUSTER APPLICATION RESOURCES, 2005), PUNCH
(ADABALA; KAPADIA; FORTES, 2000; PARK; et al., 2000), ROCKS (PAPADO-
POULOS; KATZ; BRUNO, 2001), OpenSCE (UTHAYOPAS; ANGSKUN; MANE-
ESILP, 2001, 2002; OPENSCE - SCALABLE CLUSTER ENVIRONMENT, 2005),
ferramenta de gerenciamento de clusters atrave?s de SNMP (ALVES et al., 2004;
ALVES; MARQUEZAN; GRANVILLE, 2004; ALVES et al., 2005), entre outras no
cena?rio de clusters; e Globus (FOSTER; KESSELMAN, 1997; GLOBUS - WEL-
COME TO THE GLOBUS TOOLKIT HOMEPAGE, 2005), Condor (GONZ; et
al., 2002; THAIN; TANNENBAUM; LIVYN, 2003), GridRM (BAKER; SMITH,
2003), GridSphere (NOVOTNY; RUSSELL; WEHRENS, 2004), entre va?rias outras
no escopo de gerenciamento de grids. No cena?rio de gerenciamento de mu?ltiplos
clusters na?o existe a mesma variedade de ferramentas como encontrado nos demais
cena?rios. Exemplos de ferramentas voltadas para o gerenciamento de mu?ltiplos
clusters sa?o: M3C (BRIM; et al., 2001) e HPC2N (ELMROTH; NYLE?N; OSCARS-
SON, 2005). Tambe?m identifica-se que as ferramentas que compo?em este u?ltimo
cena?rio na?o apresentam caracter??sticas importantes de sistemas distribu??dos, como
por exemplo: transpare?ncia de acesso, extensibilidade, uniformizac?a?o das operac?o?es,
completeza, interoperabilidade, capacidade de incorporac?a?o dos sistemas de geren-
ciamento legados e construc?a?o de ferramentas amiga?veis para os usua?rios.

Dadas as caracter??sticas de sistemas distribu??dos ainda na?o disponibilizadas nas
ferramentas de gerenciamento de mu?ltiplos clusters, propo?e-se o ambiente ICE -
Integrated Cluster Environment.



16

1.1 Objetivos

Os objetivos deste trabalho sa?o: (i) definic?a?o de uma arquitetura de gerenci-
amento e acesso a mu?ltiplos clusters que seja capaz de ser facilmente extens??vel,
transparente, interopera?vel e de fa?cil utilizac?a?o, configurac?a?o e manutenc?a?o; (ii) im-
plementac?a?o de um proto?tipo seguindo esta arquitetura; e (iii) validac?a?o das ide?ias
contidas na arquitetura e no proto?tipo implementado. A principal questa?o do tra-
balho desenvolvido e? permitir que os usua?rios lidem com as funcionalidades dos
clusters, como por exemplo gerenciamento de aplicac?o?es e monitoramento de recur-
sos, em alto n??vel, enquanto as especificidades de cada ferramenta sera?o abstra??das
e providas pela camada definida no trabalho. Uma alternativa seria a utilizac?a?o
das ferramentas de grid para prover essas abstrac?o?es. Entretanto, essas ferramentas
inserem um alto grau de complexidade, e dependendo do tipo de ferramenta de grid
empregada a execuc?a?o de aplicac?o?es verdadeiramente paralelas, e na?o do tipo bag of
tasks, podem ser afetadas pela forma como elas sa?o distribu??das nos recursos.

No contexto do trabalho que e? apresentado nesta dissertac?a?o defende-se a ide?ia
de trazer os conceitos da a?rea de grid, como por exemplo arquiteturas orientadas a
servic?os e interoperabilidade, para a a?rea de mu?ltiplos clusters, mantendo a simplici-
dade do cena?rio e provendo as facilidades que as ferramentas de grid disponibilizam
para os usua?rios e administradores.

1.2 Organizac?a?o

Este trabalho esta? organizado como descrito a seguir.
O Cap??tulo 2 possui o levantamento do estado da arte do gerenciamento de sis-

temas distribu??dos, mais especificamente focando em sistemas de alto desempenho
(HPC - High Performance Computing). Neste escopo sera?o apresentados tre?s gran-
des a?reas: clusters, grids e mu?ltiplos clusters. Para cada uma dessas a?reas sera?o
apresentados os cena?rios de utilizac?a?o os problemas de gerenciamento e algumas
ferramentas para lidar com esses problemas.

O Cap??tulo 3 apresenta a proposta de desenvolvimento do ambiente ICE. Um
ambiente voltado para o gerenciamento e acesso de mu?ltiplos clusters. Suas prin-
cipais caracter??sticas sa?o transpare?ncia, extensibilidade e interoperabilidade. Seu
escopo de utilizac?a?o e arquitetura sa?o descritos nas subsec?o?es desse cap??tulo.

No Cap??tulo 4 e? feita a descric?a?o do proto?tipo implementado com base nas de-
finic?o?es e na arquitetura do ambiente ICE, propostos no cap??tulo anterior. Aqui
sa?o detalhados os componentes do ambiente, seus relacionamentos, o modelo de
informac?a?o e, como resultado final, sa?o apresentadas telas do proto?tipo ICE.

O Cap??tulo 5 conte?m a avaliac?a?o do ambiente proposto e da implementac?a?o do
proto?tipo. Este cap??tulo esta? dividido em dois momentos. Primeiramente e? realizada
uma comparac?a?o entre o ambiente ICE e algumas ferramentas relacionadas. Em um
segundo momento e? descrita a avaliac?a?o quantitativa do proto?tipo desenvolvido,
onde foi medido o overhead inserido pelo proto?tipo no front-end do cluster utilizado
para testes.

Finalmente no Cap??tulo 6 sa?o apresentadas as considerac?o?es finais sobre este
trabalho e sa?o destacados alguns trabalhos relacionados ao ambiente ICE que ve?m
sendo desenvolvidos em paralelo ao andamento desta dissertac?a?o, assim como tra-
balhos futuros que podem ser desenvolvidos com base no ambiente ICE.



17

2 ESTADO DA ARTE EM GERENCIAMENTO DE

SISTEMAS DE ALTO DESEMPENHO

Neste cap??tulo sera?o discutidas as ide?ias existentes na a?rea de gerenciamento de
sistemas de alto desempenho (HPC - High Performance Computing). Para que se
possa abordar com maior riqueza de detalhes este to?pico, sera? realizada uma breve
apresentac?a?o de sistemas distribu??dos, suas caracter??sticas e alguns middlewares co-
mumente utilizados para construc?a?o de tais sistemas. Essa caracterizac?a?o inicial
sera? importante para que se possa compreender os problemas que existem na cons-
truc?a?o de ambientes de gerenciamento de sistemas de HPC. De certa forma, pode-se
dizer que eles sa?o uma especializac?a?o de sistemas distribu??dos, visto que sa?o forma-
dos por hardware e software interligados por uma rede de comunicac?a?o. A principal
diferenc?a esta? em sua especializac?a?o. Sistemas de alto desempenho tem o objetivo
espec??fico de solucionar problemas no menor tempo poss??vel. Eficie?ncia e efica?cia sa?o
para?metros considerados nesse tipo de contexto. Seu gerenciamento requer cuida-
dos, como por exemplo: menor intrusa?o poss??vel para que o desempenho do sistema
na?o seja afetado; cuidados espec??ficos na alocac?a?o de recursos para usua?rios; moni-
toramento dos recursos do sistema HPC assim como das aplicac?o?es que esta?o em
execuc?a?o; etc. Em sistemas distribu??dos esses problemas de gerenciamento tambe?m
aparecem, mas na?o sa?o cruciais. Algumas te?cnicas e soluc?o?es de gerenciamento de
sistemas distribu??dos e HPC sera?o apresentadas a seguir.

2.1 Sistemas Distribu??dos

Sistemas distribu??dos esta?o presentes em praticamente todos os segmentos da
sociedade. Estes tipos de sistemas sa?o caracterizados por componentes de software
e de hardware, localizados em computadores interligados por uma rede, que se co-
municam e coordenam suas ac?o?es atrave?s de troca de mensagens (COULOURIS;
DOLLIMORE; KINDBERG, 2005). Como exemplos de sistemas distribu??dos tem-
se: a Internet; as intranets de corporac?o?es, universidades, e de redes dome?sticas; a
computac?a?o mo?vel (desde telefones celulares a PDAs, pagers, etc); e a computac?a?o
ob??quoa (COULOURIS; DOLLIMORE; KINDBERG, 2005).

De modo geral, sistemas distribu??dos abrangem uma quantidade ampla de a?reas
de pesquisa, desenvolvimento e nego?cios dentro da computac?a?o. Cada qual com
um determinado objetivo, como por exemplo, o come?rcio eletro?nico (e-business ou
e-commerce). Nesse tipo de nego?cio, o objetivo e? manter os sistemas o mais tempo
poss??vel em funcionamento, ale?m de tentar integrar os diferentes sistemas de forne-
cedores e compradores para obtenc?a?o de maiores lucros. Um sistema de e-commerce



18

na?o dispon??vel implica em perdas financeiras para a empresa. Em contra-partida
a esses tipos de sistemas voltados para empresas, existem sistemas distribu??dos
acade?micos cuja finalidade e? manter uma rede de dados e servic?os para seus usua?rios
(no caso professores, alunos e funciona?rios). Enfim, sa?o inu?meros os tipos de siste-
mas distribu??dos e sua aplicac?a?o cotidiana.

Entretanto, o desenvolvimento e projeto de sistemas distribu??dos desperta alguns
desafios. De acordo com Coulouris em (COULOURIS; DOLLIMORE; KINDBERG,
2005) os principais desafios a serem considerados sa?o: heterogeneidade, capacidade
de extensibilidade (openness), seguranc?a, escalabilidade, tolera?ncia a falhas, con-
corre?ncia e transpare?ncia. E? bastante complexo o desenvolvimento de um sistema
distribu??do que consiga satisfazer a contento todos esses desafios. Em geral, eles
solucionam alguns deles e outros sa?o preteridos muitas vezes em raza?o do objetivo
final do sistema distribu??do projetado. Por exemplo, em um sistema distribu??do
de tempo real as caracter??sticas mais relevantes sa?o a transpare?ncia, concorre?ncia e
tolera?ncia a falhas. Nesse caso, procurar soluc?o?es o?timas para extensibilidade na?o
e? ta?o relevante. A seguir sa?o descritos com maiores detalhes cada um dos desafios
apontados em (COULOURIS; DOLLIMORE; KINDBERG, 2005).

Heterogeneidade - Este conceito esta? ligado a? variedade de redes de comunicac?a?o,
hardware dos computadores, sistemas operacionais, linguagens de programac?a?o,
implementac?o?es de diferentes desenvolvedores. Para tentar lidar com a hetero-
geneidade dos sistemas sa?o desenvolvidos middlewares, os quais sa?o camadas
de software que proporcionam abstrac?a?o e mascaramento da heterogeneidade.
Ale?m disso, middlewares tambe?m prove?em um modelo de uniformizac?a?o na
forma como os sistemas distribu??dos sa?o utilizados.

Extensibilidade - A extensibilidade em um sistema define sua capacidade de agre-
gar novas funcionalidades ou ferramentas, assim como sua capacidade de ser
reimplementado. Esse conceito, no que se refere a sistemas distribu??dos e? ba-
sicamente determinado pela capacidade de inserc?a?o de novos servic?os e de sua
disponibilizac?a?o para diferentes programas clientes. A extensibilidade pode
ser atingida atrave?s da especificac?a?o de interfaces dos componentes do sistema
e de sua publicac?a?o.

Seguranc?a - Existem tre?s componentes de seguranc?a para manter as informac?o?es
dos recursos em um sistema distribu??do: (i) confidencialidade, protec?a?o contra
usua?rios na?o qualificados e na?o autorizados; (ii) integridade, protec?a?o contra
alterac?a?o ou corrupc?a?o dos dados; e (iii) disponibilidade, protec?a?o contra as
interfere?ncias do meio no acesso aos recursos.

Escalabilidade - Caracter??stica que um sistema possui de se manter eficiente mesmo
com o aumento no nu?mero de usua?rios e de recursos. O desenvolvimento de
sistemas distribu??dos escala?veis apresenta os seguintes desafios: controle do
custo dos recursos f??sicos; controle da perda de desempenho; prevenc?a?o da
escassez de recursos de software e prevenc?a?o de gargalos.

Tratamento de falhas - Falhas em sistemas distribu??dos sa?o parciais, isto e?, alguns
componentes falham mas outros continuam funcionando. Existem algumas
te?cnicas para o tratamento de falhas: detecc?a?o, mascaramento, tolera?ncia,
recuperac?a?o de falhas e redunda?ncia.



19

Concorre?ncia - Em sistemas distribu??dos tanto servic?os quanto aplicac?o?es possuem
recursos que podem ser compartilhados por seus clientes. Nesse caso, e? preciso
garantir que o sistema funcione corretamente em um ambiente concorrente.
Para isto, mecanismos de sincronizac?a?o que mantenham os dados consistentes
devem ser adotados.

Transpare?ncia - Caracter??stica que faz com que usua?rios e aplicac?o?es de um sistema
distribu??do tenham uma visa?o u?nica do sistema ao inve?s de uma ide?ia de
um sistema formado por diferentes componentes. Existem oito formas de
transpare?ncia, sendo elas transpare?ncia de: acesso, localizac?a?o, concorre?ncia,
replicac?a?o, falha, mobilidade, desempenho e escalabilidade.

Para a construc?a?o de um sistema distribu??do, que seja capaz de suportar total
ou parcialmente as caracter??sticas detalhadas acima, e? preciso a existe?ncia de um
middleware. Esse componente e? capaz de lidar com as especificidades do sistema
como um todo e apresentar aos usua?rios (sejam pessoas ou outras aplicac?o?es) uma
abstrac?a?o em alto n??vel dos recursos do sistema distribu??do (COULOURIS; DOL-
LIMORE; KINDBERG, 2005). Middlewares como CORBA (SIEGEL, 1996), RMI
(RMI - JAVA REMOTE METHOD INVOCATION, 2005), DCOM (GRIMES, 1997)
e recentemente Web Services (CERAMI, 2002) te?m sido desenvolvidos no intuito de
prover essas abstrac?o?es.

Considerando que sistemas de alto desempenho sa?o uma especificac?a?o de sis-
temas distribu??dos, a construc?a?o de ambientes de gerenciamento e de acesso aos
recursos desse cena?rio tambe?m empregam middlewares que oferec?am as abstrac?o?es
necessa?rias. Muitas vezes existem diferenciac?o?es bem claras dos tipos de ambientes
de gerenciamento e acesso em HPC e em sistemas distribu??dos. Mas de forma geral
os objetivos da construc?a?o de ambientes de gerenciamento e de acesso a sistemas
HPC sa?o exatamente os mesmos existentes em sistemas distribu??dos. A seguir, sera?o
apresentados alguns tipos de sistemas HPC e algumas ferramentas de gerenciamento
e acesso empregadas na a?rea.

2.2 Sistemas de Alto Desempenho

Sistemas de alto desempenho sa?o tipicamente empregados na soluc?a?o de proble-
mas que precisam de uma grande quantidade de ca?lculos ou enta?o casos que existe
uma grande quantidade de dados para serem processados. O objetivo e? alcanc?ar a
resoluc?a?o desses problemas em um tempo menor ou enta?o com um grau de precisa?o
ou riqueza de detalhes maior. A linha de evoluc?a?o dos sistemas de alto desempenho
foi marcada pela utilizac?a?o de ma?quinas massivamente paralelas (MPP - Massive
Parallel Processing), ma?quinas vetoriais, clusters, constelac?o?es e grids (BUYYA,
1999).

Alguns exemplos de campos de aplicac?a?o de sistemas HPC sa?o:

• f??sica e engenharias em geral: resoluc?a?o de simulac?o?es de corpos celestes, si-
mulac?o?es de equac?o?es e sistemas em geral;

• hidrologia: simulac?a?o de impactos ambientais nos aqu??feros, simulac?a?o de cons-
truc?a?o de equipamentos para usinas hidrele?tricas, outros tipos de simulac?o?es
ligados aos recursos h??dricos;



20

Figura 2.1: Lista de aplicac?o?es existentes nas ma?quinas que fazem parte do Top 500

• geoprocessamento: simulac?a?o da existe?ncia de petro?leo em uma determinada
regia?o;

• meteorologia: previsa?o do tempo, simulac?a?o de mudanc?as clima?ticas;

• medicina e bio-informa?tica: simulac?a?o de efeitos de reme?dios, simulac?a?o de
composic?a?o de novos medicamentos;

• computac?a?o: simulac?a?o de circuitos eletro?nicos, processamento de imagens e
na renderizac?a?o de imagens, simulac?a?o de arquiteturas de computadores, entre
outras.

A Figura 2.1 apresenta algumas aplicac?o?es que sa?o executadas nas ma?quinas que
fazem parte do Top500 (TOP500 SUPERCOMPUTER SITES, 2005). Essa e? uma
lista dos 500 supercomputadores mais poderosos do mundo.

Atrave?s dessa figura pode-se observar que existem aplicac?o?es bastante variadas
nas ma?quinas pertencentes a lista do Top500. Esta variedade cobre aplicac?o?es que
va?o desde cient??ficas, como por exemplo: geo-f??sica (9,4%), benckmarking (2,6%),
pesquisas de clima e tempo (3,4%), a aplicac?o?es mais comerciais, como por exemplo:
financeiras (8,8%), telecomunicac?o?es (3,2%) e automotiva (1,0%).

Pode-se perceber que o escopo de utilizac?a?o de sistema de HPC na?o esta? mais
restrito ao campo da computac?a?o. Isto faz com que seja necessa?ria a criac?a?o de
ambientes e mecanismos que facilitem sua utilizac?a?o. Outro fator e? o tamanho
que esse tipo de sistema tem assumido. Atualmente existem ma?quinas formadas
por milhares de no?s (elementos processadores). O gerenciamento manual desse tipo
de sistemas torna-se invia?vel. Portanto, tambe?m existe uma necessidade de prover
plataformas capazes de facilitar a utilizac?a?o e manipulac?a?o desses sistemas.

Atualmente, boa parte dos trabalhos em HPC concentram-se em dois tipos de
sistemas: clusters e grids. Segundo a definic?a?o apresentada em (BAKER, 2000),
clusters sa?o sistemas computacionais locais que encerram um conjunto de computa-
dores independentes inter-conectados por uma rede. Um cluster e? local no sentido
de que todos os seus componentes e sub-sistemas esta?o localizados dentro de um
u?nico dom??nio administrativo, tipicamente localizados em uma sala e gerenciados
como um simples computador. Em contrapartida, grids assumem um cara?ter de



21

sistemas de larga escala. Grids se caracterizam pelo foco na construc?a?o de sistemas
de larga escala que sejam capazes de coordenar o compartilhamento de recursos e
que tenham a capacidade de resolver problemas em organizac?o?es virtuais dina?micas
e multi-institucionais (FOSTER; KESSELMAN; TUECKE, 2001). Ainda segundo
Ahmar Abbas (ABBAS, 2004) a computac?a?o em grid pode juntar em um u?nico
escopo todos os esforc?os realizados nas a?reas de computac?a?o de alto desempenho,
peer-to-peer (P2P) e Internet. A seguir esses dois tipos de sistemas sa?o descritos,
sendo que sa?o apresentados os problemas de gerenciamento existentes e algumas
soluc?o?es para tais problemas.

2.2.1 Clusters

Apesar do cena?rio de clusters ser uma a?rea bem desenvolvida e consolidada, com
um grande nu?mero de ferramentas e ambientes disponibilizados, na?o existe uma pa-
dronizac?a?o na forma como eles foram desenvolvidos. Mais do que isto, na?o existe
uma padronizac?a?o do que e? gerenciamento em clusters. Por exemplo, quando se fala
em gerenciamento de redes, e mais especificamente em um modelo de gerenciamento
de redes tem-se a ligac?a?o direta ao framework SNMP (CASE; et al., 1990). Em clus-
ters na?o existe uma metodologia, taxonomia, ou padra?o na a?rea de gerenciamento.

Uma tentativa de delimitar essa a?rea e? apresentada em (STERLING, 2000). Se-
gundo Thomas Sterling, os softwares que compo?e um cluster podem ser divididos
em: ambientes de programac?a?o de aplicac?o?es e softwares de gerenciamento de recur-
sos. Este u?ltimo componente pode ser dividido nas categorias de software listadas
abaixo.

• Instalac?a?o e configurac?a?o - Esta categoria lida com os desafios de desenvolvi-
mento de software que sejam capazes de implementar e manter imagens comuns
entre os no?s do cluster e que sejam de fa?cil utilizac?a?o. Este e? um problema a
ser bastante considerado quando se trabalha com clusters formados dos cen-
tenas de no?s. Exemplos de ferramentas que possuem essas caracter??sticas sa?o:
C3 (Cluster Command and Control) (FLANERY; et al., 2000), System Imager
(SYSTEM IMAGER, 2005), SSI (VALLEE; et al., 2005), entre outros.

• Escalonamento e alocac?a?o - A colocac?a?o das aplicac?o?es nos recursos distribu??dos
dos clusters requer ferramentas que sejam capazes de alocar os componen-
tes de software nos no?s e que escalonem o tempo de sua execuc?a?o. Segundo
(STERLING, 2000), a alocac?a?o pode ser realizada em diferentes n??veis de gra-
nularidade. Pode ser alocac?a?o: de trabalhos, transac?o?es, processos ou threads.
Alguns exemplos de ferramentas desenvolvidas sa?o: OpenPBS (PORTABLE
BATCH SYSTEM, 2005), OAR (CAPIT; et al., 2003, 2005), CCS (KELLER;
REINEFELD, 1998), Maui (MAUI SCHEDULER OPEN CLUSTER SOFT-
WARE, 2005), CADEO (CERA; ROSA RIGHI; PASIN, 2005), etc.

• Administrac?a?o do sistema - Para a supervisa?o do funcionamento dos sistemas
de clusters e? necessa?rio ferramentas que sejam capaz de prover: gerenciamento
das contas de usua?rios, filas de trabalhos; seguranc?a; backups; armazenamento
de dados; log de informac?o?es; terminais para usua?rios; entre outras ativida-
des simples de manutenc?a?o do sistema. Exemplos de ferramentas que ten-
tam prover essas facilidades administrativas sa?o: OSCAR (LIGNERIS; et al.,
2003; OSCAR - OPEN SOURCE CLUSTER APPLICATION RESOURCES,



22

2005), PUNCH (ADABALA; KAPADIA; FORTES, 2000; PARK; et al., 2000),
ROCKS (PAPADOPOULOS; KATZ; BRUNO, 2001), entre outros.

• Monitoramento e diagno?sticos - Nessa categoria, considera-se ferramentas que
possam ser usadas no monitoramento do estado e das operac?o?es dos elemen-
tos de sistema. Como exemplo, dessas ferramentas te?m-se: Ganglia (SA-
CERDOTI; et al., 2003), SCMS/RMS (UTHAYOPAS; PHATANAPHEROM,
2001), Supermom (SOTTILE; MINNICH, 2002), entre outros.

• Armazenamentos secunda?rios distribu??dos - Grande parte das computac?o?es
necessitam acessar fontes de armazenamento secunda?rio, isto inclui o acesso
a discos locais e remotos atrave?s do sistema de arquivos. Para prover esses
mecanismos de acesso, muitos sistemas foram desenvolvidos, como por exem-
plo: dNFSp (KASSICK et al., 2005; A?VILA et al., 2004), PVFS (HADDAD,
2000), iPVFS (OU; HE, 2005), etc.

• Disponibilidade - Na medida que o cluster se torna mais escala?vel, o seu MTBF
(MeanTime Between Failures) diminui. Isto faz com que sejam necessa?rias
medidas para garantir a continuidade da operac?a?o do sistema e minimizar o
tempo que os recursos esta?o indispon??veis. Alguns exemplos de ferramentas
empregadas com o intuito de prover mecanismos de reabilitac?a?o do sistema e
detecc?a?o e recuperac?a?o de falhas sa?o: C3 (Cornell Checkpoint (pre)Compiler)
(SCHULZ; et al., 2004; BRONEVETSKY; et al., 2004), CONDOR (THAIN;
LIVNY, 2003), entre outros.

Atrave?s da descric?a?o dos campos que envolvem o conceito de gerenciamento em
clusters, e da apresentac?a?o de va?rias ferramentas em cada uma de suas categorias,
pode-se perceber que na?o existe uma padronizac?a?o. O que existem sa?o diversas fer-
ramentas, desenvolvidas muitas vezes para suprir as necessidades de cada grupo de
pesquisa. Isto leva a existe?ncia de uma heterogeneidade de ferramentas de gerenci-
amento de clusters muito grande. Ale?m disso, muitas delas na?o foram projetadas
levando em considerac?a?o questo?es de interoperabilidade e de facilidade de uso.

No intuito de facilitar o gerenciamento dos clusters e tambe?m a forma como
os seus usua?rios utilizam seus recursos, foram desenvolvidos alguns ambiente e
plataformas de gerenciamento de clusters. A seguir, sera?o descritas as ferramen-
tas: OSCAR, OpenSCE (UTHAYOPAS; ANGSKUN; MANEESILP, 2001, 2002;
OPENSCE - SCALABLE CLUSTER ENVIRONMENT, 2005), ROCKS e o traba-
lho que tenta integrar o gerenciamento de clusters com SNMP (ALVES et al., 2004;
ALVES; MARQUEZAN; GRANVILLE, 2004; ALVES et al., 2005).

2.2.1.1 OSCAR - Open Source Cluster Application Resource

OSCAR e? uma ferramenta que visa a construc?a?o de um cluster que possua suas
ferramentas integradas em um u?nico ambiente. Seu objetivo e? permitir que usua?rios
e administradores de cluster possam criar, manter e utilizar clusters baseados em
plataforma Linux. O OSCAR e? uma colec?a?o de ferramentas comumente usadas
em clusters, as quais sa?o disponibilizadas em forma de arquivos tar que devem
ser instalados no front-end. O OSCAR e? composto pelos seguintes mo?dulos: (i)
nu?cleo da infra-estrutura e do gerenciamento, (ii) administrac?a?o e configurac?a?o, (iii)
servic?os e ferramentas de HPC e (iv) seguranc?a. Algumas ferramentas que fazem



23

parte do OSCAR sa?o: C3, System Imager, Ganglia, OpenPBS, MPI (DONGARRA;
et al., 1995), PVM (SUNDERAM, 1990), entre outras. Para utilizac?a?o do OSCAR
e? preciso um n??vel aprofundado de conhecimento da arquitetura e dos sistemas de
clusters.

2.2.1.2 OpenSCE - Open Scalable Cluster Environment

O projeto OpenSCE e? caracterizado por utilizar ferramentas pro?prias, ou seja,
que foram desenvolvidas pelo mesmo grupo que da? suporte ao projeto. O OpenSCE e?
um software constitu??do pelos seguintes tipos de ferramentas: (i) administrativas e de
monitoramento (Beowulf Builder, Ksix, SQMS), (ii) um sistema de gerenciamento de
filas (SCMS), um sistema Web de monitoramento (SCMSWeb), uma ferramenta de
visualizac?a?o e depurac?a?o de aplicac?o?es (MPView) e uma biblioteca de programac?a?o
paralela que e? baseada no MPI (MPITH). O ambiente OpenSCE pode ser uma boa
alternativa para gerenciamento de cluster quando este esta? sendo montado desde o
in??cio. Entretanto, quando ja? existe uma infra-estrutura montada, com ferramentas
diferentes das existentes no OpenSCE, pode-se tornar muito custosa a troca de todas
as ferramentas. Mais do que isso, e? preciso levar em considerac?a?o que os usua?rios
deste cluster tera?o que se readaptar a?s novas ferramentas, e muitas vezes a curva de
aprendizado na?o compensa o investimento.

2.2.1.3 NPACI Rocks Toolkit

Os trabalhos realizados no desenvolvimento do NPACI Rocks buscam encontrar
soluc?o?es para as dificuldades que existem na construc?a?o de clusters que sejam ge-
rencia?veis. De forma resumida, Rocks e? uma distribuic?a?o Linux voltada para clusters
baseada no Red Hat (RED HAT - THE OPEN SOURCE LEADER, 2005). Ale?m
disso, possui alguns pacotes adicionais e configurac?o?es programadas para automati-
zar a montagem de um cluster Linux de alto desempenho. O sistema e? baseado em
uma arquitetura tradicional de clusters: front-end, uma rede de interconexa?o Ether-
net (adicionalmente e? poss??vel uma rede de alto desempenho - Myrinet, por exem-
plo) e os no?s. Na distribuic?a?o NPACI Rocks foi desenvolvido uma infra-estrutura
de configurac?a?o com propriedades bem definidas baseadas em padro?es de facto, o
que inclui: XML (com parsers padra?o ), RedHat Kickstart, HTTP, CGI, base de
dados SQL e construtores de grafos para facilmente definir as ferramentas do cluster
(PAPADOPOULOS; KATZ; BRUNO, 2001). A ide?ia principal do projeto e? tor-
nar o uso de clusters fa?cil (ROCKS CLUSTER DISTRIBUTION: USERS GUIDE,
2005). Como apresentado em (BRUNO; et al., 2004), o objetivo do desenvolvimento
do Rocks e? permitir que pessoas que na?o sejam experientes em clusters consigam
facilmente montar e gerenciar seus clusters.

2.2.1.4 Clusters + SNMP

Esta ferramenta de gerenciamento de clusters baseado em SNMP foi desenvol-
vida no intuito de integrar o gerenciamento de clusters dentro de plataformas de
gerenciamento de redes. Visto que padronizac?a?o e? um problema nesta a?rea e que
os clusters podem apresentar uma visa?o u?nica de todo o sistema, pode-se gerenciar
clusters como se eles fossem mais um dispositivo da rede. Desta forma, consegue-se
inserir a tarefa de gerenciamento de clusters juntamente com o gerenciamento de
redes. A vantagem desta abordagem e? o aproveitamento de diversas ferramentas e



24

plataformas de gerenciamento ja? desenvolvidas no escopo de redes no contexto de
clusters. Seguindo o framework SNMP, padra?o de facto em gerenciamento de redes,
foram desenvolvidos: MIBs de gerenciamento do front-end e dos no?s, agentes que
seguem esta MIB e um gerente. A ferramenta desenvolvida possui suporte: para
monitoramento dos recursos dos clusters e das aplicac?o?es que esta?o executando;
para escalonamento das aplicac?o?es; para configurac?a?o da pol??tica de alocac?a?o do
cluster. Este ambiente foi integrado na plataforma de gerenciamento QAME (QoS-
Aware Management Environment) (GRANVILLE; TAROUCO, 2001), mostrando
que pode-se integrar o gerenciamento de clusters e redes.

2.2.1.5 Resumo sobre ambientes de gerenciamento de clusters

Existe um conjunto de ferramentas de gerenciamento de clusters bastante grande.
Nesta sec?a?o tentou-se mostrar alguns ambientes que oferecem um conjunto maior de
funcionalidades. Mesmo assim, pode-se perceber que o OSCAR e o Rocks Toolkit
sa?o ferramentas mais voltadas para a parte de configurac?a?o de clusters, direcionadas
para o momento de estabelecimento e configurac?a?o dos cluster em si. Enquanto
isto, as ferramentas OpenSCE e Cluster + SNMP sa?o mais voltadas para o uso de
clusters e monitoramento de seus recursos e aplicac?o?es.

2.2.2 Grids

Segundo Ahmar Abbas (ABBAS, 2004) existem os tipos de grids apresentados
a seguir.

• Grids Departamentais (Departamental Grids)- Sa?o constru??dos para resolver
problemas de um grupo particular dentro de uma empresa. Os recursos na?o
sa?o compartilhados por outros grupos da mesma instituic?a?o.

• Grids Empresariais (Enterprise Grids) - Sa?o formados por recursos espalhados
pela empresa e que prove?em servic?os para todos os usua?rios dentro da empresa.

• Grids Extra-empresariais (Extraprise Grids) - Caracteriza-se por ser estabe-
lecido entre companhias, seus parceiros e seus clientes. Os recursos sa?o dis-
ponibilizados atrave?s de uma rede virtual privada (VPN - Virtual Private
Network ).

• Grids Globais (Global Grids) - Sa?o formados atrave?s da Internet. Podem ser
formados no intuito de facilitar os nego?cios entre instituic?o?es.

• Grids Computacionais (Compute Grids) - Sa?o criados com o objetivo exclusivo
de prover acesso a recursos computacionais. Eles sa?o divididos de acordo com
o tipo de hardware empregado, os quais podem ser: desktops, servidores e
baseados em sistemas de alto desempenho.

• Grids de Dados (Data Grids) - Este tipo de grid e? otimizado para trabalhar
com operac?o?es orientadas a dados.

• Utility Grids - Sa?o definidos como sendo recursos computacionais que sa?o
mantidos e gerenciados por um provedor de servic?o.

Como pode-se perceber existem muitas formas diferentes de encarar a utilizac?a?o
de grids. Isto faz com que aparec?am diferentes viso?es das definic?o?es e padro?es a



25

serem adotados na a?rea de grids (BAKER; et al., 2005). No que diz respeito a parte
de gerenciamento de grids, existem muitos aspectos que devem ser considerados.
Alguns deles sa?o descritos a seguir.

1. Seguranc?a - E? preciso garantir que somente usua?rios que fac?am parte do grid
tenham acesso aos recursos, ale?m disso e? preciso restringir acesso de alguns
usua?rios a determinados servic?os existentes em grids. Neste escopo, o GSI
(Grid Security Infrastructure), promovido pelo GGF (Global Grid Forum), e?
um padra?o bem estabelecido e que trata dos problemas de seguranc?a em grid
(WELCH; et al., 2003).

2. Monitoramento - Dado o fato de grids serem estruturas extremamente dis-
tribu??das e complexas, e? preciso que essas estruturas sejam monitoradas. Se-
gundo Maozhen Li e Mark Baker (LI; BAKER, 2005), o objetivo do monito-
ramento de grids e? a medic?a?o e a publicac?a?o do estado dos recursos em um
certo per??odo de tempo. O GMA (Grid Monitoring Architecture), proposto
pelo GGF, e? uma tentativa de criac?a?o de um mecanismo padronizado para
obtenc?a?o e disponibilizac?a?o das informac?o?es de monitoramento de grids (TI-
ERNEY; et al., 2002; LI; BAKER, 2005). Alguns exemplos de ferramentas que
implementam o GMA sa?o: GridRM (BAKER; SMITH, 2003), HRIC (ZOU;
et al., 2005) e R-GMA (COOKE; et al., 2003).

3. Escalonamento e gerenciamento de recursos - As aplicac?o?es dos usua?rios sa?o
disparadas em ma?quinas que esta?o localizadas em diferentes dom??nios admi-
nistrativos. E? preciso a existe?ncia de ambientes capazes de gerenciar essas
aplicac?o?es e escalona?-las. Ferramentas como por exemplo: Globus (FOSTER;
KESSELMAN, 1997; GLOBUS - WELCOME TO THE GLOBUS TOOL-
KIT HOMEPAGE, 2005), Condor (GONZ; et al., 2002; THAIN; TANNEN-
BAUM; LIVYN, 2003), Legion (GRIMSHAW; NATRAJAN, 2005), OurGrid
(ANDRADE; et al., 2003), UNICORE (BENEDYCZAK; et al., 2005), SGE
(GENTZSCH, 2001), entre outros.

4. Facilidade de uso - Para que os desenvolvedores de aplicac?o?es de grid tenham
facilidades de acesso e utilizac?a?o de grids foi criado o conceito de Portais
de Grid. Um portal de grid e? um gateway baseado em tecnologia Web que
prove? facilidades de acesso a uma variedade de recursos. Segundo Li e Baker
(LI; BAKER, 2005) existem duas gerac?o?es de portais. Na primeira gerac?a?o
os portais eram constru??dos utilizando uma arquitetura baseada em tre?s ca-
madas. Exemplos de portais desta gerac?a?o sa?o: GridPort (THOMAS; et al.,
2001; DAHAN; et al., 2004), GridSpeed (SUZUMURA; et al., 2004), Genius
(ANDRONICO; et al., 2003), Ninf (NAKADA; et al., 2004), entre outros. A
segunda gerac?a?o de portais de grid e? caracterizada por ser constru??da a partir
de portlets, os quais sa?o componentes de software escritos em Java, gerenciados
por portlet containers, e que sa?o responsa?veis por gerenciar as requisic?o?es dos
usua?rios e pela gerac?a?o dina?mica de conteu?do (LI; BAKER, 2005). Exemplos
de portais da segunda gerac?a?o sa?o: PortalLab (LI; et al., 2003) e GridSphere
(NOVOTNY; RUSSELL; WEHRENS, 2004),

Atrave?s da descric?a?o apresentada acima sobre alguns aspectos que devem ser
considerados no gerenciamento de grids e da apresentac?a?o de algumas das ferra-



26

mentas dispon??veis, e? poss??vel perceber que o cena?rio de grids e? bastante complexo
e ainda em esta?gio de evoluc?a?o.

A utilizac?a?o de grids e? um passo natural para os usua?rios de clusters. Atual-
mente, e? cada vez mais necessa?ria a utilizac?a?o de uma quantidade maior de recursos
computacionais para solucionar os problemas existentes. Desta forma, e? bastante
plaus??vel que se integre recursos existentes em diferentes instituic?o?es, visando reduzir
o custo de aquisic?a?o de toda uma infra-estrutura computacional.

No entanto, existe um custo de adaptac?a?o de ferramentas e de reeducac?a?o dos
usua?rios de clusters que muitas vezes na?o e? considerado. Nem todos os centros
de pesquisa, corporac?o?es e empresas, esta?o prontas para migrarem suas aplicac?o?es e
usua?rios de um mundo baseado em clusters para um baseado em grids. Considerando
este cena?rio, existe um meio termo entre o contexto de grids e o de clusters. Este
contexto e? descrito a seguir.

2.2.3 Intersecc?a?o entre clusters e grids: sistemas Multicluster

Nem todos os centros de pesquisa e instituic?o?es em geral, que possuem mu?ltiplos
clusters, aderem ao cena?rio de grids para aumentar o seu potencial de processa-
mento, armazenamento, etc. Na?o e? raro que em uma mesma instituic?a?o existam
va?rios clusters que possuem seu funcionamento completamente isolado um do ou-
tro. Essa situac?a?o geralmente acontece, porque na maioria das vezes esses clusters
sa?o adquiridos em diferentes pontos do tempo.

Existem trabalhos no sentido de integrar os diferentes clusters como se eles for-
massem uma u?nica ma?quina, o que se chamou de sistemas multiclusters (BAR-
RETO; A?VILA; NAVAUX, 2000; AUMAGE, 2002). Multiclusters podem ser enca-
rados sob a visa?o dos usua?rios que utilizam os recursos ou sob a visa?o do adminis-
trador desses clusters. No caso dos usua?rios a ide?ia de multicluster faz com que seja
necessa?ria a criac?a?o de ferramentas que de?em o suporte para a comunicac?a?o entre
os diferentes clusters de forma amiga?vel. No caso de gerenciamento de mu?ltiplos
clusters, na?o e? preciso a existe?ncia dessa infra-estrutura de comunicac?a?o. Basta que
se tenha acesso aos diferentes clusters. A seguir sera?o abordados os aspectos de
gerenciamento e acesso a mu?ltiplos clusters.

A heterogeneidade e? a caracter??stica que permeia o ambiente de mu?ltiplos clus-
ters. Para gerenciar e acessar esses recursos e? preciso, primeiramente, lidar com as
diferentes ferramentas instaladas em cada um. Essa heterogeneidade passa a ser
uma dificuldade enfrentada por seus usua?rios, pois eles te?m que aprender a lidar
com cada uma dessas ferramentas para desempenharem suas atividades.

Para usua?rios acostumados com a cie?ncia da computac?a?o isto pode na?o repre-
sentar um empecilho. No entanto, para usua?rios que pertencem a outras a?reas de
atuac?a?o a dificuldade de ter de se acostumar e aprender os detalhes e sema?ntica do
grande nu?mero de ferramentas passa a ser um problema considera?vel. No intuito
de facilitar o gerenciamento e o acesso de mu?ltiplos clusters foram desenvolvidas
algumas ferramentas. Duas dessas ferramentas sera?o apresentadas a seguir.

2.2.3.1 M3C Managing and Monitoring Multiple Clusters

O M3C (BRIM; et al., 2001) proporciona uma interface gra?fica Web para admi-
nistrac?a?o de clusters e tambe?m um framework para o desenvolvimento dos sistemas
de gerenciamento das camadas inferiores. Ele foi projetado para ser extens??vel, uma
vez que novas funcionalidades de clusters podem ser inseridas no ambiente atrave?s



27

da aplicac?a?o do framework. Embora apresente a capacidade de estender o conjunto
de funcionalidades, o M3C na?o tem suporte para a extensa?o das ferramentas para
cada funcionalidade. Ou seja, ele na?o e? capaz de lidar com diferentes ferramentas
para uma mesma funcionalidade. O M3C foi implementado utilizando Java Applets
e Servlets e tambe?m utiliza arquivos que armazenam informac?o?es sobre os recur-
sos dos clusters. Este ambiente apresenta suporte para: monitoramento dos no?s,
reserva de no?s e particionamento de clusters. Questo?es de seguranc?a sa?o tratadas
utilizando o protocolo HTTPS para encriptac?a?o dos dados transmitidos, e realizando
verificac?o?es de quais clusters cada usua?rio pode utilizar. O M3C apresenta-se como
um ambiente contendo basicamente funcionalidades de acesso e monitoramento de
mu?ltiplos clusters, ale?m disto, por ser implementado em Java possui um certo grau
de independe?ncia de plataforma.

2.2.3.2 HPC2N - High Performance Computing Center North

O HPC2N (ELMROTH; NYLE?N; OSCARSSON, 2005) foi desenvolvido para ser
utilizado em sistemas de alto desempenho. Ele foi projetado para ser um ambiente
com foco nos usua?rios, provendo funcionalidades para lidar com suas aplicac?o?es e su-
porte para monitoramento. O acesso ao HPC2N e? proporcionado atrave?s da Internet
utilizando a tecnologia CGI. As comunicac?o?es entre o servidor HPC2N e os nave-
gadores dos usua?rios sa?o realizadas utilizando o protocolo HTTPS e a autenticac?a?o
dos usua?rios e? feita atrave?s do protocolo Kerberos (COULOURIS; DOLLIMORE;
KINDBERG, 2005). Diferentemente do M3C, o HPC2N na?o apresenta nenhum su-
porte para ser estendido. Ou seja, na?o e? poss??vel inserir novas funcionalidades e
tambe?m nem inserir novas ferramentas nesse ambiente.

2.2.3.3 Problemas em Aberto no Cena?rio de Gerenciamento de Mu?ltiplos Clusters

Por se tratar de um ambiente heteroge?neo, alguns aspectos ligados a prover
facilidades para os usua?rios devem ser considerados na criac?a?o de ambientes para
gerenciamento e acesso de mu?ltiplos clusters. Alguns aspectos importantes sa?o:

• transpare?ncia de acesso;

• extensibilidade;

• uniformizac?a?o das operac?o?es;

• completeza;

• interoperabilidade;

• capacidade de incorporac?a?o dos sistemas de gerenciamento legados;

• construc?a?o de ferramentas amiga?veis para os usua?rios.

Analisando as ferramentas de gerenciamento e acesso de mu?ltiplos clusters apre-
sentadas acima percebe-se que elas na?o possuem suporte para todos os aspectos des-
critos acima. Transpare?ncia de acesso e? provida tanto pelo ambiente M3C quanto
pelo HPC2N. Extensibilidade e? provida somente pelo M3C, e ainda de forma par-
cial. Uniformizac?a?o das operac?o?es esta? relacionada com a caracter??stica de extensi-
bilidade, pois implica na possibilidade de lidar com diferentes ferramentas mas com



28

a mesma funcionalidade. Nenhum dos ambientes proporcionam essa caracter??stica.
Da mesma forma como na?o apresentam completeza (capacidade de lidar com di-
ferentes funcionalidades de clusters), interoperabilidade, capacidade de lidar com
sistemas legados. Os ambientes M3C e HPC2N apresentam interfaces gra?ficas que
facilitam as operac?o?es dos usua?rios. Enfim, esses ambientes promovem facilidades no
gerenciamento e acesso a mu?ltiplos clusters, mas ainda na?o esta?o suficientemente ma-
duros para prover caracter??sticas fundamentais como extensibilidade, uniformizac?a?o,
interoperabilidade e suporte a sistemas legados.

2.3 Resumo

Neste cap??tulo foram apresentadas as principais caracter??sticas de sistemas dis-
tribu??dos e de sistemas de alto desempenho. Como visto, sistemas de alto desem-
penho podem ser considerados como uma especializac?a?o de sistemas distribu??dos.
Eles apresentam, em geral, os mesmos problemas de gerenciamento, facilidade de
utilizac?a?o, so? que possuem um fim mais espec??fico - resolver problemas de forma
eficiente. Dentro de sistemas HPC, te?m-se clusters e grids como os cena?rios mais
utilizados pelos usua?rios. O gerenciamento desse tipo de sistema apresenta desafios.
E? preciso prover aos usua?rios e administradores ferramentas de fa?cil utilizac?a?o e que
consigam gerenciar as funcionalidades desses cena?rios. Na a?rea de gerenciamento
de clusters existem muitos trabalhos consolidados e bem estabelecidos, enquanto
na a?rea de grids ainda existem discusso?es e campo aberto para inovac?o?es e novas
abordagens. Entretanto, tambe?m foi apresentado neste cap??tulo que existe uma
intersecc?a?o entre clusters e grids, denominada de multiclusters. Essa intersecc?a?o
apresenta algumas ferramentas de gerenciamento, mas elas ainda na?o prove?em ca-
racter??sticas importantes para um escopo como este, distribu??do e heteroge?neo.



29

3 PROPOSTA DE AMBIENTE INTEGRADO PARA

CLUSTERS: ICE

O ambiente ICE (Integrated Cluster Environment) (MARQUEZAN et al., 2006)
e? proposto com o intuito de prover as caracter??sticas de gerenciamento de mu?ltiplos
clusters que ainda na?o foram contempladas pelas plataformas ja? existentes, como
apresentado na Cap??tulo 2. Sendo assim, os objetivos do ambiente ICE sa?o prover
as caracter??sticas descritas abaixo.

• Extensibilidade - O ambiente tem que ser capaz de incorporar novas funciona-
lidades sem que isso impacte na sua execuc?a?o em geral. Por funcionalidades
entende-se os servic?os tipicamente disponibilizados para os usua?rios dos clus-
ters ou aqueles utilizados pelos pro?prios administradores. Exemplos de funcio-
nalidades sa?o: monitoramento dos recursos de um cluster, monitoramento das
aplicac?o?es, gerenciamento das aplicac?o?es dos usua?rios, ac?o?es administrativas
nos componentes dos clusters (no?s, front-end e rede de intercomunicac?a?o), etc.
Mas ale?m de prover extensibilidade no n??vel das funcionalidades do sistema, o
ambiente ICE tambe?m deve visar a extensibilidade de ferramentas. Isso signi-
fica que o ambiente deve suportar a utilizac?a?o de qualquer ferramenta, e que
na?o esta? restrito a um conjunto fixo de ferramentas.

• Transpare?ncia - A proposta do ambiente ICE e? prover transpare?ncia de acesso
a?s ferramentas e tambe?m transpare?ncia na forma de utilizac?a?o dessas ferra-
mentas. Dessa forma os usua?rios podera?o acessar as ferramentas sem ter a
noc?a?o de quais exatamente esta?o utilizando. A mesma ide?ia se aplica para
execuc?a?o das suas operac?o?es, isto e?, os usua?rios na?o ira?o lidar diretamente
com o formato dos para?metros e nem com os comandos de cada uma delas.

• Uniformizac?a?o das operac?o?es - Atrave?s de sua arquitetura o ambiente ICE se
propo?e a uniformizar a forma como as operac?o?es das ferramentas integradas
sa?o realizadas. O objetivo e? fazer com que os usua?rios do sistema executem
as mesmas ac?o?es independentemente do tipo de ferramenta que esta? instalada
nos clusters aos quais eles te?m acesso.

• Completeza - O ambiente proposto visa abranger as atividades tipicamente
executadas em um cluster, como por exemplo: gerenciamento de aplicac?o?es
(submissa?o, te?rmino, visualizac?a?o) monitoramento de recursos e aplicac?o?es e
intervenc?o?es administrativas. Pretende-se que o usua?rio acesse o ambiente
ICE e atrave?s dele consiga fazer todas as tarefas que anteriormente ele fazia
atrave?s de um terminal. Para isso a definic?a?o da arquitetura ICE procura ser



30

o mais flex??vel poss??vel para permitir que diferentes tipos de atividades sejam
inclu??das na plataforma.

• Interoperabilidade - Para que o ambiente ICE seja capaz de gerenciar mu?ltiplos
clusters, cada um com diferentes ferramentas, e? preciso que ele seja capaz de
lidar com os sistemas legados desses clusters. Essa e? uma caracter??stica fun-
damental e que diferencia o ambiente ICE dos demais trabalhos realizados no
mesmo escopo. O ICE e? capaz de ser instalado sem que isso interfira na infra-
estrutura existente no cluster. Ale?m disso, o ambiente ICE e? independente
de plataforma de hardware e software. Ele e? capaz de lidar com essas espe-
cificidades de maneira transparente. Portanto, dentro do ambiente pode-se
gerenciar clusters baseados em diferentes arquiteturas de hardware e sistemas
operacionais.

• Facilidade de uso - Uma caracter??stica tambe?m importante do ambiente ICE
e? prover facilidade de uso aos seus usua?rios. Esse ambiente permite que seus
usua?rios trabalhem com as ferramentas existentes nos clusters sem ter que
lidar com os para?metros e comandos espec??ficos das mesmas. Atrave?s da apre-
sentac?a?o gra?fica, eles podem realizar suas atividades sem ter que aprender a
usar as ferramentas. No cena?rio de mu?ltiplos clusters essa caracter??stica e? im-
portante. Ao inve?s de aprender a lidar com todas as ferramentas dos clusters,
aos quais os usua?rios tem acesso, eles aprendem a lidar somente com o ambi-
ente ICE. Esse, por sua vez, apresenta um ponto u?nico de acesso aos usua?rios e
trata toda a complexidade das ferramentas sem a intervenc?a?o de seus usua?rios.

O pu?blico alvo do ambiente ICE sa?o usua?rios que na?o sa?o nativos da a?rea da
cie?ncia da computac?a?o e mais especificamente da a?rea de alto desempenho, como
por exemplo, pesquisadores nas a?reas de f??sica, bio-informa?tica, geo-processamento,
etc. Esse tipo de usua?rio utiliza os recursos que os sistemas HPC disponibilizam, mas
na?o sa?o seus objetivos saber exatamente como as coisas funcionam e nem lidar com
as especificidades das ferramentas que executam as tarefas que eles precisam. Por
exemplo, usua?rios da a?rea de recursos h??dricos necessitam de clusters para executa-
rem suas simulac?o?es, mas na?o e? seu interesse saber como funciona um gerenciador de
aplicac?o?es (como o OpenPBS (PORTABLE BATCH SYSTEM, 2005)) ou uma fer-
ramenta de monitoramento (como o Ganglia (SACERDOTI; et al., 2003)). Muitas
vezes esse tipo de usua?rios tem acesso a diferentes sistemas de HPC. Para cada um
deles seria necessa?rio aprender o funcionamento das ferramentas. E? para atender
a?s necessidades desse tipo de usua?rios que o ambiente ICE torna-se mais indicado.
Entretanto, isto na?o significa que usua?rios da a?rea de computac?a?o de alto desem-
penho na?o possam tirar proveito do ambiente. Esse tipo de usua?rio, em geral com
maior conhecimento das ferramentas empregadas, pode se beneficiar das facilidades
de acesso e de visualizac?a?o que o ambiente ICE proporciona.

Outra caracter??stica do ambiente ICE e? a filosofia de incorporar e buscar adap-
tar padro?es e especificac?o?es ao contexto de gerenciamento de mu?ltiplos clusters. A
ide?ia e? evitar criar novas infra-estruturas, protocolos e especificac?o?es. O objetivo e?
fazer com que os sistemas de clusters que ja? existem sejam integrados em um u?nico
ambiente sem que isso exija a utilizac?a?o de mecanismos e middlewares que res-
trinjam extensibilidade e interoperabilidade do ambiente ICE. Organizac?o?es, como
por exemplo, W3C (W3C ARCHITECTURE DOMAIN - WEB SERVICES AC-
TIVITY, 2005) e OASIS (OASIS-WS - OASIS COMMITTEES BY CATEGORY:



31

WEB SERVICES, 2005), tem trabalhado na padronizac?a?o de protocolos e em espe-
cificac?o?es para criac?a?o de sistemas inter-opera?veis. Exemplo destes esforc?os sa?o as
especificac?o?es ligadas a Web services. Outras organizac?o?es como por exemplo GGF
(GLOBAL GRID FORUM, 2005) esta?o trabalhando na definic?a?o de especificac?o?es
e padronizac?o?es na a?rea de grids. A filosofia que se pretende adotar no ambiente
ICE e? aproveitar estas iniciativas so? que adaptando-as para o cena?rio de mu?ltiplos
clusters.

O contexto de grid e de mu?ltiplos clusters e? semelhante em alguns aspectos. Am-
bos lidam com recursos que esta?o dispersos fisicamente, o que exige a inserc?a?o de um
certo grau de seguranc?a no sistema. O acesso aos recursos tem que ser transparente e
deve existir uma maneira de uniformizar a utilizac?a?o das ferramentas dos diferentes
recursos participantes. Entretanto, o cena?rio de grids requer mais mecanismos de
controles. Em grids e? preciso levar em considerac?a?o questo?es como: dinamicidade
de recursos, manter canais de comunicac?a?o entre recursos alocados em dom??nios
administrativos diferentes, manter informac?o?es de controle de onde esta?o sendo exe-
cutadas as aplicac?o?es dentre os diversos dom??nios, entre outros (LI; BAKER, 2005).
Todos esses controles sa?o decorrentes do fato dos recursos que fazem parte dos grids
serem alocados de forma transparente e independente. Isto e?, o usua?rio solicita um
determinado nu?mero de recursos independentemente de sua localizac?a?o f??sica. No
contexto de mu?ltiplos clusters, ao qual o ambiente ICE se destina, o usua?rio sabe
onde esta?o os recursos que ele esta? solicitando. Dessa forma, na?o e? preciso manter
toda a infra-estrutura de controle de localizac?a?o de recursos neste contexto. Baseado
nas semelhanc?as e distinc?o?es entre estes dois cena?rios percebeu-se que alguns con-
ceitos, especificac?o?es e padro?es do mundo de grid podem ser mapeados e utilizados
no gerenciamento de mu?ltiplos clusters. Esses reaproveitamentos sera?o abordados
com mais detalhes no Cap??tulo 4.

A seguir sera?o apresentados os cena?rios distribu??dos nos quais o ambiente ICE
pode ser adotado e o modelo da arquitetura projetada para o ambiente ICE.

3.1 ICE como um Sistema Distribu??do

Antes de compreender a arquitetura projetada para a plataforma ICE e? preciso
entender o grau de distribuic?a?o inserido neste contexto. O primeiro ponto a ser
considerado e? o escopo em que o ambiente ICE esta? inserido. Ele foi definido para
ser empregado em um cena?rio composto por mu?ltiplos clusters, entretanto o acesso
a estes diferentes clusters e? realizado atrave?s de um u?nico ponto, o qual esconde a
complexidade de lidar com diferentes clusters distribu??dos em diferentes dom??nios
administrativos. A Figura 3.1 apresenta este cena?rio de utilizac?a?o do ambiente ICE.

Os clusters gerenciados pelo ambiente ICE podem estar em quaisquer dom??nios
administrativos, assim como os usua?rios que os acessam. Como a Figura 3.1 mostra,
existem interac?o?es entre usua?rios e o ambiente ICE e entre esse e os clusters gerenci-
ados. Esses dois tipos de interac?o?es possuem caracter??sticas diferentes. No primeiro
caso trata-se de um sistema Web. Sendo assim, e? preciso lidar com as questo?es
relacionadas a aplicac?o?es Web, como por exemplo: separac?a?o entre a apresentac?a?o
das informac?o?es da lo?gica do sistema (modelo de aplicac?a?o em 3 camadas), questo?es
de seguranc?a no servidor Web, entre outras. No segundo caso, trata-se de um mid-
dleware para gerenciamento das atividades em clusters. Questo?es de seguranc?a,
uniformizac?a?o do acesso, interoperabilidade e flexibilidade devem ser consideradas



32

Cluster 3Cluster 1 Cluster 2

ICE

Usuário A Usuário B Usuário C

Domínio
Administrativo 1

Domínio
Administrativo 2

Domínio
Administrativo 3

Figura 3.1: Visa?o geral do cena?rio de utilizac?a?o do ICE

nas deciso?es de definic?a?o dessa parte do sistema.

O segundo ponto considerado relaciona-se com o fato do ambiente ICE na?o ser
apenas um Portal Web ou um sistema rodando no front-end de um cluster. O
ambiente ICE e? a unia?o entre o ponto u?nico de acesso dos usua?rios - Portal Web -
com o middleware definido para o gerenciamento e acesso das funcionalidades dos
clusters.

3.2 Arquitetura ICE

No momento que se compreende o cena?rio no qual o ambiente ICE sera? utilizado
pode-se perceber que a arquitetura desse sistema deve ser o mais flex??vel poss??vel
para poder lidar com a heterogeneidade de seu contexto.

Uma caracter??stica principal da arquitetura ICE e? sua modularidade. O obje-
tivo e? que se possa inserir novas funcionalidades e novas ferramentas sem que isso
altere o modelo do ambiente. A ide?ia por tra?s da arquitetura do ambiente ICE e?
fazer com que existam mo?dulos ba?sicos do sistema e uma infra-estrutura para pro-
ver servic?os de clusters. Na visa?o do ambiente ICE, cada funcionalidade existente
em um cluster, como por exemplo: monitoramento de recursos, gerenciamento de
aplicac?o?es e recursos atrave?s do uso de escalonadores, e? potencialmente um servic?o
a ser disponibilizado.

Como apresentado na Sec?a?o 3.1, esse ambiente e? divido em dois contextos: ponto
u?nico de acesso (Portal Web) e o middleware para lidar com o gerenciamento e
acesso aos clusters. De modo geral o mapeamento da arquitetura ICE para esses
contextos pode ser feita da seguinte forma: os mo?dulos ba?sicos sa?o mapeados para
a infra-estrutura necessa?ria para modelagem e construc?a?o do Portal Web; enquanto
a infra-estrutura para prover os servic?os e? mapeada para o middleware.

Na concepc?a?o da arquitetura ICE, optou-se por utilizar o modelo de arquite-
tura orientada a servic?os (SOA - Service Oriented Architecture) (MCGOVERN; et



33

al., 2003) para definic?a?o do middleware que viabiliza a infra-estrutura de servic?os
nos clusters. Essa decisa?o foi tomada dadas as caracter??sticas que este modelo de
arquitetura proporciona a?s aplicac?o?es que a adotam (MCGOVERN; et al., 2003):

• os servic?os sa?o descobertos e dinamicamente localizados;

• os servic?os sa?o auto-contidos, modulares e apresentam um baixo acoplamento;

• os servic?os possuem interoperabilidade;

• os servic?os apresentam transpare?ncia de localizac?a?o e podem ser compostos
com outros servic?os;

• a arquitetura orientada a servic?os suporta a caracter??stica de auto-cura (self-
healing), ou seja, e? capaz de identificar problemas em sua estrutura e tomar
medidas para trata?-las.

A maior vantagem em utilizar uma arquitetura orientada a servic?os e? poder
separar a implementac?a?o de um servic?o de sua interface. Essa caracter??stica e? fun-
damental para a modelagem e o desenvolvimento do ambiente ICE.

A Figura 3.2 apresenta a arquitetura definida para o ambiente ICE, a qual esta?
dividida em dois componentes principais: Middleware de Servic?os e o Portal Web.
O Middleware de Servic?os e? formado por tre?s mo?dulos: Unified Service Interface
(USI), Service Implementation (SI) e Service Module (SM). Atrave?s da Figura 3.2
pode-se perceber que o SM tambe?m faz parte do Portal Web. Esse mo?dulo sera?
detalhado no decorrer desta sec?a?o e sera?o esclarecidas as razo?es pelas quais ele faz
parte dos dois componentes. Ainda formam o Portal Web os mo?dulos: Security
Module (SecM) e System Management Module (SMM). A seguir cada componente
e seus respectivos mo?dulos sera?o descritos detalhadamente.

Figura 3.2: Arquitetura ICE



34

3.2.1 Middleware de Servic?os

No Midleware de Servic?os (os mo?dulos cinza escuro dentro da a?rea achurada
na Figura 3.2) sa?o considerados os aspectos ligados a arquitetura SOA utilizada
no ambiente ICE. Esse modelo se caracteriza pela existe?ncia de um provedor, um
consumidor e um registro de servic?os. Mapeando essa estrutura para o contexto do
ambiente ICE te?m-se como provedor de servic?os os mo?dulos SI nos front-ends dos
clusters, os consumidores sa?o os mo?dulos SM e o registro de servic?os esta? no repo-
sito?rio do Portal Web. Segundo a definic?a?o da arquitetura SOA (MCGOVERN; et
al., 2003; ALONSO; et al., 2004) e? preciso que exista uma forma de descrever esses
servic?os. Essa descric?a?o, no escopo do ambiente ICE e? o mo?dulo USI. Os mo?dulos
que compo?em o Middleware de Servic?os sa?o estruturas conceituais. Atrave?s da
instanciac?a?o das mesmas e? que pode-se efetivamente definir e disponibilizar funcio-
nalidades no ambiente ICE. Por questo?es de melhor compreensa?o seus mo?dulos sera?o
apresentados na seguinte ordem: USI, SI e SM.

Ao mo?dulo Unified Service Interface (USI) e? atribu??da a func?a?o de servir
como elemento de definic?a?o da interface do servic?o a ser fornecido por um cluster. A
definic?a?o de uma interface implica na especificac?a?o das operac?o?es desse servic?o, de
seus para?metros e da sema?ntica que lhe sera? atribu??da. Atrave?s de uma USI pode-se
uniformizar o acesso a diferentes ferramentas que possuam a mesma funcionalidade.
O processo de criac?a?o de uma USI consiste na ana?lise de ferramentas semelhantes,
que sa?o tipicamente utilizadas no cena?rio de clusters, e na descoberta de carac-
ter??sticas em comum entre elas. Esse processo e? necessa?rio para que o ambiente ICE
mantenha compatibilidade com os sistemas ja? empregados nos clusters e que na?o se
criem operac?o?es e para?metros que na?o podera?o ser mapeados para as ferramentas
existentes nos mesmos. A intersecc?a?o gerada por essa ana?lise deve conter pelo me-
nos um conjunto m??nimo de operac?o?es e para?metros utilizadas pelos usua?rios dessa
funcionalidade e ao mesmo tempo deve conter o ma?ximo de caracter??sticas em co-
mum das ferramentas analisadas. Todo este processo de ana?lise e definic?a?o pode se
tornar bastante emp??rico dependendo do tipo de funcionalidade que se esta? tentando
especificar. Como um dos objetivos do ambiente ICE e? a utilizac?a?o de padro?es e
especificac?o?es ja? existentes, recomenda-se que a definic?a?o de USIs seja baseada em
alguma especificac?a?o em vigor.

O mo?dulo Service Implementation (SI) e? o elemento que prove? os servic?os
dentro da arquitetura ICE. Para que se possa modelar um SI e? preciso que exista
uma USI definida. Para uma mesma USI pode-se definir diferentes SIs, isto e?, para
cada ferramenta com a mesma funcionalidade pode-se implementar um SI. E? impor-
tante ressaltar que existem situac?o?es nas quais na?o e? poss??vel haver o mapeamento
direto entre as operac?o?es de uma USI para os comandos e para?metros da ferramenta
que esta? sendo integrada no ambiente. Neste caso, cabe ao mo?dulo SI tratar essas
situac?o?es, fazendo as adaptac?o?es necessa?rias sem que elas interfiram no funciona-
mento da ferramenta em si. Na?o interferir no funcionamento e? um requisito que
deve ser seguido para que se possa atingir a capacidade de integrac?a?o de sistemas
legados que o ambiente visa. A combinac?a?o de USI mais SIs garante a capacidade
de extensibilidade de ferramentas visada pelo ambiente ICE.

O Middleware de Servic?os ainda e? formado pelo Service Module (SM). Como
apresentado anteriormente, o SM esta? presente tanto no Portal Web quanto nesse
componente. Aqui, ele assume as responsabilidades de consumidor de servic?os. Da
mesma forma como a construc?a?o do SI depende de uma USI, o SM tambe?m deve



35

seguir a USI definida para a funcionalidade em questa?o. A caixa preenchida em
cinza escuro no mo?dulo Service Module da Figura 3.2 ilustra a diferenc?a entre as
func?o?es que ele assume em cada componente. A porc?a?o consumidora de servic?os
do SM monta a requisic?a?o para uma determinada operac?a?o atrave?s das informac?o?es
recebidas pela parte gra?fica do SM, pertencente ao Portal Web. Ao receber a res-
posta do provedor de servic?os, deve tratar as informac?o?es entregando-as para serem
novamente apresentadas na interface Web. Para cada USI e? preciso o desenvolvi-
mento de somente um SM a ser integrado no Portal Web, pois a forma de acesso
das operac?o?es deve ser uniforme e transparente.

3.2.2 Portal Web

Portal Web e? o componente cujo foco principal e? encapsular o acesso a?s funci-
onalidades dos clusters atrave?s de um u?nico ponto. Por questo?es de flexibilidade
e acessibilidade decidiu-se que esse componente sera? uma aplicac?a?o Web baseada
no modelo de tre?s camadas (ALONSO; et al., 2004) e no modelo MVC (Model
View-Controller ) (HANSEN; FOSSUM, 2005). Atrave?s desses modelos e? poss??vel,
respectivamente, concentrar a lo?gica do sistema em um middleware, resguardando
as informac?o?es e recursos do sistema; e tambe?m separar a lo?gica do sistema de sua
apresentac?a?o. Como ilustrado na Figura 3.2, o Portal Web e? composto por tre?s
mo?dulos, os quais sa?o descritos abaixo.

O Security Module (SecM) trata das questo?es de autenticac?a?o e de autorizac?a?o
dos usua?rios do ambiente ICE. Somente usua?rios autenticados podem utilizar esse
ambiente. Ale?m disso, esses usua?rios possuem restric?o?es de acesso. Eles podem
acessar somente a?reas nas quais esta?o autorizados para tanto. O processo de au-
torizac?a?o esta? ligado a? ide?ia de perfis e grupos de usua?rios. A primeira ide?ia esta?
relacionada a questo?es de autorizac?a?o no Portal Web, enquanto a segunda esta? ligada
aos front-ends dos clusters e consequ?entemente ao middleware de servic?os. Maiores
informac?o?es sobre o esquema de seguranc?a definido para o ambiente ICE podem ser
encontradas em (ILHA, 2005).

O System Management Module (SMM) foi definido no intuito de concentrar
as atividades de gerenciamento e configurac?a?o do pro?prio ambiente ICE. Ele possui
as seguintes atribuic?o?es:

• gerenciar os perfis dos usua?rios, isto inclui a definic?a?o de perfis e das permisso?es
de acesso a?s a?reas do ambiente ICE que cada tipo de perfil possuira?;

• coordenar o processo de autenticac?a?o e autorizac?a?o dos usua?rios, para tanto
sera?o utilizadas as estruturas de seguranc?a definidas no mo?dulo SecM;

• gerenciar as informac?o?es dos clusters que esta?o sob controle do ambiente ICE;

• manipular as funcionalidades oferecidas pelo ICE, isto implica a inserc?a?o,
remoc?a?o e alterac?a?o das funcionalidades existentes no ambiente; o que im-
pacta diretamente nos mo?dulos de servic?os (SM);

• gerenciar os relacionamentos entre usua?rios, clusters e funcionalidades desses
clusters;

• tratar da apresentac?a?o das pa?ginas web, uma vez que as pa?ginas sa?o geradas
dinamicamente de acordo com (i) o perfil dos usua?rios, (ii) os clusters que eles



36

te?m acesso e ainda (iii) quais funcionalidades disponibilizadas nesses clusters
que eles podem acessar.

Como destacado acima, a ide?ia de perfil de usua?rios e? um ponto relevante dentro
do Portal Web. E? atrave?s dos perfis que se definem as pol??ticas de acesso do ambiente
ICE. Essas pol??ticas esta?o ligadas a permisso?es: de gerenciamento do sistema como
um todo, de gerenciamento das informac?o?es dos clusters inseridos no ambiente, de
autorizac?a?o para utilizac?a?o de clusters e de suas funcionalidades. A Tabela 3.1
apresenta os perfis atualmente definidos no ambiente ICE.

Tabela 3.1: Descric?a?o dos perfis atualmente definidos no ambiente ICE
Perfil Descric?a?o das capacidades dentro do ICE
ICE ROOT Gerenciamento de usua?rios, clusters e funcionalidades

(inserc?a?o, remoc?a?o e alterac?o?es)
Especificac?a?o das funcionalidades dos clusters (SM)

CLUSTER ROOT Gerenciamento dos usua?rios dos clusters
Associac?a?o de funcionalidades a clusters
Associac?a?o de usua?rios a clusters
Associac?a?o de usua?rios a?s funcionalidades dos clusters
Manutenc?a?o dos relacionamentos entre usua?rios,
clusters e funcionalidades

USER Utilizac?a?o das funcionalidades associadas ao usua?rio

No ambiente ICE os perfis sa?o atribu??dos para cada usua?rio em relac?a?o a cada
cluster que ele tem permissa?o de utilizac?a?o. A excec?a?o e? o perfil ICE ROOT, pois
ele na?o esta? ligado a nenhum cluster e pode ser associado a qualquer usua?rio que
possuir as permisso?es de administrador do ambiente ICE. Atrave?s da abordagem de
associac?a?o de perfis de acordo com os clusters acess??veis, e? poss??vel que um mesmo
usua?rio tenha diferentes perfis entre os diferentes clusters. Tambe?m e? poss??vel que no
mesmo cluster esse usua?rio apresente diferentes perfis. Por exemplo, considerando
os usua?rios apresentados na Figura 3.1, e? poss??vel que o Usua?rio A possua o perfil
CLUSTER ROOT no Cluster 2 e o perfil USER no Cluster 1 ; mas nada impede que
ele tambe?m possua o perfil USER tambe?m no Cluster 2. Enfim, de forma resumida
pode-se dizer que o perfil ICE ROOT esta? relacionado com as ac?o?es de configurac?a?o
do ambiente ICE, o perfil CLUSTER ROOT esta? ligado a?s questo?es de preparac?a?o
das informac?o?es dos clusters para serem integrados no ambiente ICE, enquanto o
perfil USER esta? relacionado com as funcionalidades, ou seja SMs que os usua?rio
tera?o acesso.

O Service Module (SM) e? um mo?dulo conceitual, cujo objetivo e? servir de
framework para a incorporac?a?o de funcionalidades no sistema. Como apresentado
anteriormente, esse mo?dulo esta? presente tambe?m no Middleware de Servic?os. A
caixa cinza mais clara no mo?dulo Service Module da Figura 3.2 ilustra a porc?a?o
pertencente ao Portal Web desse mo?dulo. Nele, o SM assume as atribuic?o?es de prover
as abstrac?o?es gra?ficas e facilidade de uso para os usua?rios da funcionalidade em
questa?o. Esta porc?a?o gra?fica e? criada atrave?s dos para?metros e operac?o?es existentes
na USI definida para a funcionalidade.



37

3.2.3 Cena?rio de Aplicac?a?o da Arquitetura ICE

Para que se possa compreender melhor como cada mo?dulo da arquitetura ICE
e? empregado e como ela pode ser estendida a Figura 3.3 ilustra um cena?rio de
utilizac?a?o do ambiente ICE.

Figura 3.3: Instanciac?a?o da Arquitetura ICE

Na Figura 3.3 esta?o definidos dois mo?dulos de servic?o: Resource Monitoring
(RM-SM) e Job Management (JM-SM). Esses mo?dulos seguem, respectivamente, as
interfaces de servic?os definidas em RM-USI e JM-USI. No contexto de utilizac?a?o
apresentado na Figura 3.3 existem tre?s clusters. O Cluster I possui suporte para as
duas funcionalidades, enquanto o Cluster II possui suporte apenas para a funciona-
lidade de Resource Monitoring e o Cluster III apenas para a funcionalidade de Job
Management.

Como em um cena?rio comum em HPC, cada cluster apresenta o seu conjunto de
ferramentas. Para que se possa integrar esses clusters no ambiente de gerenciamento
ICE e? preciso preservar esta diversidade. A aplicac?a?o da arquitetura ICE nesse
contexto garante essa caracter??stica. A Figura 3.3 mostra que para as mesmas
funcionalidades e? poss??vel integrar ferramentas diferentes em clusters distintos. Cada
cluster que prove? uma funcionalidade (servic?o) tem que ter um SI implementado de
acordo com a USI da funcionalidade em questa?o. Desta forma, existe para a RM-
USI o mo?dulo RM-SI no Cluster I e no Cluster II, enquanto para a JM-USI existem
dois SIs, localizados no Cluster I e Cluster III.

A motivac?a?o para apresentac?a?o da Figura 3.3 e? ilustrar o potencial de extensibili-
dade, transpare?ncia e uniformizac?a?o que a arquitetura do ambiente ICE proporciona



38

no gerenciamento e acesso de mu?ltiplos clusters. O objetivo final do ambiente ICE e?
prover servic?os de: monitoramento de recursos, monitoramento de aplicac?o?es, geren-
ciamento de aplicac?o?es, depurac?a?o de aplicac?o?es e gerenciamento de configurac?a?o de
clusters. Sendo que essa na?o e? uma lista fechada de funcionalidades que se pretende
inserir nesse ambiente.

3.3 Resumo

O ambiente ICE e? uma plataforma de gerenciamento e de acesso a mu?ltiplos
clusters. Os principais objetivos desse ambiente sa?o prover: (i) extensibilidade das
funcionalidades agregadas na plataforma, e extensibilidade das ferramentas que po-
dem ser integradas; (ii) transpare?ncia de acesso e de uso das ferramentas existentes
nos clusters; (iii) uniformizac?a?o da forma como as operac?o?es das ferramentas dos
clusters sa?o executadas; (iv) completeza, ou seja, ser capaz de incorporar todos os
tipos de servic?os que um cluster pode disponibilizar; (v) interoperabilidade, garan-
tindo que sistemas legados sejam incorporados ao ambiente; (vi) facilidade de uso,
permitindo que os usua?rios possam interagir com uma interface gra?fica amiga?vel.

Para que se consiga atingir estes objetivos a arquitetura do ambiente ICE foi pro-
jetada para ser modular e baseada no modelo SOA (Service Oriented Architecture).
Atrave?s desse modelo e? poss??vel garantir uma independe?ncia entre especificac?a?o dos
servic?os e a forma como eles sa?o implementados. Esse tipo de caracter??stica e? fun-
damental para que se consiga prover extensibilidade, transpare?ncia e uniformizac?a?o.

A arquitetura ICE esta? dividida em dois componentes: Middleware de Servic?os
e Portal Web. O primeiro componente e? formado pelos mo?dulos de definic?a?o de
um servic?o (USI - Unified Service Interface), provedor de servic?os (SI - Service
Implementation) e consumidor de servic?os (SM - Service Module). O segundo com-
ponente e? formado pelos mo?dulos de gerenciamento do pro?prio ambiente (SMM -
System Management Module), de seguranc?a do sistema (SecM - Security Module)
e por um mo?dulo de servic?os (SM - Service Module). Esse u?ltimo mo?dulo possui
func?o?es nos dois componentes da arquitetura ICE, sendo que no Portal Web ele
assume um cara?ter mais de apresentac?a?o gra?fica das informac?o?es e no Middleware
de Servic?os possui o cara?ter de consumidor de servic?os.

Atrave?s da arquitetura proposta e? poss??vel o desenvolvimento de um ambiente de
gerenciamento e de acesso a mu?ltiplos clusters que e? capaz de incorporar os sistemas
legados dos clusters, ser estendido para ser usado com diferentes funcionalidades e
ferramentas, ser transparente e de fa?cil utilizac?a?o. O pro?ximo cap??tulo apresenta
algumas deciso?es de implementac?o?es tomadas, ale?m de descrever o proto?tipo imple-
mentado para validar a arquitetura proposta.



39

4 DESENVOLVIMENTO DO PROTO?TIPO DO AM-

BIENTE ICE

Este cap??tulo descreve o desenvolvimento do proto?tipo do ambiente ICE, o qual
segue a arquitetura proposta no Cap??tulo 3. Sera?o apresentadas as deciso?es de
projeto, os mo?dulos que foram implementados e a forma como esse processo foi
realizado.

4.1 Deciso?es de Projeto

A primeira decisa?o de projeto a ser considerada foi qual middleware deveria ser
utilizado para o desenvolvimento da infra-estrutura de servic?os. Dado o fato de que
o Middleware de Servic?o segue uma arquitetura SOA foi necessa?rio analisar plata-
formas compat??veis com essa abordagem. Como apresentado no Cap??tulo 2, existem
alguns middlewares para o desenvolvimento de sistemas distribu??dos. Dentre estes,
escolheu-se CORBA e Web Services para serem analisados visto que eles podem ser
considerados os middlewares mais representativos (COULOURIS; DOLLIMORE;
KINDBERG, 2005).

O segundo aspecto levado em considerac?a?o para o desenvolvimento do ambiente
ICE esta? relacionado com a definic?a?o de que padro?es e especificac?o?es poderiam ser in-
corporadas ao ambiente ICE. Foram analisados: o padra?o de facto em infra-estrutura
para grid - Globus Toolkit (GLOBUS - WELCOME TO THE GLOBUS TOOLKIT
HOMEPAGE, 2005), e as especificac?o?es OGSA (Open Grid Service Architecture) e
OGSI (Open Grid Service Infrastructure) (LI; BAKER, 2005).

A seguir sa?o apresentadas as considerac?o?es sobre as ana?lises destes pontos e as
deciso?es tomadas para o desenvolvimento do proto?tipo do ambiente ICE.

4.1.1 Definic?a?o do Middleware Empregado

CORBA e? um middleware amplamente usado na construc?a?o de sistemas dis-
tribu??dos, sendo que inu?meras aplicac?o?es ja? foram implementadas sobre essa plata-
forma (ALONSO; et al., 2004). Apesar de na?o apresentar uma arquitetura SOA,
CORBA possui algumas caracter??sticas da mesma, como por exemplo: prover in-
teroperabilidade (mesmo que em contextos menores (COULOURIS; DOLLIMORE;
KINDBERG, 2005)), sistema de descoberta de servic?os e capacidade de isolar o fun-
cionamento dos objetos de sua API. Essa u?ltima caracter??stica e? fundamental em
uma arquitetura SOA (como visto na Sec?a?o 3.2). Entretanto, CORBA na?o permite
que os usua?rios acessem a lo?gica dos nego?cios facilmente atrave?s da Internet (MC-
GOVERN; et al., 2003). A tecnologia Web Services, por sua vez, e? um middleware



40

baseado em uma arquitetura SOA. Ela na?o apresenta todas as caracter??sticas dessa
arquitetura (MCGOVERN; et al., 2003), mas prove? suporte para grande parte delas,
ou pelo menos para as mais importantes.

A comparac?a?o ra?pida entre CORBA e Web Services apresentada em (COULOU-
RIS; DOLLIMORE; KINDBERG, 2005) ilustra as principais diferenc?as entre esses
middleware. A Tabela 4.1 mostra essas diferenc?as.

Tabela 4.1: Comparac?a?o entre CORBA e Web Services
Aspecto CORBA Web Services

Nomeac?a?o Servic?o de nomeac?a?o CORBA DNS
Formato da Refere?ncia IOR URL

(Internet-wide Object Reference)
Ativac?a?o e Localizac?a?o Integrados Separados
Facilidade de Uso Software complexo que Infra-estrutura ja?

requer instalac?a?o e suporte instalada
(HTTP + XML)

Eficie?ncia Definido para ser eficiente Menos eficiente
(dados bina?rios) (dados textuais)

O primeiro ponto que os distingue e? quanto ao esquema de nomeac?a?o utilizado.
CORBA baseia-se em uma estrutura de nomeac?a?o pro?pria, enquanto Web Services
utiliza DNS para localizac?a?o dos seus servic?os. Essa e? uma diferenc?a importante,
pois a adoc?a?o de Web Services na?o requer a inserc?a?o de novas estruturas ou formatos
para identificac?a?o de servic?os. Ale?m disso, por ser baseado em DNS, Web Services
possuem toda uma infra-estrutura de nomeac?a?o legada para serem utilizados sem
maiores problemas na Internet (visto que DNS e? um servic?o que funciona efetiva-
mente nesse meio). O segundo aspecto esta? relacionado a? forma como os servic?os sa?o
referenciados. Atrave?s da Tabela 4.1 percebe-se que Web Services seguem o padra?o
utilizado na Internet, enquanto CORBA emprega um esquema pro?prio. Com relac?a?o
a ativac?a?o e localizac?a?o dos servic?os, Web Services trata esses eventos de forma se-
parada, enquanto em CORBA esse processo e? realizado em um mesmo instante. O
aspecto de facilidade de uso e? um ponto que os diferencia significantemente. Para
utilizac?a?o de Web Services na?o e? preciso a instalac?a?o e configurac?a?o de softwares
complexos e grandes. Como ele e? baseado em padro?es da Internet, a sua infra-
estrutura ja? esta? normalmente instalada na maioria (ou totalidade) das ma?quinas.
Em contra-partida, para utilizac?a?o de CORBA e? preciso a instalac?a?o de softwares
espec??ficos e que na?o sa?o disponibilizados de forma padra?o nas ma?quinas. Quanto ao
quesito eficie?ncia, CORBA possui um desempenho melhor visto que utiliza formatos
bina?rios, enquanto Web Services empregam XML que e? um formato textual. Atual-
mente existem iniciativas da criac?a?o de XML bina?rio (W3C-XML BINARY - XML
BINARY CHARACTERIZATION WORKING GROUP PUBLIC PAGE, 2005), o
que solucionaria o de?fiti de eficie?ncia que Web Services possui.

Considerando os aspectos ressaltados pela comparac?a?o direta entre estes mid-
dlewares, os objetivos que se pretende atingir com o ambiente ICE e a arquitetura
desse ambiente, optou-se pela utilizac?a?o de Web Services como infra-estrutura para o
desenvolvimento do Middleware de Servic?o. A tecnologia Web Service na?o e? unita?ria,
ela e? formada por tecnologias como por exemplo, SOAP para a construc?a?o das men-



41

sagens a serem trocadas entre os provedores e os consumidores de servic?os, WSDL
para a descric?a?o dos servic?os e UDDI para a localizac?a?o dos servic?os. Neste trabalho
utilizou-se apenas a tecnologia SOAP para a troca de informac?o?es entre as entidades
envolvidas.

4.1.2 Ana?lise dos Padro?es e Especificac?o?es

Como apresentado no Cap??tulo 3, uma das filosofias do ambiente ICE e? a uti-
lizac?a?o de padro?es e especificac?o?es dispon??veis. Na a?rea de gerenciamento e de acesso
a mu?ltiplos clusters na?o existem trabalhos pontuais que busquem um certo grau
de padronizac?a?o de me?todos, ferramentas, etc. Entretanto, na a?rea de grids exis-
tem muito esforc?os neste sentido. Alguns exemplos de grupos que trabalham com
este foco sa?o: GGF (Global Grid Force) (GLOBAL GRID FORUM, 2005), Globus
Aliance (THE GLOBUS ALLIANCE, 2005), OASIS (OASIS - ADVANCING E-
BUSINESS STANDARDS SINCE 1993, 2005), W3C (W3C WORLD WIDE WEB -
LEADING THE WEB TO ITS FULL POTENTIAL..., 2005), entre outros. Os dois
u?ltimos grupos apresentados na?o colaboram diretamente para a?rea de grids, mas os
padro?es gerados por eles sa?o muitas vezes empregados nesta a?rea.

O gerenciamento e o acesso de mu?ltiplos clusters possui caracter??sticas em co-
mum com a a?rea de clusters. Ambas te?m que lidar com: mu?ltiplos dom??nios ad-
ministrativos, heterogeneidade de sistemas e ferramentas, localizac?a?o dos recursos,
seguranc?a no acesso a esses recursos, entre outros. Entretanto, a forma como es-
tes aspectos sa?o tratados nestes escopos e? diferente. No gerenciamento e acesso a
mu?ltiplos clusters os usua?rios sabem exatamente que recursos eles esta?o utilizando,
podem na?o saber onde eles esta?o, mas sabem quem sa?o estes recursos. Em grids
os usua?rios na?o sabem quem sa?o e nem onde esta?o os recursos que eles solicitaram.
Essa pequena diferenc?a faz com que, no cena?rio de grids, sejam necessa?rios grandes
mecanismos de controle para prover esse tipo de transpare?ncia aos seus usua?rios.
Outra diferenc?a que tambe?m implica em n??veis de controle mais pesados, e? a dina-
micidade de um cena?rio de grid. Neste, os recursos sa?o vola?teis, ou seja, as ma?quinas
que fazem no grid em um determinado momento podem na?o fazer mais parte no
momento seguinte, enquanto que em um cena?rio de mu?ltiplos clusters esses recursos
sa?o esta?ticos. Mesmo com essas diferenc?as estruturais ainda e? poss??vel adaptar as
ide?ias da a?rea de grid para o contexto do ambiente ICE. Ale?m disso, pode-se tentar
incorporar ferramentas ja? existentes no cena?rio de grid no ambiente ICE. Seguindo
esta ide?ia analisou-se a possibilidade de utilizac?a?o de mo?dulos do Globus Toolkit
dentro do ambiente ICE.

O Globus Toolkit (GT) tem sido desenvolvido desde o final dos anos 90 com
o objetivo de dar suporte para o desenvolvimento de aplicac?o?es e infra-estruturas
de sistemas distribu??dos orientados a servic?o. Alguns componentes ba?sicos do GT
esta?o relacionados com: seguranc?a, gerenciamento e acesso de recursos, migrac?a?o
e gerenciamento de dados, descobrimento de recursos, entre outros. Atualmente o
GT esta? em sua versa?o 4, a qual e? toda baseada em Web services. A Figura 4.1
apresenta os mo?dulos que compo?em o GT4.

Seguindo a ide?ia de aproveitar o que ja? existe na a?rea de alto desempenho para
ser integrado ao ambiente ICE, pensou-se na utilizac?a?o do mo?dulo de alocac?a?o e
gerenciamento de recursos - GRAM (Grid Resource Allocation and Management).
Esse mo?dulo do GT4 e? responsa?vel pela submissa?o de processos no grid. Ele faz
o interfaceamento com os escalonadores de aplicac?o?es instalados nos recursos com-



42

Figura 4.1: Arquitetura do Globus Toolkit v4 - Figura retirada do draft (GLOBUS
Toolkit 4.0 RELEASE MANUALS, 2005)



43

putacionais. Entretanto analisando o toolkit, percebeu-se que na?o e? poss??vel isolar
este mo?dulo e integra?-lo no ambiente ICE. Isto na?o e? poss??vel pois para utilizar o
GRAM e? preciso ter todo o nu?cleo de componentes ba?sicos do GT4. Eles servem,
de forma geral, para prover a infra-estrutura de controle para gerenciamento das
informac?o?es que mante?m o recurso participando do grid. Isto implica na inserc?a?o de
comunicac?o?es, protocolos e outras abstrac?o?es que na?o sa?o necessa?rias no contexto
de clusters. Sendo assim, para evitar essa sobrecarga desnecessa?ria decidiu-se na?o
integrar o GRAM no ambiente ICE.

Da mesma forma como ocorre no caso do GRAM, os outros mo?dulos do GT
tambe?m so? podem ser utilizados caso a infra-estrutura ba?sica do GT esteja presente.
Isto faz com que a utilizac?a?o do GT para evitar novas implementac?o?es na?o seja apro-
priado no contexto do ambiente ICE. No entanto, as ide?ias por tra?s deste middleware
para grid podem ser mapeadas para clusters, como por exemplo o esquema de se-
guranc?a utilizado no GT, o qual e? baseado no GSI (Grid Security Infrastructure)
(WELCH; et al., 2003) - atualmente um padra?o do GGF.

Outro exemplo de adaptac?a?o poss??vel e? utilizar os conceitos e especificac?o?es do
OGSA (Open Grid Service Architecture) (FOSTER; et al., 2002; ABBAS, 2004)
implementando-os para o cena?rio de mu?ltiplos clusters. Atrave?s da utilizac?a?o das
especificac?o?es, como por exemplo GMA (Grid Monitoring Architecture), e? poss??vel
desenvolver um sistema para mu?ltiplos clusters que segue interfaces bem definidas.
Se no futuro o ambiente ICE for migrado para um contexto de grid, a interface de
seus servic?os ja? estara? de acordo com os padro?es utilizados.

4.2 Descric?a?o do Proto?tipo Implementado

A partir das deciso?es de projeto tomadas, principalmente da definic?a?o de que
middleware seria usado para o desenvolvimento dos mo?dulos de servic?o, partiu-se
para a definic?a?o de quais componentes fariam parte do proto?tipo.

O proto?tipo resultante deste trabalho possui a implementac?a?o de uma insta?ncia
do Middleware de Servic?os, ou seja, foi disponibilizada uma funcionalidade de clus-
ter, e foi implementado o mo?dulo de gerenciamento do sistema (SMM) no Portal
Web. O mo?dulo de seguranc?a (SecM) projetado na arquitetura ICE foi implemen-
tado no trabalho de Conclusa?o de Curso do aluno Alexandre Ilha (ILHA, 2005).

Como descrito na Sec?a?o 3.2.3, esta?o previstas va?rias funcionalidades dentro do
ambiente ICE. No contexto deste trabalho, foi implementada a funcionalidade de
gerenciamento de aplicac?o?es (Job Management). No entanto, existem trabalhos de
outros participantes do GPPD (Grupo de Processamento Paralelo e Distribu??do) de
desenvolvimento de outros mo?dulos do Middleware de Servic?os. Exemplos dessas
iniciativas sa?o os servic?os para: monitoramento de aplicac?o?es e de recursos, e para
configurac?o?es administrativas dos clusters. A Figura 4.2 apresenta a estrutura atual
do ambiente ICE. Os mo?dulos implementados no escopo deste trabalho sa?o ilus-
trados pelas caixas mais escuras, enquanto os desenvolvidos por outras pessoas sa?o
ilustrados pelas caixas mais claras.

Ale?m de apresentar os mo?dulos do ambiente ICE implementados, a Figura 4.2
tambe?m indica os clusters que foram integrados dentro do ambiente ICE: gppd e o
frontal-minuano. Como indicado na figura, cada cluster possui um tipo de ferra-
menta de gerenciamento. Essas ferramentas sa?o, respectivamente: OpenPBS (Open
Portable Batch System) (PORTABLE BATCH SYSTEM, 2005) e OAR (CAPIT; et



44

Figura 4.2: Proto?tipo ICE Implementado

al., 2005). Estes clusters pertencem ao GPPD.

4.2.1 Tecnologias Empregadas

Para implementac?a?o do proto?tipo foram adotadas as tecnologias descritas abaixo,
em cada um dos componentes da arquitetura.

• Middleware de Servic?o - Para o desenvolvimento dos Web services optou-se
pela utilizac?a?o da tecnologia Apache Axis (versa?o 2) (APACHE AXIS2, 2005).
Essa escolha tambe?m levou a? utilizac?a?o do container Tomcat (versa?o 5.0.30)
(APACHE TOMCAT - THE APACHE SOFTWARE FOUNDATION, 2006)
e do Java (j2sdk-1.4.2) (JAVA PLATFORM, STANDARD EDITION , JAVA
SE). Estas u?ltimas tecnologias foram empregadas pois sa?o pre?-requisitos para
utilizac?a?o do Axis2. E? importante ressaltar que o desenvolvimento do Mid-
dleware de Servic?o poderia ter utilizado qualquer outra tecnologia, uma vez
que Web services sa?o independente de plataforma e linguagem.

• Portal Web - Esse componente possui tre?s pontos distintos: apresentac?a?o das
informac?o?es atrave?s de pa?ginas Web, controle e separac?a?o da lo?gica do sistema
e o papel de consumidor de servic?os. A apresentac?a?o foi implementada atrave?s
da tecnologia JSP (Java Server Pages) (JAVASERVER PAGES TECHNO-
LOGY, 2006). A lo?gica do sistema esta? separada das pa?ginas Web devido
a utilizac?a?o de Java Servlets (JAVA SERVLET TECHNOLOGY, 2006). As
pa?ginas JSP na?o lidam diretamente com a intelige?ncia do sistema, elas fazem
requisic?o?es para Servlets, os quais interagem com os recursos e servic?os ne-



45

cessa?rios. E para finalizar, os consumidores dos mo?dulos de servic?o (SM) sa?o
implementados utilizando Apache Axis (versa?o 2) e tambe?m na?o sa?o acessados
diretamente pelas pa?ginas JSP. Este acesso tambe?m e? feito atrave?s de Ser-
vlets. Para o armazenamento das informac?o?es foi utilizado o SGBD (Sistema
de Gerenciamento de Base de dados) PostgreSQL versa?o 8.1 (POSTGRESQL
GLOBAL DEVELOPMENT GROUP, 2006).

A seguir, a implementac?a?o de cada componente da arquitetura ICE e? descrita em
detalhes. Sa?o apresentadas as caracter??sticas de cada mo?dulo, suas funcionalidades
e especificidades, assim como as interac?o?es entre esses mo?dulos. Inicialmente sera?
descrito o Middleware de Servic?o implementado para funcionalidade de gerencia-
mento de aplicac?o?es Job Management e em seguida sera? detalhada a implementac?a?o
do Portal Web.

4.2.2 Middleware de Servic?o para Gerenciamento de Aplicac?o?es

Neste trabalho o Middleware de Servic?os foi instanciado para que fosse poss??vel a
disponibilizac?a?o da funcionalidade de gerenciamento de aplicac?o?es junto ao ambiente
ICE. Primeiramente sera? descrita a USI definida para o gerenciamento de aplicac?o?es
(Job Management Unified Service Interface - JM-USI), em seguida sera?o apresen-
tados os provedores de servic?os desenvolvidos para essa interface (Job Management
Service Implementation - JM-SI). Finalmente sera?o descritas as func?o?es referentes
ao consumidor dos servic?os desta funcionalidade (Job Management Service Module
- JM-SM).

4.2.2.1 JM-USI - Job Management Unified Service Interface

A USI para o gerenciamento de aplicac?o?es foi definida considerando alguns
para?metros tipicamente utilizados pelos usua?rios de clusters. Para se chegar nestes
para?metros buscou-se suportar os para?metros utilizados pelo GRAM do GT4 (GLO-
BUS Toolkit 4.0 RELEASE MANUALS, 2005; GT 4.0 WS GRAM, 2005). De certa
forma, pode-se dizer que a API disponibilizada pelo GRAM tenta prover as princi-
pais operac?o?es e para?metros existentes nos escalonadores de aplicac?o?es. Alguns dos
principais para?metros utilizados no GRAM foram incorporados na funcionalidade
de gerenciamento de aplicac?o?es do ambiente ICE.

Para finalizar a ana?lise das possibilidades de operac?o?es e para?metros que podem
ser disponibilizados em uma interface uniformizada, foram estudadas as ferramentas
de escalonamento: OpenPBS, SGE, CCS e OAR. Essas ferramentas foram escolhidas
pois esta?o dispon??veis nos clusters do GPPD e de instituic?o?es de pesquisas parceiras
do grupo. Desta forma, essa infra-estrutura poderia ser utilizada no momento da
avaliac?a?o do proto?tipo que foi desenvolvido. Atrave?s da ana?lise dessas ferramentas,
foi poss??vel ter uma noc?a?o melhor das facilidades que elas ofereciam e poder ter
um maior conhecimento acerca do que poderia ser disponibilizado aos usua?rios do
ambiente ICE que utilizam a funcionalidade de gerenciamento de aplciac?o?es.

Sendo assim a definic?a?o da USI para esta funcionalidade foi baseada nas ca-
racter??sticas disponibilizadas pelo toolkit de grid mais utilizado - o Globus Toolkit
- e nas potencialidades oferecidas pelos escalonadores estudados. Como resultado
te?m-se a JM-USI denominada JobManagementProviderInterface. A Tabela 4.2
descreve as operac?o?es que fazem parte desta USI.

Uma questa?o que passa por todas as operac?o?es da JM-USI e? a seguranc?a. Na?o



46

Tabela 4.2: Operac?o?es da JM-USI
Operac?a?o Descric?a?o/Func?a?o

jobSubmissionOperation Submissa?o de aplicac?o?es
jobFinalizationOperation Finalizac?a?o de aplicac?o?es
jobStatusVerificationOperation Verificac?a?o do estado das

aplicac?o?es, o que pode
corresponder a: finalizada,
em execuc?a?o, em espera,
ou enta?o ainda na fila para
ser submetida

jobOutputRetrievalOperation Recuperac?a?o da sa??da de uma
aplicac?a?o (padra?o ou de erros)

submissionScriptVerificationOperation Apresentac?a?o do script de
submissa?o da aplicac?a?o de
acordo com o escalonador
do cluster em uso

e? poss??vel permitir que usua?rios na?o autenticados e na?o autorizados utilizem as
operac?o?es dos servic?os disponibilizados nos front-ends dos clusters. De forma resu-
mida pode-se descrever como medidas de seguranc?a do ambiente ICE a utilizac?a?o
de HTTPS para as comunicac?o?es; o emprego de mecanismo de mapeamento entre
as identidades dos usua?rios, os quais podem assumir uma identidade no ambiente
ICE e diferentes identidades nos clusters aos quais possuem acesso; e mecanismos
de autenticac?a?o mu?tua entre o Portal ICE e o front-end que esta? sendo acessado.
A construc?a?o do mo?dulo de seguranc?a segue a definic?a?o do Middleware de Servic?os,
isto e?, foram definidos os elementos: USI, SI e SM.

Estas questo?es de seguranc?a interferem na definic?a?o dos para?metros das operac?o?es
de qualquer servic?o que for criado para ser integrado ao ambiente ICE. Desta forma,
independentemente da funcionalidade que se esta? inserindo no ambiente, e? preciso
que todas as suas operac?o?es possuam a identificac?a?o do usua?rio que a esta? solici-
tando. Todo o processo de autenticac?a?o e autorizac?a?o e? realizado pelo servic?o de
seguranc?a definido e desenvolvido para o ambiente ICE. Essa caracter??stica mostra
a modularidade que este ambiente possui.

A seguir os para?metros de cada uma das operac?o?es da JM-USI sa?o descritos nas
tabelas abaixo.

• jobSubmissionOperation()

Para?metros de entrada



47

username: Conte?m a identificac?a?o do usua?rio. Esta? rela-
cionado com o sistema de seguranc?a de cada
cluster, mas na interface do servic?o JM-USI
este fator e? transparente.

basedir: Informa o direto?rio a partir de onde as
aplicac?o?es e os arquivos do usua?rio sera?o en-
contrados durante o processo de submissa?o
da aplicac?a?o.

param: Possui os para?metros que devem ser passados
para a aplicac?a?o.

main appname: Representa o nome da aplicac?a?o que deve ser
executada.

numnodes: Indica o nu?mero total de no?s que devem ser
reservados.

numproc: Possui o nu?mero de processos que devem ser
lanc?ados. Esse para?metro deve ser utilizado
no lanc?amento de aplicac?o?es paralelas.

queue: Indica a fila onde a aplicac?a?o deve ser
lanc?ada. De acordo com o tipo de escalo-
nador pode assumir outra sema?ntica que na?o
a de fila, mas continuara? indicando o local a
partir de onde a aplicac?a?o sera? lanc?ada.

walltime: Representa o tempo da reserva dos no?s.
joinout: Especifica se a sa??da padra?o e a de erro devem

ser colocadas em um u?nico arquivo.
outfile: Conte?m o nome do arquivo para onde sera?

redirecionada a sa??da padra?o.
errfile: Possui o nome do arquivo para onde sera? re-

direcionada a sa??da de erro padra?o.



48

jobtype: Especifica que tipo de aplicac?a?o sera? subme-
tida. Elas podem ser sequ?enciais ou para-
lelas. No caso das aplicac?o?es paralelas elas
ainda sa?o identificadas de acordo com o tipo
de biblioteca paralela utilizada.

numbatchiteration: E? poss??vel configurar a execuc?a?o da mesma
aplicac?a?o um determinado nu?mero de vezes.
Isto e?, existe a possibilidade de disparar uma
aplicac?a?o para ser executada como um pool
de execuc?o?es. O nu?mero de vezes que a
aplicac?a?o sera? executada e? designado por este
para?metro.

multiplescripts: Serve como complemento do para?metro ante-
rior. Indica se as va?rias execuc?o?es da mesma
aplicac?a?o devem ser lanc?adas como jobs in-
dependentes para o escalonador ou na?o. Isto
e?, deve-se escolher entre gerar tantos scripts
quantas foram as iterac?o?es, ou enta?o se deve
ser gerado apenas um script, e as repetic?o?es
da execuc?a?o da mesma aplicac?a?o devem ser
gerenciadas dentro desse script u?nico. E?
importante ressaltar que essa definic?a?o tem
impacto direto sob o para?metro relativo ao
tempo da reserva que esta? sendo solicitada
para a aplicac?a?o (walltime). Em caso de
gerac?a?o de apenas um u?nico script, e? preciso
que o tempo da reserva leve em considerac?a?o
o tempo necessa?rio para o te?rmino de todas
as iterac?o?es.

Para?metros de sa??da

jobid: Conte?m a identificac?a?o da aplicac?a?o que foi sub-
metida.

errorcode: Campo destinado a conter o valor de qualquer tipo
de erro que ocorra durante o processamento e a
montagem do script de submissa?o da aplicac?a?o,
assim como qualquer tipo de erro ocorrido no mo-
mento da submissa?o da aplicac?a?o atrave?s do esca-
lonador utilizado no cluster.

• finalizeJobOperation()

Para?metros de entrada

username: Ide?ntico ao para?metro da operac?a?o de submissa?o.
jobid: Identificac?a?o da aplicac?a?o que deve ser finalizada.



49

Para?metros de sa??da

fin output: Possui a sa??da, em modo texto, da ordem de fina-
lizac?a?o da aplicac?a?o

errorcode: Campo destinado a conter o valor de qualquer tipo
de erro que ocorra durante a ac?a?o de finalizac?a?o
da aplicac?a?o atrave?s do escalonador utilizado no
cluster.

• verifyJobStatusOperation()

Para?metros de entrada

username: Ide?ntico ao para?metro da operac?a?o de submissa?o.

Para?metros de sa??da

status: Conte?m o estado em que as aplicac?o?es se encon-
tram.

errorcode: Campo destinado a conter o valor de qualquer tipo
de erro que ocorra durante a ac?a?o de verificac?a?o do
estado da aplicac?a?o.

• jobOutputRetrievalOperation()

Para?metros de entrada

username: Ide?ntico ao para?metro da operac?a?o de submissa?o.
basedir: Informa o direto?rio a partir de onde os arquivos

referentes ao job solicitado esta?o armazenados.
file: Indica no nome do arquivo que deve ser recupe-

rado.

Para?metros de sa??da

archive: Conte?m os dados do arquivo solicitado.
errorcode: Campo destinado a conter o valor de qualquer tipo

de erro que ocorra durante a ac?a?o de recuperac?a?o
dos dados do arquivo solicitado.

• verifySubmissionScriptOperation()

Para?metros de entrada - Possui exatamente os mesmo para?metros de entrada que
a operac?a?o de submissa?o.



50

Para?metros de sa??da

script: Conte?m os dados que formam o script de sub-
missa?o de uma aplicac?a?o, de acordo com o esca-
lonador em uso no cluster.

errorcode: Campo destinado a conter o valor de qualquer tipo
de erro que ocorra durante a gerac?a?o do script de
submissa?o.

Analisando rapidamente a definic?a?o da JM-USI pode-se pensar em iguala?-la ao
WSDL do servic?o de gerenciamento de aplicac?o?es. Entretanto esta equiparac?a?o na?o
pode ser feita. Pela definic?a?o da arquitetura ICE, uma USI e? a definic?a?o de uma
funcionalidade, ou seja, de um servic?o, a qual possui as operac?o?es e os para?metros
do mesmo. WSLD e? a linguagem de descric?a?o de um Web Service, a qual possui
sua sintaxe baseada em XML e tags espec??ficas que informam, por exemplo, a forma
como sera? realizada a comunicac?a?o (one-way ou em estilo RPC) e ainda o enderec?o
do provedor do servic?o. Essas duas caracter??sticas do WSDL sa?o os pontos que o
diferem completamente de uma USI. Outro ponto que os distingue e? o fato da JM-
USI na?o estar atrelada a? tecnologia de Web Services, ela pode ser mapeada para
qualquer tecnologia que possua caracter??sticas de arquiteturas SOA.

Atrave?s das operac?o?es apresentadas acima e de seus para?metros e? poss??vel re-
alizar todas as ac?o?es ba?sicas de gerenciamento de aplicac?o?es. A JM-USI definida
disponibiliza aos seus usua?rios facilidades para lanc?amento de aplicac?o?es. Exemplos
dessas facilidades sa?o o lanc?amento de mu?ltiplas execuc?o?es da mesma aplicac?a?o e o
upload de arquivos.

E? importante ressaltar que podem ocorrer casos em que as operac?o?es e os pa-
ra?metros definidos na JM-USI na?o possam ser diretamente mapeados para as ca-
racter??sticas dos escalonadores e gerenciadores utilizados nos clusters. Esse tipo
de situac?a?o deve ser tratada pelas implementac?o?es espec??ficas da JM-USI, ou seja,
devem ser tratadas em cada implementac?a?o do mo?dulo JM-SI.

4.2.2.2 JM-SI - Job Management Service Implementation

Neste ponto sera?o detalhadas as implementac?o?es do servic?o de gerenciamento de
aplicac?o?es (JM-SI) para os escalonadores OpenPBS e OAR. Ale?m disso sera? apre-
sentado o framework definido para dar suporte no desenvolvimento dos diferentes
JM-SIs. Esse framework garante a caracter??stica de extensibilidade, que podera? ser
comprovada durante a descric?a?o das implementac?o?es espec??ficas.

O Job Management Framework (JMF), nome atribu??do ao framework de-
finido, pode ser visualizado na Figura 4.3. Essa figura ilustra o diagrama de classes,
baseado na modelagem UML (UML - UNIFIED MODELING LANGUAGE, 2006),
que formam a estrutura ba?sica da implementac?a?o do servic?o de gerenciamento de
aplicac?o?es.

Como pode-se perceber na Figura 4.3 existem classes e pacotes ja? definidos, que
servem como base para as implementac?o?es futuras e existem as classes que devem
ser implementadas. Para tanto e? necessa?rio que se utilize os mecanismos de heranc?a
presentes no modelo de programac?a?o Orientado a Objetos. A seguir cada pacote e
classe do JMF sera?o descritas e contextualizadas.



51

JobManagementSI

JobManagementProviderSI

ICESecurity

JobManagementProviderInterface

+jobSubmissionOperation()

+finalizeJobOperation()

+verifyJobStatusOperation()

+jobOutputRetrievalOperation()

+verifySubmissionScriptOperation()

Job

+generateSubmissionScript()

+launchSubmissionScript()

JobManagementProviderToBeImplemented

JobToBeImplemented

UserAuthentication

+confirmAuthentication()

+setUserInformation()

UserAutheticationToBeImplemented

JobManagementProvider

+user: UserAuthetication

+job: Job

+jobSubmissionOperation()

+finalizeJobOperation()

+verifyJobStatusOperation()

+jobOutputRetrievalOperation()

+verifySubmissionScriptOperation()

Figura 4.3: JMF - Job Management Framework

Antes de permitir que qualquer usua?rio tenha acesso para utilizar os recursos
do servic?o, e? preciso garantir requisitos de seguranc?a, como por exemplo, a auten-
ticac?a?o do usua?rio que esta? solicitando o servic?o. Este procedimento de verificac?a?o
de identidade do usua?rio na?o esta? restrito apenas ao servic?o de gerenciamento de
aplicac?o?es. Na verdade ele deve ser acionado sempre que um servic?o for solici-
tado. No intuito de garantir que somente usua?rios autenticados estara?o utilizando
os servic?os do ambiente ICE foi definido o pacote ICESecurity. Esse pacote e? for-
mado pela classe abstrata UserAuthentication, que possui os me?todos abstratos
setUserInformation() e confirmAuthentication(). Optou-se por definir essa
classe e seus me?todos como abstratos para que fosse poss??vel extende?-la e imple-
mentar seus me?todos de acordo com o sistema de autenticac?a?o de cada cluster.
Por exemplo, alguns clusters autenticam seus usua?rios atrave?s de mecanismos de
login dos sistemas UNIX, enquanto isso, existem outros que utilizam LDAP para
autenticar seus usua?rios.

A definic?a?o de classes extens??veis e? fundamental, uma vez que o objetivo desse
framework e? prover uma estrutura organizacional m??nima para que os desenvolvedo-
res dos servic?os sejam capaz de estende?-lo e adapta?-lo a?s suas necessidades. Sendo
assim, a classe UserAutheticationToBeImplemented, apresentada na Figura 4.3,
representa justamente a classe que contera? a implementac?a?o do sistema de auten-
ticac?a?o de um cluster espec??fico.

O pacote JobManagementSI (na Figura 4.3) tem a func?a?o de encapsular os pa-
cotes e classes referentes a? implementac?a?o do JM-SI. Dentro dele existe o pacote
JobManagementProviderSI, o qual possui os elementos ba?sicos de um JM-SI. Ele e?
formado pela interface JobManagementProviderInterface, e pelas classes abstra-
tas Job e JobManagementProvider. A interface possui a assinatura das operac?o?es
do servic?o de gerenciamento de aplicac?o?es, detalhadas na sec?a?o 4.2.2.1.

A classe abstrata Job possui os atributos e os me?todos que devem ser utilizados



52

sobre as informac?o?es que caracterizam uma aplicac?a?o. Essa classe sera? utilizada
pelas operac?o?es de submissa?o de aplicac?o?es e de verificac?a?o do script de submissa?o
gerado pelo JM-SI de um cluster espec??fico. Por questo?es de simplificac?a?o os atri-
butos que qualificam esta classe na?o sa?o apresentados na Figura 4.3. Os me?todos,
tambe?m abstratos, generateSubmissionScript() e launchSubmissionScript()
devem ser implementados de acordo com o tipo de ferramenta de escalonamento
de aplicac?o?es instalada em cada cluster. A classe JobToBeImlemented, que na?o
faz parte do pacote JobManagementProviderSI, devera? estender e implementar es-
ses me?todos. A classe abstrata JobManagementProvider esta? associada a?s classes
UserAuthentication e Job, atrave?s dos respectivos atributos: user e job. Estas
associac?o?es sa?o importantes pois atrave?s delas e? poss??vel autenticar o usua?rio e aces-
sar as informac?o?es de aplicac?a?o de forma transparente e extens??vel. Ale?m destes
atributos esta classe possui os me?todos abstratos correspondentes a interface que ele
esta? estendendo. A implementac?a?o das operac?o?es do servic?o deve ser feita na classe
que sera? capaz de lidar com especificidades de cada ferramenta de escalonamento
de aplicac?o?es. Nesse caso a classe JobManagementProviderToBeImplemented, como
indicado na Figura 4.3.

Visando o reuso de co?digo definiu-se o pacote JobManagementUtil, ilustrado na
Figura 4.4, o qual e? formado pela classe JobManagementAuxiliaryTools e pelas in-
terfaces JobManagementErrorCode e JobManagementConstants. A primeira possui
os co?digos de erros definidos para a funcionalidade de gerenciamento de aplicac?o?es do
ambiente ICE. A segunda interface possui as constantes necessa?rias para o desenvol-
vimento dos provedores de Web Services. A classe JobManagementAuxiliaryTools
possui me?todos que foram desenvolvidos para ajudarem na construc?a?o das imple-
mentac?o?es espec??ficas da JM-USI. Vale ressaltar que esse pacote na?o e? utilizado
apenas nos JM-SIs, mas tambe?m e? empregado no desenvolvimento do JM-SM.

Figura 4.4: Diagrama de classes para o pacote de classes auxiliares

Tendo como base o JMF e o pacote com ferramentas auxiliares passou-se para
o segundo passo do desenvolvimento do JM-SI: as implementac?o?es para cada ferra-
menta de gerenciamento de aplicac?o?es. A Figura 4.5 apresenta o diagrama de classes



53

das implementac?o?es do JM-SI para as ferramentas OpenPBS e OAR.

Por uma questa?o de organizac?a?o decidiu-se criar um pacote para cada imple-
mentac?a?o dentro do pacote JobManagementSI. A Figura 4.5 apresenta os pacotes
OpenPBSProviderSI e OARProviderSI. Dado o fato dos escalonadores OpenPBS
e OAR possuirem fortes similaridades, as implementac?o?es de seus provedores de
servic?os tornaram-se bastante semelhantes. Em virtude disso, sera? descrita a im-
plementac?a?o do OpenPBS JM-SI, a qual e? equivalente a implementac?a?o do OAR
JM-SI. Os pontos que as diferem sera?o ressaltados quando necessa?rio.

Pode-se observar na Figura 4.5 que as implementac?o?es seguem o JMF definido,
estendendo as classes necessa?rias e adicionando outras. As classes necessa?rias sa?o:
OpenPBSJobManagementProvider e OpenPBSJob. Na primeira classe sa?o efetiva-
mente implementados os provedores dos Web Services. Eles ira?o receber as men-
sagens atrave?s do container Tomcat processa?-las, executando a operac?a?o solicitada
(nesse caso executanto a chamada do me?todo que implementa a operac?a?o) e construir
a mensagem SOAP de resposta a? solicitac?a?o. As comunicac?o?es entre os consumido-
res do JM-SM (localizados junto ao Portal Web) e os provedores dos servic?os nos
front-ends e? realizada utilizando o estilo SOAP-RPC, o qual e? caracterizado por
empregar mensagens s??ncronas. Na segunda classe necessa?ria sa?o implementados os
me?todos utilizados para o processamento das operac?o?es de submissa?o de aplicac?o?es
e visualizac?a?o dos scripts de submissa?o. As demais operac?o?es definidas no JM-USI
na?o demandaram a criac?a?o de classes extras, visto que elas apresentam um com-
plexidade bem inferior as duas citadas anteriormente. Na Figura 4.5 tambe?m esta?
indicado a utilizac?a?o dos pacotes ICESecurity e JobManagementUtil que colaboram
na modularizac?a?o e simplicidade do ambiente ICE.

A seguir sera?o detalhadas as operac?o?es e sera?o descritas as estruturas de classes
definidas para as implementac?o?es das operac?o?es mais complexas.

Operac?a?o de Submissa?o de Aplicac?o?es

Atrave?s da ana?lise da Figura 4.5 percebe-se que as classes OpenPBSJob e OARJob
possuem, ale?m dos me?todos herdados da classe Job, me?todos que geram as in-
formac?o?es comuns a todos os tipos de aplicac?o?es a serem lanc?adas. Por exemplo, o
me?todo definePartialAppCall() e? utilizado em ambas implementac?o?es espec??ficas
para montar a chamada da aplicac?a?o com seus para?metros (caso sejam especifica-
dos). Entretanto, existem alguns detalhes na montagem do script de submissa?o
que devem considerar qual o tipo de aplicac?a?o que sera? lanc?ada. Nesse caso foram
definidas classes mais espec??ficas para aplicac?o?es: sequ?enciais e paralelas, sendo que
esta? u?ltima dependera? do tipo de biblioteca ou linguagem paralela que sera? em-
pregada. No ambiente ICE existe suporte para lanc?amento de aplicac?o?es paralelas
baseadas em duas bibliotecas: MPI (DONGARRA; et al., 1995) (implementac?a?o
MPICH) e DECK (BARRETO et al., 2000). Desta forma foram definidas as classes
OpenPBSSeqJob, OpenPBSMPICHParallelJob e OpenPBSDECKParallelJob, as quais
estendem a classe OpenPBSJob. Como apresentado na Figura 4.5 essas classes pos-
suem os me?todos necessa?rios para gerac?a?o das especificidades de lanc?amento de
cada tipo de aplicac?a?o. Nessa figura tambe?m percebe-se que esta mesma estrutura
de classes e? aplicada no caso do gerenciador OAR.

A Figura 4.6 ilustra o XML contendo a requisic?a?o que chega no provedor de



5
4

Figura 4.5: Diagrama de classes para as implementac?o?es do OpenPBS-SI e OAR-SI



55

servic?os do JM-SI. As informac?o?es existentes nessa requisic?a?o sa?o as mesmas tanto
para a operac?a?o de submissa?o quanto para a de visualizac?a?o dos scripts. A diferenc?a
e? que no caso da primeira operac?a?o os dois me?todos generateSubmissionScript()
e launchSubmissionScript() sa?o invocados, enquanto que na outra operac?a?o so? o
primeiro e? chamado. A utilizac?a?o desses me?todos aciona os outros me?todos apre-
sentados na Figura 4.5.

Figura 4.6: Elementos XML da requisic?a?o das operac?o?es de submissa?o e visualizac?a?o
de scripts

Para que os consumidores do mo?dulo JM-SM possam compreender o retorno
dessas operac?o?es, foram definidos alguns elementos XML que padronizam a forma
de lidar com as informac?o?es de retorno dos servic?os. A seguir sera?o apresentados os
elementos XML e exemplos de sua aplicac?a?o.

A Figura 4.7 possui o retorno da submissa?o de uma aplicac?a?o no escalonador
OpenPBS, Figura 4.7 (a); e no OAR, Figura 4.7 (b). Nesse exemplo pode-se perceber
que a aplicac?a?o conseguiu ser submetida aos gerenciadores com sucesso, visto que a
informac?a?o retornada e? a identificac?a?o do job submetido.

Figura 4.7: Exemplo de retorno da operac?a?o de submissa?o. (a) Submissa?o com
OpenPBS (b) Submissa?o com OAR

Ja? a Figura 4.8 apresenta o retorno de um erro identificado antes do processo de
submissa?o a? ferramenta de gerenciamento de aplicac?o?es do cluster. Neste exemplo,
o usua?rio na?o esta? devidamente autenticado para utilizar os recursos do cluster em
questa?o.



56

Figura 4.8: Exemplo de retorno da operac?a?o de submissa?o contendo erro.

Essa mesma estrutura de retorno de erro e? empregada nas demais operac?o?es,
sendo que o que pode variar e? o tipo de erro que esta? sendo reportado.

Operac?a?o Visualizac?a?o dos Scripts de Submissa?o

Como apresentado anteriormente, as diferenc?as em termos de diagrama de clas-
ses e o funcionamento das operac?o?es de submissa?o e visualizac?a?o sa?o pequenas. Mas
existem diferenc?as em relac?a?o aos elementos XML de retorno. A seguir sera?o deta-
lhados os elementos XML para a operac?a?o de visualizac?a?o dos scripts de submissa?o.

Na Figura 4.9 sa?o ilustrados os elementos ba?sicos de retorno: script, content.
Como a JM-USI permite que mais de um script seja montado para a mesma aplicac?a?o,
a operac?a?o de visualizac?a?o tem suporte para retornar as informac?o?es de mais de um
script, facilitando a identificac?a?o dessa multiplicidade no JM-SM.

Figura 4.9: Elementos XML de retorno da operac?a?o de visualizac?a?o de scripts.

O elemento script delimita as informac?o?es dos eventuais diferentes componentes
da mensagem de retorno, enquanto o elemento name indica o nome do script que
deveria ser submetido e o content representa cada linha do script.

Abaixo sa?o apresentados alguns exemplos de poss??veis mensagens de retorno
dessa operac?a?o. A Figura 4.10 e? um exemplo da visualizac?a?o de um script de
submissa?o de uma aplicac?a?o MPI que deve ser lanc?ada em 2 scripts diferentes.
Nesse exemplo, a operac?a?o de visualizac?a?o foi requisitada no cluster que possui
como gerenciador de aplicac?o?es o OpenPBS.

A Figura 4.11 possui a visualizac?a?o de um script gerado no cluster que opera
com o gerenciador OAR. Nesse exemplo, simula-se o lanc?amento de uma aplicac?a?o
DECK que devera? ser executada 5 vezes mas apenas um script devera? ser gerado.

Comparando os dois exemplos e? poss??vel verificar que os dois escalonadores apre-
sentam caracter??sticas bastante semelhantes. Dessa forma, tanto na operac?a?o de
submissa?o quanto na de visualizac?a?o de scripts na?o foram necessa?rias grandes mu-
danc?as na estrutura de desenvolvimento. As diferenc?as se concentraram na forma



57

Figura 4.10: Exemplo de retorno da operac?a?o de visualizac?a?o em um cluster com
OpenPBS.

Figura 4.11: Exemplo de retorno da operac?a?o de visualizac?a?o em um cluster com
OAR.

como as aplicac?o?es sa?o lanc?adas (no caso da submissa?o) e nos para?metros que for-
mam o script de submissa?o, como mostraram os exemplos apresentados acima.

Analisando a Figura 4.5 e? poss??vel perceber que na?o existem classes e pacotes
espec??ficos para o desenvolvimento das operac?o?es de finalizac?a?o de aplicac?o?es, veri-
ficac?a?o do estado e recuperac?a?o dos arquivos das sa??das padra?o. Essa caracter??stica
ocorre visto que a aquisic?a?o e as interac?o?es com os gerenciadores de aplicac?o?es sa?o
simples. Para essas operac?o?es na?o e? necessa?rio a criac?a?o de scripts, basta receber
processar o conteu?do dos para?metros recebidos pelo provedor e gerar os comandos
que interagem com as ferramentas de gerenciamento de aplicac?o?es. A seguir sera?o
apresentados os elementos XML das mensagens de requisic?a?o e resposta de cada
uma das operac?o?es ainda na?o descritas.



58

Operac?a?o de Finalizac?a?o

Baseado nas definic?o?es da JM-USI, a Figura 4.12 (a) apresenta os elementos XML
que formam a mensagem de requisic?a?o de te?rmino de uma aplicac?a?o submetida.
Atrave?s da utilizac?a?o do elemento jobid uma aplicac?a?o e? finalizada.

Figura 4.12: Elementos XML de requisic?a?o e retorno da operac?a?o de finalizac?a?o.

Na implementac?a?o dessa operac?a?o, o comando de finalizac?a?o do gerenciador do
cluster e? chamado e o elemento jobid e? passado como para?metro. Caso algum
erro seja detectado antes do comando ser chamado enta?o o provedor retornara? a
mensagem de erro ja? descrita anteriormente. Caso contra?rio ele informara? o sucesso
da finalizac?a?o, como ilustrado na Figura 4.12 (b), ou podera? retornar a mensagem
de erro passada pelo gerenciador do cluster, como apresentado na Figura 4.12 (c).

Operac?a?o de Verificac?a?o de Status

A mensagem de requisic?a?o dessa operac?a?o conte?m somente a identificac?a?o do
usua?rio. Como apresentado na USI, outros para?metros na?o sa?o necessa?rios. A
Figura 4.13 apresenta um exemplo de uma mensagem de retorno contendo o estado
de uma aplicac?a?o submetida em um cluster que utiliza OAR.

Esta informac?a?o, quando recebida pelo JM-SM, devera? ser tratada para ser apre-
sentada aos usua?rios de uma forma mais fa?cil e clara.

Operac?a?o de Recuperac?a?o das Sa??das

Esta operac?a?o e? utilizada tanto para recuperar o conteu?do dos arquivos com a
sa??da padra?o como a sa??da padra?o de erro. Atrave?s dos elementos basedir e file,
ilustrados na Figura 4.14 (a), pode-se definir qual devera? ser o arquivo recuperado.
Na Figura 4.14 (b) te?m-se o retorno de um erro ocorrido no momento da recu-
perac?a?o do arquivo solicitado. A Figura 4.14 (c) apresenta um exemplo de retorno
de mensagem com o conteu?do da sa??da padra?o de uma aplicac?a?o.

Nesse caso de recuperac?a?o de arquivos sempre sera?o recuperadas informac?o?es em
modo texto. Por isso na?o sa?o necessa?rios cuidados especiais com relac?a?o a formac?a?o



59

Figura 4.13: Exemplo de retorno da operac?a?o de verificac?a?o de estado das filas do
gerenciador OAR.

da mensagem de retorno.

Resumindo a implementac?a?o dos JM-SIs pode-se dizer que os dois tipos de es-
calonadores sa?o bastante semelhantes. Esta caracter??stica facilitou o processo de
desenvolvimento dos JM-SIs uma vez que na?o foi necessa?rio a criac?a?o de mecanismo
complexos de mapeamento entre a JM-USI e os JM-SIs. Dentre as operac?o?es de
gerenciamento de aplicac?o?es disponibilizadas no ambiente ICE, a submissa?o foi a
que demandou mecanismos mais complexos e extensos de implementac?a?o.

4.2.2.3 Job Management Service Module

Como descrito no Cap??tulo 3 o componente Service Module possui uma parte
de sua implementac?a?o no Middleware de Servic?os e outra parte no Portal Web.
Neste ponto sera?o apresentadas as caracter??sticas de implementac?a?o do componente
JM-SM referente aos consumidores de Web Services.

Assim como os provedores de servic?os, os consumidores foram desenvolvidos
utilizando a tecnologia Apache Axis 2. Entretanto, tambe?m foi necessa?rio utilizar
Servlets java para repassar o resultado dos provedores de servic?os para os mo?dulos
do JM-SM dentro do Portal Web. A Figura 4.15 ilustra o diagrama de classes
definido para organizar as classes e me?todos empregados no desenvolvimentos dos
consumidores.

No intuito de organizar a implementac?a?o, foi definido o pacote JobManagementSM.
Como pode ser visto na Figura 4.15, as classes dentro desse pacote utilizam os
recursos ja? definidos no pacote JobManagementUtil. Dentro do pacote que organiza
o JM-SM foi definido o pacote JobManagementSMConsumer. Ele foi criado com o
intuito de separar toda e qualquer implementac?a?o relacionada a tecnologia Web
Service do restante da lo?gica do JM-SM. Com essa filosofia pode-se simplesmente
mudar o tipo de tecnologia empregado na comunicac?a?o e preservar o restante do
funcionamento do JM-SM.

O pacote JobManagementSMConsumer e? formado pelas classes que lidam com os



60

Figura 4.14: Elementos XML da requisic?a?o e resposta da operac?a?o de recuperac?a?o
de arquivos de sa??da.

provedores de servic?os existentes no diferentes clusters integrados no sistema. Para
cada operac?a?o disponibilizada na JM-USI foram definidas classes que preparam as
mensagens a serem enviadas aos provedores (buildxxxxxxOperationPayload())
e que efetivamente acessam os servic?os (callxxxxxOperation()). Visando uma
melhor modelagem do sistema foi definida uma classe que trata das especificidades da
submissa?o de aplicac?o?es (JobSubmission). As classes que tratam da chamada para a
submissa?o de aplicac?o?es (JobSubmissionOperation()) e a de verificac?a?o dos scripts
de submissa?o (VerifySubmissionScriptOperation()) herdam as caracter??sticas da
classe JobSubmission.

Ainda no pacote JobManagementSM existe a classe WSConnections. E? atrave?s
dessa classe que o JM-SM se comunica com o Portal Web. Esta classe e? instanci-
ada dentro do Portal Web a partir das pa?ginas JSP’s e e? responsa?vel por isolar as
chamadas e primitivas empregadas em Web Services existentes nas classes do pa-
cote JobManagementSMConsumer. Entretanto, para cada operac?a?o existe um me?todo
correspondente na classe WSConnections. Nesses me?todos e? que as informac?o?es re-
tornadas sera?o manipuladas e por fim armazenadas no banco de dados.

Como Web Service e? uma tecnologia stateless foi preciso a definic?a?o de algumas
tabelas para o armazenamento das informac?o?es retornadas, para que futuramente
se pudesse acessa?-las. Esta estrutura de tabelas sera? apresentada juntamente com o
diagrama entidade relacionamento (ER) do Portal Web a seguir.

4.2.3 Portal Web

Para iniciar a descric?a?o da implementac?a?o dos mo?dulos que compo?e o Portal
Web sera? apresentado o modelo de dados adotado nesse ambiente. Em seguida sera?o
descritas as implementac?o?es dos mo?dulos System Management Module (SMM) e a
parte referente ao Service Module (SM) do Portal.

O modelo de informac?a?o do ambiente ICE e? ilustrado na Figura 4.16. Ele esta? ba-
seado no modelo Entidade Relacionamento (ER). As entidades e suas caracter??sticas
relevantes sera?o descritas a seguir.



61

Figura 4.15: Diagrama de classes do JM-SM

Para compreender a forma como o Portal Web foi implementado e? preciso en-
tender alguns elementos ba?sicos do ICE e os seus relacionamentos. Cada usua?rio
cadastrado possui um grupo com o nome exatamente igual ao seu usua?rio. Esta
estrutura se reflete nas entidades User, Group e UsersPerGroup. E? poss??vel que um
usua?rio fac?a parte de mais de um grupo, mas o grupo que possui o nome exatamente
igual ao usua?rio deve ser unita?rio. Esse tipo de associac?a?o e? similar ao que ocorre
nos sistemas do tipo Unix e Linux. A partir dessa associac?a?o a manipulac?a?o dos
relacionamentos dos usua?rios com as demais entidades sera? realizada atrave?s do(s)
grupo(s) ao(s) qual(is) ele esta? associado.

Ale?m do usua?rio estar associado a um grupo, e? preciso que esse grupo esteja as-
sociado a um perfil. As entidades Profile e ProfileAssociatedToGroup ilustram
esse requisito. Atrave?s do perfil de um usua?rio e? que se podera? definir quais sa?o os di-
reitos que este usua?rio tera? dentro do ambiente ICE. A princ??pio ja? existem tre?s perfis
ba?sicos: ICE ROOT, CLUSTER ROOT e USER, descritos na Sec?a?o 3.2.2, mas o
modelo de dados e o ambiente foram projetados para suportarem definic?o?es de novos
perfis. Entretanto, como o ambiente ICE e? voltado para o gerenciamento e suporte
de utilizac?a?o de mu?ltiplos clusters e? poss??vel que o usua?rio apresente diferentes per-
fis de acordo com o sistema de alto desempenho que esta? utilizando. Para garantir
essa caracter??stica te?m-se as entidades Cluster e ClusterAccessAssociation. Se-



62

Figura 4.16: Diagrama Entidade Relacionamento (ER) do ambiente ICE

guindo a linha de relacionamentos apresentados na Figura 4.16, para um usua?rio,
associado a um grupo, que esta? associado a um perfil, que esta? associado a um
cluster, podem existir diferentes funcionalidades.

O ambiente ICE permite que sejam definidas diversos tipos de funcionalidades
de clusters. Neste trabalho esta? sendo disponibilizada a funcionalidade de gerencia-
mento de aplicac?o?es, mas outras como monitoramento de recursos, de aplicac?o?es en-
tre outras que podem ser incorporadas. Uma funcionalidade e? formada por operac?o?es
que sa?o formadas por para?metros. As entidades listadas a seguir representam es-
tas associac?o?es: Parameter, Operation, Funcionality, OperationParameters,
FunctionalityOperations. Uma ou mais funcionalidades podem estar associadas
a um ou mais clusters. A entidade FunctiolalityPerCluster ilustra esta situac?a?o
na Figura 4.16.

Com as associac?o?es apresentadas ate? o momento e? poss??vel definir as permisso?es
de acesso dos usua?rios a?s funcionalidades de cada cluster. Dentro do ambiente ICE
para que um usua?rio esteja devidamente autorizado a utilizar alguma funcionalidade
e? preciso que ele possua um grupo, um perfil, um cluster e uma funcionalidade asso-
ciados. Isto e? representado na entidade ClusterFunctionalityUsagePermission.

O gerenciamento dos dados do modelo de informac?a?o do ICE e? realizado pelo
SMM. No momento em que usua?rio acessa o Portal Web, identificando-se atrave?s
de um login e senha, o SMM verifica quais os clusters, perfis e funcionalidades que
esse usua?rio possui permissa?o de utilizac?a?o. A partir dessa verificac?a?o e? montada
a pa?gina Web que sera? enviada ao browser do usua?rio. A Figura 4.17 ilustra um
usua?rio que tem perfil de administrador do ICE (ICE ROOT) e do cluster gppd
(CLUSTER ROOT) (cluster pertencente ao Grupo de Processamento Paralelo e
Distribu??do do Instituto de Informa?tica da UFRGS), ale?m disso ele tambe?m possui
o perfil de usua?rio comum (USER).

Na Figura 4.17 tem-se a sobreposic?a?o de duas telas. A que esta? em segundo
plano e? a pa?gina principal do menu de operac?o?es. A partir dela e? poss??vel inserir
novas operac?o?es, remover e visualizar as existentes, assim como altera?-las. A tela
em primeiro plano ilustra exatamente a pa?gina de alterac?a?o das informac?o?es de uma
operac?a?o, no caso a Job Submission, pertencente a? funcionalidade de gerenciamento
de aplicac?o?es (Job Management).

A? esquerda das telas existe o menu principal. Esse e? o menu montado a partir



63

Figura 4.17: Snapshot Portal Web - Configurac?a?o de operac?o?es

do tipo de perfil que o usua?rio possui. O menu ICE Configuration representa as
opc?o?es que existem para os usua?rio que possuem o perfil ICE ROOT. O menu ICE
Cluster Configuration representa as possibilidades de configurac?a?o disponibilizadas
pelo perfil CLUSTER ROOT. E, finalmente, o menu Common User representa o
perfil de usua?rio comuns (USER). A estrutura de telas apresentada na Figura 4.17
e suas opc?o?es e? ide?ntica para os demais itens do menu ICE Configuration.

Foi definido que usua?rios com direito de configurac?a?o de ambiente ICE so? po-
dem inserir usua?rios administrativos, isto e?, administradores do ICE e de clusters.
Usua?rios comuns so? podera?o ser cadastrados pelos administradores dos clusters. Fo-
ram criadas restric?o?es para fazer com que os administradores de clusters so? tivessem
acesso para configurar os requisitos dos clusters os quais sa?o responsa?veis. Dessa
forma se um usua?rio tem acesso a mais de um cluster que e? gerenciado por diferentes
administradores e? preciso que cada administrador o cadastre como usua?rio de seu
cluster.

O gerenciamento de grupos e? feito exclusivamente pelos administradores dos
clusters. A Figura 4.18 apresenta a tela principal da configurac?a?o de grupos.

E? neste momento que os administradores de clusters podem associar usua?rios
(Users), perfis (Profile), clusters (Resource) e funcionalidades (Functionality) aos
grupos. Atrave?s do preenchimento dos formula?rios disponibilizados nessa etapa de
configurac?a?o do ambiente ICE e? que as tabelas de dados, geradas a partir do modelo
ER apresentado na Figura 4.16, sera?o atualizados. Os processos associac?o?es de
recursos e funcionalidades a grupos sa?o os mais complexos.

Para estabelecer o relacionamento entre um grupo e um cluster sa?o preciso dois
passos. A Figura 4.19 mostra ao fundo o primeiro passo e mais na frente o segundo
passo.

A associac?a?o de um recurso a um usua?rio implica que esse usua?rio esteja as-
sociado a um grupo, o qual deve estar associado a um perfil. No primeiro passo



64

Figura 4.18: Snapshot Portal Web - Pa?gina principal da configurac?a?o de grupos

Figura 4.19: Snapshot Portal Web - Associac?a?o de clusters a grupos



65

Figura 4.20: Snapshot Portal Web - Associac?a?o de funcionalidades a grupos

o administrador pode escolher qual perfil do grupo vai associar a um determinado
cluster. No segundo passo sa?o apresentados os clusters que esse grupo tem acesso e
a quais clusters esse grupo ja? esta? associado.

Na associac?a?o de funcionalidades a grupos sa?o necessa?rios os mesmos passos
anteriores e mais um terceiro. A Figura 4.20 ilustra esse u?ltimo passo.

Depois de definido qual o perfil e qual o recurso, o administrador devera? indicar
qual a funcionalidade que sera? associada. No passo tre?s, apresentado na Figura 4.20,
sa?o disponibilizadas somente as funcionalidades associadas com o cluster escolhido.
A associac?a?o entre cluster e funcionalidade e? estabelecida no menu Resource do
administrador do ambiente ICE, ela e? ilustrada na Figura 4.21.

Nessa figura sa?o disponibilizadas aos administradores as funcionalidades provi-
das pelo ambiente. O administrador deve selecionar as que deseja prover em cada
cluster e deve indicar o enderec?o do provedor dos servic?os. Nesse ponto acontece a
ligac?a?o entre o Portal Web e o mo?dulos JM-SIs existentes nos front-ends dos clusters.

Os snapshots apresentados neste trabalho resumem as telas mais importantes do
ambiente ICE. Resumidamente pode-se dizer que os itens nos menus referentes ao
ICE ROOT e ao CLUSTER ROOT tem a funcionalidade de configurar o ambiente
ICE para que ele possa ser utilizado pelos usua?rios dos clusters gerenciados neste
ambiente. Como as informac?o?es disponibilizadas aos usua?rios sa?o todas dina?micas
tornam-se necessa?rias essas ac?o?es de configurac?a?o do ambiente realizadas dentro do
SMM.

Ale?m das operac?o?es realizadas pelo SMM ainda existem aquelas executadas pelos



66

Figura 4.21: Snapshot Portal Web - Associac?a?o de funcionalidades a clusters

SMs. No caso deste trabalho tem-se as ac?o?es relacionadas ao SM-JM. Analisando os
snapshots apresentados acima pode-se perceber que no menu principal do ambiente
ICE tambe?m existe o menu Common User, relacionado com o perfil USER. Isto
significa que o usua?rio com perfil USER no cluster gppd possui a permissa?o de
execuc?a?o da funcionalidade de gerenciamento de aplicac?o?es (Job Management). A
Figura 4.22 ilustra a pa?gina principal do menu de gerenciamento de aplicac?o?es.

Atrave?s dessa pa?gina e? poss??vel submeter aplicac?o?es, gerar um script de visu-
alizac?a?o de teste, finalizar uma aplicac?a?o ou enta?o recuperar os arquivos de sa??da
padra?o e sa??da de erro padra?o de uma aplicac?a?o que esteja rodando ou que tenha sido
finalizada. A tabela da Figura 4.22 apresenta as aplicac?o?es que estavam associadas
ao usua?rio naquele momento. E? poss??vel perceber que ele possu??a uma aplicac?a?o
que estava sendo executada (12444.gppd.gppd) e outra que estava ainda na fila do
gerenciador do cluster gppd (12456.gppd.gppd). Nesse caso o gerenciador instalado
nesse cluster e? a ferramenta OpenPBS, mas como se pode perceber esta informac?a?o
na?o e? necessa?ria para o usua?rio do ambiente ICE.

As operac?o?es disponibilizadas na Figura 4.22 sa?o exatamente as mesmas definidas
na JM-USI. Entretanto, algumas delas na?o aparecem explicitamente. A operac?a?o de
verificac?a?o de estado das aplicac?o?es na?o e? disponibilizada atrave?s de um bota?o, como
por exemplo, a operac?a?o de submissa?o. Entretanto, a tabela ilustrada na pa?gina
Web dessa figura e? formada atrave?s da consulta a esta operac?a?o. As operac?o?es de
finalizac?a?o e recuperac?a?o das sa??das padra?o tambe?m sa?o disponibilizadas atrave?s
dessa tabela. A operac?a?o de submissa?o de uma aplicac?a?o e? ilustrada na Figura 4.23.

Os para?metros informados sera?o processados pelo SM-JM do Portal Web, sendo
que o consumidor de servic?os do Middleware de Servic?os sera? contactado e dessa
forma a requisic?a?o ao Web service do cluster em questa?o sera? acionado. A mensagem
de retorno do provedor de servic?os sera? tratada e os dados devidamente apresentados
ao usua?rio. Caso algum erro ocorra o usua?rio recebera? uma pa?gina especificando
esse erro. Caso contra?rio, as informac?o?es da aplicac?a?o sera?o armazenadas em uma



67

Figura 4.22: Snapshot Portal Web - Pa?gina principal do menu Job Management

Figura 4.23: Snapshot Portal Web - Pa?gina de submissa?o de aplicac?o?es



68

Figura 4.24: ER da funcionalidade de Job Management

tabela que foi acrescentada no modelo de informac?a?o do ambiente. A Figura 4.24
apresenta as entidades e relacionamentos definidos para armazenar as informac?o?es
relacionadas com a funcionalidade de submissa?o de aplicac?o?es.

Dado o fato dos Web Services na?o manterem o estado de suas operac?o?es foi
necessa?ria a definic?a?o da entidade JobInformation. Ela esta? relacionada com a en-
tidade ClusterAccessAssociation pois uma aplicac?a?o para ser submetida deve estar
associada a um usua?rio com o perfil que permita esse tipo de operac?a?o em um
determinado cluster.

Para visualizar como funcionam as opc?o?es de submissa?o de aplicac?o?es e ter cer-
teza de que o script que devera? ser submetido condiz com a sema?ntica que o usua?rio
deseja foi definida a operac?a?o de visualizac?a?o de scripts. Para utilizar essa operac?a?o
o usua?rio deve preencher o um formula?rio ide?ntico ao formula?rio da operac?a?o de
submissa?o. Entretanto, ao inve?s de submeter uma aplicac?a?o ele recebera? o conteu?do
do script que devera? ser submetido ao gerenciador do cluster. As Figuras 4.25 e 4.26
ilustram exemplos de visualizac?a?o de scripts, apresentando algumas das opc?o?es de
submissa?o disponibilizadas no ambiente.

Figura 4.25: Exemplo 1 de visualizac?a?o de scripts de submissa?o

Nesse primeiro exemplo foi solicitada a visualizac?a?o de script para o lanc?amento
de uma aplicac?a?o paralela utilizando a biblioteca MPICH, onde deveriam ser alo-
cados 2 no?s mas deve ser disparado apenas 1 processo. Atrave?s da interface Web
especificou-se que essa aplicac?a?o deveria ser executada 2 vezes (referente ao campo



69

Figura 4.26: Exemplo 2 de visualizac?a?o de scripts de submissa?o

Batch iterations na Figura 4.23) e que na?o ela na?o deveria ser disparada em mu?ltiplos
arquivos (referente ao campo Multiple scrips na Figura 4.23). A diferenc?a entre os
exemplos da Figura 4.25 e 4.26 esta? no fato de que a segunda habilita a criac?a?o de
diferentes scripts para cada uma das iterac?o?es especificadas.

Comparando as informac?o?es apresentadas na interface Web com as informac?o?es
originais recebidas no consumidor (comparando as figuras da Sec?a?o 4.2.2.2) percebe-
se que em alguns casos o XML recebido no consumidor e? apresentado praticamente
sem alterac?o?es na pa?gina Web, como no caso da visualizac?a?o de scripts. Em com-
pensac?a?o, existem casos em que o conteu?do do XML e? completamente adaptado
para facilitar sua apresentac?a?o aos usua?rios, como no caso da verificac?a?o do estado
das aplicac?o?es submetidas.

De acordo com o projeto do ambiente ICE e? poss??vel a adic?a?o de outras operac?o?es
na funcionalidade de gerenciamento de aplicac?o?es sem que isto altere o que ja? esta?
disponibilizado na ferramenta.

4.3 Resumo

Neste cap??tulo foram apresentadas as deciso?es de projeto e a descric?a?o da im-
plementac?a?o do proto?tipo desenvolvido. No que diz respeito a?s deciso?es de projeto
discutiu-se qual seria o middleware adotado e quais deveriam ser os padro?es atu-
ais que poderiam ser empregados no ambiente ICE. Atrave?s de uma comparac?a?o
entre dois bem documentados e estabelecidos middlewares (Corba e Web Services)
optou-se por utilizar Web Services na implementac?a?o do Middlware de Servic?os do
ambiente ICE. Com relac?a?o aos padro?es que poderiam ser utilizados no ambiente
optou-se por na?o utilizar o Globus e seu mo?dulo de gerenciamento de recursos (WS-
GRAM). Essa decisa?o foi tomada pois verificou-se que na?o poss??vel utilizar apenas
o mo?dulo WSGRAM. Para utiliza?-lo e? preciso instalar tambe?m os mo?dulos princi-



70

pais do Globus, o que corresponde a praticamente todo o Globus, como mostrado
na Sec?a?o 4.1.2. Depois desta ana?lise inicial passou-se para a descric?a?o em si da
implementac?a?o do proto?tipo. Foram apresentados cada um dos componentes do
ambiente e seus mo?dulos. Para tanto foram empregados exemplos de estruturas
XML que descrevem o funcionamento do Middleware de Servic?os assim como foram
apresentados o modelo de informac?a?o e alguns snapshots do Portal Web. Atrave?s
da leitura deste cap??tulo pode-se perceber que o ambiente ICE possui uma comple-
xidade em seus relacionamentos e interligac?o?es de mo?dulos. Entretanto, acredita-se
que essa caracter??stica na?o compromete a capacidade de extensibilidade do ambiente
e transpare?ncia de uso propostos no ambiente. Ao contra?rio, a modularidade de seus
componentes e a arquitetura definidas facilitam o oferecimento dessas caracter??sticas.



71

5 AVALIAC?A?O DO PROTO?TIPO ICE

Este cap??tulo esta? dividido em duas partes. A primeira trata de uma comparac?a?o
entre o ambiente ICE, proposto neste trabalho, e algumas ferramentas relacionadas.
A segunda possui uma ana?lise quantitativa do ambiente desenvolvido. Sera?o apre-
sentados gra?ficos avaliando o overhead inserido pelo ambiente ICE.

5.1 Avaliac?a?o Qualitativa

A avaliac?a?o qualitativa foi realizada atrave?s da comparac?a?o das caracter??sticas
de ferramentas de gerenciamento de alto desempenho com as caracter??sticas do am-
biente ICE. Esta comparac?a?o na?o foi realizada entre todos os ambientes porque,
como apresentado no Cap??tulo 2, existem escopos diferentes de emprego das ferra-
mentas (clusters, grids e mu?ltiplos clusters). Sendo assim, as ferramentas de clusters
e grids na?o podem ser diretamente comparadas ao ambiente ICE, o qual e? voltado
para mu?ltiplos clusters. Isto faz com que seja realizada uma ana?lise qualitativa entre
o ambiente ICE e as ferramentas HPC2N e M3C, apresentadas no Cap??tulo 2.

O objetivo desta ana?lise qualitativa e? verificar qual a capacidade que esses ambi-
entes apresentam para prover facilidades aos usua?rios e desenvolvedores dos sistemas,
considerando os desafios existentes na construc?a?o de sistemas distribu??dos.

5.1.1 Metodologia

A metodologia utilizada, para realizar a comparac?a?o entre o ambiente ICE e os
trabalhos relacionados na a?rea de gerenciamento de mu?ltipos clusters, esta? baseada
nos desafios apresentados por Coulouris em (COULOURIS; DOLLIMORE; KIND-
BERG, 2005) sobre a construc?a?o de sistemas distribu??dos. A lista dos crite?rios de
avaliac?a?o e? apresentada abaixo.

• Heterogeneidade - A comparac?a?o entre os ambientes, considerando este que-
sito, foi realizada seguindo as caracter??sticas listadas abaixo.

1. Independe?ncia de plataforma: considera se o ambiente e? capaz de operar
sobre diferentes plataformas de hardware e de sistemas operacionais.

2. Independe?ncia de linguagem: avalia se os mo?dulos que compo?em o am-
biente podem ser desenvolvidos em qualquer linguagem de programac?a?o.

3. Interoperabilidade: compara a capacidade que o middleware, empregado
no desenvolvimento do ambiente, possui para integrar sistemas legados e
para lidar com as diferenc?as que existem entre os mo?dulos que compo?em
o sistema.



72

• Openness - Este crite?rio esta? relacionado com a extensibilidade do sistema.
Neste trabalho foram considerados dois tipos de extensibilidade, descritas
abaixo.

1. Extensibilidade de funcionalidades: esta? relacionada com a capacidade
que o ambiente apresenta para prover mecanismos de inserc?a?o de novas
funcionalidades.

2. Extensibilidade na integrac?a?o de ferramentas: considera o suporte dis-
pon??vel para prover abstrac?o?es para inserc?a?o de novas ferramentas que
apresentam a mesma funcionalidade. Por exemplo, se ja? existe uma fer-
ramenta integrada no sistema para o gerenciamento de aplicac?o?es, este
crite?rio mede quais os recursos que os ambientes apresentam para inserir
uma nova ferramenta de gerenciamento, sem que isso altere o funciona-
mento global do ambiente.

• Transpare?ncia - Sa?o considerados os seguintes tipos de transpare?ncia: acesso,
localizac?a?o, concorre?ncia, replicac?a?o, falha, mobilidade, desempenho e escala-
bilidade.

• Tratamento de falhas - Considera a capacidade que o ambiente apresenta para
lidar com as falhas de seus componentes.

• Escalabilidade - Avalia a capacidade que o ambiente tem de lidar com o au-
mento do nu?mero de usua?rios e recursos, sem que isso afete o seu desempenho.

• Concorre?ncia - Considera a modelagem de seguranc?a no acesso aos recursos,
quando o ambiente e? concorrente e existe a possibilidade de ocorrer condic?o?es
de corrida, deadlocks, etc.

• Seguranc?a - A ana?lise deste crite?rio foi baseada na existe?ncia de canais de
comunicac?a?o com encriptac?a?o e na existe?ncia de autenticac?a?o de usua?rios.

5.1.2 Comparac?a?o entre HPC2N, M3C e ICE

A tabela 5.1 apresenta a existe?ncia ou na?o dos crite?rios avaliados em cada uma
das ferramentas: HPC2N, M3C e ICE.

Os primeiros cinco crite?rios da Tabela 5.1 sa?o relacionados com as caracter??sticas
de heterogeneidade e openness dos sistemas distribu??dos. O ambiente ICE prove? es-
tes cinco crite?rios devido a decisa?o de utilizar como middleware Web services. O
ambiente M3C pode apresentar independe?ncia de plataforma, uma vez que foi de-
senvolvido utilizando Applets e Servlets Java. Este ambiente tambe?m pode prover
extensibilidade de funcionalidades, visto que ele pode permitir que os seus usua?rios
insiram novas funcionalidades no sistema. Entretanto o M3C na?o proporciona um
framework para lidar com as diferentes ferramentas que possuem a mesma funciona-
lidade e nem a capacidade de lidar com os sistemas legados dos clusters. Ao contra?rio
dos dois primeiros ambientes discutidos, o HPC2N na?o prove? caracter??sticas de hete-
rogeneidade e nem de openness porque e? restrito a um conjunto fechado de ferramen-
tas. Ale?m disso, ele foi desenvolvido utilizando tecnologias que na?o proporcionam
a capacidade de serem interopera?veis e, tambe?m, na?o disponibilizam um framework
para que ele se torne extens??vel.



73

Tabela 5.1: Comparac?a?o entre o ambiente ICE e algumas ferramentas relacionadas

Para?metros de Comparac?a?o HPC2N M3C ICE
1 Independe?ncia de plataforma x x
2 Independe?ncia de linguagem x
3 Interoperabilidade x
4 Extensibilidade de funcionalidades x x
5 Extensibilidade de integrac?a?o de ferramentas x
6 Transpare?ncia de acesso x x x
7 Transpare?ncia de localizac?a?o
8 Transpare?ncia de concorre?ncia x x x
9 Transpare?ncia de replicac?a?o
10 Transpare?ncia de falhas
11 Transpare?ncia de mobilidade x x x
12 Transpare?ncia de desempenho
13 Transpare?ncia de escalabilidade x x
14 Tratamento de falhas
15 Escalabilidade - - -
16 Concorre?ncia x x x
17 Seguranc?a x x x

Todos os ambientes comparados apresentam transpare?ncia de acesso, concorre?ncia
e mobilidade. Seus usua?rios podem acessa?-los sem levar em considerac?a?o quais sa?o
os passos e operac?o?es realizados para que se alcance, efetivamente, o dispositivo.
Va?rios usua?rios podem acessar estes ambientes de forma concorrente atrave?s de seus
browsers. Eles tambe?m podem mudar sua localizac?a?o f??sica sem afetar os sistemas
aqui comparados. Entretanto, nenhum dos ambientes apresentam transpare?ncia de
localizac?a?o, visto que eles sabem quais clusters esta?o utilizando. Os crite?rios de
transpare?ncia de replicac?a?o e falhas, da mesma forma como o tratamento de falhas,
na?o sa?o providos por estes ambientes, pois seus mo?dulos na?o foram desenvolvidos
para identificarem e lidarem com as eventuais falhas que possam ocorrer nos com-
ponentes que os formam.

Com relac?a?o ao crite?rio de escalabilidade, na?o se pode fazer nenhuma afirmac?a?o,
ja? que na?o foram realizados estudos e experimentos capazes de estabelecer a esca-
labilidade destes ambientes. Levando em considerac?a?o o fato dos tre?s ambientes
serem baseados em plataformas Web e que sa?o executados sob servidores Web, tais
como Apache e Tomcat, pode-se especular que suas escalabilidades podem ser limi-
tadas pela configurac?a?o desses servidores. Da mesma forma como a escalabilidade,
o crite?rio de concorre?ncia esta? relacionado com a infra-estrutura sobre a qual os
ambientes ICE, M3C e HPC2N sa?o executados. Cada acesso a estes ambientes e?,
primeiramente, tratado pelos servidores Web, sendo que a concorre?ncia e? tratada
nestes servidores. Considerando o aspecto de seguranc?a, te?m-se os tre?s ambientes
utilizando HTTPS para garantir a seguranc?a do canal de comunicac?a?o. Entretanto,
cada um apresenta um conjunto de te?cnicas distintas para garantir as caracter??sticas
de autenticac?a?o e autorizac?a?o de seus usua?rios.

Finalmente, analisando a Tabela 5.1, pode-se perceber que o ambiente ICE pos-



74

sui suporte para quase todos os crite?rios de sistemas distribu??dos, considerados nesta
comparac?a?o. Comparado com os outros ambientes de gerenciamento de mu?ltiplos
clusters, apresentados neste trabalho, pode-se dizer que o ambiente ICE e? o que
possui mais recursos. As caracter??sticas que mais o distinguem das demais ferra-
mentas sa?o sua capacidade de extensibilidade e openness. Em um cena?rio onde e?
preciso prover facilidade de uso para os usua?rios de mu?ltiplos clusters, cujo custo
de implantac?a?o da soluc?a?o seja o menos oneroso, tem-se o ambiente ICE como uma
boa opc?a?o.

5.2 Avaliac?a?o Quantitativa

O objetivo dos testes quantitativos com o ambiente ICE foi verificar qual o
overhead inserido pela infra-estrutura deste ambiente nos processos que eram, tipi-
camente, executados atrave?s de linha de comando pelos usua?rios.

Foram realizados experimentos com as cinco operac?o?es providas pela JM-USI:

• submissa?o de aplicac?o?es;

• verificac?a?o do script de submissa?o gerado pelo JM-SI da respectiva ferramenta;

• finalizac?a?o de aplicac?o?es;

• verificac?a?o do estado das filas de execuc?a?o das ferramentas;

• recuperac?a?o dos arquivos de sa??da (padra?o ou de erro).

A seguir sa?o apresentadas algumas informac?o?es que caracterizam como foi condu-
zida a avaliac?a?o quantitativa do ambiente ICE. Em primeiro lugar, sera? apresentado
exatamente o que foi medido, em seguida qual a metodologia de medic?a?o e por fim
os resultados encontrados para cada uma das operac?o?es suportadas no mo?dulo de
gerenciamento de aplicac?o?es do ambiente ICE.

5.2.1 Definic?a?o do escopo das medic?o?es

O overhead observado leva em considerac?a?o apenas o tempo de processamento
dentro do front-end do cluster empregado nos testes. A Figura 5.1 ilustra os ele-
mentos que fizeram parte da aquisic?a?o das medidas apresentadas nesta sec?a?o.

As as tre?s entidades que foram empregadas na realizac?a?o desses testes sa?o carac-
terizadas a seguir.

• Ferramenta de Gerenciamento: corresponde a? ferramenta de gerenciamento de
aplicac?o?es, instalada no cluster onde as medidas esta?o sendo realizadas.

• Provedor JM-SI: representa as implementac?o?es do JM-SI. No caso dos experi-
mentos aqui apresentados existe o OpenPBS JM-SI e o OAR JM-SI.

• Consumidor standalone: consiste em uma aplicac?a?o que consulta o provedor
de servic?os da JM-SI. Esta aplicac?a?o na?o esta? integrada ao ambiente ICE, por
isso foi considerada standalone. Ela e? utilizada para acessar tanto o OpenPBS
JM-SI quanto o OAR JM-SI. Entretanto, para evitar a interfere?ncia da rede
de comunicac?a?o, ela foi instalada no front-end utilizado para os testes.



75

Figura 5.1: Cena?rio de aquisic?a?o das medidas de overhead

O consumidor standalone faz as requisic?o?es das operac?o?es do servic?o de gerencia-
mento de aplicac?o?es ao provedor da respectiva JM-SI. Ao receber estas requisic?o?es o
JM-SI faz o tratamento das informac?o?es e realiza os passos necessa?rios, interagindo
com a ferramenta de gerenciamento. Apo?s esta interac?a?o, retorna para o consumidor
os dados resultantes da execuc?a?o da operac?a?o.

Nos testes que foram realizados, o que se mediu foi a soma dos tempos T1, T2
e T3 indicados na Figura 5.1. T1 representa o tempo despendido pelo provedor
para reconhecer os para?metros da operac?a?o e traduzi-los para serem solicitados a?
ferramenta de gerenciamento. T2 consiste no tempo que o provedor tem que esperar
para receber os dados da operac?a?o executada. T3 e?, por fim, o tempo que o provedor
gasta para processar os dados e montar a mensagem de retorno ao consumidor.
Nestes experimentos, na?o foi considerado o tempo gasto pelo container Tomcat
para receber a requisic?a?o e encaminha?-la ao devido provedor de servic?os, assim
como, tambe?m, na?o foi considerado o tempo que este mesmo elemento leva para
encaminhar a resposta do servic?o solicitado.

Para que se pudesse identificar qual o overhead inserido pelo ambiente ICE foi
realizado o seguinte ca?lculo:

Overhead = Tice ? Tf erramenta

onde Tice representa o tempo total (T1+T2+T3) de execuc?a?o de uma operac?a?o do
provedor JM-SI, e Tferramenta representa o tempo de total execuc?a?o de uma ac?a?o
da ferramenta sem intervenc?a?o do ambiente ICE. Desta forma, durante as ana?lises
o overhead e? apresentado em termos absoluto, ou seja, quantos milisegundos de
overhead foi observado.

5.2.2 Metodologia

Foram realizados experimentos e ana?lises para todas as operac?o?es definidas na
JM-USI e implementadas na JM-SI. Para cada uma destas ana?lises foram executadas
300 iterac?o?es. Definiu-se este tamanho de amostra depois de sucessivas rodadas de
execuc?a?o com diferentes tamanhos de amostras. Atrave?s da ana?lise dos desvios



76

padro?es encontrados percebeu-se que amostras a partir de 300 iterac?o?es mantinham
o desvio padra?o constante. Os valores apresentados nos gra?ficos correspondem a?s
me?dias encontradas de acordo com esta amostra. Ale?m dos gra?ficos com as me?dias,
tambe?m sera?o apresentadas algumas tabelas com o desvio padra?o e a representac?a?o
percentual desse desvio em relac?a?o a? me?dia encontrada. Ainda foram realizados
testes utilizando, diretamente, os comandos dos gerenciadores de aplicac?o?es. Estes
testes tambe?m seguiram a metodologia de aquisic?a?o de valores relatada acima.

A aquisic?a?o do tempo, dentro da implementac?a?o dos provedores JM-SI, foi reali-
zada atrave?s da chamada a? primitiva System.currentTimeMillis() logo no in??cio
de cada operac?a?o e no final das mesmas, exatamente antes de retornar a resposta aos
consumidores. Esta primitiva faz parte da linguagem Java, e retorna o tempo em mi-
lisegundos no momento de sua chamada. Para que se possa ter o tempo de execuc?a?o
da operac?a?o, faz-se a subtrac?a?o do tempo final menos o inicial. Para aquisic?a?o dos
tempos, no caso da execuc?a?o direta das ferramentas, foi utilizado o comando time,
que retorna o tempo de cpu em segundos utilizado por uma aplicac?a?o. Este tempo
em segundos foi convertido para milisegundos no intuito de uniformizar as medic?o?es
realizadas neste trabalho.

Nas ana?lises feitas, com as operac?o?es de submissa?o de aplicac?o?es e de visualizac?a?o
de scripts de submissa?o, foram realizadas variac?o?es na forma como os para?metros
das aplicac?o?es podem ser utilizados. Estas variac?o?es consideram os para?metros de:
mu?ltiplas execuc?o?es de mesma aplicac?a?o e no caso de serem solicitadas mu?ltiplas
execuc?o?es, e? preciso que sejam criados diferentes scripts. Ale?m disso tambe?m foram
testadas aplicac?o?es sequ?enciais e paralelas com as bibliotecas MPICH e DECK. A
nomenclatura utilizada nas legendas dos gra?ficos dos experimentos destas operac?o?es
e? apresenta na Tabela 5.2.

Tabela 5.2: Descric?a?o das variac?o?es de para?metros para submissa?o de aplicac?o?es.
Opc?o?es Descric?a?o
DECK Aplicac?a?o paralela baseada na biblioteca DECK
MPICH Aplicac?a?o paralela baseada na biblioteca MPICH
Seq Aplicac?a?o sequ?encial
Multiple Indica se a aplicac?a?o foi executada mais de uma vez.

No caso dos testes realizados, quando habilitado,
significa que a aplicac?a?o foi executada cinco vezes.

Single Indica que a aplicac?a?o foi executada uma u?nica vez.
Arc Indica que somente um script foi gerado,

independentemente do nu?mero de vezes
que a aplicac?a?o sera? executada

Arcs Indica que foram gerados tantos scripts quantas foram as
vezes que a aplicac?a?o devera? ser executada. Neste caso,
quando habilitado, significa que foram gerados cinco
scripts de submissa?o da mesma aplicac?a?o.

5.2.3 Plataforma

Os experimentos foram realizados no cluster frontal-minuano pertencente ao
GPPD, o qual tem suas principais caracter??sticas apresentadas na Tabela 5.3. Neste



77

cluster foram instaladas as duas ferramentas de gerenciamento de aplicac?o?es: OpenPBS
e OAR.

Tabela 5.3: Descric?a?o do cluster utilizado para os experimentos
Caracter??stica Frontal-minuano

Sistema Operacional Linux, kernel 2.6.12
Distribuic?a?o Debian Sarge
Processador Pentium III 598.998 MHz
Cache 512 KB
Memo?ria RAM 256 MB
Swap 494 MB

Decidiu-se realizar os experimentos em um u?nico cluster para que se pudesse
comparar, diretamente, a intrusa?o do ambiente ICE em cada uma das ferramentas
de gerenciamento de aplicac?o?es suportadas.

5.2.4 Experimentos com a operac?a?o de submissa?o de aplicac?o?es

O primeiro experimento realizado considerou a operac?a?o de submissa?o de aplicac?o?es.
Os gra?ficos da Figura 5.2 ilustram os tempos de submissa?o encontrados com a im-
plementac?a?o OpenPBS JM-SI.

Figura 5.2: Gra?fico Operac?a?o de Submissa?o - OpenPBS

No caso dos testes apresentados na Figura 5.2, o tempo medido para gerac?a?o
e submissa?o de um u?nico script manteve-se em me?dia entre 147 e 153 milisegun-
dos, enquanto a gerac?a?o e submissa?o de mu?ltiplos scripts manteve-se em torno de
730 e 740 milisegundos. O tempo me?dio de submissa?o de uma aplicac?a?o, atrave?s
dos comandos do gerenciador OpenPBS, manteve-se na casa de 56,1 milisegundos.
Observando os gra?ficos dessa figura pode-se perceber que a submissa?o de um u?nico
script apresentou um comportamento bastante homogeneo, independentemente do
tipo de aplicac?a?o que estava sendo lanc?ada. No caso do disparo de mu?ltiplos scripts



78

de execuc?a?o da mesma aplicac?a?o, houve uma pequena diminuic?a?o no tempo da
gerac?a?o para aplicac?o?es MPICH. Este comportamento pode ter sido causado devido
a?s condic?o?es do front-end, onde, possivelmente, devem ter ocorrido variac?o?es na sua
carga de trabalho que acabou se refletindo nos testes realizados. Esta hipo?tese e?
reforc?ada quando se analisa o desvio padra?o deste experimento, que pode ser encon-
trado na Tabela 5.4. Pode-se perceber que o desvio padra?o, nos casos de submissa?o
de mu?ltiplos scripts para a biblioteca DECK e para aplicac?o?es sequ?enciais, manteve-
se na casa de 18 milisegundos, enquanto para a biblioteca MPICH o desvio padra?o
encontrado foi de aproximadamente 19 milisegundos. Com estes valores, pode-se
considerar que o tempo de execuc?a?o em me?dia foi menor do que nos demais, entre-
tanto apresentou uma variac?a?o maior. Ale?m das informac?o?es de desvio padra?o, a
Tabela 5.4 tambe?m apresenta o percentual de quanto cada desvio padra?o representa
em relac?a?o a? respectiva me?dia do experimento.

Comparando os valores das me?dias encontradas, pode-se dizer que o tempo de
processamento de uma submissa?o de aplicac?o?es no ambiente ICE e?, em me?dia, tre?s
vezes o tempo necessa?rio para o disparo direto. Sendo assim, o ambiente ICE insere
um overhead de duas vezes o valor alcanc?ado na submissa?o direta. Por exemplo,
na submissa?o de uma aplicac?a?o a ser lanc?ada em u?nico script (em geral o tempo
gasto, independentemente se e? uma aplicac?a?o sequ?encial ou paralela, com ou sem
mu?ltiplas execuc?o?es dentro do mesmo script) o overhead do ambiente ICE e? de
aproximadamente 90,9 milisegundos.

Considerar este valor de overhead, sem levar em considerac?a?o a facilidade de uso
que o ambiente ICE traz, na?o e? muito encorajador. Entretanto, apesar de possuir
um tempo de processamento maior que o tempo de submissa?o direta, via OpenPBS,
ainda pode ser mais vantajoso para os usua?rios perderem alguns milisegundos na
submissa?o e contarem com a transpare?ncia e facilidade de uso que o ambiente ICE
oferece.

Os gra?ficos da Figura 5.3 ilustram os tempos de submissa?o encontrados com a
implementac?a?o OAR JM-SI.

Figura 5.3: Gra?fico Operac?a?o de Submissa?o - OAR



79

Analisando os gra?ficos da Figura 5.3, percebe-se que, de maneira geral, o compor-
tamento do ambiente ICE, com a implementac?a?o OAR JM-SI, mostrou-se similar
ao OpenPBS JM-SI. A diferenc?a esta? no fato de que os tempos medidos na pri-
meira implementac?a?o foram, consideravelmente, maiores do que na segunda. Esta
diferenc?a existe dadas as caracter??sticas das ferramentas de gerenciamento sobre as
quais foram implementadas as JM-SIs. Muitas das informac?o?es que o OAR pos-
sui esta?o armazenadas em um banco de dados MySQL, enquanto isso o OpenPBS
utiliza arquivos. Ale?m disso, as caracter??sticas de implementac?a?o de cada uma sa?o
bastante diferentes. Tudo isso faz com que elas possuam tempos de execuc?a?o bem
distintos. Esses tempos na?o sa?o maiores somente na comparac?a?o entre as imple-
mentac?o?es do JM-SI, mas tambe?m na comparac?a?o direta entre as ferramentas. A
ferramenta OpenPBS apresenta o tempo de submissa?o na casa de 56,1 milisegundos,
enquanto a OAR possui tempo de execuc?a?o 1863,53 milisegundos.

Observando os tempos encontrados com o OAR JM-SI, que ficaram em torno de
1930 milisegundos no caso da gerac?a?o e submissa?o de um u?nico script, tem-se um
desempenho bem pro?ximo ao da pro?pria ferramenta. Isto faz com que os usua?rios
disponham da facilidade de utilizac?a?o da ferramenta de gerenciamento de aplicac?o?es
que o ambiente ICE disponibiliza na?o sendo penalizado com um overhead significa-
tivo. Por exemplo, para aplicac?o?es sequ?enciais lanc?adas em um u?nico arquivo tem-se
um overhead de aproximadamente 66,47 milisegundos.

Na Tabela 5.4 tambe?m sa?o apresentados os valores de desvio padra?o e o per-
centual de desvio padra?o encontrado para os testes de submissa?o da implementac?a?o
OAR JM-USI. Pode-se perceber que os valores dos desvios padra?o mantiveram-se
esta?veis quando comparados entre os tipos de experimentos realizados. Por exem-
plo, no caso de mu?ltiplos scripts, os desvios padra?o ficaram perto de 2 milisegundos,
enquanto que para um u?nico script ele se manteve em 4 milisegundos.

Tabela 5.4: Desvio padra?o para operac?a?o de submissa?o
OpenPBS OAR

Variac?o?es Desvio Padra?o Percentual Desvio Padra?o Percentual
Ferramenta 2,89 5,11 12,95 0,69
DECKMultipleArc 7,32 4,95 104,2 5,39
DECKMultipleArcs 18,92 2,42 1543,61 10,61
DeckSingleArc 6,35 4,3 149,19 7,75
MPIMultipleArc 7,33 4,96 108,13 5,6
MPIMultipleArcs 19,18 2,6 1508,67 10,34
MPISingleArc 6,7 4,55 117,65 6,1
SeqMultipleArc 6,17 4,14 105,28 5,45
SeqMultipleArcs 18,16 2,32 1722,46 11,67
SeqSingleArc 5,69 3,71 123,69 6,4

Atrave?s das ana?lises realizadas acima, sobre os resultados encontrados nos expe-
rimentos, verificou-se que o ambiente ICE insere um overhead no processo de sub-
missa?o de aplicac?o?es. Este overhead foi mais acentuado no caso da implementac?a?o
OpenPBS JM-SI (90,9 milisegundos, que um tempo representa aproximadamente 2
vezes mais tempo do que a execuc?a?o isolada da ferramenta) e foi bastante suavizado



80

na implementac?a?o OAR JM-SI (66,47 milisegundos, que representa 3,6% do tempo
de execuc?a?o da ferramenta isolada). Estes experimentos na?o podem ser analisados
de forma pontual, ou seja, sem levar em considerac?a?o as caracter??sticas do ambiente
ICE. Eles servem como um indicativo do custo a ser pago pelas facilidades de uso
que esta?o sendo inseridas no uso do cluster.

5.2.5 Experimentos com a operac?a?o de visualizac?a?o de scripts de sub-
missa?o

A operac?a?o de visualizac?a?o de scripts de submissa?o na?o possui um comando
correspondente nas ferramentas de gerenciamento utilizadas neste trabalho. Isto
tambe?m se deve ao fato de que sa?o os usua?rios que montam os scripts de submissa?o,
e sendo assim, eles ja? conhecem o seu conteu?do. No ambiente ICE os usua?rios
na?o lidam com os detalhes de mais baixo n??vel, mas podem necessitar a verificac?a?o
da sema?ntica e a correc?a?o dos scripts que lanc?ara?o suas aplicac?o?es, como ja? ar-
gumentado na Sec?a?o 4.2.2.1. As Figuras 5.4 e 5.5 apresentam os tempos me?dios
de processamento da operac?a?o de visualizac?a?o de scripts, respectivamente, para as
implementac?o?es OpenPBS JM-SI e OAR JM-SI.

Figura 5.4: Gra?fico Operac?a?o de Visualizac?a?o de Scripts - OpenPBS

Analisando os gra?ficos das Figuras 5.4 e 5.5, e? poss??vel perceber que os tempos
me?dios de processamento da operac?a?o de verificac?a?o de scripts, nas duas imple-
mentac?o?es, aproximaram-se bem mais do que no caso da operac?a?o de submissa?o. Os
tempos me?dios para a gerac?a?o de scripts u?nicos ficou em torno de 12 milisegundos
para a implementac?a?o OpenPBS JM-SI e em torno de 11 segundos para a OAR
JM-SI. No caso da verificac?a?o dos mu?ltiplos scripts, o tempo me?dio de execuc?a?o
ficou na casa de 15 e 14 milisegundos respectivamente.

Comparando a execuc?a?o das duas implementac?o?es, pode-se perceber que, em
geral, existe uma diferenc?a de 1 milisegundo. Esta diferenc?a se deve a?s diferenc?as
que existem no processo de traduc?a?o dos para?metros para as estruturas de cada
ferramenta.

Nos gra?ficos da Figura 5.4, ainda pode-se perceber uma pequena variac?a?o dos



81

Figura 5.5: Gra?fico Operac?a?o de Visualizac?a?o de Scripts - OAR

tempos me?dios no caso da verificac?a?o de um u?nico script. Mais uma vez essa al-
terac?a?o pode ter sido causada pela carga do cluster frontal-minuano.

A Tabela 5.5 apresenta os valores de desvio padra?o e percentual de desvio padra?o
verificados nos experimentos com esta operac?a?o para as duas implementac?o?es. Ana-
lisando os valores encontrados, pode-se perceber que na?o houve nenhuma variac?a?o
muito acentuada dos tempos de execuc?a?o desta operac?a?o.

Tabela 5.5: Desvio padra?o para operac?a?o de verificac?a?o de scripts

OpenPBS OAR
Variac?o?es Desvio Padra?o Percentual Desvio Padra?o Percentual
DECKMultipleArc 1,09 8,94 0,89 7,57
DECKMultipleArcs 2,18 13,75 1,26 8,76
DeckSingleArc 1,49 11,54 0,8 6,74
MPIMultipleArc 1,35 10,01 0,88 7,39
MPIMultipleArcs 1,95 12,57 1,21 8,55
MPISingleArc 1,32 10,75 0,9 7,69
SeqMultipleArc 1,29 9,85 0,79 6,73
SeqMultipleArcs 1,86 12,01 1,18 8,36
SeqSingleArc 1,5 12,31 0,79 6,75

De maneira geral, pode-se dizer que os tempos encontrados na execuc?a?o desta
operac?a?o sa?o satisfato?rios. A vantagem de poder verificar o conteu?do e a sema?ntica
do script de submissa?o, e esta operac?a?o ser executada em um intervalo de tempo
pequeno, so? traz ganhos para o ambiente ICE.



82

5.2.6 Experimentos com a operac?a?o de finalizac?a?o de aplicac?o?es

A Tabela 5.6 apresenta os tempos me?dios, desvio padra?o e o percentual do desvio
padra?o em relac?a?o a? me?dia. Foram realizados experimentos com a submissa?o direta
via OpenPBS, via OAR e atrave?s das implementac?o?es JM-SI sobre cada uma das
ferramentas.

Tabela 5.6: Desempenho da operac?a?o de finalizac?a?o de aplicac?o?es

Me?dia Desvio Padra?o Percentual
OpenPBS JM-SI 138,96 3,61 2,59
OpenPBS 50,5 2,54 5,03
OAR JM-SI 2070,76 408,85 19,74
OAR 1989,19 405,42 20,38

A primeira constatac?a?o que se pode fazer, analisando esta tabela, e? a grande
diferenc?a de tempos entre as ferramentas. Ao utilizar o OpenPBS diretamente o
tempo me?dio de finalizac?a?o e? 50,5 milisegundos, enquanto com o OAR o tempo me?dio
fica em 1989,19 milisegundos. Isto ocorre em vista das diferenc?as das ferramentas
(ver Sec?a?o 5.2.4).

Comparando os resultados de cada ferramenta com a respectiva infra-estrutura
no ambiente ICE, tem-se as situac?o?es apresentadas a seguir. No caso da comparac?a?o
entre o gerenciador OpenPBS e o OpenPBS JM-SI, pode-se visualizar a diferenc?a
que existe nos tempos me?dios de finalizac?a?o. Enquanto o gerenciador apresenta
o tempo me?dio de 50,5 milisegundos, a infra-estrutura ICE possui o tempo me?dio
de 138,96 milisegundos. Isto mostra que o tempo total de finalizac?a?o no ambiente
ICE, considerando a infra-estrutura para a ferramenta OpenPBS, e? em geral tre?s
vezes maior que o tempo de execuc?a?o da ferramenta de forma isolada. O overhead
verificado foi de 88,46 milisegundos. Apesar deste nu?mero parecer um pouco elevado,
e? preciso lembrar que se tratam de milisegundos e que o maior objetivo do ambiente
ICE e? prover facilidades de uso aos seus usua?rios. Portanto, mais uma vez se torna
interessante gastar 138,96 milisegundos para finalizar uma aplicac?a?o, havendo a
possibilidade de na?o ter que lidar com os comandos espec??ficos do gerenciador de
aplicac?a?o.

Considerando o gerenciador OAR e o OAR JM-SI, pode-se perceber, atrave?s
dos dados da Tabela 5.6, que a infra-estrutura ICE apresenta um tempo me?dio de
execuc?a?o bem pro?ximo ao do gerenciador. A finalizac?a?o direta, via OAR, levou em
me?dia 1989,76 milisegundos, enquanto a finalizac?a?o atrave?s do ambiente ICE, via
o OAR JM-SI, levou em me?dia 2070,76 milisegundos. Esta diferenc?a implica em
overhead de 81 milisegundos. Este valor mostra que a maior parte do tempo de
processamento da infra-estrutura ICE, nesta implementac?a?o, e? gasto pela chamada
ao comando do gerenciador OAR.

Analisando o desvio padra?o encontrado nos experimentos, e? poss??vel notar, que
no caso da ferramenta OAR, existe uma variac?a?o considera?vel que fica na casa de
20% em relac?a?o a? me?dia observada, tanto na execuc?a?o direta da ferramenta quanto
na implementac?a?o OAr JM-SI. No caso do OpenPBS, obteve-se uma estabilidade
maior, ou seja, os tempos de execuc?a?o da amostra na?o tiveram uma variac?a?o muito
grande. Ela ficou na casa de 3,61 milisegundos para a finalizac?a?o atrave?s da im-



83

plementac?a?o OpenPBS JM-SI e 2,54 milisegundos atrave?s da operac?a?o direta da
ferramenta OpenPBS. Em termos percentuais, estes valores representam uma va-
riac?a?o nos tempos de execuc?a?o das amostras de 2,59% e 5,03% em relac?a?o a? me?dia,
respectivamente, aos tipos de experimentos.

Da mesma forma como constatado nas demais operac?o?es, percebe-se que o overhead,
imposto pelo ambiente ICE, pode ser tolerado visto a facilidade que ele insere.

5.2.7 Experimentos com a operac?a?o de visualizac?a?o de status

A visualizac?a?o do status das aplicac?o?es consiste na listagem de todas as aplicac?o?es
que esta?o presentes nas filas dos gerenciadores. A Tabela 5.7 possui os dados ob-
tidos com os experimentos de visualizac?a?o dos status das aplicac?o?es para ambas
ferramentas e implementac?o?es das JM-SIs.

Tabela 5.7: Desempenho da operac?a?o de verificac?a?o do status das aplicac?o?es

Me?dia Desvio Padra?o Percentual
OpenPBS JM-SI 123,22 3,9 3,17
OpenPBS 36,34 0,99 2,73
OAR JM-SI 857,67 6,77 0,79
OAR 765,92 12,95 1,69

Atrave?s da ana?lise dos tempos das ferramentas de gerenciamento percebe-se que
o gerenciador OpenPBS apresenta um desempenho superior ao verificado com o
OAR. Eles possuem, respectivamente, os tempos me?dios de execuc?a?o de 36,34 e
765,92 milisegundos.

Assim como em experimentos anteriores, a ana?lise dos experimentos sobre a
plataforma OpenPBS mostrou, mais uma vez, que a implementac?a?o OpenPBS JM-
SI apresentou um tempo de execuc?a?o me?dio tre?s vezes maior do que o da execuc?a?o
direta. Os valores mostrados na Tabela 5.7 mostram que a implementac?a?o OpenPBS
JM-SI apresentou um tempo me?dio de 123,22 milisegundos, enquanto a execuc?a?o
direta utilizou 36,34 milisegundos. O overhead medido ficou em 86,88 milisegundos.

No caso dos experimentos com a ferramenta OAR, verificou-se que o tempo
de execuc?a?o dessa operac?a?o, diretamente pelo OAR, foi de 765,92 milisegundos,
enquanto o tempo de execuc?a?o, via implementac?a?o OAR JM-SI, foi igual a 857,67
milisegundos. O overhead inserido foi de 91,75 milisegundos.

Comparando os tempos das execuc?o?es diretas das ferramentas com as imple-
mentac?o?es das JM-SI, pode-se observar que se obteve o mesmo comportamento da
operac?a?o de finalizac?a?o. Isto e?, a implementac?a?o OpenPBS JM-SI apresentou uma
diferenc?a de tempo de execuc?a?o maior, se comparada com a verificac?a?o do estado, di-
retamente via ferramenta, do que na implementac?a?o OAR JM-SI. E da mesma forma
como as demais operac?o?es comparadas com as execuc?o?es diretas das ferramentas,
pode-se considerar que os tempos despendidos pelas implementac?o?es JM-SI, para
operac?a?o de verificac?a?o dos estado das filas, e? compensador.

5.2.8 Experimentos com a operac?a?o de recuperac?a?o das sa??das padra?o

A operac?a?o de recuperac?a?o de informac?a?o das sa??das padra?o na?o possui corres-
pondente nos gerenciadores, visto que o arquivo pode ser visualizado pelo usua?rio



84

em sua pro?pria conta. Sendo assim, a Tabela 5.8 apresenta os resultados alcanc?ados
com a avaliac?a?o da execuc?a?o das implementac?o?es JM-SI do ICE para cada uma das
ferramentas.

Tabela 5.8: Desempenho da operac?a?o de recuperac?a?o das sa??das padra?o

Me?dia Desvio Padra?o Percentual
OpenPBS JM-SI 11,28 2,54 22,5
OAR JM-SI 10,74 0,74 6,93

Analisando os tempos me?dios apresentados na Tabela 5.8, verifica-se que os
dois JM-SI apresentam um resultado muito semelhante. Como neste caso a infra-
estrutura ICE na?o precisa interagir com os gerenciadores, a diferenc?a de tempo exis-
tente entre eles e? decorrente, unicamente, das diferenc?as entre as implementac?o?es das
JM-SIs e tambe?m da carga do front-end no momento dos experimentos. No caso do
OpenPBS JM-SI sa?o gastos em me?dia 11,28 milisegundos, enquanto no OAR JM-SI
sa?o necessa?rios 10,74 milisegundos para recuperar a informac?a?o dos arquivos.

Para evitar que o tamanho do arquivo recuperado interferisse nas medic?o?es re-
alizadas, optou-se por utilizar um arquivo vazio. Assim o tempo gasto para ler as
linhas dos arquivos na?o foi computada neste experimento. Os resultados apresen-
tados na Tabela 5.8 representam exatamente o overhead que a infra-estrutura ICE
necessita para recuperar arquivos.

5.3 Resumo

Este cap??tulo foi dividido em dois momentos. O primeiro conte?m uma ana?lise
comparativa entre o ambiente ICE, proposto neste trabalho, e algumas ferramentas
relacionadas que possuem o mesmo escopo de aplicac?a?o. Para tanto foram escolhi-
das duas ferramentas, que se enquadravam nestas caracter??sticas, para serem com-
paradas com o ICE, as quais sa?o: HPC2N e M3C. A comparac?a?o teve como base
os crite?rios estabelecidos por Coulouris em (COULOURIS; DOLLIMORE; KIND-
BERG, 2005) como sendo os desafios da construc?a?o de um sistema distribu??do.
Alguns dos crite?rios adotados foram: transpare?ncia, extensibilidade, portabilidade,
seguranc?a entre outros. No segundo momento foi apresentada uma ana?lise quantita-
tiva do ambiente ICE, onde foram verificados os tempos de execuc?a?o dos provedores
do servic?o de gerenciamento de aplicac?o?es, implementados e disponibilizados no am-
biente. Conforme esperado, o ambiente ICE inseriu um certo overhead na execuc?a?o
das operac?o?es, se comparado com a execuc?a?o das operac?o?es diretamente atrave?s da
ferramenta. Em alguns casos, o aumento foi significativo, principalmente nos experi-
mentos realizados sobre a ferramenta OpenPBS. Entretanto, esse aumento no tempo
de processamento na?o chega ser um fator limitante para a utilizac?a?o do ambiente
ICE, visto que os objetivos principais do ambiente ICE sa?o prover: transpare?ncia
das operac?o?es de mais baixo n??vel a seus usua?rios, extensibilidade e flexibilidade.
Portanto, o overhead das operac?o?es, disponibilizadas por ele, na?o e? um crite?rio deci-
sivo para definir a adoc?a?o ou na?o do ambiente ICE. Ele serve como um indicativo do
custo/benef??cio que se deseja oferecer aos usua?rios do ambiente de alto desempenho.



85

6 CONCLUSA?O

O ambiente ICE - Integrated Cluster Environment - foi proposto com base em
aspectos que ainda na?o tinham sido considerados pelas ferramentas atuais de ge-
renciamento de mu?ltiplos clusters, como por exemplo: interoperabilidade, extensi-
bilidade, integrac?a?o de sistemas legados, transpare?ncia, entre outros. O foco deste
ambiente e? prover gerenciamento e acesso a mu?ltiplos clusters de forma transpa-
rente, extens??vel e interopera?vel. Como apresentado no Cap??tulo 3, o ambiente ICE
tem uma se?rie de objetivos, mas se pode considerar que aqueles que o distinguem
dos demais ambientes sa?o: (i) capacidade de uniformizac?a?o do modo como as ferra-
mentas de clusters sa?o utilizadas e, tambe?m, a uniformizac?a?o na maneira como os
clusters sa?o acessados; (ii) transpare?ncia de acesso e uso dos clusters; e (iii) capa-
cidade de extensibilidade em dois n??veis: o primeiro refere-se a? extensibilidade do
nu?mero de funcionalidades (servic?os) providas pelo sistema e o segundo esta? relaci-
onado com a capacidade do sistema lidar com o uso de diferentes ferramentas que
possuem a mesma funcionalidade. Estas principais capacidades podem ser providas
pelo ambiente ICE devido ao tipo de arquitetura e ao tipo de middleware adotados
neste ambiente. Optou-se por utilizar uma arquitetura orientada a servic?os (SOA) e
como middleware utilizou-se Web Services. As principais razo?es para estas escolhas
sa?o: o fato da arquitetura SOA poder separar a implementac?a?o de um servic?o de
sua interface; e Web Services ser uma tecnologia baseada em arquitetura SOA e em
padro?es Web, os quais ja? esta?o estabelecidos e amplamente utilizados. Estas ca-
racter??sticas sa?o fundamentais para a modelagem e o desenvolvimento do ambiente
ICE. E? importante ressaltar que o ambiente ICE na?o e? apenas um portal ou enta?o
um sistema rodando no front-end de um cluster. O ambiente ICE e? a unia?o entre o
ponto u?nico de acesso dos usua?rios com o middlware definido para o gerenciamento
e acesso das funcionalidades dos clusters.

Uma caracter??stica principal da arquitetura ICE e? sua modularidade. O objetivo
e? que se possam inserir novas funcionalidades e novas ferramentas sem que isso al-
tere o modelo do ambiente ICE. A ide?ia, por tra?s da arquitetura do ambiente ICE,
e? fazer com que existam mo?dulos ba?sicos do sistema e uma infra-estrutura capaz de
prover servic?os de clusters. Com base nestas caracter??sticas, definiu-se a arquitetura
do ambiente ICE, a qual e? formada por dois componentes: Middleware de Servic?os
e Portal Web. No Middleware de Servic?os sa?o considerados os aspectos ligados a?
arquitetura SOA empregada no ambiente ICE. Este componente e? formado pelos
seguintes mo?dulos: Unified Service Interface (USI), Service Implementation (SI), e
Service Module (SM). Uma USI e? um mo?dulo conceitual, onde o servic?o e? especi-
ficado em alto n??vel. O SI e? um mo?dulo que deve ser desenvolvido de acordo com
uma USI. Ele e? a implementac?a?o do provedor de um servic?o. O SM caracteriza-se



86

por ser a implementac?a?o do consumidor de um servic?o, tambe?m especificado por
uma USI e que fara? as requisic?o?es ao respectivo SI. O Portal Web e? formado por
tre?s mo?dulos: Service Management Module (SMM), Security Module (SecM) e Ser-
vice Module (SM). O SMM e? responsa?vel pela lo?gica de controle do sistema Web.
No SecM sa?o definidas as ac?o?es para o controle de autenticac?a?o e autorizac?a?o do
ambiente ICE. O SM e? responsa?vel pela apresentac?a?o, no Portal Web, dos dados
provenientes da porc?a?o SM pertencente ao Middleware de Servic?o. Para cada fun-
cionalidade de cluster, que se deseja inserir no ambiente ICE, e? preciso a criac?a?o da
infra-estrutura de servic?os e o registro das informac?o?es desta nova funcionalidade no
Portal Web.

O suporte para extensibilidade, transpare?ncia e capacidade de integrac?a?o com
sistemas legados, projetado na arquitetura ICE, po?de ser comprovado na imple-
mentac?a?o de um proto?tipo do ambiente ICE (Cap??tulo 4). Neste proto?tipo foram
desenvolvidas as estruturas ba?sicas do Portal Web e foi provido o suporte para a fun-
cionalidade de gerenciamento de aplicac?o?es (Job Management - JM). Foi definida
uma USI para esta funcionalidade (JM-USI) e foram desenvolvidos dois provedo-
res deste servic?o (JM-SIs) e um consumidor (JM-SM). Os JM-SIs desenvolvidos
levaram em considerac?a?o a integrac?a?o dos clusters existentes no Grupo de Proces-
samento Paralelo e Distribu??do (GPPD) do Instituto de Informa?tica da UFRGS. O
cluster denominado gppd possui o OpenPBS como ferramenta de gerenciamento de
aplicac?o?es, enquanto o cluster denominado frontal-minuano possui a ferramenta
OAR para tal finalidade.

No Cap??tulo 4 foram explorados os detalhes da definic?a?o da USI desta funcionali-
dade; foi especificado um framework para dar suporte a? extensa?o da funcionalidade
de gerenciamento de aplicac?o?es para diferentes ferramentas, o qual foi denominado
Job Management Framework (JMF); foram detalhadas as implementac?o?es dos pro-
vedores deste servic?o, denominadas OpenPBS JM-SI e OAR JM-SI, e do consumidor,
denominado (JM-SM). Ale?m da descric?a?o completa dos componentes do Middleware
de Servic?os, neste cap??tulo, tambe?m foram apresentadas as estruturas que formam
o Portal Web; o modelo de informac?a?o definido para armazenar os dados e os relaci-
onamentos dos mo?dulos que compo?em o ambiente ICE; e foram ilustradas algumas
telas do proto?tipo desenvolvido com alguns dos processos de configurac?a?o e uti-
lizac?a?o, atualmente disponibilizados neste ambiente.

No intuito de comparar o ambiente ICE com alguns ambientes voltados para o
mesmo fim (gerenciamento de mu?ltiplos clusters), foram realizadas comparac?o?es
entre estes ambientes. Estas comparac?o?es levaram em considerac?a?o os desafios
para a construc?a?o de sistemas distribu??dos apresentados por Coulouris em (COU-
LOURIS; DOLLIMORE; KINDBERG, 2005). A partir desta comparac?a?o direta,
po?de-se verificar que o ambiente ICE, dentre os ambientes comparados (M3C e
HPC2N), possui o maior grau de extensibilidade, interoperabilidade e capacidade
de lidar com sistemas legados. Ale?m da comparac?a?o das caracter??sticas do ambi-
ente tambe?m verificou-se qual o overhead inserido pela infra-estrutura do ambiente
ICE nos processos de gerenciamento de aplicac?o?es. A comparac?a?o foi realizada en-
tre a execuc?a?o direta dos comandos das ferramentas OpenPBS e OAR, instalados
no cluster frontal-minuano, e a execuc?a?o das operac?o?es das respectivas imple-
mentac?o?es dos mo?dulos JM-SIs, instalados no mesmo cluster. Apesar de existir a
possibilidade de utilizac?a?o dos dois clusters do GPPD, optou-se por instalar a ferra-
menta OpenPBS tambe?m no cluster frontal-minuano para que se pudesse realizar



87

a comparac?a?o direta entre as ferramentas e as implementac?o?es JM-SI.
Conforme esperado, verificou-se a existe?ncia de um overhead, que em alguns

casos mostrou-se significativo. Analisando os resultados, pode-se dizer, de maneira
geral, que a implementac?a?o OpenPBS JM-SI apresenta um tempo de execuc?a?o tre?s
vezes maior, se comparado com a execuc?a?o via ferramenta OpenPBS diretamente.
Considerando os experimentos que utilizaram a ferramenta OAR percebeu-se um
aumento no tempo de execuc?a?o. Entretanto, este aumento na?o ocorreu somente no
tempo de execuc?a?o da implementac?a?o OAR JM-SI. A execuc?a?o direta das operac?o?es
do OAR possuem um tempo maior comparando-se com o OpenPBS. Isto se deve a?s
diferenc?as de construc?a?o e concepc?a?o dessas ferramentas. Atrave?s dos experimentos
realizados, percebeu-se que o tempo de execuc?a?o das implementac?o?es JM-SI, assim
como o overhead inserido pelo ambiente ICE, dependem muito da ferramenta de
gerenciamento de aplicac?o?es sobre a qual o mo?dulo e? constru??do. Outra conclusa?o
esta? na relac?a?o custo/benef??cio que o ambiente ICE proporciona aos seus usua?rios.
Acredita-se que, ainda, e? mais vantajoso para os usua?rios gastarem um pouco mais
de tempo utilizando o ambiente ICE do que realizando as operac?o?es diretamente
atrave?s das ferramentas, onde, apesar de economizarem alguns segundos, teriam
que lidar com as especificidades de cada ferramenta.

Enfim, atrave?s da definic?a?o do ambiente ICE, de sua arquitetura bastante mo-
dular e extens??vel e do desenvolvimento de um proto?tipo, po?de-se constatar que e?
via?vel a construc?a?o de um sistema de gerenciamento e acesso a mu?ltiplos clusters,
que seja capaz de lidar com as especificidades de diferentes clusters e de suas di-
ferentes ferramentas de maneira transparente e interopera?vel. Ale?m disso, tambe?m
verificou-se que na?o houve a necessidade de mudar a infra-estrutura ja? estabelecida
no cluster do GPPD, onde o ambiente ICE esta? em operac?a?o. Esta caracter??stica
e? bastante relevante, pois isso, mais uma vez, mostra a flexibilidade da arquitetura
proposta e da implementac?a?o realizada.

A partir da infra-estrutura ba?sica, definida e implementada nesta dissertac?a?o,
verificou-se a existe?ncia de espac?o para estender o ambiente, inserindo novas funci-
onalidades e suportando diferentes ferramentas. Sendo assim, foram desenvolvidos
alguns trabalhos em paralelo com base no trabalho descrito nesta dissertac?a?o, e
tambe?m foram identificados outros trabalhos para serem futuramente realizados. A
seguir esses trabalhos sa?o apresentados.

6.1 Trabalhos em Paralelo

Dentre os trabalhos em paralelo, que esta?o sendo realizados, citam-se os seguin-
tes:

• incorporac?a?o dos mecanismos de seguranc?a desenvolvidos no trabalho realizado
por Alexandre Ilha em (ILHA, 2005), onde um framework de seguranc?a para
o ambiente ICE foi definido com base na arquitetura proposta neste trabalho;

• incorporac?a?o da funcionalidade de monitoramento de aplicac?o?es e recursos de
clusters;

• sistema de upload de arquivos para os front-end dos clusters.

• implementac?a?o do ambiente ICE, levando em considerac?a?o o modelo MVC
(Model View Controller );



88

• porte da implementac?a?o atual para uma versa?o mais robusta utilizando tec-
nologias J2EE;

As quatro u?ltimas atividades listadas, assim como o desenvolvimento do mo?dulo
de gerenciamento de aplicac?o?es desta dissertac?a?o, foram realizadas no a?mbito do
projeto Java-WSPAD (Java - Web Services para Processamento de Alto Desempe-
nho) (PORTAL DO PROJETO JAVA-WSPAD, 2006), cujo obejtivo e? a construc?a?o
de uma plataforma para a computac?a?o distribu??da de alto desempenho, baseada em
Web Services e Peer-to-Peer.

Atrave?s do desenvolvimento destes trabalhos, os integrantes do grupo GPPD
conseguiram inserir, na plataforma ba?sica do ambiente ICE, outras funcionalidades
de maneira bastante modular. Estas novas funcionalidades contribuem para que se
consiga ter o ambiente completo de gerenciamento e acesso de mu?ltiplos clusters
almejado na proposta desta dissertac?a?o.

6.2 Trabalhos Futuros

Como trabalhos futuros, alguns podem ser citados:

• prover suporte para um nu?mero maior de ferramentas de gerenciamento de
clusters;

• prover suporte para lanc?amento de aplicac?o?es paralelas, baseadas em ou-
tras bibliotecas e ambientes paralelos, como por exemplo: LAN-MPI, PVM,
cJAVA (SILVA; LOBOSCO; AMORIM, 2003; LOBOSCO; LOQUES; AMO-
RIM, 2005), etc;

• definic?a?o de servic?os de gerenciamento administrativo de clusters, isto e?, per-
mitir que os administradores dos clusters possam realizar intervenc?o?es de con-
figurac?a?o nos clusters por eles gerenciados a partir do ambiente ICE;

Ainda como um trabalho futuro, mas na?o ligado diretamente a? implementac?a?o
do ambiente, pretende-se colocar este ambiente em um centro de pesquisa formado,
basicamente, por usua?rios que na?o sa?o nativos da a?rea de computac?a?o. O objetivo
desta avaliac?a?o e? perceber quais as dificuldades que esses usua?rios possuem ao inte-
ragir com o ambiente ICE e, atrave?s do feedback dos mesmos, melhora?-lo. Enfim,
de maneira geral, pode-se dizer que o ambiente ICE possui uma base so?lida para
ser estendido e melhorado a fim de se tornar um ambiente amplamente utilizado no
cena?rio de mu?ltiplos cluster.



89

REFERE?NCIAS

ABBAS, A. (Ed.). Grid Computing: a practical guide to technology and applica-
tions. [S.l.]: Charles River Media, 2004. 408p.

ADABALA, S.; KAPADIA, N. H.; FORTES, J. A. B. Performance and intero-
perability issues in incorporating cluster management systems within a wide-area
network-computing environment. In: ACM/IEEE CONFERENCE ON SUPER-
COMPUTING, 2000, Dallas, Texas, USA. Proceedings. . . Washington: IEEE
Computer Society, 2000. 1 CD-ROM.

ALONSO, G. et al. Web Services: conepts, architectures and applications. [S.l.]:
Springer, 2004. 354p.

ALVES, R. S.; MARQUEZAN, C. C.; GRANVILLE, L. Z. Experiences in the Im-
plementation of an SNMP-Based High Performance Cluster Management System.
In: IEEE SYMPOSIUM ON COMPUTERS AND COMMUNICATIONS, ISCC 9.,
2004, Alexandria, Egito. Proceedings. . . [S.l.]: IEEE Computer Society, 2004.

ALVES, R. S.; MARQUEZAN, C. C.; GRANVILLE, L. Z.; NAVAUX, P. O. A.
High Performance Cluster Management Based on SNMP: experiences on integra-
tion between network patterns and cluster management concepts. In: INTERNATI-
ONAL CONFERENCE ON TELECOMMUNICATIONS, ICT, 2004, Fortaleza. Te-
lecommunications and Networking - ICT, 2004: Proceedings. Berlin: Sprin-
ger, 2004. p.782–791. (Lecture Notes in Computer Science, v.3124).

ALVES, R. S.; MARQUEZAN, C. C.; GRANVILLE, L. Z.; NAVAUX, P. O. A. In-
tegrac?a?o do Gerenciamento de Clusters de Alto Desempenho com o Gerenciamento
de Redes atrave?s de Soluc?a?o baseada em SNMP. In: SIMPo?SIO BRASILEIRO DE
REDES DE COMPUTADORES, SBRC, 23., 2005, Fortaleza, Brasil. Anais. . . For-
taleza: Sociedade Brasileira de Computac?a?o, 2005. p.3–16.

ANDRADE, N. et al. OurGrid: an approach to easily assemble grids with equitable
resource sharing. In: JOB SCHEDULING STRATEGIES FOR PARALLEL PRO-
CESSING, 2003. Proceedings. . . Berlin: Springer, 2003. p.61–86. (Lecture Notes
in Computer Science, v.2862).

ANDRONICO, G. et al. The GENIUS web portal: grid computing made easy. In:
INTERNATIONAL SYMPOSIUM ON INFORMATION TECHNOLOGY. ITCC,
2003. Proceedings. . . [S.l.]: IEEE Computer Society, 2003. p.425–431.



90

APACHE AXIS2, A. W. to. Dispon??vel em: ?http://ws.apache.org/axis2/?.
Acesso em: nov. 2005.

APACHE TOMCAT - The Apache Software Foundation. Dispon??vel em: ?http:
//tomcat.apache.org/?. Acesso em: maio 2006.

AUMAGE, O. Heterogeneous Multi-Cluster Networking with the Madeleine III
Communication Library. In: INTERNATIONAL PARALLEL AND DISTRIBU-
TED PROCESSING SYMPOSIUM, IPDPS, 2002. Proceedings. . . [S.l.]:IEEE
Computer Society, 2002.

BAKER, M. Cluster Computing White Paper. Dispon??vel em: ?http://dsg.
port.ac.uk/?mab/Links/tfcc/WhitePaper/final-paper.pdf?. Acesso em: dez.
2005.

BAKER, M. et al. Emerging Grid Standards. IEEE Computer, [S.l.], v.38, n.4,
p.43–50, Apr. 2005.

BAKER, M.; SMITH, G. GridRM: an extensible resource monitoring system.
In: IEEE INTERNATIONAL CONFERENCE ON CLUSTER COMPUTING,
2003, Tsim Sha Tsui, Kowloon, Hong Kong. Proceedings. . . [S.l.: s.n.], 2003.
p.207– 214. Dispon??vel em: ?http://ieeexplore.ieee.org/iel5/8878/28041/
01253317.pdf?tp=&amp;amp;arnumber=1253317&amp;amp;isnumber=28041?. Acesso em: dez. 2005.

BARRETO, M.; A?VILA, R.; NAVAUX, P. The MultiCluster Model to the Integra-
ted Use of Multiple Workstation Clusters. In: WORKSHOP ON PERSONAL COM-
PUTER BASED NETWORKS OF WORKSTATIONS, 3., 2000, Cancun. Procee-
dings. . . Berlin: Springer-Verlag, 2000. p.71–80. (Lecture Notes in Computer Sci-
ence, v.1800).

BARRETO, M.; A?VILA, R.; OLIVEIRA, F. de; CASSALI, R.; NAVAUX,
P. DECK: an enviroment for parallel programming on clusters of multipro-
cessors. In: SYMPOSIUM ON COMPUTER ARCHITECTURE AND HIGH-
PERFORMANCE COMPUTING - SBAC-PAD, 12., 2000, Sa?o Pedro, SP. Pro-
ceedings. . . Sa?o Carlos: UFSCAR, 2000. p.321–329.

BENEDYCZAK, K. et al. UNICORE as Uniform Grid Environment for Life Sci-
ences. In: EUROPEAN GRID CONFERENCE, ADVANCES IN GRID COMPU-
TING - EGC, 2005, Amsterdam, Netherlands. Proceedings. . . [S.l.]: Springer,
2005. p.364–373. (Lecture Notes in Computer Science, v.3470).

BRIM, M. et al. M3C: managing and monitoring multiple clusters. In: INTERNATI-
ONAL SYMPOSIUM ON CLUSTER COMPUTING AND THE GRID, CCGRID,
2001, Brisbane, Australia. Proceedings. . . [S.l.]: IEEE Computer Society, 2001.
p.386–393.

BRONEVETSKY, G. et al. Application-level checkpointing for shared memory
programs. In: INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUP-
PORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS, 11.,
2004, Boston, MA, USA. Proceedings. . . New York: ACM Press, 2004. p.235–247.



91

BRUNO, G. et al. Rolls: modifying a standard system installer to sup-
port user-customizable cluster frontend appliances. In: IEEE INTERNA-
TIONAL CONFERENCE ON CLUSTER COMPUTING, 2004, San Diego,
California, USA. Proceedings. . . [S.l.: s.n.], 2004. p.421–430. Dispon??vel
em: ?http://www.rocksclusters.org/rocks-documentation/4.1/papers/
cluster2004-roll.pdf?. Acesso em: dez. 2005.

BUYYA, R. (Ed.). High Performance Cluster Computing: architectures and
systems. Upper Saddle River: Prentice Hall PTR, 1999. 849p.

CAPIT, N. et al. Expe?riences autour d’une nouvelle approche de conception
d’un gestionnaire de travaux pour grappe. [S.l.]: HAL - CCSd - CNRS, 2003.

CAPIT, N. et al. A batch scheduler with high level components. In: INTERNA-
TIONAL SYMPOSIUM ON CLUSTER COMPUTING AND GRID, CCGRID, 5.,
2005. Proceedings. . . [S.l.: s.n.], 2005.

CASE, J. et al. A Simple Network Management Protocol (SNMP): IETF
RFC 1098. Dispon??vel em: ?http://www.ietf.org/rfc/rfc1098.txt?number=
1098?. Acesso em: dez. 2005.

CERA, M. C.; ROSA RIGHI, R. da; PASIN, M. Alocac?a?o Dina?mica e Transparente
de Computadores Ociosos em Java. In: WORKSHOP EM SISTEMAS COMPU-
TACIONAIS DE ALTO DESEMPENHO, WSCAD, 6., 2005, Rio de Janeiro, RJ.
Anais. . . Rio de Janeiro: Sociedade Brasileira de Computac?a?o, 2005.

CERAMI, E. Web Services Essentials. [S.l.]: O’Reilly &amp;amp; Associates, 2002.

COOKE, A. et al. R-GMA An Information Integration System for Grid Monito-
ring. In: INTERNATIONAL CONFERENCE ON COOPERATIVE INFORMA-
TION SYSTEMS, COOPIS, 11., 2003, Catania, Sicily, Italy. Proceedings. . .
[S.l.: s.n.], 2003. Dispon??vel em: ?http://scholar.google.com/url?sa=U&amp;amp;q=http:
//www.macs.hw.ac.uk/dis4g/publications/coopis.pdf?. Acesso em: dez. 2005.

COULOURIS, G.; DOLLIMORE, J.; KINDBERG, T. (Ed.). Distributed Sys-
tems - Concepts and Design. Fourth Edition. [S.l.]: Addison Wesley, 2005.
832p.

DAHAN, M. et al. Grid Portal Toolkit 3.0 (GridPort). In: INTERNATIONAL
SYMPOSIUM ON HIGH-PERFORMANCE DISTRIBUTED COMPUTING, 13.,
HPDC, 2004, Honolulu, Hawaii, USA. Proceedings. . . [S.l.]: IEEE Computer So-
ciety, 2004. p.272–273.

DONGARRA, J. et al. An Introduction to the MPI Standard. Knoxville, USA:
University of Tennessee, 1995. (Technical report, CS-95-274).

ELMROTH, E.; NYLE?N, M.; OSCARSSON, R. A User-Centric Cluster and Grid
Computing Portal. In: ICPP WORKSHOPS, 2005. Proceedings. . . [S.l.]:IEEE
Computer Society, 2005. p.103–110.

FLANERY, R. et al. Cluster command and control (CS) tools suite. In: KACSUK,
P.; KOTSIS, G. (Ed.). Distributed and parallel systems: from instruction pa-
rallelism to cluster computing. Norwell, MA, USA: [S.l.]: Springer, 2000. p.205–214.



92

FOSTER, I. et al. The Physiology of the Grid: an open grid services architecture
for distributed systems integration. [S.l.:s.n.], 2002.

FOSTER, I.; KESSELMAN, C. Globus: a metacomputing infrastructure toolkit.
The International Journal of Supercomputer Applications and High Per-
formance Computing, [S.l.], v.11, n.2, p.115–128, Summer 1997.

FOSTER, I.; KESSELMAN, C.; TUECKE, S. The Anatomy of the Grid: enabling
scalable virtual organizations. International Journal of High Performance
Computing Applications, Thousand Oaks, CA, USA, v.15, n.3, p.200–222, 2001.

GENTZSCH, W. Sun Grid Engine: towards creating a compute power grid. In: IE-
EE/ACM INTERNATIONAL SYMPOSIUM ON CLUSTER COMPUTING AND
THE GRID, 1., 2001, Brisbane, Australia. Proceedings. . . [S.l.]: IEEE Computer
Society, 2001. p.35–36.

GLOBAL Grid Forum. Dispon??vel em: ?http://www.gridforum.org/?. Acesso em:
nov. 2005.

GLOBUS - Welcome to the Globus Toolkit Homepage. Dispon??vel em: ?http://
www.globus.org/toolkit/?. Acesso em: nov. 2005.

GLOBUS Toolkit 4.0 Release Manuals. Dispon??vel em: ?http://www.globus.org/
toolkit/docs/4.0/key/?. Acesso em: nov. 2005.

GONZ, F. J. et al. Condor grid computing from mobile handheld devices. SIGMO-
BILE Mob. Comput. Commun. Rev., New York, NY, USA, v.6, n.2, p.18–27,
2002.

GRANVILLE, L.; TAROUCO, L. QAME - QoS-aware management environ-
ment. In: COMPUTER SOFTWARE AND APPLICATIONS CONFERENCE,
COMPSAC, 25., 2001, Chicago, Illinois, USA. Proceedings. . . [S.l.: s.n.], 2001.
p.269–274. Dispon??vel em: ?http://ieeexplore.ieee.org/iel5/7609/20754/
00960627.pdf?tp=&amp;amp;arnumber=960627&amp;amp;isnumber=20754?. Acesso em: dez. 2005.

GRIMES, R. Professional DCOM Programming. [S.l.]: Birmingham, UK:
Wrox, 1997.

GRIMSHAW, A. S.; NATRAJAN, A. Legion: lessons learned building a grid ope-
rating system. Proceedings of the IEEE, [S.l.], v.93, n.3, p.589–603, 2005.

GT 4.0 WS GRAM. Dispon??vel em: ?http://www-unix.globus.org/toolkit/
docs/4.0/execution/wsgram/?. Acesso em: dez. 2005.

HADDAD, I. F. PVFS: a parallel virtual file system for linux clusters. Linux Jour-
nal, Seattle, WA, USA, v.2000, n.80es, p.5, 2000.

HANSEN, S.; FOSSUM, T. V. Refactoring model-view-controller. Journal of
Computing Sciences in Colleges, USA, v.21, n.1, p.120–129, 2005.

ILHA, A. S. Uso de Web Services no Controle de Acesso a Clusters.
2005. Trabalho de Conclusa?o (Cie?ncia da Computac?a?o) - Instituto de Informa?tica,
UFRGS, Porto Alegre.



93

JAVA Platform, Standard Edition (Java SE). Dispon??vel em: ?http://java.sun.
com/javase/index.jsp?. Acesso em: maio 2006.

JAVA SERVLET Technology. Dispon??vel em: ?http://java.sun.com/products/
servlet/?. Acesso em: maio 2006.

JAVASERVER Pages Technology. Dispon??vel em: ?http://java.sun.com/
products/jsp/?. Acesso em: maio 2006.

KASSICK, R.; MACHADO, C.; HERMANN, E.; A?VILA, R.; NAVAUX, P.; DEN-
NEULIN., Y. Evaluating the performance of the dNFSP file system. In: IEE-
E/ACM INTERNATIONAL SYMPOSIUM ON CLUSTER COMPUTING AND
THE GRID, CCGRID, 5., 2005, Cardiff, UK. Proceedings. . . [S.l.: s.n.], 2005.

KELLER, A.; REINEFELD, A. CCS resource management in networked HPC sys-
tems. In: HETEROGENEOUS COMPUTING WORKSHOP, HCW, 7., 1998, Or-
lando, Florida. Proceedings. . . [S.l.: s.n.], 1998. p.44–56.

LI, M.; BAKER, M. The Grid Core Technologies. [S.l.]: John Wiley &amp;amp; Sons,
2005. 423p.

LI, M. et al. PortalLab: a web services toolkit for building semantic grid portals.
In: INTERNATIONAL SYMPOSIUM ON CLUSTER COMPUTING AND THE
GRID, 3., CCGRID, 2003, Tokyo, Japan. Proceedings. . . [S.l.]: IEEE Computer
Society, 2003. p.190–197.

LIGNERIS, B. des et al. Open Source Cluster Application Resources (OS-
CAR): design, implementation and interest for the Computer scientific community.
White paper. Dispon??vel em: ?http://www.ncassr.org/projects/cluster-sec/
papers/oscar03.pdf?. Acesso em: dez. 2005.

LOBOSCO, M.; LOQUES, O.; AMORIM, C. L. de. Reducing Memory Sharing
Overheads in Distributed JVMs. In: INTERNATIONAL CONFERENCE HIGH
PERFORMANCE COMPUTING AND COMMUNICATIONS, HPCC, 1., 2005,
Sorrento, Italy. Proceedings. . . [S.l.]: Springer, 2005. p.629–639. (Lecture Notes
in Computer Science, v.3726).

MARQUEZAN, C. C.; ROSA RIGHI, R. da; SCHNORR, L.; CARISSIMI, A.;
MAILLARD, N.; NAVAUX, P. O. A. ICE: a service oriented approach to uniform
the access and management of cluster environments. In: IEEE INTERNATIONAL
SYMPOSIUM ON CLUSTER COMPUTING AND THE GRID, CCGRID, 6., 2006,
Singapore. Proceedings. . . [S.l.]:IEEE Computer Society, 2006. 1 CD-ROM.

MAUI Scheduler Open Cluster Software. Dispon??vel em: ?http://mauischeduler.
sourceforge.net/?. Acesso em: dez. 2005.

MCGOVERN, J. et al. Java Web Services Architecture. [S.l.]: Morgan Kauf-
mann, 2003. 833p.

NAKADA, H. et al. The design and implementation of a fault-tolerant RPC
system: ninf-c. In: INTERNATIONAL CONFERENCE ON HIGH PERFOR-
MANCE COMPUTING AND GRID IN ASIA PACIFIC REGION, 7., 2004,



94

Omiya Sonic City, Tokyo Area, Japan. Proceedings. . . [S.l.: s.n.], 2004. p.9–
18. Dispon??vel em: ?http://ieeexplore.ieee.org/iel5/9244/29307/01324011.
pdf?tp=&amp;amp;arnumber=1324011&amp;amp;isnumber=29307?. Acesso em: dez. 2005.

NOVOTNY, J.; RUSSELL, M.; WEHRENS, O. GridSphere: an advanced portal
framework. In: EUROMICRO CONFERENCE, 30., 2004, Rennes, France. Proce-
edings. . . [S.l.]: IEEE Computer Society, 2004. p.412–419.

OASIS - Advancing E-Business Standards Since 1993. Dispon??vel em: ?http://www.
oasis-open.org/home/index.php?. Acesso em: nov. 2005.

OASIS-WS - OASIS Committees by Category: web services. Dispon??vel em: ?http:
//www.oasis-open.org/committees/tc cat.php?cat=ws?. Acesso em: nov. 2005.

OPENSCE - Scalable Cluster Environment. Dispon??vel em: ?http://opensce.org?.
Acesso em: dez. 2005.

OSCAR - Open Source Cluster Application Resources. Dispon??vel em: ?http://
oscar.openclustergroup.org/?. Acesso em: dez. 2005.

OU, L.; HE, X. Design and Evaluation of a High Performance Parallel File System.
In: ANNUAL IEEE CONFERENCE ON LOCAL COMPUTER NETWORKS, 30.,
LCN, 2005, Sydney, Australia. Proceedings. . . [S.l.]: IEEE Computer Society,
2005. p.100–105.

PAPADOPOULOS, P. M.; KATZ, M. J.; BRUNO, G. NPACI Rocks Clusters: tools
for easily deploying and maintaining manageable high-performance linux clusters.
In: RECENT ADVANCES IN PARALLEL VIRTUAL MACHINE AND MESSAGE
PASSING INTERFACE, PVM/MPI, 8., 2001, Santorini/Thera, Greece. Proce-
edings. . . [S.l.]: Springer, 2001. p.10–11. (Lecture Notes in Computer Science,
v.2131).

PARK, I. et al. Towards an integrated, web-executable parallel programming tool
environment. In: ACM/IEEE CONFERENCE ON SUPERCOMPUTING, 2000,
Dallas, Texas, USA. Proceedings. . . Washington: IEEE Computer Society, 2000.
1 CD-ROM.

PORTABLE Batch System. Dispon??vel em: ?http://www.openpbs.org/?. Acesso
em: nov. 2005.

PORTAL do Projeto Java-WSPad. Dispon??vel em: ?http://www.lcp.coppe.ufrj.
br:9673/JavaWSPad?. Acesso em: jul. 2006.

POSTGRESQL Global Development Group. Dispon??vel em: ?http://www.
postgresql.org/?. Acesso em: maio 2006.

RED HAT - The Open Source Leader. Dispon??vel em: ?http://www.redhat.com/?.
Acesso em: dez. 2005.

RMI - Java Remote Method Invocation. Dispon??vel em: ?http://java.sun.com/
j2se/1.4.2/docs/guide/rmi/spec/rmiTOC.html?. Acesso em: dez. 2005.



95

ROCKS Cluster Distribution: users guide. Dispon??vel em: ?http:
//www.rocksclusters.org/rocks-documentation/4.1/introduction.html?.
Acesso em: dez. 2005.

SACERDOTI, F. D. et al. Wide Area Cluster Monitoring with Ganglia. In: IEEE
INTERNATIONAL CONFERENCE ON CLUSTER COMPUTING, CLUSTER,
2003, Hong Kong. Proceedings. . . [S.l.: s.n.], 2003. p.289–298.

SCHULZ, M. et al. Implementation and Evaluation of a Scalable Application-Level
Checkpoint-Recovery Scheme for MPI Programs. In: ACM/IEEE CONFERENCE
ON SUPERCOMPUTING, 2004, Washington, DC, USA. Proceedings. . . [S.l.]:
IEEE Computer Society, 2004. 1 CD-ROM.

SIEGEL, J. CORBA Fundamentals and Programming. [S.l.]: John Wiley &amp;amp;
Sons, 1996.

SILVA, A. F. da; LOBOSCO, M.; AMORIM, C. L. de. An Evaluation of cJava
System Architecture. In: SYMPOSIUM ON COMPUTER ARCHITECTURE AND
HIGH PERFORMANCE COMPUTING, SBAC-PAD, 15., 2003, Sa?o Paulo, SP, BR.
Proceedings. . . [S.l.]: IEEE Computer Society, 2003. p.91–99.

SOTTILE, M. J.; MINNICH, R. G. Supermon: a high-speed cluster monitoring
system. In: IEEE INTERNATIONAL CONFERENCE ON CLUSTER COMPU-
TING, 2002, Chicago, Illinois. Proceedings. . . [S.l.]: IEEE Computer Society,
2002. p.39–46.

STERLING, T. An Introduction to PC Clusters for High Performance
Computing. Cap??tulo 1 do white-paper entitulado Cluster Computing White Pa-
per, editado por Mark Baker. Dispon??vel em: ?http://dsg.port.ac.uk/?mab/
Links/tfcc/WhitePaper/final-paper.pdf?. Acesso em: dez. 2005.

SUNDERAM, V. PVM: a framework for parallel distributed computing. Concur-
rency: Practice and Experience, Chichester, UK, v.2, n.4, p.315–339, 1990.

SUZUMURA, T. et al. GridSpeed: a web-based grid portal generation server.
In: INTERNATIONAL CONFERENCE ON HIGH PERFORMANCE COM-
PUTING AND GRID IN ASIA PACIFIC REGION, 7., 2004, Omiya So-
nic City, Tokyo Area, Japan. Proceedings. . . [S.l.: s.n.], 2004. p.26–33. Dis-
pon??vel em: ?http://ieeexplore.ieee.org/iel5/9244/29307/01324013.pdf?
tp=&amp;amp;arnumber=1324013&amp;amp;isnumber=29307?. Acesso em: dez. 2005.

SYSTEM Imager. Dispon??vel em: ?http://www.systemimager.org/?. Acesso em:
dez. 2005.

THAIN, D.; LIVNY, M. Building Reliable Clients and Servers. In: FOSTER, I.;
KESSELMAN, C. (Ed.). The Grid: blueprint for a new computing infrastructure.
[S.l.]: Morgan Kaufmann, 2003.

THAIN, D.; TANNENBAUM, T.; LIVYN, L. Condor and the Grid. Cap??tulo 11
do livro entitulado G?rid Computing - Making the Global Infrastructure a Reality.?.
Editado por F. Berman, A. Hey e G. Fox. Publicado por John Wiley &amp;amp; Sons. Dis-
pon??vel em: ?http://scholar.google.com/url?sa=U&amp;amp;q=http://searchoracle.



96

techtarget.com/searchOracle/downloads/Grid Computing Chap11.pdf?.
Acesso em: dez. 2005.

THE GLOBUS Alliance. Dispon??vel em: ?http://www.globus.org/alliance/?.
Acesso em: nov. 2005.

THOMAS, M. et al. The GridPort Toolkit: a system for building grid portals. In:
INTERNATIONAL SYMPOSIUM ON HIGH PERFORMANCE DISTRIBUTED
COMPUTING, HPDC, 10., 2001, San Francisco, CA, USA. Proceedings. . . [S.l.]:
IEEE Computer Society, 2001. p.216–227.

TIERNEY, B. et al. A Grid Monitoring Architecture. White paper. Dispon??vel
em: ?http://www-didc.lbl.gov/GGF-PERF/GMA-WG/papers/GWD-GP-16-3.pdf?.
Acesso em: dez. 2005.

TOP500 Supercomputer Sites. Dispon??vel em: ?http://www.top500.org/?. Acesso
em: dez. 2005.

UML - Unified Modeling Language. Dispon??vel em: ?http://www.uml.org/?.
Acesso em: maio 2006.

UTHAYOPAS, P.; ANGSKUN, T.; MANEESILP, J. SCE: a fully integrated soft-
ware tool for beowulf cluster system. In: LINUX CLUSTER: THE HPC REVO-
LUTION, A CONFERENCE FOR HIGH-PERFORMANCE LINUX CLUSTER
USERS AND SYSTEM ADMINISTRATORS, 2001, Ubana,Illinois, USA. Procee-
dings. . . [S.l.: s.n.], 2001.

UTHAYOPAS, P.; ANGSKUN, T.; MANEESILP, J. On the Building of the
Next Generation Integrated Environment for Beowulf Clusters. In: INTERNATIO-
NAL SYMPOSIUM ON PARALLEL ARCHITECTURES, ALGORITHMS AND
NETWORKS, I-SPAN, 2002, Makati City, Metro Manila, Philippines. Procee-
dings. . . [S.l.: s.n.], 2002. p.159–164.

UTHAYOPAS, P.; PHATANAPHEROM, S. Fast and Scalable Real-Time Monito-
ring System for Beowulf Clusters. In: EUROPEAN PVM/MPI USER’S GROUP
MEETING ON RECENT ADVANCES IN PARALLEL VIRTUAL MACHINE
AND MESSAGE PASSING INTERFACE, PVM/MPI, 8., 2001. Proceedings. . .
London: Springer-Verlag, 2001. p.201–208. (Lecture Notes in Computer Science,
v.2131).

VALLEE, G. et al. SSI-OSCAR: a cluster distribution for high performance compu-
ting using a single system image. In: INTERNATIONAL SYMPOSIUM ON HIGH
PERFORMANCE COMPUTING SYSTEMS AND APPLICATIONS, HPCS, 19.,
2005, Guelph,Ontario, Canada?. Proceedings. . . [S.l.: s.n.], 2005. p.319–325.

W3C Architecture Domain - Web Services Activity. Dispon??vel em: ?http://www.
w3.org/2002/ws/?. Acesso em: nov. 2005.

W3C World Wide Web - Leading the Web to Its Full Potential... Dispon??vel em:
?http://www.w3.org/?. Acesso em: nov. 2005.

W3C-XML BINARY - XML Binary Characterization Working Group Public Page.
Dispon??vel em: ?http://www.w3.org/XML/Binary/?. Acesso em: dez. 2005.



97

WELCH, V. et al. Security for Grid Services. In: IEEE INTERNATIONAL SYM-
POSIUM ON HIGH PERFORMANCE DISTRIBUTED COMPUTING, 12., 2003,
Seatle, Washington. Proceedings. . . [S.l.: s.n.], 2003. p.48–57.

ZOU, H. et al. HRIC: hybrid resource information service architecture based on gma.
In: IEEE INTERNATIONAL CONFERENCE ON E-BUSINESS ENGINEERING,
ICEBE, 2005, Beijing, China. Proceedings. . . [S.l.: s.n.], 2005. p.541–544.

A?VILA, R.; NAVAUX, P.; LOMBARD, P.; LEBRE, A.; DENNEULIN, Y. Per-
formance evaluation of a prototype distributed NFS server. In: SYMPOSIUM ON
COMPUTER ARCHITECTURE AND HIGH PERFORMANCE COMPUTING,
SBAC-PAD, 16., 2004, Foz do Iguac?u, PR, BR. Proceedings. . . [S.l.]: IEEE Com-
puter Society, 2004. p.100–105.


</field>
	</doc>
</add>