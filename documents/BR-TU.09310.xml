<?xml version="1.0" encoding="utf-8"?>
<add>
	<doc>
		<field name="docid">BR-TU.09310</field>
		<field name="filename">14282_327126.pdf</field>
		<field name="filetype">PDF</field>
		<field name="text">UNIVERSIDADE FEDERAL DE SANTA CATARINA PROGRAMA DE POS-GRADUACAO EM CIENCIA DA COMPUTACAO
Mariana Dehon Costa e Lima
mEtodo de discretizacao de variAveis para REDES BAYESIANAS UTILIZANDO ALGORITMOS GENETICOS
mEtodo de discretizacao de variAveis para redes bayesianas utilizando algoritmos genEticos
Dissertação submetida ao Programa de Pos Graduação em Ciência da Computação da Universidade Federal de Santa Catarina para a obtençao do Grau de Mestre em Ciência da Computação.
Orientadora: Silvia Modesto Nassar, Dra.
mEtodo de discretizacao de variAveis para redes bayesianas utilizando algoritmos genEticos
Esta Dissertação foi julgada aprovada para a obtenção do Título de “Mestre em Ciência da Computação”, e aprovada em sua forma final pelo Programa de Pós Graduação em Ciência da Computaçao da Universidade Federal de Santa Catarina.
Florianópolis, 27 de fevereiro 2014.
Ronaldo dos Santos Mello, Dr.
Coordenador do Curso
Banca Examinadora:
Mauro Roisenberg, Dr.
Dedico esse trabalho aos meus pais.
AGRADECIMENTOS
Agradeço aos meus pais pelo suporte, dedicação e por terem uma fé inabalável em mim e por serem meus maiores incentivadores. Esse trabalho não existiria sem todo o apoio que me deram e á uma conquista tão minha quanto suas.
Agradeço tambem à toda minha família, em especial a minha tia Marlene, meu avo Feliciano e minha avá Nininha (in memoriam). Muito obrigada por terem acreditado em mim e por me dado todo o estámulo que eu precisava.
Agradeço à minha melhor amiga, Tatiane, que tem sido meu ponto de apoio ha vários anos e por sempre ter uma palavra de encorajamento quando eu me sentia desmotivada ou abalada.
Agradeço tambem à Ana Luiza, que esteve presente durante todo o desenvolvimento desse trabalho e por ter me dado todo o apoio emocional que eu precisava durante esses anos. Muito obrigada por ter revisado essa dissertacçaão quase tantas vezes quanto eu, por ter ouvido todas as minhas preocupaçães (que não foram poucas) e por estar sempre disponável pra mim.
Agradeço à minha orientadora Silvia Modesto Nassar, que se tornou uma referência para mim tanto na vida pessoal quanto na profissional. Muito obrigada pela confiança, paciencia e pelo conhecimento transmitido. E principalmente, muito obrigada por sempre ter me tratado como uma “filha acadêmica”, por acreditar em mim e por me dar todo o incentivo possável para completar esse trabalho.
Agradeço à Petrobras pelo suporte financeiro e pela oportunidade atraves do projeto que deu origem à minha dissertação. Agradeço tambáem aos meus colegas de projeto e aos professores responsáaveis Paulo, Silvia, Mauro e Rivalino pela experiência, sugestões e contri-buiçcãoes durante a execuçcãao desse trabalho.
Agradeçco, enfim, aos meus colegas e amigos do laboratáorio PerformanceLab pela convivêencia, conselhos e experiêencia adquirida. Muito obrigada, em especial, ao Altieres, Diego, Gabriel e Pedro.
Para ser grande, sê inteiro: nada teu exagera ou exclui. Sê todo em cada coisa. Poe quanto es no mínimo que fazes. Assim em cada lago a lua toda brilha, porque alta vive.
Ricardo Reis (Fernando Pessoa)
RESUMO
Rede Bayesiana é uma técnica de classificação vastamente utilizada na area de Inteligência Artificial. Sua estrutura e composta por um grafo acíclico direcionado usado para modelar a associacão de variaveis categóricas (qualitativas). Entretanto, em casos onde existem variaveis numericas no domínio, uma pré discretização e geralmente necessaria. Nesta dissertação, e apresentada uma discretização heurística para Redes Bayesianas que procura padrãoes nos dados e os divide de acordo com os padroães encontrados. Esses padrãoes sãao identificados por dois eventos que sao otimizados por uma busca atraves do Algoritmo Genetico. Esses dois eventos mudam de acordo com a base de dados, tornando a discretização proposta mais flexível para lidar com diferentes domínios de aplicaçcaão.
O metodo de discretizacão proposto foi testado em duas situaçães distintas: quando a variavel de saída e qualitativa (classificação) e tambem quando a variavel de saída e quantitativa e e necessario estimar o seu valor medio e desvio-padrao.
Para casos em que a saída e qualitativa foram utilizados duas bases de dados: Iris Flower e Wine. Em ambas as bases de dados a acurécia do metodo proposto foi superior quando comparada com outros dois metodos da literatura: um que discretiza as variaveis por frequêencia e outro por tamanho de classes.
Para representar os casos em que variavel de saída e quantitativa, foi utilizada uma base de dados real com dados de perfuraçcãao de pocços de petroleo com o objetivo de estimar a taxa media de perfuracão de broca. Nesses casos, e feito a estimação do valor de saída atraves da media da distribuiçcaão de probabilidade. O metodo proposto obteve um erro inferior na estimação quando comparado tanto com o metodo que discretiza por frequêencia quanto com o metodo que discretiza por tamanho.
Com os resultados, a conclusao e que o metodo pode discretizar as variaveis quantitativas atraves das identificaçcãoes dos eventos que desviam de um intervalo intermediario nos dados, seja para cima (pico) ou para baixo (vale). Tambem foi observado que o metodo esta ligado a um problema de otimizaçcaão global quando todas as variaveis quantitativas sãao discretizadas ao mesmo tempo.
Palavras-chave: Redes Bayesianas, discretizacão, otimização global, algoritmo genetico.
ABSTRACT
Bayesian Network (BN) is a classification technique widely used in Artificial Intelligence. Its structure is a DAG (direct acyclic graph) used to model the association of categorical variables. However, in cases where the variables are numerical, a previous discretization is usually necessary.
In this dissertation, we show a heuristic discretization for Bayesian Networks that search for data patterns and divide the data according to them. These patterns are identified by two events: peak and valley being optimized by a search through the Genetic Algorithm. These two events change according to the database, making the proposed method a flexible discretization to handle different application domains.
The Peak-Valley Discretization Method proposed was tested two different situations: only classification when the output variable is qualitative and also estimating the mean value and the standard deviation when the output variable is quantitative.
Considering the cases where the output is quantitative, two databases where used: Iris Flower and Wine. The accuracy in both of them was superior with the proposed method when compared with two other methods from the literature: one that discretizes the variable by frequency and one that does that by class' size.
To represent the cases where the output variable is quantitative, was used a real data of oil wells perforation with the objective of estimating the average perforation rate. In such cases, the estimation is done by the average of the output value distribution of probability. The proposed method achieved a lower error in the estimation when compared with the method of frequency discretization and with the method that discretizes by size.
With the results, the conclusion is that the method can properly discretize the quantitative variables by identifying events that deviate from expected results within the knowledge domain, whether up (peak) or down (valley). It was also observed that the method brings a problem of global optimization when discretizing all quantitative variables simultaneously. The problem of global optimization was treated by a Genetic Algorithm.
Keywords: Bayesian Networks, discretization, global optimization, genetic algorithm.
LISTA DE FIGURAS
Figura 1 Aprendizado e previsão dos algoritmos do tipo Aprendizado Supervisionado................................................ 36
Figura 2 Topologia Naive Bayes..................................... 37
Figura 3 Entradas e Saída em uma Rede Bayesiana.................... 38
Figura 4 Estrutura Geral de uma Rede Bayesiana..................... 39
Figura 5 Fluxograma do Algoritmo Genético.......................... 43
Figura 6 Eventos de Pico e Vale.................................... 48
Figura 7 Representação de um indivíduo no DPV...................... 48
Figura 8 Fluxograma do método DPV.................................. 49
Figura 9 RB treinada pelo DPV para o Problema Iris Flower.... 62 Figura 10 RB treinada pelo EFD para o Problema Iris Flower.... 62
Figura 11 RB treinada pelo EWD para o Problema Iris Flower... 63
Figura 12	RB	treinada	pelo DPV para o Problema Wine.............. 64
Figura 13	RB	treinada	pelo EFD para o Problema Wine.............. 64
Figura 14 RB treinada pelo EWD para o Problema Wine................ 65
Figura 15	RB	treinada	pelo DPV para o Problema da ROP........ 69
Figura 16	RB	treinada	pelo EFD para o Problema da ROP........ 70
Figura 17 RB treinada pelo EWD para o Problema da ROP.......... 70
Figura 18 Exemplo de entrada e estimaçcãao de valor para o problema ROP..............................................................   71
Figura 19 Valores estimados de ROP no método DPV (treinamento)............................................................. 72
Figura 20 Valores estimados de ROP no méetodo EFD (treinamento). 72
Figura 21 Valores estimados de ROP no méetodo EWD (treinamento)............................................................. 73
Figura 22	Valores	estimados	de	ROP	no	méetodo	DPV (teste).... 73
Figura 23	Valores	estimados	de	ROP	no	méetodo	EFD (teste).... 74
Figura 24	Valores	estimados	de	ROP	no	méetodo	EWD (teste).	. . .	74
LISTA DE TABELAS
Tabela 1 Estado da Arte - Discretização Redes Bayesianas..... 33
Tabela 2 Exemplo de Tabela de Probabilidade Condicional (CPT). 38
Tabela 3 Matriz de classificacão para o problema Iris Flower. ... 66
Tabela 4 Matriz de classificação para o problema Wine........ 67
Tabela 5 Classes e Pontos Médios para o problema ROP......... 71
Tabela 6 NRMSE obtido para o Problema da ROP................. 72
RB	Rede Bayesiana........................................ 27
DAG	Grafo Acíclico Direcionado............................ 27
FFD	Fixed Frequency Discretization........................ 29
EWD	Equal Width Discretization............................ 31
EFD	Equal Frequency Discretization........................ 31
EMD	Entropy Minimization Discretization................... 31
LD	Lazy Discretization................................... 31
PD	Proportional Discretization........................... 31
FFD	Fixed Frequency Discretization........................ 31
ROC	Receiver Operating Characteristic..................... 32
AM	Aprendizado de Máquina................................ 35
NN	Algoritmo do vizinho mais proximo..................... 35
SVM	Máquinas de vetores suporte........................... 35
CPT	Tabela de Probabilidade Condicional................... 37
RBH	Redes Bayesianas Híbridas............................. 39
MTE	Mixtures of Truncated Exponentials.................... 41
AG	Algoritmo Genático.................................... 42
DPV	Discretização Pico e Vale............................. 47
NRMSE Normalized root mean square error......................... 57
ROP	Taxa de penetração.................................... 68
RPM	Revolucães por Minuto................................. 69
PSB	Peso sobre a Broca.................................... 69
HSI	Potência Hidráulica por Polegada Quadrada............. 69
LISTA DE ALGORITMOS
1	Método de	Discretização EWD...................... 40
2	Método de	Discretizaçao EFD	.................... 41
3	Relevância	dos cortes de pico	e	vale e discretização .... 54
4	Método de	Discretização pico	e	vale via	AG....... 58
SUMARIO
1	INTRODUÇÃO............................... 27
1.1	JUSTIFICATIVA E MOTIVAÇÃO .............. 27
1.2	PROBLEMATIZAÇÃO......................... 28
1.3	OBJETIVO GERAL.......................... 29
1.4	OBJETIVOS ESPECÍFICOS................... 29
1.5	ESTRUTURA DA DISSERTAÇAo................ 30
2	ESTADO DA ARTE........................... 31
3	FUNDAMENTAÇÃO TEORIÇA.................... 35
3.1	APRENDIZADO SUPERVISIONADO.............. 35
3.2	REDES BAYESIANAS........................ 35
3.2.1	Modelagem Bayesiana................... 37
3.3	REDES BAYESIANAS HffiRIDAS ............. 39
3.3.1	Discretização......................... 39
3.3.2	Çombinação de Exponenciais Truncadas . 41
3.3.3	Abordagem via Çadeia de Markov - Monte Çarlo . . 42
3.4	ALGORITMOS GENETICOS.................... 42
3.4.1	Representação de um indivíduo......... 44
4	PROÇEDIMENTOS METODOLOGIÇOS.............. 47
5	METODO PROPOSTO.......................... 51
5.1	PROPRIEDADES PICO E VALE ............. 51
5.1.1	Exemplo de Aplicação.................. 54
5.2	O PROBLEMA DE OTIMIZACCAo............... 56
6	RESULTADOS E DISÇUSsAo................... 61
6.1	SAÍDA QUALITATIVA - BASE DE DADOS..... 61
6.1.1	O Problema Iris Flower ............... 61
6.1.2	O Problema Wine ...................... 63
6.1.3	Resultados e Çomparaçao............... 65
6.1.4	Discussao............................. 65
6.2	SAIDA QUANTITATIVA - BASE DE DADOS...... 68
6.2.1	Problema da Taxa de Penetracão da Broca (ROP) . 68
6.2.2	Resultados e Çomparação............... 69
6.2.3	Discussao............................. 71
7	ÇONSIDERAÇOES FINAIS .................... 77
REFEREnÇIAS................................. 79
1	INTRODUÇÃO
Uma Rede Bayesiana (RB) (PEARL, 1988) é um modelo de representação e raciocínio de incerteza que utiliza a probabilidade condicional entre as variáveis categóricas (qualitativas) de um domínio e as expressa via um grafo acíclico direcionado (Directed Acyclic Graph - DAG). Sua estrutura gráfica consegue mapear as correlaçães entre as variaveis, sendo uma linguagem apropriada e com recursos eficientes para a representacão da distribuição conjunta de probabilidades sobre um conjunto randômico de variaveis (FRIEDMAN; GEIGER; GOLDSZ-MIDT, 1997).
Entretanto, a distribuicão conjunta de probabilidades dentro da RB pode ser muito grande e o raciocínio Bayesiano (inferôncia) não á uma tarefa trivial. A utilização do componente de fatorização tende a diminuir a complexidade da inferôencia exata. Entre os algoritmos da área pode-se citar aqueles que sao exatos (SHENOY; SHAFER, 2008); (MADSEN; JENSEN, 1999) e os que sãao aproximados para facilitar a inferôncia em RBs complexas. Os algoritmos aproximados são divididos em dois tipos: estocasticos (FUNG; CHANG, 1990); (SALMERON; CANO; MORAL, 2000) ou determinísticos (JENSEN; LAURITZEN; OLESEN, 1990); (CANO; MORAL; SALMERON, 2000).
A inferôencia Bayesiana claíssica íe realizada em casos onde o domínio de aplicação e exclusivamente qualitativo. Para que a tícnica possa ser aplicada quando o conjunto de variíveis í híbrido, ou seja, apresente variíaveis numíericas (quantitativas) e variíaveis qualitativas, íe necessíario usar míetodos alternativos que possibilitem a inferôencia dentro da RB. Mítodos de discretizacão ou de simulação (LANGSETH et al., 2009) são geralmente empregados em Redes Bayesianas de domínios híbridos e proporcionam uma inferôencia aproximada.
1.1 JUSTIFICATIVA E MOTIVACAo
Entre os míetodos de inferôencia aproximada, o mais comum para lidar com Rede Bayesianas Híbridas e o de Discretizacão. Esse metodo, muda o valor numíerico da variíavel por um correspondente qualitativo, de acordo com alguma mítrica ou critério específico. As abordagens de discretizaçcaão saão usualmente feitas atravíes da distribuiçcãao de probabilidades ou usando parôametros estíaticos, como a frequôencia de cada classe. Alguns fatores favorecem a discretizacao (categorizacão) de variíveis
para RBs, como:
•	Falta de algoritmos eficientes para o aprendizado e a inferência para dados contínuos (FRIEDMAN; GOLDSZMIDT, 1996);
•	Facilidade em compreender características categorizadas em detrimento as contínuas (LIU et al., 2002);
•	Classificadores utilizando dados discretos (em intervalos) tendem
a ser menos complexos e mais precisos que utilizando dados contínuos (FRANK; WITTEN, 1999);
•	Menor complexidade computacional o que acarreta em uma maior rapidez no aprendizado e inferência (FRIEDMAN; GOLDSZMIDT, 1996),(ROUSU, 2001), (YANG, 2003).
A discretizaçao também pode ser feita por especialistas da área de forma manual. Entretanto, essa pode ser uma tarefa complexa: há casos onde os dados nao seguem nenhum padrâo visível e quando seguem, esse padrâo pode mudar em diferentes ocasiões. Portanto, á necessario discretizar os dados com o conhecimento dos préprios dados, porque não há nenhum conhecimento prévio do seu comportamento.
1.2	problematizacAo
Esta pesquisa visa propor um máetodo de discretizacçãao de eventos, aqui chamados pico e vale, observáveis em um vetor de dados. Este máetodo seráa implementado em algoritmo e testado em base de dados de forma que possam ser avaliados seus resultados. Portanto, trata-se de uma pesquisa de base tecnológica.
Embora haja varios de dados, A maioria dos algoritmos para discretização deles possui como objetivo principal a clusterização das variáveis. Para realizar a discretização no domínio da RB, acreditase que seja necessáario considerar as distribuiçcãoes condicionais de cada variaável no processo e como elas se distribuem globalmente na rede. Dentre as abordagens de discretizacao utilizadas em Redes Bayesianas, as mais comuns sãao:
•	Discretização de igual largura (Equal Width Discretization - EWD) (CATLETT, 1991); (KERBER, 1992); (DOUGHERTY; KOHAVI; SAHAMI, 1995) - divide os valores de v em k intervalos (definidos por parâmetro) de igual largura w = (xmax — xmin)/k;
•	Discretizaçao de igual frequância (Equal Frequency Discretization -EFD) (CATLETT, 1991); (KERBER, 1992); (DOUGHERTY; KOHAVI; SAHAMI, 1995) - ordena os valores de v e os divide em m k intervalos (definidos por parâmetro), sendo que cada intervalo contenha aproximadamente o mesmo nUmero de instâncias;
•	Discretizaçao da Minimizaçao da Entropia (Entropy Minimization Discretization - EMD) (FAYYAD; IRANI, 1993) - Ordena os valores de v e testa possíveis pontos de corte através do ponto medio de cada par x¿, xi+i. Os dados sao então discretizados em dois intervalos e a entropia ée calculada. Para avaliar o corte, a abordagem seleciona aquele com a menor entropia e entãao repete o processo recursivamente, sempre selecionando o melhor ponto de corte.
Outras tecnicas também são aplicadas, como a “Discretização Preguiçosa” (Lazy Discretization - LD) (HSU; HUANG; WONG, 2000); (HSU; HUANG; WONG, 2003), “Discretizacao Proporcional” (Proportional Discretization - PD) e “Discretização de Frequência Fixa” (Fixed Frequency Discretization - FFD) (YANG; WEBB, 2009).
Um importante aspecto quanto as RBs estaé na sua propriedade de inferâencia: a distribuicçaão de probabilidades de uma variéavel influencia diretamente a outra. Portanto, ée necesséario realizar uma otimizacçãao global para reduzir o erro na RB e, por consequâencia, aumentar a sua acuraécia.
E então possével encontrar um metodo de discretizaçao que contribua para a descoberta do conhecimento e aumento da acuracia em Redes Bayesianas?
1.3	OBJETIVO GERAL
Propor um metodo de discretizaçcãao baseado em dados para Redes Bayesianas atraves da otimização global das variaveis do doménio de aplicaçcãao.
1.4	OBJETIVOS ESPECIFICOS
•	Identificar regiãoes com eventos de pico e vale nos dados;
•	Explorar a viabilidade e as funçcãoes objetivo do Algoritmo Genetico
como método de otimização global para variével de saída qualitativa e para variavel de saída quantitativa;
•	Integrar as propriedades mateméticas do método proposto com o Algoritmo Genético;
•	Avaliar o méetodo proposto.
1.5	ESTRUTURA DA DISSERTACAo
Esta dissertacão esté dividida em oito capítulos. No Capétulo 1 e mostrada a introducão ao problema e dada uma visão geral da dis-sertaçao alem do objetivo geral e dos objetivos específicos pretendidos.
No Capétulo 2 é uma feita revisao bibliográfica dos principais metodos de discretizaçcaão para Redes Bayesianas.
No Capétulo 3 é feita a apresentaçao dos fundamentos teóricos utilizados: Redes Bayesianas, Redes Bayesianas Híbridas e Algoritmos Geneticos.
No Capétulo 4 sao definidos os procedimentos metodologicos adotados na dissertacçãao.
No Capítulo 5 são mostradas as propriedades matematicas do metodo proposto e definidas as funcoes dos dois pontos de cortes: pico e vale. Tambem é feita a mesclagem entre as propriedades de discre-tizaçcaão propostas e o Algoritmo Genetico. Sãao propostas as funçcãoes objetivo para duas situações: quando a variavel de saéda é qualitativa e quando a variavel de saída é quantitativa.
No Capítulo 6 são mostrados estudos de caso em que foram aplicados o metodo proposto e outros metodos da literatura e seus devidos desempenhos para diferentes bases de dados e esses resultados são discutidos e analisados.
E finalmente, no Capétulo 7 o estudo é concluído e são feitas as consideraçcãoes finais alem da indicaçcaão de trabalhos futuros em complemento ao apresentado.
2	ESTADO DA ARTE
Os dois métodos mais comuns para a discretização em Redes Bayesianas utilizam pontos de corte fixos para a definição de intervalos. O primeiro, chamado EWD (Equal Width Discretization - EWD) (CATLETT, 1991); (KERBER, 1992); (DOUGHERTY; KOHAVI; SAHAMI, 1995) divide o conjunto de dados em tamanhos de igual largura e cada uma das divisães equivale à uma classe na RB, o segundo Discretização de igual frequencia (Equal Frequency Discretization -EFD) divide o conjunto de dados de forma que as classes possuam aproximadamente a mesma quantidade de dados. Ambos os métodos nao levam em con-sideraçcãao qualquer relaçcãao entre as variéaveis ou a melhoria da acuréacia da rede. Ambos os métodos são utilizados em RBs devido à sua simplicidade e boa performance (HSU; HUANG; WONG, 2003).
Fayyad e Irani (1993) propoe um metodo heurístico Discretização da Minimização da Entropia (Entropy Minimization Discretization -EMD), que ao contrério dos metodos EWD e EFD é um método de aprendizado supervisionado. Dougherty, Kohavi e Sahami (1995) aplicou esse metodo em várias bases de dados do repositório da UCI utilizando Redes Bayesianas e obteve bons resultados.
Hsu, Huang e Wong (2000) propãe o método de “Discretização Preguicosa” (Lazy Discretization - LD) que deriva diretamente das propriedades da Distribuiçao de Dirichlet. Nesse método, a discretização é adiada até o momento da classificacão. Ele espera até que a instância de teste seja apresentada para entãao determinar os pontos de corte e estimar as probabilidades de cada classe. O méetodo foi aplicado em bases de dados do repositoério da UCI e obteve bons resultados quando comparado com outros méetodos de discretizacçãao.
Matsuura (2003) propãoe um méetodo de discretizaçcãao para Redes Bayesianas chamado de Discretizacão via Tabela de Probabilidades. O algoritmo discretiza todas as variéveis conténuas via EWD ou EFD e usa um algoritmo de aprendizado de estrutura para gerar a mais adequada aos dados discretizados. E realizado um loop para a uniãao de intervalos e eles sãao avaliados por uma metrica chamada de Diferençca Media Quadratica. O metodo foi aplicado em uma base de dados relacionado a protecão ao voo, obtendo resultados promissores e superiores quando comparados ao metodo EFD.
Yang e Webb (2009) propoe dois metodos heurísticos de discretização:	“Discretização Proporcional” (Proportional Discretization -
PD) e “Discretização de Frequância Fixa” (Fixed Frequency Discreti-
zation - FFD). O método PD procura minimizar o bias e a variância através de um conjunto com n instâncias de treinamento e determinando o valor de s (quantidade de dados em cada intervalo) e t (nUmero de intervalos), de forma que s = t e s x t = n. O método FFD atua de forma semelhante ao metodo EFD, porém em vez de delimitar k intervalos, ele delimita a quantidade ménima de dados em cada estado e a quantidade maxima de intervalos. Ambos os métodos apresentados obtiveram erros inferiores em bases de dados do repositorio da UCI quando comparados com outros méetodos.
Wong (2012) propõe uma medida não parametrica de avaliar o nével de dependâencia entre um atributo conténuo e uma classe, e então essa medida é utilizada em um método híbrido de discretizaçao de forma que a acuracia em um classificador do tipo Na'ive Bayes seja melhorada. O método proposto combina quatro métodos (EWD, EFD, PD e EMD) e obteve uma acurácia geralmente superior que os metodos usados individualmente ao utilizar bases de dados do repositéorio da UCI.
Kurtcephe e Güvenir (2013) propoe um metodo de discretizacão global, estético e supervisionado baseado na curva ROC (receiver operating characteristic) utilizando o algoritmo QuickHill (PREPARATA; SHAMOS, 1993) que possui complexidade esperada de O(n log n) e O(n2) no pior caso. O méetodo funciona de forma que a éarea sob a curva ROC seja maximizada e sãao propostos uma nova medida de discretizaçcãao e um novo critéerio de parada. Ao comparar o méetodo proposto com outros da literatura, ele obteve um desempenho superior utilizando bases de dados do repositéorio UCI. Os autores enfatizam que embora o método tenha resultados promissores o tempo necessario para à con-vergâencia pode ser alto.
A Tabela 1 sintetiza os métodos apresentados neste capétulo.
Método	Descrição
EWD (1191, 1995)	discretização por igual largura.
EFD (1991, 1995)	discretização por igual frequência.
EFD (1991, 1995)	discretização por minimização da entropia.
LD (2000)	discretização na hora da classificação.
via Tabela de Probabilidades (2003)	utiliza EWD ou EFD, algoritmo de aprendizagem e um loop que une intervalos.
PD (2009)	determina a quantidade de dados em cada intervalo e o número de intervalos.
FFD (2009)	delimita a quantidade mínima de dados em cada estado e a quantidade máxima de intervalos.
método de Wong (2012)	avalia o nível de dependência entre um atributo contínuo e uma classe e combina os métodos EWD, EFD, PD e EMD.
método de Kurtcephe (2013)	método de discretização global, estático e supervisionado baseado na curva ROC.	w
34
3	FUNDAMENTAÇAO TEÓRICA
Esse capítulo objetiva apresentar os principais conceitos utilizados para a composiçao do método proposto. Na Seção 3.1 é introduzido o conceito de aprendizado supervisionado e o méetodo adotado de pre-visãao nesse tipo de aprendizado.
Na Seçao 3.2 é definido formalmente uma Rede Bayesiana, assim como sua modelagem. Na Secão 3.3 é incorporado o conceito de Redes Bayesianas Hébridas e suas tecnicas de inferência aproximada.
E, finalmente, na Seçao 3.4 sao discutidos os principais conceitos dos Algoritmos Genéeticos e as formas de representacçãao de um indivéduo.
3.1	APRENDIZADO SUPERVISIONADO
O Aprendizado de Máquina (AM) é um conjunto de técnicas computacionais que tem por objetivo a criação de sistemas capazes de adquirir e organizar o conhecimento de forma automatica (MITCHELL, 1997).
Uma das técnicas do AM é o Aprendizado Supervisionado (Supervised learning) que consiste em criar uma funcao através de um conjunto de treinamento (MITCHELL, 1997).
Esse conjunto possui pares de objetos de entrada (tipicamente vetores) e saéda desejada, que pode ser um numero real (para casos de regressãao) ou um réotulo de uma classe (para casos de classificacçaão).
O objetivo do Aprendizado Supervisionado ée utilizar a funçcãao criada para prever o valor de saéda (resultado) atravées dos dados de entrada (Figura 1). Os algoritmos principais dessa técnica são: Redes Neurais, algoritmo do vizinho mais préoximo (Nearest Neighbor - NN), érvores de decisão, as maquinas de vetores suporte (Support Vector Machines - SVM) e as Redes Bayesianas.
3.2	REDES BAYESIANAS
Definindo formalmente, uma Rede Bayesiana é uma dupla (G,P), onde G=(V,E) é um DAG nos quais os nodos V = vi,v2,...,vn representam as variéveis e as arestas E = e1,... ,en representam uma direta correlacão entre cada nodo de V. O item P é definido como os parêametros probabilésticos expressos atravées de tabelas: dada uma de-
Figura 1 - Aprendizado e previsão dos algoritmos do tipo Aprendizado Supervisionado.
terminada variável e feita a distribuição de probabilidade condicional de cada uma de suas classes (estados) X = x1,...,xn em relação a cada uma das classes de seus pais.
Ou seja, a RB estabelece que uma variável á independente de todas as outras variáveis, exceto de seus descendentes no grafo dado o estado de seus pais. A inferência na RB á feita pelo Teorema de Bayes:
P (V = v|X = x)= P (X =	= v)	(3.1)
A probabilidade conjunta á determinada pela chamada “regra da cadeia” e assume a independencia condicional entre as variáveis:
n
P (vi,..., Vn) = n P (Vi\pai(Vi))	(3.2)
i=1
onde pai(Vi) determina o conjunto de nodos pais do nodo V).
A inferência exata seguindo a Equaçao 3.2 nao á uma tarefa trivial, pois a distribuicão de probabilidade conjunta pode ser muito grande. Por exemplo, em um caso em que haja 10 nodos discretos com 2 estados cada. A distribuição de probabilidade á de 210 — 1 = 1023 valores expressos em uma tabela. E a tabela de cresce forma exponencial: se fossem 11 nodos haveriam 211 — 1 = 2047 valores expressos.
O raciocínio Bayesiano á estabelecido em dois cenarios distintos:
se “entrada” entao “saída”
se “saída” entao “entrada”
3.2.1	Modelagem Bayesiana
A modelagem de uma RB é feita a partir de uma estrutura (DAG) que incorpora a categorização das variaveis continuas. A partir dessa modelagem e necessario estabelecer as tabelas de probabilidade (força de associacão entre variaveis) atraves do aprendizado do dominio a ser trabalhado. Existem três formas de realizar essa aprendizagem: exclusivamente dos dados (base de dados), exclusivamente dos especialistas do dominio ou aprender de forma híbrida tanto dos dados quanto dos especialistas.
Entre as possíveis topologias de rede e vastamente conhecida a estrutura Naive Bayes, que e o mais simples entre esses modelos. dado o contexto de classe. Embora esse modelo nãao traduza a realidade na maioria das tarefas do mundo real ele e bastante efetivo, pois os parametros de cada atributo podem ser aprendidos separadamente, facilitando o processo de aprendizagem (MCCALLUM; NIGAM et al., 1998).
A topologia Na'ive Bayes e, portanto, um conjunto de variaveis de entrada independentes entre si que possuem em conjunto um unico pai (no de saída). Um exemplo da topologia Naive Bayes pode ser vista na Figura 2. Nesse caso, o nodo A e a saida e os nodos B, C e D sãao as entradas.
Figura 2 - Topologia Naive Bayes.
Além da topologia da rede, é necessário especificar a Tabela de Probabilidade Condicional (Conditional Probability Table - CPT) de cada nodo, o que descreve a probabilidade de cada classe do nodo em combinacão com cada classe de seus pais. Um exemplo de CPT para a
RB da Figura 2 é mostrado na Tabela 2.
Tabela 2 - Exemplo de Tabela de Probabilidade Condicional (CPT).
A	P (B = stateO)	P (B = statel)	P (B = state2)
state0	0.2	0.2	0.5
statel	0.1	0.5	0.4
state2	0.1	0.05	0.85
O nodo de saída, em uma Rede Bayesiana, tem seus valores de probabilidade calculados por inferência (Equacão 3.2) ao utilizar a evidência expressa pelos nodos de entrada. A criação de uma topologia de rede hierárquica e feita ao adicionar nodos intermediarios entre as entradas e a saída (Figura 3).
Figura 3 - Entradas e Saída em uma Rede Bayesiana.
Ou seja, cada variavel do domínio de aplicação se torna um nodo na RB e uma ligaçao entre dois nodos denota uma relação de “causa e efeito” (probabilidade condicional). Para especificar a “forca” dessa relação são feitas (usualmente) tabelas do tipo CPT para cada um dos nodos (Figura 4).
Figura 4 - Estrutura Geral de uma Rede Bayesiana.
3.3	REDES BAYESIANAS HÍBRIDAS
Sao chamadas de Redes Bayesianas Híbridas (RBH) aquelas que possuem tanto nodos contínuos quanto nodos discretos. Nos casos em que há apenas nodos discretos no domínio, a distribuição condicional pode ser representada atraves de uma tabela com valores de probabilidade.
Entretanto, o problema se torna mais complexo nas Redes Bayesianas Híbridas, pois trabalha-se com modelos para expressar a distri-buicão de probabilidade condicional e nao com tabelas (CPT) como nas RBs clássicas. A acurácia da inferência depende do domínio a ser representado e nem sempre í exata (LANGSETH et al., 2009).
Para lidar com os casos em que a inferêencia exata naão íe possível, são utilizados mítodos aproximados em Redes Bayesianas Híbridas. Os mítodos mais populares de inferência aproximada são: Discretizaçao (Seçao 3.3.1), Combinação de exponenciais truncadas (Secão 3.3.2) e ainda uma abordagem utilizando Cadeia de Markov (Seção 3.3.3).
3.3.1	Discretização
A tíecnica mais comum para lidar com a inferêencia em Redes Bayesianas Híbridas é a discretização. Essa tícnica consiste em trocar
o valor contínuo x da variável por seu valor discreto equivalente x'.
Os metodos EWD e EFD são muito utilizados para discretizacão em Redes Bayesianas por sua baixa complexidade computacional e facilidade de de implementacão, alem de sua boa performance (HSU; HUANG; WONG, 2003).
O metodo EWD possui complexidade O(n) em um vetor ordenado, pois divide uma variavel em intervalos de igual largura, ou seja, os pontos de corte são definidos de forma que exista k intervalos com tamanhos de:
i xmax xmin\	\
w = (------k------)	(3.4)
onde xmax é o maior valor da variavel e xmin o menor valor (Algoritmo 1).
Algoritmo 1 Metodo de Discretização EWD	
1:	v	a variável quantitativa ordenada a ser discretizada
2:	xmin	menor valor em v
3:	xmax	maior valor em v
4:	k	quantidade de intervalos
5:	xm,ax	xmin
6:	corte	w
7:	indice	0
8:	classeatuai	classe + indice (nome da primeira classe - a que possui os menores valores)
9:	for all xi em v do
10:	if xi &gt; corte then
11:	corte	corte + w
12:	indice	indice + 1
13:	classeatuai	classe + indice
14:	end if
15:	discretize xi para classeatuai
16:	end for
17:	return v discretizada (v*)
O método EFD também possui complexidade O(n) em um vetor ordenado, porem divide uma variavel em tamanhos de igual frequencia. Ou seja, os pontos de corte são definidos de forma que cada classe possua aproximadamente o mesmo numero de registros (Algoritmo 2).
Nos metodos EWD e EFD e necessario fornecer a variavel ordenada. O problema da ordenação possui complexidade 6(n log n), e
Algoritmo 2 Método de Discretização EFD
1: v a variavel quantitativa ordenada a ser discretizada
2: n quantidade de registros em v
3: k quantidade de intervalos
4: range n
5: igual FALSO
6: indiceclasse	1
7: indiceregistro	0
8: classeatuai	classe + indice
9: classeanterior
10: while indiceregistro &amp;lt;n do
11:	if i &amp;lt;(range * j) OU igual = verdadeiro then
12:	discretize Xi para classeatuai
13:	else
14:	indiceclasse — v[indiceregistro]
15:	end if
16:	classeanterior — indice + 1
17:	if v[indiceregistro I 1] — classeanterior then
18:	// caso v[indiceregistro + 1] == n foi omitido para fins de
simplicidade
19:	igual	VERDADEIRO
20:	else
21:	igual	FALSO
22:	end if
23: end while
24: return v discretizada (v*)
portanto a complexidade total dos metodos e O(n) * 0(n log n).
3.3.2	Combinaçao de Exponenciais Truncadas
O Modelo de Combinação de Exponenciais Truncadas (Mixtures of Truncated Exponentials - MTE) (MORAL; RUMÍ; SALMERON, 2001) pode ser entendido como uma generalização da discretização (LANG-SETH et al., 2009).
Entretanto, ao inves de utilizar pontos de corte para discretizar cada regiãao do conjunto de dados, essa discretizaçcaão e feita por uma combinacçaão linear de funçcoães exponenciais.
Nesse metodo, as funcões densidade das variaveis são representa
das pelas médias das MTEs, que agem como um modelo geral e pode se aproximar à distribuiçao da variavel de forma satisfatória (LANGSETH et al., 2009).
O principal benefício dessa abordagem e uma maior flexibilidade para se aproximar da função de distribuiçao da variavel.
3.3.3	Abordagem via Cadeia de Markov - Monte Carlo
Nesse tipo de abordagem, e utilizada a ideia de amostragem. E garantido que se a quantidade de amostras for suficientemente grande, feita de forma independente e com a mesma distribuição, e possível obter qualquer grau de precisão desejada na estimação:
1 N
EZ (P (T =1|Z )) =	P (T =1|zi)	(3.5)
i=1
onde Zi,..., zn sao amostras de f (z).
Essa e uma tecnica de inferência que tira vantagem da estrutura Bayesiana para aumentar a velocidade do processo de simulação.
Nesse tipo de tecnica e importante ficar atento quanto à es-timaçcaão de eventos raros, ja que muitas amostras devem ser geradas para poder obter uma unica amostra desse evento (LANGSETH et al., 2009).
3.4	ALGORITMOS GENETICOS
Algoritmos Geneticos (AGs) sao otimizadores de funçães, ou seja, metodos que procuram os extremos de uma função objetiva f (x) baseando nos princípios da seleção natural e da genetica populacional (GOLDBERG, 1989) (CANTÚ-PAZ, 1995) (WEILE; MICHIELSSEN, 1997). A funçao objetivo do problema e usualmente usada para expressar a função fitness no AG.
Um aspecto importante em relaçao à função fitness esta na sua responsabilidade de medir a performance da solucçãao (funçcãao objetiva) como uma maneira de gerar uma alocacçãao de recursos para a reproduçcãao (WHITLEY, 1994).
Um indivíduo e definido como uma soluçao candidata valida no AG, expressa ou por uma string binaria ou por um vetor de numeros reais (JANIKOW; MICHALEWICZ, 1991) (WRIGHT et al., 1991), onde um conjunto de indivíduos e considerado uma populacão. Três operadores
sao comumente usados: seleçao, crossover e mutação (Figura 5).
Figura 5 - Fluxograma do Algoritmo Genético.
O operador de seleção usa o fitness de cada individuo para escolher aqueles que sao os mais adaptados da população atual para gerar uma nova população. Há várias maneiras de realizar essa seleção de individuos, mas ela sempre garante que os individuos mais adaptados (melhores fitness) possuam uma maior probabilidade de serem selecionados.
A reprodução á feita pelos operadores de crossover e mutação. O primeiro á o mecanismo primário de exploração do AG: ele escolhe aleatoriamente um par de indivíduos prá-selecionados e troca in-formacão (uma substring, no caso de representação binária) entre os dois indiváduos para criar novos indiváduos.
O operador de mutaçcãao áe geralmente considerado como um operador secundario e á usado para prevenir que a soluçao fique estagnada em algum mánimo ou máaximo local. A mutacçãao áe feita atraváes da seleçao randômica de uma substring em um indiváduo e trocando o valor da mesma. O percentual da populaçcãao atingido por esse operador áe geralmente muito menor que o percentual atingido pelo operador de crossover.
O AG começa com uma população atual e entao a seleçao á aplicada para criar uma população intermediaria. Recombinação (mutação e crossover) á então usada para criar a próxima população. O processo
entre a população atual ate a próxima população.
A convergencia do AG tende a evolúir atraves de súcessivas geracoes ate qúe o fitness do melhor individúo e a media de fitness da popúlacão se aproximarem do otimo global (BEASLEY; MARTIN; BULL, 1993).
Algoritmos Geneticos não garantem qúe a solúçao otima vai ser encontrada, e súa efetividade é determinada pelo tamanho da popúlação n. O tempo reqúerido para qúe o AG convirja e de O(n log n) avaliacães de fúnçoes (GOLDBERG, 1989).
3.4.1	Representação de um indivíduo
Um cromossomo representa um indivíduo, que e uma solução candidata do problema a ser resolvido. Entre as representaçcoães mais comuns de um indivíduo no AG, encontram-se: codificação binaria, codificação em ponto flutuante, maquina de estados finitos e arvores.
A representaçao mais comum e a codificação binaria, que descreve o cromossomo por um vetor de bits. A representaçcãao binaria de um numero real esta sujeita à seguinte precisão:
2 &gt; (xmax - xmin) * 10p	(3.6)
onde l o tamanho da cadeia de bits,s p corresponde a precisão, k a quantidade de bits e xmin, xmax definem o intervalo real [xmin,xmax] ao qual o valor a ser representado xr pertence. Logo, quanto maior a precisaão desejada maior a quantidade de bits necessaria para obtêe-la.
E necessario, porem, que as cadeias de bits tenham o mesmo tamanho para a execução dos operadores de reprodução do AG (crossover, mutação). Outro quesito importante esta na quantidade de bits no cromossomo: ele deve ser grande o suficiente para permitir uma boa troca de informaçcoães durante a reproduçcãao. Portanto, a tecnica de mapeamento e utilizada.
A tecnica de mapeamento funciona como uma regra de trêes. Ao pegar um numero binario (b2) tradicional, ela aumenta a quantidade de bits necessaria para representa-lo. Para isso, utiliza-se os valores de xmin,xmax de forma similar à Equaçao 3.6:
xr
— xmin
+ (xmax
min )
b10
21 — 1
(3.7)
x
onde b10 eqúivale ao valor na base decimal.
Por exemplo, considere o cromossomo de 16 bits:
xb2 = 1011010101010101	(3.8)
Ao decodificéa-lo da base 2 para a base 10, ée obtido o valor:
xb10 = 46421	(3.9)
Considere ainda que o intervalo [xmin, xmax] é definido por [0, 50].
Logo, o mapeamento na b10 desse valor é:
46421
xr = 0 + (50 - 0)^----i = 35.42	(3.10)
Caso o nuémero representado seja um inteiro, ée séo realizar o arredondamento, para cima ou para baixo, dependendo do critéerio definido.
46
4	PROCEDIMENTOS METODOLÓGICOS
No metodo de discretização Pico e Vale proposto (DPV) assume-se que uma variável numerica v e V possui valores em intervalos extremos e em um intervalo intermediíario. Ao analisar o intervalo intermediario á possível obter os intervalos de valores extremos (valores acima e valores abaixo dos limites do intervalo intermediíario) e estabelecer suas probabilidades condicionais, assim como suas relaçcãoes de causa e efeito: “O que causou esse comportamento? O que ele implica?”.
Observando o comportamento de uma variáavel, áe possável inferir se um valor x esta fora do intervalo intermediario, seja de forma positiva (alta) ou negativa (baixa). A delimitacão dos intervalos utiliza dois pontos de corte expressos em percentil: o primeiro (pico) e restrito à área considerada “alta” e o segundo (vale) cobre a área considerada “baixa”.
O uso do percentil como medida para os pontos de corte incorpora o conceito de frequêencia dos dados (seguindo a linha do EFD, EMD e FFD). Porám, o metodo DPV nao segue uma regra pré-definida de cortes, ou seja, a quantidade de dados em cada classe áe descoberta em tempo de processamento. Aláem disso, ao utilizar a medida de per-centil áe possável restringir a áarea de cobertura de cada um dos cortes, definindo seus limites de atuaçcãao.
O uso dos dois pontos de corte sugere que uma variaável numáerica possui trêes comportamentos distintos: “baixo”, “máedio” e “alto”. Entretanto, essa premissa nem sempre áe verdadeira e a utilizaçcãao desses trêes comportamentos pode nãao trazer benefácios para a criaçcãao de uma RB. Isso acontece quando os pontos de corte estão muito próximos dos valores limites, por exemplo, o corte de vale estáa muito proáximo do menor percentil da variáavel ou o corte de pico estáa muito proáximo do maior percentil. Eá possável ainda que os dois cortes estejam taão perto um do outro que um intervalo intermediaário áe considerado irrelevante.
A Figura 6 mostra dois exemplos de dados classificados com o DPV. O primeiro gráafico possui trêes comportamentos distintos: um intermediáario, um superior e um inferior. O segundo gráafico mostra apenas dois comportamentos um superior e outro inferior.
O ponto fundamental para estabelecer os percentis dos cortes está no algoritmo de busca, nesse caso, o Algoritmo Genático. A escolha do AG deve-se pela sua implementacao simples, resultados eficientes e adequação ao problema (WRIGHT et al., 1991).
Figura 6 - Eventos de Pico e Vale.
No método DPV, cada variável numérica v do conjunto de dados tem seus dois pontos de corte. Esses pontos são encontrados através da busca pelo AG e e escolhido o conjunto mais “bem adaptado” à funçao objetivo no que diz respeito à RB. O conjunto de pontos, que representa um indivíduo, pode ser visto na Figura 7.
Figura 7 - Representacao de um indivíduo no DPV.
A Figura 8 mostra a visão geral do metodo proposto.
E importante ressaltar que o metodo DPV e de discretizaçcãao visando a descoberta de conhecimento na RB, ou seja, o conjunto de
Figura 8 - Fluxograma do método DPV.
variáveis discretizadas deve reforçar o processo de aprendizagem. Dessa forma, a distribuição de probabilidade dentro de cada nodo da RB nao necessariamente seréa siméetrica.
A escolha do melhor indivéduo no AG esta diretamente associada ao mecanismo de classificaçao na Rede Bayesiana. Cada nodo em uma RB é expresso por um vetor probabilidades, sendo que cada um de seus estados tem uma probabilidade de ser “verdadeiro”. Nesta dissertação foi adotado o méetodo de classificacçaão que escolhe o maior valor no vetor de probabilidades do nodo de saída para classificar a instância.
Existem duas situacães possíveis para se estabelecer o fitness de um indivíduo durante a execução do DPV: quando a variavel de saída íe qualitativa e quando a variíavel de saída íe quantitativa.
Quando a variável de saída é qualitativa, um maior desempenho da rede estí diretamente ligado à classificaçao correta dos dados atraves da variaível de saída. Portanto, a medida de desempenho nesses casos í a prípria acurícia e o objetivo do algoritmo í a sua maximização. Ou seja, o melhor indivíduo de uma população í aquele que possui a maior acuríacia.
Entretanto, quando a variível de saída í quantitativa, objetivase estimar valores atravíes do seu vetor de probabilidade. Esses valores correspondem aos valores medios da distribuicão e o desempenho da
rede esta ligado à minimização da taxa de erro entre os valores estimados e os valores numéricos da variável de saida.
Outro ponto fundamental para as Redes Bayesianas esta na sua topologia. Neste trabalho foi utilizada a estrutura Naive Bayes e, portanto, todas as variiaveis sãao consideradas de evidêencias de entrada com a exceçao da variavel de saida. A escolha dessa topologia é justificada pela sua efetividade e simplicidade ao facilitar o processo de aprendizagem (Seção 3.2.1).
Para avaliar o desempenho do DPV, o método foi aplicado em tres bases de dados com o objetivo de testar os casos onde a variével de saída é qualitativa e onde ela e quantitativa.
No primeiro caso (saéda qualitativa), foram utilizadas duas bases de dados publicas que retratam problemas de classificaçao, sendo a primeira uma base de características de diferentes tipos de flores Iris e a segunda uma analise quémica de diferentes tipos de vinho.
No segundo caso (saéda quantitativa) foi utilizada uma base de dados de um doménio real que apresenta variéaveis de um sistema de perfuraçcãao de poçcos de petréoleo e sua respectiva taxa de perfuracçãao. O objetivo, nesse caso, ée estimar o valor da taxa de perfuraçcãao.
O méetodo proposto foi comparado com dois outros méetodos da literatura: EFD e EWD. A escolha desses métodos se deve à sua grande popularidade, eficiêencia, baixo custo computacional e utilizacçãao de forma hébrida com uma grande quantidade de méetodos de discre-tizaçcãao (Seçcãao 2).
5	MÉTODO PROPOSTO
O método proposto (DPV) é composto de dois mecanismos fundamentais:
•	a anélise dos pontos de corte (pico e vale) estabelecidos ao determinar a sua relevancia;
•	a escolha dos pontos de corte mais bem adaptados ao problema através do Algoritmo Genético.
O metodo DPV é paramétrico e define a relevancia dos pontos de corte através do coeficiente a. Esse coeficiente determina a proximidade máxima permitida entre os pontos de corte e os valores limites da variéavel (extremos).
Caso os pontos de corte estejam muito próximos, eles serão unidos. Caso um dos pontos de corte esteja muito préximo de um dos valores extremos da variéavel, este ponto seraé desconsiderado. E, finalmente, se ambos os pontos de corte estiverem muito proéximos dos pontos extremos, ée criado um novo ponto de corte atravées da méedia dos dois pontos (pico e vale).
As propriedades do método DPV em relacão aos pontos de corte são mostradas na Seção 5.1 e as configurações em relação ao Algoritmo Genético, assim como as funçães objetivo empregadas, são mostradas na Seção 5.2.
5.1 PROPRIEDADES PICO E VALE
Para descrever as propriedades dos pontos de corte no míetodo DPV, os seguintes conceitos são definidos no contexto de uma variível
v¿:
•	p(x) como uma função que recebe um valor x como entrada e retorna o percentil que esse valor se encontra;
•	P-1 (y) como a função inversa da funçao p(x): recebe um percentil y como entrada e retorna o valor x que ele representa;
•	vale como o percentil expresso pelo ponto de corte vale;
•	pico como o percentil expresso pelo ponto de corte pico;
• vale &amp;lt;pico;
•	X* = x*,... ,xn como o vetor discretizado do conjunto de valores de vi (X = xi,..., x„).
•	como o percentil que representa o menor valor (xmin) em ví;
•	PXmax como o percentil que representa o maior valor (xmax) em ví;
lí possível mesclar ou desprezar pontos de corte se eles nao forem relevantes para a solucão. A relevancia dos pontos de corte e sua proximidade com os valores extremos (xmin e xmax) são expressos por um coeficiente de relevância a (0 &amp;lt;a &amp;lt;1) definido por parâmetro, que determina quãao perto o ponto de corte estía desses valores. A proximidade para os dois pontos de corte segue as seguintes equacães:
xmin
p-1(vale)
p 1(vale) &gt;
xmin
a
p 1(pico)	___ -1 .
---------- &amp;lt;a =^ p (pico) &amp;lt;Xmax * a xmax
como vale &amp;lt;pico, a seguinte inequação é válida:
(5.1)
(5.2)
xmin
a
&amp;lt;xmax * a
(5.3)
&amp;lt;a
Ou seja, para que exista um valor válido de a á necessário satisfazer a inequação:

2	xmin
a2 &gt; ----
xmax
xmin
xmax
(5.4)
a &gt;
lí necessario, portanto, aplicar uma correção em a para assegurar que os pontos sempre possuam um intervalo de valores considerado relevante independente da proximidade de xmin e xmax. O valor ajustado do coeficiente, a', á definido por:
a' = ((1 — S) • a) + S
(5.5)
onde S é o coeficiente limite entre xmin e xmax, definido por:

S=
xmin
xmax
(5.6)
com essa definição, é possível inferir que o limite da Equação 5.5 quando ô 0 é:
lim((1 — ô) • a) + ô = a	(5.7)
Ou seja, quando a distância entre xmin e xmax for muito grande (tender ao infinito), o valor de ô tende a zero e a' = a. A relevância dos cortes, e portanto determinada pelo coeficiente ajustado a'. O menor valor relevante de vale é dado por:
p-1(va/emi„) = '	(5.8)
a'
e o maior valor relevante de pico é:
p (picomax)	xmax • a	(5.9)
Através das Equacães 5.8 e 5.9 e considerando que ambos os pontos de corte possuam diferentes definiçães, é possével definir a seguinte hierarquia:
pXmin &amp;lt;vale &amp;lt;y &amp;lt;pico &amp;lt;pXmax	(5.10)
onde y = valemin +representa o limite entre pico e vale.
Os seguintes critéerios sãao usados para mesclar ou desprezar pontos de corte:
vale+pico
2
&gt;	a', então mescle por
&gt;	a', então despreze o corte de pico
&gt;	a', entao despreze o corte de vale caso 3 entao mescle
caso	1:	se
caso	2:	se
caso	3:	se
caso	4:	se
p—1 (vale) p—1 (pico} p—1 (pico)
xmax xmin p—1 (vale) caso 2 e

por vale+pico por 2
(5.11) A característica da RB de representar o conhecimento de forma explácita cria uma preocupacão quanto ao nome das classes em X *, que devem ser intuitivas e expressar suas propriedades. Dessa forma, os nomes das classes foram escolhidos levando em consideração a Equação 5.11.
O cálculo da relevôncia dos cortes pico e vale no método DPV á feito de forma paramáetrica e áe necessáario a definiçcãao de alguns paraômetros, como: a variáavel a ser discretizada, o valor do coeficiente de relevôancia a, e os dois pontos de corte respeitando a hierarquia da Equaçao 5.5. Esses parôametros sãao definidos como entrada para o cáalculo.
Apos a definicçaão dos paraêmetros de entrada, e aplicada a correçcaão do valor alpha pela Equaçao 5.5 e são calculados os valores de caso1, caso2 e caso3 atraves da Equação 5.11. Os valores calculados deter-minarãao a quantidade de classes para a discretizaçcãao e o rotulo das mesmas.
O fluxo geral do calculo da relevêancia dos cortes e sua respectiva discretizaçao pelo metodo DPV e expresso no Algoritmo 3.
Algoritmo 3 Relevôncia dos cortes de pico e vale e discretizaçao
1: v a variavel quantitativa a ser discretizada
2: a algum coeficiente de relevancia a, (0 &amp;lt;a &amp;lt;1)
3: vale algum percentil de acordo com a Equação 5.10
4: pico algum percentil de acordo com a Equacão 5.10
5: a' correcao do a (Equaçao 5.5)
6: casol
&lt;-
p 1(vale) p-1 (pico) p—1 (pico)
xmax xmin p-1 (vale)
&gt; a' or ( caso2 &gt; a'
7: caso2
8: caso3
9: if casol
10:	discretize v usando “baixo” e
11: else if caso2 &gt; a1 then
and caso3 &gt; a' ) then “alto” (2 classes)

12:	discretize v usando “baixo” e “medio” (2 classes)
13: else if caso3 &gt; a' then
14:	discretize v usando “medio” e “alto” (2 classes)
15: else
16:	discretize v usando “baixo”, “medio” e “alto” (3 classes)
17: end if
18: return v discretizada (v*)
5.1.1	Exemplo de Aplicaçao
Imagine uma situação onde xmin = 10, xmax = 12 e a = 0.8. Caso fosse aplicado o coeficiente de relevêancia sem efetuar a correçcãao (a = a'), o menor valor possível para o vale ser considerado relevante, pela Equacçãao 5.8, e:
p-1(valemin) = .^min = 10) = 12.5	(5.12)
(a' = a = 0.8)
De forma analoga, o maior valor possível para o pico ser consi-
derado relevante, pela Equacao 5.9, é:
p í(picomax) = (xmax = 12) • (a' = a = 0.8) = 9.6
(5.13)
Esses valores geram uma contradicão, pois nunca seria aceito como relevante nenhum corte de vale ou de pico.
Para realizar a correcão em a e necessario calcular o ô pela Equacao 5.6 e aplicar a correção do coeficiente de relevância pela Equação 5.5:
(5.14)
a' = ((1 - (ô = 0.8334)) • (a = 0.8)) + (ô = 0.8334) = 0.96668 (5.15)
E por consequencia, os valores de p-1(valemin) e p-1 (picomax) sãao alterados pelas Equacçãoes 5.8 e 5.9:
(5.16)
p 1(picomax) = (xmax = 12) • (a' = 0.96668) = 11, 6001	(5.17)
Apos definido os pontos de corte e necessario analisar a relevância dos mesmos (Equaçcãao 5.11).
Vamos supor que o algoritmo tenha definido os pontos de corte, e em uma variavel Vi tenha sido encontrado o valor de vale = 20. Ou seja, o corte de vale encontra-se no percentil 20. Vamos supor ainda, que por interpolacão linear fosse encontrada p-1(vale) = 10.3. Da mesma forma para o corte de pico, imagine que pico = 98 e p-1(pico) = 11.9 Observe que nesse caso o corte de pico seria desprezado: o unico corte valido seria o de vale.
Caso vale = 10, p-1(vale) = 10.15, pico = 80 e p-1(pico) = 11.5, seria desprezado o corte de vale.
Caso os valores estejam próximos, por exemplo,vale = 48, p-1(vale) = 10.98, pico = 52 e p-1(pico) = 11, nesse caso os dois cortes são unidos e e criado um novo corte pela Equacão 5.11:
(vale = 48) + (pico = 52)
--------------------------= 50
(5.18)
e seu percentil estabelecido por interpolaçao linear. Nesse caso, corte = 50 e p-1(corte) = 10.99.
Se o valor de vale for muito baixo e o de pico muito alto simultaneamente áe feito um novo corte de forma similar ao exemplo anterior. Nunca ocorreráa um caso em que o corte de vale seja muito alto ou que o pico seja muito baixo, pois esses cortes obedecem a hierarquia estabelecida na Equaçcao 5.10.
5.2 O PROBLEMA DE OTIMIZACÇAO
Os seguintes conceitos sao definidos:
•	vout como a variável de saída emV;
•	V* = v*,... ,vn como o vetor de todas as variáveis discretizadas em V: originalmente qualitativas ou discretizadas pelo DPV;
•	v*out como a variável de saída em V*;
•	X = x1,... ,xn comos os valores previstos de v0ut pela RB;
•	X1 = xI,... ,xi como os valores previstos corretamente de vout pela RB;
•	X0 = x0,... ,xn como os valores previstos incorretamente de v*out pela RB;
•
n
ev(x)	beliefi • pontomedioi	(5.19)
i=1
como uma funçcao que retorna o valor quantitativo esperado de uma classe em vout, baseado nas probabilidades da rede (beliefs ) e em uma lista com os nuámeros reais que representam cada classe de vout. A lista de nuámeros reais áe criada atraváes dos pontos máedios de cada classe de vout comparados com vout.
A discretizacao de uma variavel v no DPV depende dos pontos de corte pico e vale, aláem de um coeficiente de relevêancia práe-definido (a). Entretanto, a distribuiçao de probabilidade em v influencia o processo de inferêencia de toda a RB (Equaçcao 3.1).
Portanto, é necessário discretizar todas as variáveis simultaneamente, o que gera um Problema de Otimizaçao Global (HORST; RO-MEIJN, 2002), ou seja, encontrar o melhor conjunto de condicoes aceitáveis para atingir um objetivo formulado por termos matemáticos.
Nesta dissertacao, a funçao objetivo consiste em discretizar todas as variaáveis do conjunto de dados, de forma que o erro de previsao da variável de saéda seja o menor possével.
Assumindo que vout pode tanto ser quantitativa quanto qualitativa, duas funcoes objetivo diferentes podem ser usadas. Se vout for qualitativa,
encontre V* = maxacuracia(v*ut)	(5.20)
onde
acuracia(v*ut) =
|X0| + ¡X1!
(5.21)
Porém, se vout for quantitativa, a função objetivo é dada pela mi-nimização do erro NRMSE (normalized root mean square error), dado por
encontre V* = minNRMSE(vout)	(5.22)
onde
NRMSE(vout) =
100	n ZXi(x - ev(£i)')‘2
xmax
xmin
(5.23)
O erro NRMSE é calculado a partir do erro RMSE, que é considerado uma boa medida de desempenho embora seja dependente de escala. A normalizaçcao do erro traz a vantagem de independente de escala e a possível comparaçao entre diferentes bases de dados (HYNDMAN; KOEHLER, 2006).
A execuçao do método DPV segue o fluxo geral de execucão do Algoritmo Genético (Figura 5). Porém, é necessário definir o valor de a (o mesmo para toda a execucão), as variáveis V do domínio e a variável de saéda vout.
Apás a definição de parâmetros, o algoritmo segue o fluxo do AG, com a criaçao randâmica de indivéduos, a avaliação da população atraves da funcão fitness, a selecão, o crossover e a mutacão.
Para cada indivíduo da população, é feita a discretização de todas as variéveis quantitativas (utilizando o Algoritmo 3), cria-se uma RB utilizando as variéveis discretizadas e as qualitativas do tipo naive Bayes e calcula-se o valor de fitness seja pela acurécia (vout qualitativa) ou pelo erro (vout quantitativa). Caso vout seja quantivativa, o DPV procura minimizar o fitness (erro) e caso vout seja qualitativa, o DPV procura maximizar o fitness (acurérica).
O resultado do método é aquele considerado o melhor indivíduo da execucão, ou seja, com o melhor fitness. Portanto são retornados os pontos de corte para cada variéavel e a RB criada atravées desses pontos de corte.
O algoritmo expresso em Algoritmo 4 mostra o fluxo de trabalho que satisfaz as funçães objetivo (Equaçães (5.20) e (5.22)), utilizando a tecnica de Algoritmos Geneticos (AG).
Algoritmo 4 Método de Discretização pico e vale via AG
1: a algum coeficiente de relevancia a, (0 &amp;lt;a &amp;lt;1)
2: V variéveis de algum doménio de aplicacao
3: vout variével de saéda em V
4: P = indi,..., indn	o vetor de indivéduos randomicos contendo
os cortes de pico e vale para cada variével quantitativa em V (po-pulacçãao)
5: while não encontrou soluçao do
6:	for all indi in P do
7:	discretize todas as variéveis quantitativas (Algoritmo 3)
8:	RBi uma RB com todas as variéveis - qualitativas e quan-
titativas apos discretizacão - topologia Naive Bayes
9:	if vout is qualitativa then
10:	fitnessi	acuracia(vout) (Equ (5.20))
11:	else
12:	fitness^	NRMSE(vout) (Equation (5.22))
13:	end if
14:	end for
15:	seleçcãao()
16:	crossover()
17:	mutacçãao()
18: end while
19: return o melhor indi em P (aquele com o melhor fitness) e RBi (a RB criada por esse indivéduo)
A complexidade computacional do metodo DPV, assim como o fluxo geral do algoritmo, e semelhante a do Algoritmo Genetico classico, que, em uma populacão de n indivíduos possui complexidade de O(nlog n) * O(fitness) para à convergência do algoritmo (GOLDBERG, 1989).
A função fitness no DPV utiliza dois metodos fundamentais: a discretizacão de todas as variaveis e a propria inferência Bayesiana. A funcao de discretizacao possui a complexidade de O(k * m) onde k e a quantidade de variaveis contínuas e m e a quantidade de registros em cada variável. Portanto a complexidade geral do metodo DPV, e dada pela formula:
O(nlog n) * [O(k * m) + O(inference)]	(5.24)
Sendo que O(inference) depende do algoritmo de inferência Bayesiana utilizado, que e considerado um problema do tipo NP-hard (COOPER, 1990). O algoritmo utilizado nesse trabalho foi implementado no shell Netica1 da Norsys Software Corp e utiliza técnicas do tipo “join tree” (SPIEGELHALTER et al., 1993).
1Disponível em: http://wwww.norsys.com/netica.html
60
6	RESULTADOS E DISCUSSÃO
Para avaliar a performance do DPV duas situaçcãoes foram testadas: quando a base de dados tem uma saída qualitativa (Secao 6.1) e quando a variavel de saída í quantitativa (Secão 6.2).
Quando a variível de saída é qualitativa, o objetivo do algoritmo íe realizar a classificaçcãao da variíavel estimando a probabilidade de cada uma de suas classes. Portanto, a funcçãao objetivo do problema de otimizacçãao estía em maximizar a acuríacia (classificaçcaão correta).
Quando a variíavel de saída íe quantitativa o objetivo do algoritmo vai alíem da classificaçcãao: íe necessíario que a míedia estimada pelo vetor de probabilidade reflita o comportamento da variavel. Portanto, nesse caso, a funçcãao objetivo estaí relacionada com a minimaçcãao do erro (NRMSE) entre a míedia estimada e o valor real de cada registro.
O DPV e um mítodo de Aprendizagem Supervisionada (MITCHELL, 1997) e os dados sao divididos em dois conjuntos: treinamento e teste.
O coeficiente de relevancia a adotado neste trabalho foi de 0.8. Esse valor foi escolhido apíos uma busca por coeficientes melhores adaptados aos problemas apresentados.
Os resultados obtidos foram comparados com dois míetodos de discretizacão para Redes Bayesianas: EWD e EFD.
6.1	SAÍDA QUALITATIVA - BASE DE DADOS
Para representar os casos em que a variíavel de saída íe qualitativa, duas base de dados foram usadas: Iris Flower (Seção 6.1.1) e Wine (Seção 6.1.2) e foram separadas randomicamente de forma que (0.5n) dos dados pertencessem ao conjunto de treinamento e (0.5n) dos dados ao conjunto de teste.
A variaível de saída em ambas as bases dados íe chamada de class (tipo de flor iris ou de vinho).
6.1.1	O Problema Iris Flower
Publicado por Fisher em 1936 (FISHER, 1936), a base de dados Iris Flower í uma das mais populares na literatura especializada em reconhecimento de padrãoes. Existem 150 registros nessa base, que foi
adquirida através do repositório público UCI.
A base de dados possui três classes de 50 instancias cada, onde cada classe refere ao tipo de planta Iris: Iris-virginica, Iris-versicolor e Iris-setosa. Os parêmetros de entrada possuem valores quantitativos, chamados: sepal length (comprimento da sepala), sepal width (largura da sepala), petal length (comprimento da petala) e petal width (largura da petala).
Foram treinadas três Redes Bayesianas atraves do conjunto do treinamento com a topologia Naive Bayes. A distribuição dos dados entre os conjuntos de treinamento e teste foi feita de forma randêomica e estratificada, ou seja, a quantidade de dados de cada classe e a mesma em ambos os conjuntos. Portanto, cada tipo de flor Iris possui 25 registros no conjunto de treinamento e 25 registros no conjunto de teste.
A RBs treinadas da base Iris Flower podem ser vistas na Figura 9 (DPV), Figura 10 (EFD) e Figura 11 (EWD).
Figura 9 - RB treinada pelo DPV para o Problema Iris Flower.
plength				class			
baixo	34.5			iris set	33.3	i	
medio	33.3			iris versi	33.3		
alto	32 1			iris virgi	33.3		
«width		
baixo	4Q.5	
medio	32.1	
alto	27.4	

p width		
baixo	34.5	
medio	33.3	
alto	32.1	
«length		
baixo	35.7	
medio	35.7	
alto	28.6	
Figura 10 - RB treinada pelo EFD para o Problema Iris Flower.
plength			class					pwidth		
baixo	3.57 medio	13.1 alto	83.3	■: : :		iris set	33.3 iris versi	33.3 iris virgi	33.3					baixo	8.33 medio	21.4 alto	7Q.2	1 :	
										

swidth		
baixo	7.14	
medio	11.9	i:	:	:
alto	81.0	
slength		
baixo	3.57	
medio	13.1	■: : :
alto	83.3	
Figura 11 - RB treinada pelo EWD para o Problema Iris Flower.
6.1.2	O Problema Wine
A base de dados Wine tambem é muito popular na literatura de reconhecimento de padrões. Ha 178 registros nessa base, também adquirida através do repositério UCI.
Essa base de dados possui três classes, onde cada uma se refere a um tipo de vinho cultivado na mesma regiõao da Itéalia, mas com diferentes características: a classe 1 possui 59 registros, a classe 2 possui 71 registros e a classe 3 possui 48 registros.
As variéveis de entrada possuem valores quantitativos, chamados: alcohol (élcool), malic acid (écido mélico), ash (cinza), alkalinity of ash (alcalinidade das cinzas), magnesium (magnésio), total phenols (fenóis totais), flavonoids (flavonoides), non-flavonoid phenols (fenóis nõo flavonéides), pro-anthocyanins (pré-antocianinas), color intensity (intensidade de cor), hue (tonalidade), OD280/OD315 of diluted wines (OD280/OD315 de vinhos diluédos) e proline (prolina).
De forma similar ao problema Iris Flower, foram treinadas trêes Redes Bayesianas utilizando a topologia Naive Bayes. Embora a dis-tribuiçõo de dados tenha sido feita de forma randêmica e estratificada, algumas classes de vinho possuem uma quantidade émpar de registros. Portanto, a distribuiçcõao de dados adotada foi a seguinte:
•	winel:	30	registros	no	treinamento,	29	registros	no	teste
•	wine2:	35	registros	no	treinamento,	36	registros	no	teste
•	wine3:	24	registros	no	treinamento,	24	registros	no	teste
A RBs treinadas da base Wine podem ser vistas na Figura 12 (DPV), Figura 13 (EFD) e Figura 14 (EWD).
color
od280
alcohol
macid
winel wine2 wine3
baixo medio alto
baixo medio alto
baixo alto
baixo medio alto
proliine
magnesium
proant	
baixo	8.16 medio	73.4 alto	18.4	—
nonflava	
baixo	29.6 medio	34.8 alto	35.6	-
flava	
baixo 51.5 alto	48.5	
total phe	
baixo	18.2 medio	55.2 alto	28 6	
Figura 12 - RB treinada pelo DPV para o Problema Wine.
color
od280
alcohol
macid
winel wine2 wine3
baixo medio alto
baixo medio alto
baixo medio alto
baixo medio alto
baixo medio alto
proliine
magnesium
proant	
baixo	33.5 M
medio	34.8	:
alto	31.7 ■■	:
nonflava	
baixo	37.8
medio	31.7 Hi : :
alto	30.5 ■■
flava	
baixo	33.4 M
medio	32.9 M :
alto	33 8 ■■	:
total phe	
baixo	33.5 ■■
medio	32.8 M :
high	33.8 ■■
Figura 13 - RB treinada pelo EFD para o Problema Wine.
color
odíñO
alcohol
macid
wine! wine2 wine3
baixo medio alto
baixo medio alto
baixo medio alto
baixo medio alto
baixo medio alto
baixo medio alto
baixo medio alto
baixo medio alto
baixo medio alto
7 18 I ; ; ;
9 24 i : : :
proliine
magnesium
totalphe
Figura 14 - RB treinada pelo EWD para o Problema Wine.
proant	
baixo	4.09 medio	3.06 alto	92.9	
nontlava	
baixo	4.09 medio	3.06 alto	92.9	
	
6.1.3	Resultados e Comparaçao
Os dados foram separados de forma estratificada e randêomica nos conjuntos de treinamento e teste para ambas as bases que possuem variavel de saída qualitativa. No conjunto de treinamento foram aplicados os metodos EWD, EFD e DPV para a geraçcãao das Redes Baye-sianas. O conjunto de teste foi utilizado para confrontar a acuracia das redes ao se depararem com dados desconhecidos.
A matriz de classificaçcãao (confusãao) para o Problema Iris Flower e mostrada na Tabela 3 e a matriz de classificacçãao para o Problema Wine e mostrada na Tabela 4.
6.1.4	Discussão
Dois exemplos de base de dados para variaveis de saída qualitativas foram utilizados: Iris Flower e Wine. A base Iris Flower possui três classes de saída, cada uma representando um tipo de flor (Iris-setosa, Iris-virginica e Iris-versicolor). Uma classe de saída (Iris-setosa) e linearmente separavel das outras duas contribuindo para esse ser considerado um domínio simples.
Tabela 3 - Matriz de classificacão para o problema Iris Flower.
Mótodo	Real	Previsto setosa versi. virgi.			Total	Acurócia
Treinamento						
EWD	setosa	21	0	4	25	65.33%
	versi.	0	6	19	25	
	virgi.	0	3	22	25	
EFD	setosa	25	0	0	25	98.66%
	versi.	0	24	1	25	
	virgi.	0	2	23	25	
DPV	setosa	25	0	0	25	98.66%
	versi.	0	24	1	25	
	virgi.	0	0	25	25	
Teste						
EWD	setosa	17	0	8	25	62.66%
	versi.	0	7	18	25	
	virgi.	0	2	23	25	
EFD	setosa	25	0	0	25	93.33%
	versi.	0	24	1	25	
	virgi.	0	4	21	25	
DPV	setosa	25	0	0	25	96%
	versi.	0	24	1	25	
	virgi.	0	2	23	25	
A base Wine também é considerada um domínio simples e possui estruturas de classes “bem-comportadas”, sendo recomendada para métodos novos de classificacão principalmente por sua alta dimensionalidade atraves de 13 variéveis de entrada.
Essas características em ambas as bases de dados asseguram uma alta acurécia na classificaçao. Nessas bases de dados observou-se uma acurécia inferior na classificacão quando o método EWD foi usado e uma boa acurécia tanto com o método DPV quanto com o método EFD.
O problema Iris Flower em particular, mostra que os metodos
Tabela 4 - Matriz de classificação para o problema Wine.
Metodo	Real	Previsto wine1 wine2 wine3			Total	Acurácia
Treinamento						
EWD	wine1	27	0	3	30	74.16%
	wine2	9	20	16	35	
	wine3	3	2	19	24	
EFD	wine1	30	0	0	30	98.88%
	wine2	0	34	1	35	
	wine3	0	0	24	24	
DPV	wine1	30	1	0	30	100%
	wine2	0	35	0	35	
	wine3	0	0	24	24	
Teste						
EWD	wine1	28	0	1	29	82.02%
	wine2	4	27	5	36	
	wine3	6	0	18	24	
EFD	wine1	28	1	0	29	94.38%
	wine2	1	33	2	36	
	wine3	0	1	23	24	
DPV	wine1	28	1	0	29	94.38%
	wine2	4	32	0	36	
	wine3	0	0	24	24	
DPV e EFD classificaram sem erros o tipo de flor “setosa” (linearmente separável) e obtiveram alguns erros ao classificar os tipos “virginica” e “versicolor”. Entretanto, o metodo DPV possui uma melhor acuracia com o tipo “virginica”, o que indica uma melhor separaçcaão de classes com esse metodo.
Nas RBs treinadas para essa base de dados (Figuras 9, 10 e 11) e possível observar uma distribuição de probabilidade diferente em cada uma das redes. Na RB treinada pelo DPV nãao ha um padrãao de distribuiçcãao e ele varia de acordo com cada variavel, na RB treinada pelo EFD as distribuiçcãoes tendem a ser iguais em todas as variaveis.
E, na RB treinada pelo metodo EWD as variáveis tendem para uma assimáetrica na distribuicçaão de probabilidade, entretanto a distribuiçcaão áe muito semelhante em todas as variáaveis.
No problema Wine, ambos os mátodos DPV e EFD obtiveram alguns erros ao classificarem o tipo Wine2. O metodo EFD tambám possui classificacões incorretas em relacão à classe Wine3. Embora a acurácia dos dois metodos seja a mesma, o metodo DPV separa as classes Wine2 e Wine3 com uma maior eficiencia que o mátodo EFD e nãao háa casos de classificaçcaão errada de uma classe como outra.
Quando observadas as RBs treinadas para a base Wine (Figuras 12, 13 e 14) tambem á possável notar diferentes distribuiçães de probabilidade em cada uma das redes. Na rede treinada pelo DPV cada variável possui sua distribuicão particular e duas variáveis, proliine e ash, foram discretizadas em duas classes. A RB treinada pelo EFD possui uma distribuiçcaão de probabilidade com frequôencias iguais. A rede treinada pelo máetodo EWD mostrou um padrãao em quase todas as variáveis: mais de 75% dos dados foram considerados de uma classe especáfica.
6.2 SAróA QUANTITATIVA - BASE DE DADOS
Para representar o caso em que a variáavel de saáda áe quantitativa, foi utilizada uma base de dados de Taxa de Penetraçao da broca (Rate of Penetration - ROP) em poços de petráleo sendo randomicamente separada de forma que (0.7n) dos registros pertencessem ao conjunto de treinamento e (0.3n) ao conjunto de teste. A variavel de saáda á chamada de “ROP”.
6.2.1	Problema da Taxa de Penetraçao da Broca (ROP)
O problema da Taxa de Penetração da Broca (ROP) é característico de ambientes com grande complexidade e risco que procuram otimizar o custo da perfuraçcãao de poçcos. A minimizacçãao desses custos estáa diretamente relacionada com a maximizaçcãao da ROP.
Para reduzir os custos, áe necessaário planejar de forma correta as operaçcoães de perfuraçcaão. O tempo necessaário para perfurar um poçco tem que ser estimado com uma alta precisaão, pois a maior parte dos custos saão associados ao aluguel dos equipamentos necessáarios para essa operação (GANDELMAN, 2012).
Entretanto, cada operação possui propriedades únicas que tornam essa tarefa extremamente complexa. Muitas dessas propriedades variam durante a perfuração, como o tipo de rocha, a porosidade da rocha, a presenca de gás, a pressao, a taxa de desgaste da broca, entre outras. Todas essas propriedades afetam a ROP, assim como outros muitos parâmetros que são controlados pelo operador de broca.
A base de dados utilizada possui 277 registros sobre um tipo especifico de broca de perfuracão, usando o valor da ROP de forma quantitativa (m/s).
As variaáveis de entrada que possuem valores quantitativos, sãao chamadas: RPM (Revoluçães por Minuto), PSB (Peso sobre a Broca), HSI (Potencia Hidráulica por Polegada Quadrada) e metros acumulados. As trâes primeiras variaáveis sãao intránsecas ao processo de per-furaçao e a ultima (metros acumulados) possui um comportamento acumulativo e linear trazendo informacçãao sobre o desgaste da broca.
Existe ainda uma variáavel de entrada qualitativa, práe-discretizada por especialistas no dominio: UCS (Unconfined Compressive Strength - Resistância à Compressao Simples) relacionada à geologia do solo.
A RBs foram treinadas utilizando a topologia Naive Bayes e podem ser vistas na Figura 15 (DPV), Figura 16 (EFD) e Figura 17 (EWD).
Figura 15 - RB treinada pelo DPV para o Problema da ROP.
HSI	
baixo	4 93 medio	75.3 alto	19.6	
RPM		
baixo	78.5	
alto	21.5	■ !
PSB		
baixo	117.3	■ :
medio	43.8	
alto	39.0	
UCS		
mole media	3.89	
media	376	
medio dura	53.2	
duro	5.36	■
metro8 ac u mu lados		
baixo	14.2 medio	58.1 alto	27.7	UM	
6.2.2	Resultados e Comparaçao
Foram criadas três Redes Bayesianas, cada uma relacionada à um método de discretização. As RBs criadas no caso de saída quantitativa tem como objetivo a estimacao do valor médio da variével. Portanto,
Figura 16 - RB treinada pelo EFD para o Problema da ROP.
Figura 17 - RB treinada pelo EWD para o Problema da ROP.
HSI		
baixo	2.71	
medio	2.25	
alto	95 0	
		
ROP				
baixo	7.11 medio	6.09 alto	86.8				
9.26 ±3	.2			
UCS		
mole media	3.72	
media	38.2	
media dura	52.9	
dura	5.22	
RPM		
baixo	32.4	
medio	49.9	
alto	17.6	
PSB		
baixo	23.7	
medio	30.7	
alto	45.6	
metros acii mu lados		
baixo	62.0	
medio	36.6	
alto	1.38	
cada classe da variável de saída possui pontos mádios ligados à ela (Tabela 5).
Com os pontos medios de cada classe á possável estimar o valor da variavel de saáda, pela Equação 5.19. Um exemplo de entrada á mostrado na Figura 18, utilizando a rede DPV. Para isso são instanciadas as evidâncias de entrada (HSI = mádio, RPM = baixo, PSB = alto, metros acumulados = medio e UCS = media). O valor estimado de saáda nesse caso é de 6.02 com desvio padrào de 4.
O conjunto de treinamento foi utilizado para a criaçcãao das RBs e a estimaçcãao dos valores pela rede em comparaçcãao com os valores reais podem ser vistos nas Figura 19 (DPV), Figura 20 (EFD) e Figura 21 (EWD).
A validaçcãao das redes foi feita com o auxilio do conjunto de teste e os gráaficos com os valores reais e valores estimados podem ser vistos nas Figura 22 (DPV), Figura 23 (EFD) e Figura 24 (EWD).
Para avaliar o desempenho das mesmas, foi considerado o erro
Tabela 5 - Classes e Pontos Mádios para o problema ROP.
Máetodo	Classe	Ponto Máedio
EWD	baixo	0.645
	medio	1.32
	alto	10.52
EFD	baixo	1.247
	medio	3.106
	alto	11.71
DPV	baixo	1.48
	medio	4.27
	alto	12.68
Figura 18 - Exemplo de entrada e estimação de valor para o problema ROP.
entre o valor máedio previsto e o valor real da variaável. O erro adotado nesse trabalho á o NRMSE e o NRMSE(vout) obtido para cada abordagem pode ser visto na Tabela 6.
6.2.3	Discussão
Para ilustrar as situaçcãoes onde a variáavel de saáda áe quantitativa foi utilizada uma base de dados real sobre a ROP da broca em poços de petróleo. A ROP á derivada do processo de perfuraçao sob a influencia
Figura 19 - Valores estimados de ROP no metodo DPV (treinamento).
Figura 20 - Valores estimados de ROP no metodo EFD (treinamento).
Tabela 6 - NRMSE obtido para o Problema da ROP.
Conjunto	EWD	EFD	DPV
Treinamento	32.45%	19.61%	12.18%
Teste	51.39%	46.51%	16.09%
de varios fatores, como: o equipamento dos operadores, a geologia e os sensores de medida. Portanto, os dados não são sempre confiaveis e a o domínio de aplicação e considerado complexo.
O comportamento das distribuiçães de probabilidade nas redes treinadas (Figuras 15, 16 e 17) foi semelhante ao comportamento das
Figura 21 - Valores estimados de ROP no método EWD (treinamento).
Figura 22 - Valores estimados de ROP no metodo DPV (teste).
bases Iris Flower e Wine. O metodo DPV obteve distribuições diferentes para cada variavel e a variavel RPM foi discretizada em duas classes, o metodo EFD obteve frequências aproximadamente iguais e o metodo EWD teve tendência para a assimetria na maioria dos casos.
Na Tabela 5 e mostrado os pontos medios de cada variavel e ha uma certa semelhança nos valores dos metodos DPV e EFD. Entretanto, a media dos dados e diferente devido ao vetor de probabilidades do nodo de saída.
Pelas Figuras 15, 16 e 17 e possível observar que a media da ROP para essa distribuyo e de 4.32 com desvio padrâo de 3.64 na rede Bayesiana criada pelo metodo DPV, 5.37 com desvio padrõo de 4.57 na RB criada pelo metodo EFD e 9.26 com desvio padrõo de 3.23 na RB criada a partir do metodo EWD.
Figura 23 - Valores estimados de ROP no método EFD (teste).
Figura 24 - Valores estimados de ROP no metodo EWD (teste).
Tanto pelos valores medios da rede quanto pela Tabela 5 nota-se uma maior similaridade entre os valores dos metodos EFD e DPV, de forma similar aos casos em que a variavel de saída e qualitativa.
Em relação ao NRMSE na base da ROP, o metodo DPV mostra um erro inferior tanto no conjunto de treinamento quanto no conjunto de teste mostrando sua capacidade de generalização (Tabela 6). No conjunto de treinamento o metodo DPV possui um erro 37% inferior ao erro do metodo EFD e 62% inferior ao metodo EWD. Quando analisado o conjunto de teste, a diferença e ainda mais evidente: o DPV possui um erro 65% inferior ao metodo EFD e 68% inferior ao metodo EWD.
Nas Figuras 19, 20 e 21 sao mostrados os graficos dos valores estimados por cada um dos metodos no conjunto de treinamento. Ao
75 observar a aderência da curva de valores estimados a curva de valores reais, nota-se uma maior aderêencia do DPV, uma aderêencia míedia do míetodo EFD e uma curva superestimada no míetodo EWD.
Essas diferencças sãao mais ainda acentuadas nas Figuras 22, 23 e 24 que mostram o comportamento dos valores estimados para o conjunto de teste. Nota-se uma aderêencia maior do míetodo DPV e tanto o míetodo EFD quanto o míetodo EWD tendem a superestimar a curva. Esses resultados reforçcam a boa capacidade de generalizaçcãao do DPV.
76
7	CONSIDERACOES FINAIS
Esse trabalho apresentou um metodo de discretizaçcãao para Redes Bayesianas utilizando Algoritmo Genetico para a obtenção de dois pontos de corte que identificam eventos de vale (“baixo”), eventos de pico (“alto”) e eventos intermediarios (“medio”).
O metodo foi aplicado em trêes bases de dados diferentes: duas com saída qualitativa tirados do repositório UCI e uma com saída quantitativa de um domínio real com dados de perfuração de poços de petríoleo.
O míetodo DPV realiza a discretizaçcãao com foco no desempenho geral da RB e nãao apenas na distribuicçãao de frequêencia de cada variíavel. Ao observar as RBs geradas nota-se que a distribuiçcãao de probabilidade de cada classe (“baixo”,”míedio” e “alto”) pode tender para simetria ou para a assimetria, o que reforçca a adequaçcaão do míetodo para performance geral da rede. Ou seja, nãao hía um padrãao príe-estabelecido de pontos de corte: os valores mudam de acordo com os dados.
Nos casos em que a variível de saída í qualitativa, (classificação) observou-se uma alta acuraícia tanto no conjunto de treinamento quanto no conjunto de teste, mostrando uma adaptacçãao do míetodo para esse tipo de problema. Tambíem observou-se que o míetodo DPV teve um desempenho superior aos outros mítodos utilizados (EFD e EWD) na separaçcãao e delimitaçcãao de cada classe e um maior potencial de gene-ralizaçcãao.
Quando a variíavel de saída íe quantitativa, o objetivo do míetodo nãao íe a classificaçcãao da variíavel e sim diminuir o erro ao estimar o valor míedio. O comportamento da curva de valores estimados pelo DPV conseguiu ter uma boa aderência a curva dos dados reais e não houve uma grande discrepêancia entre os erros dos dados de treinamento e dos dados de teste. Em ambos os conjuntos de dados o comportamento do mítodo DPV foi superior ao dos mítodos EFD e EWD.
Embora o míetodo DPV tenha obtido uma baixa taxa de erro na estimaçcãao dos valores míedios, ele naão foi capaz de estimar com precisaão valores extremos (muito altos ou muito baixos). Esse resultado era esperado pela característica das Redes Bayesianas em fornecer um vetor com a distribuiçcãao de probabilidade da variíavel, e, nesse caso em estimar o valor míedio da distribuiçcãao.
Ao analisar os resultados, observa-se um bom comportamento do míetodo proposto tanto nos casos em que a variíavel de saída íe qualitativa quanto nos casos em que ela íe quantitativa. A utilizaçcãao do
Algoritmo Genético para tratar o problema de otimização global se mostrou eficiente e atendeu aos requisitos do problema.
Apesar dos resultados promissores apresentados, esse trabalho possui algumas limitaçcãoes, como a utilizaçcaão de uma uénica topologia de rede (Naive Bayes) e a falta de transicão entre as classes (“baixo”,”medio” e “alto”).
Outro ponto relevante está na complexidade computacional do DPV: ela é superior à dos metodos EWD e EFD e esté diretamente ligada à quantidade de registros do doménio e de variéveis, além da busca via AG e da inferância Bayesiana (problema do tipo NP-hard). Essas caracterésticas podem tornar o méetodo proibitivo em alguns doménios de aplicaçcãao.
Considerando as limitacães apresentadas e tendo como objetivo a melhoria do méetodo proposto, ée possével citar os seguintes trabalhos futuros:
•	A utilizaçao de técnicas hébridas para a suavizar a transição entre as classes, como por exemplo a técnica Fuzzy-Bayes (BRIGNOLI, 2013) ou a adição de mais pontos de corte no algoritmo;
•	A utilização de outras topologias de rede além da Naive Bayes apresentada e a combinação do método a um método de aprendizagem de estrutura de rede;
•	Estudo de desempenho do uso do Algoritmo Genéetico no méetodo e a sua eficéacia quando comparado com outros algoritmos de busca;
•	Aplicaçcãao do méetodo em bases com grande quantidade de dados.
•	Estudo aprofundado da complexidade do méetodo DPV.
REFERÊNCIAS
BEASLEY, D.; MARTIN, R.; BULL, D. An overview of genetic algorithms: Part 1. fundamentals. University computing, WHURR PUBLISHERS, v. 15, p. 58-58, 1993.
BRIGNOLI, J. T. Um Modelo para Suporte ao Raciocinio Diagnóstico diante da Dinâmica do Conhecimento sobre Incertezas. Tese (Doutorado) — Universidade Federal de Santa Catarina, 2013.
CANO, A.; MORAL, S.; SALMERON, A. Penniless propagation in join trees. International Journal of Intelligent Systems, Wiley Online Library, v. 15, n. 11, p. 1027-1059, 2000.
CANTU-PAZ, E. A summary of research on parallel genetic algorithms. 1995.
CATLETT, J. On changing continuous attributes into ordered discrete attributes. In: SPRINGER. Machine learning-EWSL-91. [S.l.], 1991. p. 164-178.
COOPER, G. F. The computational complexity of probabilistic inference using bayesian belief networks. Artificial intelligence, Elsevier, v. 42, n. 2, p. 393-405, 1990.
DOUGHERTY, J.; KOHAVI, R.; SAHAMI, M. Supervised and unsupervised discretization of continuous features. In: MORGAN KAUFMANN PUBLISHERS, INC. MACHINE LEARNINGINTERNATIONAL WORKSHOP THEN CONFERENCE-. [S.l.], 1995. p. 194-202.
FAYYAD, U.; IRANI, K. Multi-interval discretization of continuous-valued attributes for classification learning. 1993.
FISHER, R. A. The use of multiple measurements in taxonomic problems. Annals of eugenics, Wiley Online Library, v. 7, n. 2, p. 179-188, 1936.
FRANK, E.; WITTEN, I. H. Making better use of global discretization. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 1999.
FRIEDMAN, N.; GEIGER, D.; GOLDSZMIDT, M. Bayesian network classifiers. Machine learning, Springer, v. 29, n. 2-3, p. 131-163, 1997.
FRIEDMAN, N.; GOLDSZMIDT, M. Discretizing continuous attributes while learning bayesian networks. In: MORGAN KAUFMANN PUBLISHERS, INC. MACHINE LEARNINGINTERNATIONAL WORKSHOP THEN CONFERENCE-. [S.l.], 1996. p. 157-165.
FUNG, R. M.; CHANG, K.-C. Weighing and integrating evidence for stochastic simulation in bayesian networks. In: NORTH-HOLLAND PUBLISHING CO. Proceedings of the Fifth Annual Conference on Uncertainty in Artificial Intelligence. [S.l.], 1990. p. 209-220.
GANDELMAN, R. A. ROP prediction and real-time optimization of operational parameters on drilling offshore oil Wells. Dissertação (Mestrado) — Federal University of Rio de Janeiro, 2012.
GOLDBERG, D. E. Genetic algorithms in search, optimization, and machine learning. Addison-Wesley Professional, 1989.
HORST, R.; ROMEIJN, H. E. Handbook of global optimization. [S.l.]: Kluwer Academic Pub, 2002.
HSU, C.-N.; HUANG, H.-J.; WONG, T.-T. Why discretization works for na ve bayesian classifiers. In: Proceedings of the Seventeenth International Conference on Machine Learning, Morgan Kaufmann, San Francisco, CA. [S.l.: s.n.], 2000. p. 399-406.
HSU, C.-N.; HUANG, H.-J.; WONG, T.-T. Implications of the dirichlet assumption for discretization of continuous variables in naive bayesian classifiers. Machine Learning, Springer, v. 53, n. 3, p. 235-263, 2003.
HYNDMAN, R. J.; KOEHLER, A. B. Another look at measures of forecast accuracy. International Journal of Forecasting, v. 22, n. 4, p. 679 - 688, 2006. ISSN 0169-2070. Disponível em:&amp;lt;http://www.sciencedirect.com/science/article/pii/S0169207006000239&gt;.
JANIKOW, C. Z.; MICHALEWICZ, Z. An experimental comparison of binary and floating point representations in genetic algorithms. In: SAN DIEGO, CA. Proceedings of the fourth international conference on genetic algorithms. [S.l.], 1991. v. 31, p. 36.
JENSEN, F. V.; LAURITZEN, S. L.; OLESEN, K. G. Bayesian updating in causal probabilistic networks by local computations.
Computational statistics quarterly, v. 4, p. 269-282, 1990.
KERBER, R. Chimerge: Discretization of numeric attributes. In: AAAI PRESS. Proceedings of the tenth national conference on Artificial intelligence. [S.l.], 1992. p. 123-128.
KURTCEPHE, M.; GUVENIR, H. A. A discretization method based on maximizing the area under receiver operating characteristic curve. International Journal of Pattern Recognition and Artificial Intelligence, World Scientific, v. 27, n. 01, 2013.
LANGSETH, H. et al. Inference in hybrid bayesian networks. Reliability Engineering &amp;amp; System Safety, Elsevier, v. 94, n. 10, p. 1499-1509, 2009.
LIU, H. et al. Discretization: An enabling technique. Data mining and knowledge discovery, Springer, v. 6, n. 4, p. 393-423, 2002.
MADSEN, A. L.; JENSEN, F. V. Lazy propagation: a junction tree inference algorithm based on lazy evaluation. Artificial Intelligence, Elsevier, v. 113, n. 1, p. 203-245, 1999.
MATSUURA, J. P. Discretização para Aprendizagem Bayesiana: Aplicação no Auxílio à ValidaQão de Dados em ProteQão ao Voo. Tese (Doutorado) — Dissertação de Mestrado, Instituto Tecnológico de Aeronautica, São Jose dos Campos, 2003.
MCCALLUM, A.; NIGAM, K. et al. A comparison of event models for naive bayes text classification. In: AAAI-98 workshop on learning for text categorization. [S.l.: s.n.], 1998. v. 752, p. 41-48.
MITCHELL, T. M. Machine learning. 1997. Burr Ridge, IL: McGraw Hill, v. 45, 1997.
MORAL, S.; RUMI, R.; SALMERON, A. Mixtures of truncated exponentials in hybrid bayesian networks. In: Symbolic and Quantitative Approaches to Reasoning with Uncertainty. [S.l.]: Springer, 2001. p. 156-167.
PEARL, J. Probabilistic Reasoning in Intelligent Systems: Networks of Plausble Inference. [S.l.]: Morgan Kaufmann Pub, 1988.
PREPARATA, F. P.; SHAMOS, M. I. Computational geometry: An introduction (monographs in computer science). Monographs in Computer Science (Springer-Verlag, New York, 1985), ISBN 35)0961313, 1993.
ROUSU, J. Efficient range partitioning in classification learning. In: Department of Computer Science, University of Helsinki. [S.l.: s.n.], 2001.
SALMERON, A.; CANO, A.; MORAL, S. Importance sampling in bayesian networks using probability trees. Computational Statistics &amp;amp; Data Analysis, Elsevier, v. 34, n. 4, p. 387-413, 2000.
SHENOY, P. P.; SHAFER, G. Axioms for probability and belief-function propagation. In: Classic Works of the Dempster-S’hafer Theory of Belief Functions. [S.l.]: Springer, 2008. p. 499-528.
SPIEGELHALTER, D. J. et al. Bayesian analysis in expert systems. Statistical science, Institute of Mathematical Statistics, v. 8, n. 3, p. 219-247, 1993.
WEILE, D. S.; MICHIELSSEN, E. Genetic algorithm optimization applied to electromagnetics: A review. Antennas and Propagation, IEEE Transactions on, IEEE, v. 45, n. 3, p. 343-353, 1997.
WHITLEY, D. A genetic algorithm tutorial. Statistics and computing, Springer, v. 4, n. 2, p. 65-85, 1994.
WONG, T.-T. A hybrid discretization method for naive bayesian classifiers. Pattern Recognition, Elsevier, v. 45, n. 6, p. 2321-2325, 2012.
WRIGHT, A. H. et al. Genetic algorithms for real parameter optimization. Foundations of genetic algorithms, v. 1, p. 205-218, 1991.
YANG, Y. Discretization for Naive-Bayes Learning. [S.l.]: Monash University, 2003.
YANG, Y.; WEBB, G. I. Discretization for naive-bayes learning: managing discretization bias and variance. Machine learning, Springer, v. 74, n. 1, p. 39-74, 2009.</field>
	</doc>
</add>