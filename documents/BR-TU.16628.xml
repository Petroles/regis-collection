<?xml version="1.0" encoding="utf-8"?>
<add>
	<doc>
		<field name="docid">BR-TU.16628</field>
		<field name="filename">23293_ferro_l_dr_rcla.pdf</field>
		<field name="filetype">PDF</field>
		<field name="text">
 

UNIVERSIDADE ESTADUAL PAULISTA 

Instituto de Geociências e Ciências Exatas 

Campus de Rio Claro 

 

 

LUCIANO FERRO 

 

 

 

APLICAÇÃO DA REDE NEURAL MLP (MULTILAYER PERCEPTRON) EM 

INDÚSTRIA DE PISOS E REVESTIMENTOS DO PÓLO CERÂMICO DE SANTA 

GERTRUDES – SP 

 

 

 

Tese de Doutorado apresentada ao Instituto 

de Geociências e Ciências Exatas do 

Campus de Rio Claro, da Universidade 

Estadual Paulista Júlio de Mesquita Filho, 

como parte dos requisitos para obtenção do 

título de  Doutor em Geociências e Meio  

Ambiente 

 

 

 

Orientador: Prof. Dr. José Ricardo Sturaro  

 

Rio Claro – SP 

2013



 

Ferro, Luciano
     Aplicação da rede neural MLP (Multilayer Perceptron) em
indústria de pisos e revestimentos do pólo cerâmico de Santa
Gertrudes - SP / Luciano Ferro. - Rio Claro, 2013
     143 f. : il., figs., gráfs., tabs.

     Tese (doutorado) - Universidade Estadual Paulista,
Instituto de Geociências e Ciências Exatas
     Orientador: José Ricardo Sturaro

     1. Redes neurais (Computação). 2. Redes neurais
artificiais. 3. Rede MLP. 4. Argila. 5. Variáveis físicas. I.
Título.

 
006.32
F395a

	 Ficha Catalográfica elaborada pela STATI - Biblioteca da UNESP
Campus de Rio Claro/SP



 

COMISSÃO EXAMINADORA 

 

 

Dr. JOSÉ RICARDO STURARO 

IGCE/UNESP/Rio Claro (SP) 

 

Dr. PAULO MILTON BARBOSA LANDIM 

IGCE/UNESP/Rio Claro (SP) 

 

Dr. RICARDO EGYDIO DE CARVALHO 

IGCE/UNESP/Rio Claro (SP) 

 

Dr. ALESSANDRO FIRMIANO DE JESUS 

Divisão de Ensino/Academia da Força Aérea/Pirassununga (SP) 

 

Dr. ALEXANDRE CAMPANE VIDAL 

IG/UNICAMP/Campinas (SP) 

 

 

 

Doutorando: LUCIANO FERRO 

 

RESULTADO: APROVADO 

 

Data da Defesa: 25/04/2013 

 



 

 

 

 

 

 

 

DEDICATÓRIA 

 

 

 

 

 

 

 

 

 

 

 

 

Aos meus familiares e, em 

especial, à Maria, ao Gastone e 

Violetta (in memoriam), e Claudia, 

Rodrigo, Giovanna e Gustavo.



AGRADECIMENTOS 

 

Ao Prof. Dr. José Ricardo Sturaro pela orientação deste trabalho, pela dedicada 

amizade e confiança, pelo apoio e estímulos e pelos conhecimentos transmitidos. 

 

À COPEMA (Comissão Permanente do Magistério), da Academia da Força Aérea, 

pelo apoio e pelas facilidades concedidas para que pudéssemos concluir este trabalho. 

 

Aos professores e funcionários do Instituto de Geociências e Ciências Exatas da 

UNESP de Rio Claro (SP) pelo apoio, incentivos e ensinamentos transmitidos. 

 

Aos colegas de curso Luiz Batista Castanheira, Natale Chierice Júnior e Roseli 

Aparecida Fernandes Chierice pela amizade e incentivos e ao Rogers R. da Rocha, 

proprietário das Cerâmicas Triunfo e Rochaforte que, além da amizade, proporcionou os 

materiais e os resultados das variáveis físicas, da sua tese de doutorado, para a nossa 

consequente aplicação através das Redes Neurais Artificiais. 

 

Ao Sr. Mario Castellano Pieroni, da Mineração Pieroni Ltda., aos Srs. Antonio Vitti, 

Antonio Cláudio Vitti e Fábio Ramos Vitti, da Santa Amábile Agropecuária e Mineração 

Ltda., ambas de Rio Claro, por permitirem a visita e a tomada de fotos das minas para este 

trabalho. Ao amigo Aldo José Colabone, Diretor de Meio Ambiente da Santa Amábile 

Agropecuária e Mineração Ltda., por ter intermediado a permissão das visitas e ao seu técnico 

ceramista Sérgio Antonio Castilho. 

 

A todos aqueles que, de alguma forma, colaboraram para a realização deste trabalho. 

 

 



RESUMO 

 

As Redes Neurais Artificiais se constituem numa alternativa à computação 

programada tradicional e foram aplicadas em quase todos os ramos do conhecimento humano. 

Em Geotecnologia, no entanto, ainda são escassas as aplicações de maneira que, com este 

trabalho, procura-se mostrar que elas também podem ser aplicadas em indústrias de pisos e 

revestimentos cerâmicos do Pólo Cerâmico de Santa Gertrudes, Estado de São Paulo. Para 

isso, foram utilizados corpos-de-prova elaborados, testados e analisados nas indústrias 

Triunfo Cerâmica e Rochaforte Cerâmica, com argilas oriundas de nove minas da região que 

constitui o Pólo Cerâmico de Santa Gertrudes, dentre aquelas que representavam toda a 

coluna estratigráfica da Formação Corumbataí com amostras bem diferenciadas. Os dados 

obtidos relativos às variáveis físicas foram gentilmente cedidos pelo proprietário das 

indústrias acima citadas e as variáveis físicas usadas neste estudo são a Densidade de 

Prensagem (DP), a Densidade Aparente de Corpos-de-Prova Secos (DAS), a Retração Linear 

de Secagem (RLS), a Retração Linear de Queima (RLQ), a Perda ao Fogo (PF), a Carga de 

Ruptura (CR), a Absorção de Água (Abs) e o Módulo de Resistência à Flexão (MRF). Para a 

análise, os corpos-de-prova foram submetidos a quatro temperaturas de queima 1000°C, 

1020°C, 1040°C e 1060°C, onde cada um destes valores deu origem a uma rede neural MLP 

(Multilayer Perceptron) de três camadas, para as quais foi usada a Regra do Aprendizado de 

Retropropagação do Erro (Backpropagation, do original em inglês). 

 

Palavras-chave: redes neurais artificiais, rede MLP, argila e variáveis físicas. 

 

 

 



ABSTRACT 

 

Artificial Neural Networks constitute an alternative to traditional programmed 

computation and have been applied in almost all branches of human knowledge. However, 

they are rarely applied in Geotechnology, so this work aims to show that they can be applied 

in the flooring and ceramic tile industries in the Principial Ceramic Region of Saint Gertrudes, 

São Paulo State. For this purpose, proof specimens elaborated, tested and analyzed in the 

industries of Triunfo Cerâmica and Rochaforte Cerâmica were used. These proof specimens 

were composed of well differentiated clays from nine mines in the Principial Ceramic Region 

of Saint Gertrudes, and these mines are representative of all the stratigraphic column of the 

Corumbataí Formation. The data relative to physical variables were graciously provided by 

the owner of the above mentioned industries, and the physical variables used in this study are 

Pressing Density (DP), Bulk Density of Dry Specimens (DAS), Linear Shrinkage Drying 

(RLS), Linear Shrinkage Firing (RLQ), Loss on Ignition (PF), Tensile Strength (CR), Water 

Absorption (Abs) and Flexural Modulus of Resistance (MRF). For analysis, the proof 

specimens were subjected to four firing temperatures, 1000° C, 1020° C, 1040° C and 

1060°C. Each one of these values gave rise to a neural network MLP (Multilayer Perceptron) 

of three tiers for which the Backpropagation rule of learning was used. 

 

Key words: artificial neural networks, MLP network, clay and physical variables. 



LISTA DE FIGURAS 

 

Figura 1 - O Neurônio Biológico Natural.................................................................................21 

Figura 2 - Uma rede direta. ....................................................................................................... 23 

Figura 3 - Um neurônio artificial. ............................................................................................. 25 

Figura 4 - A função logística e a sua derivada. ........................................................................ 25 

Figura 5 - A função tangente hiperbólica e a sua derivada. ..................................................... 26 

Figura 6 - O neurônio de McCulloch/Pitts. .............................................................................. 28 

Figura 7 - O Perceptron de Rosenblatt. .................................................................................... 29 

Figura 8 - Uma rede direta de três camadas. ............................................................................ 34 

Figura 9 - Fotos da mina Pieroni. ............................................................................................. 46 

Figura 10 - Fotos da mina Santa Amábile. ............................................................................... 47 

Figura 11 - Resultado encontrado para os dados do ANEXO E, com N = 7 e 242 epochs. .... 59 

Figura 12 - Resultado encontrado para os dados do ANEXO E, com N =34 e 164 epochs. ... 62 

Figura 13 - Resultado encontrado para os dados do ANEXO F, com N = 27 e 88 epochs. ..... 66 

Figura 14 - Resultado encontrado para os dados do ANEXO F, com N = 33 e 83 epochs. ..... 67 

Figura 15 - Resultado encontrado para os dados do ANEXO F, com N = 90 e 60 epochs. ..... 68 

Figura 16 - Resultado encontrado para os dados do ANEXO G, com N= 27 e 85 epochs. ..... 70 

Figura 17 - Resultado encontrado para os dados do ANEXO G, com N = 33 e 82 epochs. .... 71 

Figura 18 - Resultado encontrado para os dados do ANEXO G, com N = 34 e 73 epochs. .... 72 

Figura 19 - Resultado encontrado para os dados do ANEXO H, com N = 38 e 215 epochs. .. 74 

Figura 20 - Resultado encontrado para os dados do ANEXO H, com N = 38 e 94 epochs. .... 75 

Figura 21 - Resultado encontrado para os dados do ANEXO H, com N =40 e 127 epochs. ... 76 

Figura 22 - Resultado encontrado para os dados do ANEXO H, com N = 40 e 126 epochs. .. 77 

Figura 23 - Resultado encontrado para os dados do ANEXO H, com N = 40 e 118 epochs. .. 78 

Figura 24 - Resultado encontrado para os dados do ANEXO H, com N = 40 e 78 epochs. .... 79 

Figura 25 - Resultado encontrado para os dados do ANEXO I, com N = 56 e 64 epochs. ...... 84 

Figura 26 - Resultado encontrado para os dados do ANEXO I, com N = 58 e 158 epochs. .... 85 

Figura 27 - Resultado encontrado para os dados do ANEXO I, com N = 60 e 263 epochs. .... 86 

Figura 28 - Resultado encontrado para os dados do ANEXO I, com N = 60 e 269 epochs. .... 87 

Figura 29 - Resultado encontrado para os dados do ANEXO J, com N = 60 e 80 epochs. ..... 89 

Figura 30 - Resultado encontrado para os dados do ANEXO J, com N = 62 e 74 epochs. ..... 90 

Figura 31 - Resultado encontrado para os dados do ANEXO J, com N = 62 e 81 epochs. ..... 91 

Figura 32 - Resultado encontrado para os dados do ANEXO J, com N = 63 e 66 epochs. ..... 92 

Figura 33 - Resultado encontrado para os dados do ANEXO J, com N = 65 e 75 epochs. ..... 93 



Figura 34 - Resultado encontrado para os dados do ANEXO J, com N = 65 e 167 epochs. ... 94 

Figura 35 - Resultado encontrado para os dados do ANEXO K, com N = 55 e 76 epochs. .... 96 

Figura 36 - Resultado encontrado para os dados do ANEXO K, com N = 55 e 92 epochs. .... 97 

Figura 37 - Resultado encontrado para os dados do ANEXO K, com N = 65 e 65 epochs. .... 98 

Figura 38 - Resultado encontrado para os dados do ANEXO K, com N = 67 e 69 epochs. .... 99 

Figura 39 - Resultado encontrado para os dados do ANEXO K, com N = 67 e 97 epochs. .. 100 

Figura 40 - Resultado encontrado para os dados do ANEXO K, com N = 70 e 81 epochs. .. 101 

Figura 41 - Resultado encontrado para os dados do ANEXO K, com N = 71 e 60 epochs. .. 102 

 

 



LISTA DE TABELAS 

Tabela 1 - Classificação das placas cerâmicas quanto à porosidade. ....................................... 40 

Tabela 2 - Especificações da NBR 13818 (ABNT, 1997). ....................................................... 41 

Tabela 3 - Coordenadas UTM obtidas por GPS das minas estudadas (Datum: SAD 69). ....... 43 

Tabela 4 - Médias dos ANEXOS A, B, C e D, média geral e o padrão de produto estabelecido 
neste trabalho. .......................................................................................................... 51 

Tabela 5 - As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS E, F, G e H,com 
N = 7 e 242 epochs. ................................................................................................. 61 

Tabela 6 - As fórmulas (4.5), (4.6) e (4.7) aplicadas as dados dos ANEXOS E, F, G e H, com 
N = 34 e 164 epochs. ............................................................................................... 63 

Tabela 7 - As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS E, F, G e H, com 
N = 35 e 122 epochs. ............................................................................................... 63 

Tabela 8 - Aplicação do método resiliente aos dados do ANEXO F. ...................................... 64 

Tabela 9 - Complementação da TABELA 8. ........................................................................... 65 

Tabela 10 - As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS E, F, G e H, 
com N = 27 e 88 epochs. ....................................................................................... 66 

Tabela 11 - As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS E, F, G e H, 
com N = 33 e 83 epochs. ....................................................................................... 67 

Tabela 12 - As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS E, F, G e H, 
com N = 90 e 60 epochs. ....................................................................................... 68 

Tabela 13 - Aplicação do método resiliente aos dados do ANEXO G..................................... 69 

Tabela 14 - Complementação dos dados da TABELA 13. ...................................................... 69 

Tabela 15 - As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS E, F, G e H, 
com N = 27 e 85 epochs. ....................................................................................... 71 

Tabela 16 - As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS E, F, G e H, 
com N = 33 e 82 epochs. ....................................................................................... 72 

Tabela 17 - As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS E, F, G e H, 
com N = 34 e 73 epochs. ....................................................................................... 73 

Tabela 18 - Aplicação do método resiliente aos dados do ANEXO H..................................... 73 

Tabela 19 - Complementação dos dados da TABELA 18. ...................................................... 74 

Tabela 20 - As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS E, F, G e H, 
com N = 38 e 215 epochs. ..................................................................................... 75 

Tabela 21 - As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS E, F, G e H, 
com N = 38 e 94 epochs. ....................................................................................... 76 

Tabela 22 - As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS E, F, G e H, 
com N = 40 e 127 epochs. ..................................................................................... 77 

Tabela 23 - As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS E, F, G e H, 
com N = 40 e 126 epochs. ..................................................................................... 78 



Tabela 24 - As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS E, F, G e H, 
com N = 40 e 118 epochs. ..................................................................................... 79 

Tabela 25 - As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS E, F, G e H, 
com N = 40 e 78 epochs. ....................................................................................... 80 

Tabela 26 - Aplicação do método resiliente aos dados do ANEXO I. ..................................... 82 

Tabela 27 - Complementação dos dados da TABELA 26. ...................................................... 83 

Tabela 28 - As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N 
= 56 e 64 epochs. ................................................................................................... 84 

Tabela 29 - As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N 
= 58 e 158 epochs. ................................................................................................. 85 

Tabela 30 - As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N 
= 60 e 263 epochs. ................................................................................................. 86 

Tabela 31 - As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N 
= 60 e 269 epochs. ................................................................................................. 87 

Tabela 32 - Aplicação do método resiliente aos dados do ANEXO J. ..................................... 88 

Tabela 33 - Complementação dos dados da TABELA 32. ...................................................... 88 

Tabela 34 - As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N 
= 60 e 80 epochs. ................................................................................................... 89 

Tabela 35 - As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N 
= 62 e 74 epochs. ................................................................................................... 90 

Tabela 36 - As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N 
= 62 e 81 epochs. ................................................................................................... 91 

Tabela 37 - As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N 
= 63 e 66 epochs. ................................................................................................... 92 

Tabela 38 - As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N 
= 65 e 75 epochs. ................................................................................................... 93 

Tabela 39 - As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N 
= 65 e 167 epochs. ................................................................................................. 94 

Tabela 40 - Aplicação do método resiliente aos dados do ANEXO K..................................... 95 

Tabela 41 - Complementação dos dados da TABELA 40. ...................................................... 95 

Tabela 42 - As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N 
= 55 e 76 epochs. ................................................................................................... 97 

Tabela 43 - As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N 
= 55 e 92 epochs. ................................................................................................... 98 

Tabela 44 - As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N 
= 65 e 65 epochs. ................................................................................................... 99 

Tabela 45 - As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N 
= 67 e 69 epochs. ................................................................................................. 100 

Tabela 46 - As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N 
= 67 e 97 epochs. ................................................................................................. 101 



Tabela 47 - As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N 
= 70 e 81 epochs. ................................................................................................. 102 

Tabela 48 - As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N 
= 71 e 60 epochs. ................................................................................................. 103 

Tabela 49 - Bloco B1 da matriz IW, fórmula (4.9). ............................................................... 106 

Tabela 50 - Bloco B2 da matriz IW, fórmula (4.9). ............................................................... 107 

Tabela 51 - Bloco B3 da matriz IW, fórmula (4.9). ............................................................... 108 

Tabela 52 - Bloco B4 da matriz IW, fórmula (4.9). ............................................................... 109 

Tabela 53 - Cálculo dos desvios, D(%), para os valores da TABELA 46.............................. 110 

Tabela 54 - Cálculo dos desvios, D(%), para os valores da TABELA 47.............................. 111 

Tabela 55 - Bloco B1 da matriz IW , fórmula (4.12). ............................................................ 112 

Tabela 56 - Bloco B2 da matriz IW, fórmula (4.12). ............................................................. 113 

Tabela 57 - Bloco B3 da matriz IW, fórmula (4.12). ............................................................. 114 

Tabela 58 - Bloco B4 da matriz IW, fórmula (4.12). ............................................................. 115 

Tabela 59 - Bloco B5 da matriz IW, fórmula (4.12). ............................................................. 116 

Tabela 60 - Bloco B6 da matriz IW, fórmula (4.12). ............................................................. 117 

 



LISTA DE ANEXOS 

Anexo A - Variáveis físicas obtidas na temperatura de queima de 1000°C ........................... 133 

Anexo B - Variáveis físicas obtidas na temperatura de queima de 1020°C ........................... 134 

Anexo C - Variáveis físicas obtidas na temperatura de queima de 1040°C ........................... 135 

Anexo D - Variáveis físicas obtidas na temperatura de queima de 1060°C. .......................... 136 

Anexo E - Variáveis físicas obtidas na temperatura de queima de 1000°C. (Minas CF, CR, 
PG, PT e PI) .......................................................................................................... 137 

Anexo F - Variáveis físicas obtidas na temperatura de queima de 1020°C. (Minas CF, CR, 
PG, PT e PI) .......................................................................................................... 138 

Anexo G - Variáveis físicas obtidas na temperatura de queima de 1040°C. (Minas CF, CR, 
PG, PT e PI) .......................................................................................................... 139 

Anexo H - Variáveis físicas obtidas na temperatura de queima de 1060°C. (Minas CF, CR, 
PG, PT e PI) .......................................................................................................... 140 

Anexo I - Variáveis físicas obtidas na temperatura de queima de 1020°C. (Minas CF, CR, PG, 
PT, PI e TU).......................................................................................................... 141 

Anexo J - Variáveis físicas obtidas na temperatura de queima de 1040°C. (Minas CF, CR, PG, 
PT, PI e TU).......................................................................................................... 142 

Anexo K - Variáveis físicas obtidas na temperatura de queima de 1060°C. (Minas CF, CR, 
PG, PT, PI e TU) .................................................................................................. 143 

 

 

 



LISTA DE ABREVIATURAS E SIGLAS 

 

ABNT   Associação Brasileira de Normas Técnicas 

Abs   Absorção de Água 

APL   Arranjo Produtivo Local 

ASPACER   Associação Paulista das Cerâmicas de Revestimento 

CF 1   Cristofoletti Mina 1 

CF 2   Cristofoletti Mina 2 

CR   Carga de Ruptura 

CR 1   Cruzeiro Frente 1 

CR 2   Cruzeiro Frente 2 

DAS   Densidade Aparente de Corpos de Prova Secos 

DP   Densidade de Prensagem 

GPS   Global Positioning System ou Sistema de Posicionamento Global 

MRF   Módulo de Resistência à Flexão 

PCSG  Pólo Cerâmico de Santa Gertrudes 

PF   Perda ao Fogo 

PG  Mina Paganoti 

PI   Mina Pieroni 

PT 1  Partezani Mina 1 

PT 2   Partezani Mina 2 

PT 3   Partezani Mina 3 

RLQ   Retração Linear de Queima 

RLS   Retração Linear de Secagem 

TU   Mina Tute 

UTM   Sistema Universal de Coordenadas Transverso de Mercator 

    

 



SUMÁRIO 

1.  INTRODUÇÃO ................................................................................................................ 16 

1.1. Objetivo Geral e Hipótese .......................................................................................... 17 

1.2. Estrutura ..................................................................................................................... 18 

2.  REVISÃO BIBLIOGRÁFICA ......................................................................................... 19 

2.1. O Neurônio Biológico Natural ................................................................................... 19 

2.2. As Redes Neurais Artificiais ...................................................................................... 21 

2.3. Breve Histórico sobre as Redes Neurais Artificiais ................................................... 27 

2.4. Regra Delta e Regra Delta Generalizada ................................................................... 32 

2.4.1. Regra Delta ....................................................................................................... 32 

2.4.2. Regra Delta Generalizada ................................................................................. 33 

3.  MATERIAIS E MÉTODOS ............................................................................................. 39 

3.1. O Pólo Cerâmico de Santa Gertrudes ........................................................................ 39 

3.2. Caracterização das Argilas e das Cerâmicas .............................................................. 39 

3.3. Os Corpos-de-Prova ................................................................................................... 43 

3.4. Determinação das Variáveis Físicas dos Corpos-de-Prova........................................ 48 

3.5. Metodologia ............................................................................................................... 50 

4.  RESULTADOS E DISCUSSÕES .................................................................................... 55 

4.1. Introdução .................................................................................................................. 55 

4.2. Regra de Aprendizado Backpropagation (Formato Básico) ...................................... 55 

4.3. Regra de Aprendizado Backpropagation com Momento (traingdm) ........................ 57 

4.4. Regra de Aprendizado Backpropagation Resiliente (trainrp) ................................... 58 

4.5. Regra de Aprendizado Backpropagation de Levenberg-Marquadt (trainlm) .......... 103 

4.6. Conclusões ............................................................................................................... 104 

5.  CONSIDERAÇÕES FINAIS ......................................................................................... 121 

REFERÊNCIA BIBLIOGRÁFICA........................................................................................123 

BIBLIOGRAFIA COMPLEMENTAR..................................................................................127 

ANEXOS ................................................................................................................................ 132 

 
 

 



16 
 

1. INTRODUÇÃO 
 

Para uma exploração economicamente viável e sustentada dos recursos minerais e sua 

consequente industrialização com uma minimização, na medida do possível, dos impactos 

causados inevitavelmente ao meio ambiente, há atualmente a possibilidade de se aplicar 

alguns dos modernos conceitos da Matemática, da Estatística, da Computação. Neste trabalho, 

consideram-se a mineração de argila e, em especial, a industrialização correspondente de 

pisos e revestimentos cerâmicos da região do Estado de São Paulo denominada de Pólo 

Cerâmico de Santa Gertrudes (PCSG) formado pelos municípios de Limeira, Cordeirópolis, 

Rio Claro, Ipeúna, Piracicaba, Araras e Santa Gertrudes. Este segmento da indústria, 

conforme dados da Associação Paulista das Cerâmicas de Revestimento, ASPACER, 

(ASPACER, 2012), é responsável por aproximadamente 15000 empregos diretos e 200000 

empregos indiretos. Está inserido no Arranjo Produtivo Local (APL) de Pisos e 

Revestimentos Cerâmicos de Santa Gertrudes, o qual de acordo com Poletto (2007), é o maior 

produtor da América Latina e o quarto maior do mundo. É formado por indústrias de pequeno 

e de médio porte, utiliza basicamente o processo de monoqueima rápida, com produção, 

preponderantemente, pela chamada Via Seca usando argilas de boa qualidade. 

A preocupação com o meio ambiente tem-se intensificado sobremaneira nos últimos 

anos em vista das recentes pesquisas sobre a poluição das águas e da atmosfera, a ocupação 

dos espaços nas terras e nos mares, o desmatamento das florestas remanescentes, a exploração 

dos recursos minerais disponíveis, recursos estes indispensáveis à espécie humana. 

Paralelamente, a indústria de transformação procura meios para aumentar a produção 

diminuindo os custos dos processos produtivos, aumentando os lucros e melhorando a 

qualidade do seu produto final. Atualmente, ela também busca por certificações ambientais 

como uma forma de demonstrar que também está consciente dos impactos ambientais que ela 

mesma provoca ao meio ambiente, utilizando-se para isso de medidas mitigadoras (planos de 

recuperação, de remediação, de reabilitação), mesmo que, às vezes, de forma incipiente, 

apesar dos baixos custos, quando bem planejadas. Contudo, persistem ainda os conflitos entre 

as mineradoras e indústrias e a comunidade onde elas estão inseridas. Se, por um lado, as 

mineradoras e as indústrias provocam a poluição ambiental, por outro lado promovem o 

desenvolvimento econômico gerando empregos e renda, que são muito importantes para a 

população. O controle de qualidade do produto final, o controle de qualidade nos processos e 

nos mecanismos de industrialização associados a uma diminuição do consumo de energia 



17 
 

podem ser, quando bem planejados, instrumentos de promoção na busca pelas certificações 

ambientais. 

Dentro deste contexto, pode-se admitir que com a escassez de aplicações do conceito 

estatístico-matemático das Redes Neurais Artificiais (RNAs) na indústria cerâmica, 

vislumbrou-se um farto e interessante material para esta pesquisa, tendo em vista que com 

este conceito pode-se estabelecer com bastante rigor o valor das variáveis, dentro dos seus 

próprios limites de tolerância de especificação, estabelecidos pela Associação Brasileira de 

Normas Técnicas (ABNT), as quais determinam as características do produto final, no caso, 

piso e revestimento cerâmico. Ademais, dentro das sete ferramentas estatísticas para o 

controle de qualidade de um produto industrial introduzidas por Kaoru Ishikawa, podem ser 

estabelecidos o limite inferior de controle e o limite superior de controle com valores bem 

próximos um do outro, diminuindo sensivelmente a amplitude do intervalo. Para este 

propósito será utilizado o software MATLAB® (MATrix LABoratory) 7.0, da “MathWorks, 

Inc.” – User’s Guide : Neural Networks Toolbox. 

Originariamente, em função dos dados e do problema apresentados, delimitou-se a 

aplicação entre o conceito das RNAs e o conjunto de técnicas de otimização conhecido como 

Metodologia de Superfície de Resposta, introduzido por G. E. P. Box e K. B. Wilson no início 

dos anos cinquenta do século passado, já que ambos se inserem, em alguns casos, no conceito 

de Aproximação de Funções da Análise Numérica. 

A escolha recaiu sobre as RNAs tendo em vista a facilidade de aplicação, o baixo 

custo para a obtenção e o rigor na determinação das especificações do produto industrial, o 

qual é o próprio padrão de saída da RNA. 

 

1.1.  Objetivo Geral e Hipótese 

 

O objetivo geral deste trabalho é o de encontrar uma rede neural artificial, dentre as 

várias arquiteturas e as diversas regras de aprendizado, aquela que melhor se adapta para 

maximizar o aproveitamento das argilas na indústria cerâmica do PCSG e melhorar a 

qualidade dos pisos e revestimentos cerâmicos ali produzidos, mediante uma rigorosa 

caracterização. 

Este objetivo geral é consequência da seguinte hipótese: 



18 
 

“Mostrar que é possível aplicar o conceito das redes neurais artificiais na indústria de pisos e 

revestimentos cerâmicos”. 

 

1.2.  Estrutura 

 

Este trabalho foi dividido em cinco capítulos, onde, além deste introdutório, no 

Capítulo 2, será desenvolvida uma revisão bibliográfica sobre o conceito estatístico-

matemático das Redes Neurais Artificiais, dentro daquilo que se concebeu como Inteligência 

Artificial. 

Em seguida, no Capítulo 3, serão apresentados os materiais utilizados, a localização 

das minas e a metodologia usada. 

No Capítulo 4, serão comentados e discutidos os resultados encontrados com a 

aplicação da rede neural artificial MLP (Multilayer Perceptron), mostrando que, de fato, elas 

podem ser utilizadas, inclusive em futuro próximo, para melhorar a qualidade dos pisos e 

revestimentos cerâmicos dentro das especificações técnicas do produto já previamente 

estabelecidas. Finalizando este capítulo, foi acrescentado um roteiro para os interessados em 

aplicar as RNAs. 

No Capítulo 5, serão mostradas as conclusões encontradas e feitas as considerações 

finais com sugestões para trabalhos futuros. 

Por fim, são indicados na Referência Bibliográfica os livros, os artigos e outras 

publicações e as publicações pesquisadas junto à rede “internet”, usados neste trabalho. Foi 

também acrescentada uma Bibliografia Complementar, importante para o entendimento das 

RNAs. 



19 
 

2. REVISÃO BIBLIOGRÁFICA 

2.1. O Neurônio Biológico Natural 

 

O cérebro humano é constituído principalmente por dois tipos diferentes de células, as 

células glias e os neurônios. As células glias, com um número aproximado dez vezes maior do 

que o número de neurônios, são responsáveis pela sustentação do cérebro, enquanto os 

neurônios são basicamente as unidades ou elementos de processamento dos sinais (ou pulsos) 

e estímulos que recebe e sobre eles estão concentrados os principais estudos no sentido de se 

simular ou de se modelar mediante um algoritmo o seu comportamento. O neurônio é 

constituído por um corpo celular ou soma, pelo axônio e pelos dendritos. O cérebro humano 

através da rede de neurônios e de suas interconexões, onde ocorrem sinapses, é responsável 

pelo pensamento, emoção, cognição. As sinapses ocorrem entre axônios de diferentes 

neurônios, entre o axônio e o soma e, também, entre os dendritos de um mesmo neurônio, 

segundo Wasserman (1989). Ainda, de acordo com o mesmo autor, o cérebro humano, que 

pesa somente 2% da massa corporal, consome 20% de todo o oxigênio liberado no corpo 

humano e somente de 20 a 30 W de potência para o seu saudável funcionamento. 

É através das sinapses que um neurônio se interconecta a até outros 10.000 neurônios, 

num total de aproximadamente 100 bilhões de neurônios dentro do nosso cérebro. Eles não 

são todos idênticos, diferem quimicamente, estruturalmente ou funcionalmente e já foram 

encontrados mais de 250 tipos diferentes de neurônios. A região intersináptica é 

eletroquimicamente ativa, local onde já foram descobertas mais de 50 substâncias químicas 

que executam a rápida intercomunicação, via corrente iônica, entre um neurônio pré-sináptico 

e um pós-sináptico, por isso essas substâncias são denominadas apropriadamente de 

neurotransmissores. 

A membrana do neurônio é de grande importância, em função da intensa 

intercomunicação neuronal, porque é através dela que os neurotransmissores atuam podendo 

ser excitadores quando a despolarizam mediante a atuação do potencial elétrico de ação ou 

impulso nervoso ou podendo ser inibidores quando a hiperpolarizam, visto que em repouso a 

membrana encontra-se a -70 mV, segundo Eccles (1957). A complexidade do funcionamento 

de um único neurônio reside, principalmente, no fato de que a concentração dos 

neurotransmissores depende cada um deles de uma série de fatores que até o presente 

momento não são bem conhecidos e por isso encontra-se, ainda, em fase de intensos estudos. 



20 
 

É importante observar que o mesmo neurotransmissor pode ser excitatório para uma sinapse e 

inibitório para outra, de acordo com Wasserman (1989). 

O neurônio foi identificado pelo neurologista espanhol Ramón y Cajal; as suas 

manifestações elétricas foram observadas pela primeira vez por DuBois Reymond, com o 

auxílio de galvanômetros (KOVÁCS, 2002). Identificado pelo pesquisador E. D. Adrian, o 

potencial de ação consiste em um pacote de ondas e é obtido pela despolarização da 

membrana do neurônio com o auxílio da chamada bomba de sódio/potássio, a qual permite a 

entrada do sódio e a consequente saída do potássio. Depois de emitido o impulso, ocorre a 

hiperpolarização com os íons fazendo o caminho inverso. Toda esta operação ocorre no 

intervalo de tempo de 1 a 3 ms, com a superposição de um período refratário e um estado de 

repouso ou relaxação, o que impossibilita uma retomada pelo neurônio de um novo impulso 

durante esse período de tempo (WASSERMAN, 1989), como pode ser visto na Figura 1. O 

neurônio natural apresenta muitas entradas (conexões sinápticas), mas uma só saída (um único 

impulso nervoso), de forma que, dependendo dos sinais ou estímulos que recebe, um neurônio 

pode ou não emitir um pulso. A superposição de todos esses sinais ou estímulos, excitatórios 

ou inibitórios, é o que constitui a atividade cerebral. 

Os neurônios organizados em sistemas são adaptativos, visto que com mudanças nas 

interconexões sinápticas, eles aprendem e se auto-organizam fazendo emergir ordem da 

desordem e, também, o que é mais importante, estes sistemas apresentam propriedades 

coletivas (memória), mediante a competição e cooperação entre os neurônios constituintes, as 

quais diferem das propriedades individuais. 

O cérebro humano é um sistema dinâmico complexo com uma grande quantidade de 

elementos organizados em sistemas que, por sua vez, também se organizam em outros 

sistemas cada vez mais complexos, de maneira similar à própria origem da vida e a evolução 

das espécies, conforme Damásio (1996). De acordo com Haykin (2001), o cérebro humano 

processa informações como se fosse um computador altamente complexo, não linear e 

paralelo. Em função dessa complexidade, atualmente há pelo menos dois grandes ramos da 

neurociência; o primeiro que estuda a estrutura e o funcionamento dos sistemas nervosos da 

maneira como interessam aos biólogos e aos profissionais de estudos correlacionados, 

enquanto que o segundo estuda o processamento computacional dos dados de maior interesse 

para os físicos e matemáticos no intuito de se construírem computadores úteis ou máquinas 

mais efetivas, segundo Nussenzveig (1999). 

 



21 
 

Figura 1 - O potencial de ação durante a aplicação de um estímulo nervoso. 
 

 

Fonte: Disponível em:&amp;lt;http://faculty.washington.edu.chudler/ap.html&gt;. 
Acesso em: 03 mar. 2007. Tradução nossa.  

 

2.2. As Redes Neurais Artificiais 

As redes neurais artificiais, constituídas de neurônios artificiais em analogia com os 

neurônios naturais humanos, são modelos estatístico-matemáticos que buscam simular alguns 

processos biológicos do Sistema Nervoso Central. Elas são modelos adaptáveis, da mesma 

maneira que os neurônios naturais, e é dessa forma que devemos entendê-las, pois através da 

variação de alguns parâmetros de controle elas conseguem aprender e, consequentemente, 

realizar operações de controle, de classificação e de reconhecimento de padrões previamente 

fixados. A simulação deve ser feita com o auxílio de algoritmos e de programas 

computacionais. Estes modelos surgiram na mesma época em que foram construídos os 

primeiros computadores, década de 40 do século passado, para se lidar com problemas 

complexos, especialmente, aqueles da teoria matemática dos sistemas dinâmicos, onde grande 



22 
 

quantidade de dados deve ser modelada e analisada estatisticamente e computacionalmente, 

de acordo com Abdi et al. (1999), Haykin (2001), Kovács (2002) e Braga et al. (2007). As 

RNAs  constituem uma alternativa à computação programada tradicional conduzida pela 

elaboração de um algoritmo, por um conjunto de rotinas e por uma sólida base lógica, visto 

que são capazes de reagir e de se auto-organizar; elas aprendem, sua maior virtude, mas, 

infelizmente, como qualquer ser humano, também esquecem. Elas são capazes de gerar as 

suas próprias regras e é importante ressaltar que no processo de adaptação elas podem gerar 

regras internas desconhecidas. São muito utilizadas, em função destas características, como 

simulação dos processos mentais na ciência da cognição, conforme Abdi et al.(1999), Haykin 

(2001) e Braga et al. (2007). Braga et al. (2007) ressaltam que as redes neurais são utilizadas 

em sistemas de processamento paralelo e distribuído. 

A ideia original foi a de se aproximar o máximo possível do funcionamento dos 

sistemas de neurônios humanos, mas apesar de todo o avanço tecnológico que se seguiu e da 

sua rápida transferência aos modernos computadores, esta ideia teve que ser deixada de lado, 

pois paralelamente avançou-se também na compreensão da fisiologia de um único neurônio 

biológico, onde se percebeu que o seu funcionamento era demasiadamente complexo, o qual 

reunido a outros neurônios em estruturas cada vez mais complexas dentro do Sistema Nervoso 

Central ainda não permite que esta desejada aproximação se concretize. No entanto, o estudo 

das redes neurais artificiais revelou-se muito importante na resolução de problemas de 

controle, pois mesmo quando a informação é parcial, a rede pode escolher um padrão (ou 

informação de saída) mais próximo ao desejado, pois uma característica muito importante das 

redes neurais é a de que elas são tolerantes a falhas (NUSSENZVEIG, 1999). As redes 

neurais, atualmente, são aplicadas em inúmeros campos do conhecimento humano, onde se 

destacam a filtragem de ruídos (em telefonia), a análise do eletrocardiograma e do 

eletroencefalograma, a compressão de imagens e a realização de efeitos especiais na indústria 

de entretenimento, a simulação de vôo e dos sistemas de controle de aviões na indústria 

aeroespacial, a avaliação da aplicação de créditos no sistema bancário. Mas, conforme foi 

salientado no capítulo anterior, há poucas aplicações notadamente na indústria cerâmica, 

objeto deste estudo.  

Na constituição de uma rede neural artificial, as unidades ou elementos de 

processamento, denominados apropriadamente de neurônios artificiais, são organizados 

basicamente em camadas: uma camada de entrada, que recebe os dados ou padrão de entrada, 

uma camada de saída, que fornece as respostas da rede ou padrão de saída, ambas interligadas 



23 
 

ao exterior e, entre elas, podem ser incluídas uma ou mais camadas chamadas de 

intermediárias ou ocultas (internas). A cada elemento de processamento é associado um peso 

sináptico, que é o responsável pelas conexões em uma analogia com a realidade das sinapses. 

É através das mudanças destes pesos sinápticos de ligação entre as conexões que uma rede 

neural aprende. Esta rede, pela sua arquitetura ou topologia, recebe o nome de rede direta ou 

feedforward ou, ainda, de acordo com Haykin (2001), rede alimentada para frente (Figura 2). 

Além desta, há outras arquiteturas ou topologias.  

 

Figura 2 - Uma rede direta. 

 
Fonte: Abdi et al. (1999). 
 
           A primeira simplificação introduzida no modelo do elemento de processamento foi a 

determinação de que cada neurônio podia assumir somente dois estados: ativo, quando o 

axônio emite um sinal ou inativo quando o axônio não emite sinal. 

           Uma rede neural artificial exibe duas fases de operação: o aprendizado e a 

recapitulação. No aprendizado, os pesos sinápticos são adaptados ou modificados no sentido 

de se obter o resultado desejado. Quando a resposta (padrão de saída) é conhecida, diz-se que 

o aprendizado é supervisionado e quando a resposta não é conhecida, mas é esperada uma 

saída ótima para resolver o problema, diz-se que o aprendizado é não supervisionado, porque 

neste caso não há a necessidade de um supervisor (ou um professor) que encaminhe a rede 

para o resultado esperado. Complementando, a rede pode ser heteroassociativa quando o vetor 

(padrão) de saída for diferente do vetor (padrão) de entrada e autoassociativa quando eles 

forem iguais. Na fase de recapitulação (ou operação propriamente dita), tem-se que o 



24 
 

resultado é obtido em resposta a um dado exemplo, sem que ocorra qualquer modificação nos 

pesos das conexões, de acordo com Campanha (1994). 

Em resumo, pode-se dizer que as características fundamentais de uma rede neural 

artificial são a sua arquitetura ou topologia (a forma como as unidades de processamento 

estão interligadas), as suas próprias unidades de processamento e as suas regras de 

aprendizado. 

Quanto à arquitetura, dentre os vários tipos de redes, destacam-se as redes diretas ou 

feedforward, do original em inglês, ou rede alimentada adiante com camada única ou com 

múltiplas camadas, conforme denominação de Haykin (2001), onde os elementos de 

processamento aparecem interconectados em camadas sequencialmente colocadas da entrada 

até a saída, passando pelas camadas ocultas ou intermediárias (ou ainda, hidden) - que será 

utilizada neste trabalho -, e as redes recorrentes, em especial a rede de Hopfield, a qual é uma 

rede simétrica (a matriz dos pesos sinápticos é simétrica) com ciclos ou com realimentação 

(ou retroalimentação), como quando um neurônio se auto realimenta da mesma forma que um 

neurônio biológico natural. 

Basicamente, uma unidade de processamento é constituída de vários sinais de entrada 

(x 1 ,...,x n ), cada qual com o seu peso sináptico (conexão) associado (w 1 ,...,w n ). A soma 

ponderada 

? ix . w i              (2.1) 

é a entrada efetiva que dará origem a uma só saída. Se a soma ponderada (2.1) for superior a 

um limiar (threshold) previamente estabelecido, quando necessário, então o sinal de saída é 

gerado mediante a aplicação de uma função de transferência (ativação) ou função de saída do 

neurônio, a qual deve ser uma função que mais se aproxima do efeito de ativação do potencial 

de ação do neurônio natural. Pode ser estabelecida por qualquer função contínua, 

monotonicamente crescente, com imagens reais dentro dos intervalos [0,1] ou [-1,1]. Caso 

contrário, se a soma (2.1) for inferior ao limiar, nada acontece, ou seja, não há sinal de saída 

(Figura 3). 

As principais funções de transferência são: a função linear 

f(x) = kx,            (2.2)  

a função logística (Figura 4) 



25 
 

f(x) = ( 1 + e kx? ) 1?            (2.3)  

 

Figura 3 - Um neurônio artificial. 

 

Fonte: Abdi et al. (1999). 

 
Figura 4 – A função logística e a sua derivada. 
 

-10 -8 -6 -4 -2 0 2 4 6 8 10
-0.2

0

0.2

0.4

0.6

0.8

1

Função logística

-6 -4 -2 0 2 4 6
-0.05

0

0.05

0.1

0.15

0.2

0.25

Derivada da função logística

Fonte: Ferro (2007). 

e a função tangente hiperbólica (Figura 5), 

f(x) = tanh(kx) .                               (2.4) 

 

 

 

 



26 
 

Figura 5 – A função tangente hiperbólica e a sua derivada. 
 

-10 -8 -6 -4 -2 0 2 4 6 8 10

-1

-0.8

-0.6

-0.4

-0.2

0

0.2

0.4

0.6

0.8

1

Função tangente hiperbólica

-6 -4 -2 0 2 4 6
-0.2

0

0.2

0.4

0.6

0.8

1

Derivada da função tangente hiperbólica

 

Fonte: Ferro (2007). 

 

Em todas estas funções, k é uma constante (em especial, o MATLAB usa k = 1). As 

derivadas das duas últimas funções apresentam características sigmoidais. 

De acordo com Abdi et al. (1999), o processamento do conhecimento é conseguido 

através das mudanças nos pesos das conexões (sinápticos) entre os elementos de 

processamento. Em termos estatísticos, este processo é equivalente à interpretação de que os 

valores dos pesos das conexões entre os elementos de processamento são parâmetros 

estimadores. O processo de aprendizado especifica o algoritmo usado para estimar esses 

parâmetros. O aprendizado, então, está ligado a uma regra que modifica ou adapta os pesos 

das conexões.  

Em 1949, o neuropsicólogo Donald Hebb propôs a regra, chamada de Regra do 

Aprendizado de Hebb, a qual estabelece que “quando um axônio da célula A está próximo o 

suficiente para excitar uma célula B e repetidamente ou persistentemente a estimula, algum 

processo de crescimento ou mudança metabólica ocorre em uma ou em ambas as células de 

tal forma que a eficiência de A, como uma das células que estimula B, aumenta”, conforme 

Abdi et al. (1999), tradução nossa. Na década de 60, do século passado, B. Widrow e M. Hoff 

desenvolveram a Regra do Aprendizado Delta, que é baseada no método dos mínimos 

quadrados (WIDROW &amp;amp; HOFF, 1960), dentro daquilo que é chamado em matemática de 

ajustamento de curvas ou aproximação polinomial de funções; no caso particular em que o 

polinômio de aproximação é do primeiro grau, o método recebe o nome bem conhecido de 



27 
 

regressão linear. Neste método, o mínimo da função do erro acontece no vértice da sua 

superfície de resposta, visto que o erro de aproximação é quadrático com curva ou superfície 

de mínimo. A Regra de Aprendizado Delta foi generalizada por Rumelhart e outros 

pesquisadores, ainda na década de 80 do século passado, com o nome de Regra Delta 

Generalizada ou Regra de Retropropagação do Erro (Backpropagation), tornando-se a mais 

poderosa regra de aprendizado das redes neurais artificiais. Esta regra foi desenvolvida 

originalmente por P. Werbos, (1974), e redescoberta, independentemente, por  D. Parker,  

(1982), Y. LeCun, (1985), e por Rumelhart et al., (1986), de acordo com os seus próprios 

idealizadores. Ela usa o método do gradiente descendente para a correção do erro. Como o 

gradiente é um vetor cujo sentido aponta sempre na direção do crescimento máximo da 

função sobre a qual ele é aplicado, quando o seu sentido é invertido, ele passa a apontar na 

direção do decrescimento máximo da função. Assim, o mínimo da função de erro, neste caso, 

depende do gradiente com sinal negativo. A Regra Delta e a Regra Delta Generalizada serão 

apresentadas ainda neste capítulo. Acrescentam-se a estas regras, dentre outras, a Regra de 

Aprendizagem de Kohonen, (KOHONEN, 1982), a Regra de Aprendizagem por Cooperação-

Competição de von der Malsburg e Grossberg, segundo seus próprios criadores von der 

Malsburg (1973) e Grossberg (1976, 1980). 

 

2.3. Breve Histórico sobre as Redes Neurais Artificiais 

A seguir destacam-se os fatos históricos relevantes para o objetivo geral deste trabalho 

e para isto foram adotados os textos de Campanha (1994), Abdi et al. (1999), Azevedo et al. 

(2000), Haykin (2001) e Kovács (2002). 

Em 1943, o neurologista Warren S. McCulloch junto com o seu aluno Walter Pitts, 

estatístico, publicaram o artigo “A logical calculus of the ideas immanent in nervous activity” 

no Bulletin of Mathematical Biophysics, considerado o marco zero no desenvolvimento das 

redes neurais artificiais. Acredita-se que este artigo tenha influenciado cientistas como John 

von Neumann que se voltou para a construção de cérebros eletrônicos ou computadores, 

Marvin Minsky que se dedicou a aplicação da inteligência artificial em sistemas autônomos, 

Norbert Wiener que se preocupou com aquilo que veio a denominar-se cibernética e, 

principalmente, Frank Rosenblatt que, preocupado com o aspecto computacional da visão, 

criou o “Perceptron”, em 1957. Alguns historiadores acreditam que McCulloch e Pitts foram, 



28 
 

por seu turno, também influenciados pelas pesquisas de Alan Turing e do próprio John von 

Neumann. 

O neurônio de McCulloch/Pitts é do tipo lógico, que admite somente dois estados: 

dispara ou não dispara o pulso ou sinal. As suas entradas pertencem ao conjunto formado 

pelos números 0 e 1, possui um limiar constante igual a 1 e as suas sinapses tanto excitatórias 

quanto inibitórias possuem valores idênticos. Neste caso, as sinapses são excitatórias quando 

o peso da conexão é positivo e são inibitórias quando o peso da conexão é negativo. Se a 

soma ponderada for superior ao limiar o neurônio dispara o sinal e em, caso contrário, não 

dispara o sinal, conforme estabelecido pelos próprios autores McCulloch &amp;amp; Pitts (1943), 

(Figura 6). 

 

Figura 6 – O neurônio de McCulloch/Pitts. 

 

Fonte: McCulloch/Pitts (1943). 

 

Frank Rosenblatt, na Universidade de Cornell, construiu o Perceptron, Figura 7, 

equipamento que se constituía em uma rede neural de uma só camada e era um classificador 

de padrões capaz de identificar formas geométricas. Também foi construído com neurônios do 

tipo lógico, entretanto já apresentava conexões modificáveis, mas a saída fornecia os mesmos 

resultados 0 e 1. Usava a regra Hebbiana para o aprendizado. Os neurônios do Perceptron 

eram discriminadores (ou separadores) lineares, tendo em vista que a soma ponderada: 

? iw . x i  = ? ,                      (2.5) 



29 
 

onde ?  é o limiar, comportava-se no caso em que n = 2, onde n é o número de entradas, 

como a equação de uma reta no plano formado pelos eixos x 1  e x 2 . Assim, se  

w 1 .x 1  + w 2 .x 2  ?  ?            (2.6) 

é obtida uma parte do plano (acima da reta) e se 

w 1 .x 1  + w 2 .x 2  &amp;lt;?           (2.7) 

é encontrada a outra parte do plano (abaixo da reta). Quando n = 3 o elemento separador 

w 1 x 1 + w 2 x 2 + w 3 x 3  = ?  passa a ser um plano no espaço gerado pelas coordenadas x 1 , x 2 e 

x 3 . Quando n é superior a 3, tem-se aquilo que os matemáticos chamam de hiperplanos 

(ROSENBLATT,1958). 

 

Figura 7 – O Perceptron de Rosenblatt. 

 

Fonte: Rosenblatt (1958). 

Na Universidade de Stanford, em 1959, Bernard Widrow desenvolveu filtros que 

eliminaram os ruídos nas linhas telefônicas e se confirmou como a primeira aplicação das 

redes neurais artificiais ao mundo real. Estudou também a aplicação das redes neurais no 

reconhecimento da fala e de objetos, na previsão do tempo, no ajuste de antenas parabólicas, 

na regulação da pressão sanguínea. Criou o ADALINE (ADAptative LINear Element), onde as 

unidades de processamento são adaptadores lineares e também o MADALINE (Multiple 

Adaline) com várias camadas ADALINE. Com Marcian Hoff desenvolveu a Regra de 



30 
 

Aprendizado Delta (ou de Widrow-Hoff), aquela que estabeleceu que “quando você comete 

um erro, preste menos atenção à célula de entrada que levou você a cometer este erro e preste 

mais atenção à célula de entrada que não levou você a cometer este mesmo erro”. Esta regra 

causou uma grande euforia junto a outros pesquisadores crentes na imensa potencialidade de 

aplicações das redes neurais, acreditando, naqueles tempos, que havia sido descoberta uma 

chave da inteligência humana (WIDROW e HOFF, 1960). 

Entretanto, M. Minsky e S. Papert provaram, posteriormente, que o Perceptron não 

seria capaz de discriminar a operação lógica do tipo “ou exclusivo (XOR)” e o seu 

complemento e, significava que, para os conjuntos “mais interessantes” (linearmente não 

separáveis) ele não conseguiria exibir nenhum conjunto de pesos sinápticos (w i ) para efetuar 

a classificação de padrões (MINSKY e PAPERT, 1969). Isto provocou uma estagnação nos 

estudos sobre as redes neurais, principalmente, pela indisponibilidade de verbas para as 

pesquisas por quase duas décadas, de acordo com as referências indicadas no início deste 

item. 

Mesmo assim, isto não impediu que Teuvo Kohonen, Stephen Grossberg, James 

Anderson, Igor Aleksander (redes sem pesos), Kunihiko Fukushima (cognitron e 

neocognitron), notadamente, continuassem os seus trabalhos de pesquisa dentro da 

computação neural. De acordo com Wasserman (1989), o choque provocado pelo livro 

“Perceptrons” de Minsky e Papert permitiu que houvesse um período de latência de modo a se 

atingir a necessária maturidade neste campo de estudos. Hoje as redes neurais artificiais 

rotineiramente resolvem alguns dos problemas que foram colocados no livro de Minsky e 

Papert. As redes diretas exigem apenas camadas intermediárias para solucionar os problemas 

não linearmente separáveis. Provou-se, mais tarde, que bastava uma só camada, no entanto, a 

solução ficaria ainda na dependência da quantidade de neurônios desta camada oculta. 

Destaca-se que Kohonen, da Universidade de Helsinque, nos anos 70 do século 

passado, desenvolveu a Regra do Aprendizado Competitivo e a rede neural correspondente, 

na qual as unidades de processamento competem entre si. Também desenvolveu alguns 

algoritmos adaptativos locais (mapa auto-organizável). James Anderson, na Universidade de 

Brown, desenvolveu um Modelo Associativo Linear similar aos modelos biológicos de 

memória e reconhecimento. Por seu turno, Stephen Grossberg utilizando-se da clássica 

experiência de Pavlov sobre reflexos condicionados, construiu modelos em computação 

neural. Desenvolveu, também, a partir dos anos 60, a Teoria da Ressonância Adaptativa 

(ART), para classificar padrões, com os modelos ART1 (padrões binários), ART2 (padrões 



31 
 

analógicos) e, junto com Gail Carpenter, o ART3, redes diretas espaço temporais, esta em 

1978.  

Com a publicação do artigo “Neural networks and physical systems with emergent 

collective computational abilities” na renomada Proceedings of the National Academy of 

Sciences por J. J. Hopfield, da Caltech, em 1982, promoveu-se o retorno à respeitabilidade 

dos estudos sobre as redes neurais artificiais. Hopfield desenvolveu uma Rede Associativa 

com neurônios do tipo lógico, introduzindo o conceito de função de energia (ou função de 

Lyapunov) para analisar a evolução da rede. O mecanismo de resposta da rede é uma 

particularização dos trabalhos de Grossberg, no entanto, ele concebeu uma notável síntese de 

ideias com um adequado tratamento matemático, conforme destaca Wasserman (1989). 

Complementando, dentre outros precursores, pode-se destacar o neuroanatomista 

Brian G. Gragg e o físico H. N. V. Temperley que, ainda em 1954, estabeleceram uma 

analogia entre as Redes de Neurônios e as Redes de Átomos com Spins. Analogia que foi 

aproveitada por William A. Little, em 1974, que associou as redes neurais e os sistemas de 

spins (Ising). Em 1976, D. Marr e T. Poggio estudando como o cérebro humano construía a 

noção de profundidade, que permite a visão tridimensional, desenvolveram um algoritmo 

cooperativo semelhante ao modelo de Hopfield . 

Em 1986, com a publicação do clássico “Parallel Distributed Processing” por D. 

Rumelhart e J. McClelland consolidou-se definitivamente o avanço nos estudos sobre as redes 

neurais com a criação de associações de pesquisadores e estudiosos, com a fundação de 

empresas de software para a sua exploração comercial e com uma proliferação de publicações 

e de periódicos. Com a introdução do algoritmo de Retropropagação do Erro 

(Backpropagation) houve um aumento substancial nas aplicações das redes neurais. 

Atualmente, há diversos grupos utilizando-as em Neurologia, Psicologia, Medicina, Ciência 

da Computação e em outros ramos do conhecimento humano, o que demonstra a grande 

capacidade que elas possuem para a resolução de problemas de grande complexidade com 

resultados impressionantes. Para finalizar, em 1987, os pesquisadores Sejnowsky e Rosenberg 

desenvolveram redes para a conversão de texto e a correspondente representação fonética. 

Burr desenvolveu redes para o reconhecimento de caracteres escritos manualmente enquanto 

Cottrell, Munro e Zipser desenvolveram redes para a compressão de imagens. Acrescentam-se 

as redes de Funções de Base Radial, desenvolvida por M. J. D. Powell, em 1985, Renals, em 

1989, Moody e Darken, em 1990, Poggio e Girosi, também em 1990 e Park e Sandberg, em 



32 
 

1991, que são utilizadas para a classificação e para a aproximação de funções introduzidas 

pela função de ativação gaussiana nos neurônios das camadas intermediárias. 

 

2.4. Regra Delta e Regra Delta Generalizada 

2.4.1 Regra Delta 

 
Basicamente as regras de aprendizado aqui analisadas consistem em um processo 

iterativo de ajuste dos pesos das conexões da rede. Na forma matricial, se a matriz dos pesos 

das conexões (pesos sinápticos) for W, então o processo iterativo é desenvolvido pela 

fórmula: 

W(n + 1) = W(n) + ? W(n),             (2.8) 

onde W(n + 1) representa a matriz dos pesos no instante (ou passo) n + 1, W(n) a matriz no 

instante n e ? W(n) representa a matriz de ajustes aplicado aos pesos das conexões. Para cada 

maneira diferente de se calcular ? W(n) há um algoritmo de aprendizado diferente. 

A regra delta elaborada por B. Widrow e M. Hoff (WIDROW &amp;amp; HOFF, 1960), com 

base na regra de Hebb, é um processo de minimização do erro calculado entre a saída 

desejada (alvo) e a saída obtida através da aproximação pelo método dos mínimos quadrados. 

É uma regra supervisionada, segundo Abdi et al. (1999), Haykin (2001) e Braga et al. (2007). 

A regra básica (FÓRMULA 2.8) na forma matricial pode ser decomposta para cada peso, 

conforme a equação a seguir  

)()()1( nwnwnw ijijij ???? ,        (2.9) 

onde i representa a célula de entrada e j a célula de saída. Considerando jt  o valor desejado, 

jt?  o valor encontrado pela rede no instante n , o erro de aproximação é  

jjj tte ˆ?? .                  (2.10) 

Assim, 

)(..)()1( nxenwnw ijijij ????                               (2.11) 



33 
 

onde ?  é uma constante positiva denominada de taxa de aprendizado, ??  [0,1], e )(nxi  é o 

valor de ativação (entrada) i do neurônio. Ainda, de acordo com Braga et al., 2007, a Equação 

(2.11) aparece também no algoritmo de treinamento do Perceptron, no algoritmo de 

treinamento das redes ADALINE, de B. Widrow, e na generalização do algoritmo de 

Retropropagação do Erro (Backpropagation). 

A minimização do erro deve ocorrer quando se aplicam as derivadas 

parciais na soma dos erros quadráticos das saídas 

 ?
?

?
p

j
jee

1

22

2

1
,                     (2.12) 

onde p é o número de exemplos de treinamento da rede, o número ½ é um fator de controle e 

não afeta o resultado, de acordo com o ajustamento pelo Método dos Mínimos Quadrados. No 

caso do aprendizado Hebbiano, tem-se:  

)().(. nxnyw ijij ??? ,                    (2.13) 

onde )(ny j  é a saída j do neurônio e )(nxi  é a entrada i do neurônio. Especificamente para o 

Perceptron, os autores Braga et al.  (2007), apresentam este algoritmo de aprendizado com o 

auxilio do que chamam de Portas de Limiar (Threshold gates) Linear, Quadrática e 

Polinomial e, novamente, neste caso, fica bem evidente a similaridade entre os conceitos de 

Portas de Limiar e a Análise de Regressão, com fórmulas semelhantes e com a minimização 

efetuada com a fórmula da soma quadrática dos erros. Ressaltam ainda que o ADALINE é 

também um aproximador linear de funções. 

 

2.4.2 Regra Delta Generalizada 

 

A Regra Delta Generalizada ou Regra de Retropropagação do Erro ou, ainda, Regra 

Backpropagation, é de aprendizado supervisionado e pode ser aplicada à qualquer rede direta 

multicamadas com unidades de processamento não lineares. A diferença entre uma unidade 

linear e outra não linear é a função de transferência ou de ativação, que no segundo caso é não 

linear. 



34 
 

O desenvolvimento do algoritmo será feito, sem perda de generalidade, com uma 

camada de entrada, uma camada oculta (intermediária ou hidden) e uma camada de saída, 

conforme a Figura 8. 

 

Figura 8 – Uma rede direta de três camadas. 

 

Fonte: Abdi et al. (1999). 

 

Na Figura 8, a camada de entrada é constituída por I neurônios, a camada oculta é 

constituída por L neurônios e a camada de saída é constituída por J neurônios. A matriz dos 

pesos sinápticos Z, que conecta os neurônios da camada de entrada aos neurônios da camada 

oculta (no MATLAB, esta matriz é denotada por IW) é de ordem I X L, a matriz de pesos 

sinápticos W, que conecta os neurônios da camada oculta aos neurônios da camada de saída 

(no MATLAB, ela é denotada por LW) é de ordem L X J, o vetor do padrão de entrada x k  é 

de ordem I X 1. Da mesma forma, o vetor h k  é de ordem L X 1, kt?  é de ordem J X 1. A saída 

desejada é o vetor kt , também de ordem J X 1, de acordo com Abdi et al. (1999). 

A diferença entre a resposta obtida (estimada) kt?  de uma unidade de saída e a resposta 

desejada kt  é o erro produzido pela rede. As células da camada de saída usam este erro 



35 
 

diretamente para corrigir os seus pesos nas conexões. O mesmo não acontece com as células 

da camada oculta, que não estão em contato direto com alguma fórmula de erro, de forma que 

elas estimam os próprios erros. Neste ponto entra o erro de retropropagação. O crescimento 

do erro cometido é inicialmente um sinal de erro proporcional à razão de mudança (tangente 

ou derivada) da função de ativação não linear. Este sinal de erro é passado para trás através 

dos pesos das conexões da unidade oculta. Em seguida, todos os pesos das conexões são 

atualizados, conforme Abdi et al. (1999).  

Na Regra Delta de unidades lineares a correção dos pesos é proporcional ao erro 

cometido e ao valor da célula de entrada, enquanto na Regra Delta Generalizada, a correção 

dos pesos sinápticos é proporcional à razão de mudança da função de transferência não linear 

(ABDI et al.,1999). Na camada de saída, tem-se, então, 

? W lj  = f ´(a j ). ? ?jj tt ˆ?  .h l  =                   (2.14) 

 = f ´(a j ).e j .h l  =                     (2.15) 

= lj h.? ,                      (2.16) 

onde a j é a saída para a função de transferência. O sinal de erro é j?  = f’(a j ).e j . Na camada 

oculta o erro é estimado mediante a aplicação da Equação (2.17) 

le?  = ?
j

jljw ?  e,                      (2.17)  

consequentemente, na camada de entrada tem-se 

? Z il  = [f ´(a l ). le? ].x i  =                    (2.18) 

= il x.? ,                      (2.19) 

 onde a l  é a saída para a função de transferência. 

Na modalidade de único estímulo (termos com índice k), de acordo com Abdi et al. 

(1999), é mostrado o funcionamento da Regra Delta Generalizada através da sequência das 

equações que seguem abaixo. Na ida ou no sentido da propagação do erro, a sequência se 

caracteriza pelos seguintes passos, onde o símbolo T,  que acompanha as matrizes Z e W, 

significa que a matriz é transposta: 



36 
 

Camada de entrada x k ; 

Camada oculta h k  = f(Z
T x k ); 

Camada de saída kt?  = f(W
T h k ); 

Erro de sinal kt?  = f(W
T h k ). 

Agora, no retorno, no sentido da retropropagação do erro, tem-se: 

Camada de saída (s) e k  = kk tt ˆ? ; 

ks,? = f ´(W
T .h k ).e k ; 

W(n +1) = W(n) + ? .h k .
T

ks,?  = W(n) + ? W(n). 

Camada oculta (o) ko,? = f ´(Z
T .x k ).e k ; 

Z(n + 1) = Z(n) + ? .x k .
T

ko,?  = Z(n) + ? Z(n). 

Ao retornar à camada de entrada, inicia-se novamente o processo de ida e de volta até 

que o erro seja minimizado para resolver o problema. No MATLAB, cada uma dessas 

passagens é considerada como um passo ou um epoch, do original em inglês. Para facilitar os 

cálculos, pode-se também colocar, de forma simplificada (Formulas 2.20), 

b = Z T .x                      (2.20) 
h = f(b)  
a = W T .h  
t?  = f(a)  
e = tt ˆ?   

s?  = f ´(a).e  
e?  = W. s?   

o?  = f ´(b). e?   
 

Z(n + 1) = Z(n) + ? .x. To?   

W(n + 1) = W(n) + ? .h. Ts?  



37 
 

Nestas fórmulas, o índice s está relacionado à camada de saída, o índice o está 

relacionado à camada oculta e ?  é a taxa de aprendizado. 

O objetivo deste algoritmo é o de encontrar iterativamente o mínimo da função do erro 

(superfície de erro), usando o método do gradiente descendente. O gradiente é um operador 

diferencial que aplicado a uma função de no mínimo duas variáveis aponta, sempre, no 

sentido do crescimento máximo dessa função. No sentido contrário, encontra-se o mínimo. 

Esta regra está intimamente relacionada com a técnica estatística da Análise de Regressão 

Não Linear. Considerando novamente a forma matricial do erro e usando a regra da cadeia 

adaptada para matrizes, demonstra-se o método do gradiente descendente conforme 

desenvolvimento a seguir. Assim, se 

? ? ? ? ? ?kTkkTkkTkkkTkkk ttttttttttE ˆ2ˆˆ
2

1ˆˆ
2

1
??????                 (2.21) 

é a função de erro, então o gradiente aplicado a esta função é 

W

hW

hW

t

t

E

W

E
E k

T

k
T
k

k

kk
kW ?

?
?
?

?
?

?
?
?

??
ˆ

ˆ
,                 (2.22) 

onde cada derivada parcial, calculada em separado, fornece os resultados 

? ?kk
k

k tt
t

E
ˆ

ˆ
???

?
?

,                    (2.23) 

? ? ? ?kT
k

T
k

T

k
T
k hWf

hW

hWf

hW

t
'

ˆ
?

?
?

?
?
? , e                     (2.24) 

T
k

k
T

h
W

hW
2?

?
? .                      (2.25) 

Donde se conclui que 
T
kkskW hE ,???? , e                 (2.26) 

? ? T kskhnW ,???? .                    (2.27) 

Novamente, usando a regra da cadeia adaptada para matrizes e considerando que: 

? ?? ?kTTk xZfWft ?ˆ  ,                    (2.28) 



38 
 

 
Z

xZ

xZ

h

Z

xZ

xZ

h

h

hW

hW

t

t

E

Z

E
E k

T

k
T

k
ko

k
T

k
T

k

k

k
T

k
T
k

k

kk
kZ ?

?
?
?

??
?

?
?
?

??
?

?
??
?

?
?

?
?
?

?
?

?
?
?

?? ,
ˆ

ˆ
? .               (2.29)  

Calculando cada derivada parcial em separado, encontram-se as expressões: 

W
h

hW

k

k
T

?
?

? ,                      (2.30) 

? ? ? ?kT
k

T
k

T

k
T

k xZf
xZ

xZf

xZ

h
'?

?
?

?
?
?  , e                   (2.31) 

T
k

k
T

x
Z

xZ
2?

?
? .                      (2.32) 

Consequentemente, 

T
kkokZ xE ,????  , e                    (2.33) 

? ? T kokxnZ ,???? .                     (2.34) 

Com estes resultados conclui-se a demonstração do método do gradiente descendente, 

aplicado na elaboração da Regra Delta Generalizada, conforme Abdi et al. (1999). 

Além das três regras de aprendizado aqui apresentadas, Haykin (2001) acrescenta o 

processo de aprendizagem baseado em memória e a aprendizagem de Boltzmann com 

excelente detalhamento. Também complementa com outros tópicos relacionados com as redes 

neurais artificiais. 

Por seu turno, Braga et al. (2007) apresenta em detalhes o modelo ADALINE, as redes 

auto-organizáveis, o desenvolvimento dos Sistemas Neurais Híbridos, tema de pesquisa mais 

recente, onde um dos subsistemas é uma rede neural artificial, e as redes neurais sem pesos. 

 

 

 

 

 

 

 



39 
 

3. MATERIAIS E MÉTODOS 

3.1. O Pólo Cerâmico de Santa Gertrudes 

 

O Pólo Cerâmico de Santa Gertrudes (PCSG) constituído pelos municípios de Limeira, 

Cordeirópolis, Santa Gertrudes, Rio Claro, Ipeúna, Piracicaba e Araras é integrado por 34 

indústrias cerâmicas, de um total de 47, dentro do Estado de São Paulo, que são filiadas à 

Associação Paulista das Cerâmicas de Revestimento, ASPACER, (ASPACER, 2012). 

Por ter acompanhado as inovações e mudanças tecnológicas pela importação, 

principalmente, de máquinas e equipamentos de países sobre os quais foram impostas severas 

regras para o controle ambiental da produção industrial de transformação, ainda de acordo 

com Poletto (2007), os impactos ambientais causados pelas indústrias do já citado APL de 

Santa Gertrudes sofreram uma sensível redução, contrário ao das mineradoras que ainda, na 

ocasião em que foi elaborada a sua pesquisa, não haviam modernizado os seus processos de 

lavra. Contudo, o mesmo autor pondera que com uma fiscalização mais rigorosa por parte dos 

órgãos e das entidades governamentais associadas a melhorias técnicas advindas das 

modernas máquinas e dos equipamentos de mineração e, também, com mudanças na forma de 

extração das argilas, inclusive as mineradoras estão se esforçando no sentido de diminuir os 

impactos ambientais por elas causados, dentro das possibilidades e orientações de cada uma 

delas. Novamente, Poletto (2007) considera que o maior impacto ambiental causado pelas 

mineradoras é aquele da desfiguração topográfica, para a qual sugere algumas soluções como, 

por exemplo, a revegetação, além da reversão da área já explorada em terras produtivas e 

autosustentáveis. 

 

3.2. Caracterização das Argilas e das Cerâmicas 

 

De acordo com Christofoletti e Moreno (2004), as argilas da Formação Corumbataí 

constituída basicamente de argilitos, siltitos, arenitos, calcários, predominante no APL de 

Santa Gertrudes, são argilominerais que permitem, dentre outros, o seu aproveitamento 

dentro, principalmente, do grupo BIIb, segundo a norma NBR 13818 da Associação Brasileira 

de Normas Técnicas, ABNT, (ABNT, 1997). Conforme os textos de Christofoletti et al. 

(2005), Poletto (2007b) e Prado et al. (2008), as argilas de boa qualidade dessa formação 

ocorrem no interior do Estado de São Paulo, onde se localiza o PCSG com aproximadamente 



40 
 

90 m de espessura (altura) numa largura que atinge aproximadamente 10 km. De acordo com 

a ASPACER (2012) esta formação possui uma continuidade aproximada de 200 km. 

Em sua composição mineralógica destacam-se a illita, a montmorillonita, a clorita, o 

quartzo, o feldspato, os carbonatos (em níveis localizados), os compostos de Fe e Ti , além da 

presença de argilominerais interestratificados, segundo Gaidzinski (2006) e Prado et al. 

(2008), ressaltando que a ocorrência das argilas não é homogênea. Ocorrem variações nos 

teores de uma mina para outra e, mesmo dentro de uma cava de lavra, a composição das 

argilas pode variar tanto horizontalmente quanto verticalmente, esta, porém, menos frequente. 

Em complemento, de acordo com Christofoletti et al. (2005), Gaidzinski (2006), Poletto 

(2007) e Prado et al. (2008), essas argilas apresentam em sua composição química alguns 

óxidos, principalmente, SiO 2  e AlO 3 , ambos responsáveis pelas propriedades refratárias, os 

carbonatos (terrosos) CaO, MgO, Na 2 O, que juntos ao óxido K 2 O são responsáveis pelas 

propriedades de queima (ou sinterização). Completam esta lista Fe 2 O 3 , TiO 2 , MnO, P 2 O 5  e 

Mn 2 O 3 . Da mesma maneira há variações, visto que a porcentagem destes óxidos presentes 

nas argilas é diferente em cada cava de lavra. 

Prado (2007) verificou que as indústrias cerâmicas misturam, no máximo, argilas 

provenientes de três fontes diferentes e que na preparação da massa para moagem executam 

uma mistura de matérias-primas. Também, complementa que o grupo BIIb especificado na 

norma NBR 13818 (ABNT, 1997) caracteriza um produto semi-poroso, com alta absorção de 

água mas com baixa resistência mecânica. Devido à porosidade, medida em porcentagem pela 

variável física absorção de água (Abs), a classificação das placas cerâmicas é apresentada na 

Tabela 1. 

 

Tabela 1 – Classificação das placas cerâmicas quanto à porosidade. 
PRODUTO Abs (%) 

Poroso 10 – 20 

Semi-poroso 6 – 10 

Semi-grês 3 – 6 

Grês 0.5 – 3 

Porcelanato &amp;lt;0.5 

Fonte: Prado (2007). 

 



41 
 

O porcelanato é um produto cerâmico de alto valor agregado e a sua produção é a que 

mais cresce no mundo. Paralelamente aos dados da Tabela 1, Prado (2007) acrescenta a 

Tabela 2 (como consta da norma NBR 13818, ABNT,1997). 

 

Tabela 2 – Especificações da NBR 13818 (ABNT, 1997). 
 MÉTODOS DE FABRICAÇÃO 

Abs(%) Extrudado (A) Prensado (B)  Outros (C) 

    

    

Abs ?  0.5 AI BIa CI 

0.5 &amp;lt;Abs ?  3.0 AI BIb CI 

3.0 &amp;lt;Abs ?  6.0 AIIa BIIa CIIa 

6.0 &amp;lt;Abs ?  10.0 AIIb BIIb CIIb 

Abs &gt; 10.0 AIII BIII  CIII 

Fonte: (ABNT, 1997) e (PRADO, 2007). 

 

A indústria de revestimentos cerâmicos está inserida na indústria de materiais de 

construção que, por sua vez, faz parte da indústria de transformação. No APL de Santa 

Gertrudes, o principal processo de fabricação é o chamado Processo Via Seca (CORREIA et 

al., 2007), com o qual, em princípio, não há misturas e nem homogeneização, mas inferioriza 

a qualidade do produto final, ou seja, os pisos e revestimentos cerâmicos. Porém, este 

processo industrial é largamente usado porque é aplicado com menores custos energéticos, 

menores custos de manutenção, menores impactos ambientais como um importante resultado, 

apesar de apresentar limitações tecnológicas. Conforme Moreno et al. (2009), a argila é moída 

a seco, os ciclos são de monoqueima rápida com intervalos de tempo de 20 a 30 minutos, com 

uma temperatura máxima superior a 1000° C. Christofoletti (1999) reforça que na moagem a 

seco a umidade é de valor aproximado de 5%, enquanto na prensagem a seco a umidade é de 

ordem aproximada a 10%. A monoqueima rápida é considerada melhor do que a biqueima, 

outro processo de fabricação. Recentemente foi implantado por algumas indústrias brasileiras, 

um terceiro processo de fabricação denominado de terceira queima, o qual permite alguns 

efeitos especiais na produção. Em resumo, Christofoletti (1999) e Prado (2007) consideram as 

seguintes fases nas operações de mineração: 

1. Pesquisa mineral; 

2. Remoção da cobertura vegetal (ou desmatamento); 



42 
 

3. Decapeamento; 

4. Desmonte (por explosivos ou por meios mecânicos); 

5. Carregamentos; 

6. Britagem (ou trituração); 

7. Sazonamento (em pilhas) ou 

8. Pátio de secagem e de homogeneização; e 

9. Armazenamento (para o posterior transporte de argila). 

E, para as operações industriais: 

1. Extração e transporte da argila; 

2. Preparação da massa (via seca) e moagem; 

3. Atomização; 

4. Prensagem (ou moldagem) e secagem; 

5. Preparação do esmalte e esmaltação (ou não esmaltados) – faz parte do conceito de 

decoração; 

6. Queima (monoqueima rápida); 

7. Retífica e polimento; 

8. Classificação do produto e embalagem; 

9. Transporte interno (empilhamento e carregamento de caixas); 

10. Lavagem de equipamentos e pisos; 

11. Geração de resíduos sólidos; e 

12. Seleção, expedição e comercialização. 

 

Com relação à homogeneização das argilas, Gaidzinski (2006) fez uma análise de 

sazonamento, que consiste na estocagem das argilas a céu aberto por períodos de tempo que 

variam de 6 meses a 2 anos, e concluiu que para se conseguir esta homogeneização o período 

de tempo deve ser superior a um ano, com as pilhas mantidas em local aberto. Para os seus 

estudos, também usou argilas provenientes do APL de Santa Gertrudes. 

Finalmente, a argila, como qualquer outro mineral, é um recurso não renovável, 

significando que uma mina é temporária e, levando isto em consideração, pode-se dizer que 

ela possui um ciclo de vida (AMBIENTE BRASIL, 2009). Este ciclo de vida de uma mina é 

constituído pelas seguintes fases: 

1. Planejamento (pesquisa mineral, estudos ambientais, estudos de viabilidade); 



43 
 

2. Implantação; 

3. Operação; 

4. Desativação (preparação para o fechamento); 

5. Fechamento; e 

6. Transferência de responsabilidade. 

Neste momento, convém salientar que o Ambiente Brasil (2009) sugere que para se 

aumentar o ciclo de vida de uma mina, as alternativas são a redução de consumo e a 

reciclagem das matérias-primas. 

 

3.3. Os Corpos-de-Prova 

 

As amostras para a confecção e moldagem dos corpos-de-prova foram retiradas das 

nove minas: Cristofoletti minas 1 e 2, Cruzeiro frentes 1 e 2, Paganoti, Partezani minas 1, 2 e 

3, Pieroni e Tute, identificadas e localizadas conforme os dados que constam da Tabela 3. 

 

Tabela 3 – Coordenadas UTM obtidas por GPS das minas estudadas (Datum: SAD 69). 
MINAS SIGLA COORDENADA

N - S 

COORDENADA 

E – W 

ALTITUDE 

(m) 

Cristofoletti Mina 1 CF1 0231532 7521059 540 

Cristofoletti Mina 2 CF2 0230592 7526781 598 

Cruzeiro Frente 1 CR1 0248481 7505118 654 

Cruzeiro Frente 2 CR2 0249023 7505458 641 

Paganoti PG 0251306 7531241 690 

Partezani Mina 1 PT1 0234656 7506449 573 

Partezani Mina 2 PT2 0235424 7507416 567 

Partezani Mina 3 PT3 0234843 7507990 560 

Pieroni PI  0232239 7512374 552 

Tute TU 0241575 7535652 639 

Fonte: Rocha (2012). 

 

A Tabela 3 apresenta as coordenadas de localização das minas, zona 23K e, também, 

as altitudes das minas escolhidas, as quais foram obtidas com a utilização do Sistema de 

Posicionamento Global, GPS (Global Positioning System). 



44 
 

A Mina Cruzeiro pertence ao município de Limeira, a Mina Paganoti faz parte do 

município de Araras, enquanto as Minas Cristofoletti, as Minas Partezani, a Mina Pieroni e a 

Mina Tute pertencem ao município de Rio Claro. As áreas das minas Cruzeiro Frente 1 e 

Cruzeiro Frente 2, embora próximas, possuem características distintas e dessa maneira foram 

consideradas em separado. As minas Partezani 1, 2 e 3 encontram-se dentro da mesma 

propriedade, mas estão localizadas em áreas bem distintas devido a grande extensão territorial 

da propriedade. De acordo com Rocha (2012), a escolha destas minas foi feita em função de 

que elas permitiram a obtenção de amostras bem diferenciadas e, principalmente, por 

representarem toda a coluna estratigráfica da Formação Corumbataí. 

Ainda, conforme Rocha (2012), para a coleta sistemática das amostras foi utilizada 

uma perfuratriz de acionamento pneumático e máquinas escavadeiras. O material recolhido 

foi armazenado em sacos plásticos com a respectiva identificação, a qual consistiu das 

seguintes informações: o nome da amostra (com referência à bancada – designadas por B1, 

B2, B3, B4 e B5, respectivamente, da base ao topo da mina – e a mina de onde ela foi 

recolhida), a data da coleta, a profundidade do furo e o posicionamento obtido pelo GPS. Foi 

estabelecida uma quantidade mínima de 30 kg por amostra de forma a permitir que fossem 

feitos todos os testes necessários durante a etapa laboratorial. Os corpos-de-prova foram 

preparados pelo método de moagem pela Via Seca, à semelhança do método empregado no 

PCSG. 

A Figura 9 apresenta duas fotos da mina Pieroni, as quais não expõem claramente a 

superposição das bancadas. Por isto, foi acrescentada a Figura 10 que apresenta duas fotos da 

mina Santa Amábile, também de Rio Claro que, apesar de não constar da relação das minas 

estudadas, no momento das fotos, expôs de forma bem clara a superposição das bancadas. 

Nestas fotos, entre a base e o topo tem-se aproximadamente um desnível de 45 m, onde a 

bancada B1 de coloração bem escura e contaminada com calcário, possui uma espessura 

aproximada de 25 m, a seguir são visíveis   bancadas de coloração roxa, todas elas com uma 

espessura total de 15 a 20 m. Ainda, a bancada B2 apresenta argilas duras, B3 argilas 

intermediárias e, por fim, a bancada B4 apresenta argilas moles, seguindo o modelo utilizado 

pelas indústrias da região do PCSG, de acordo com Rocha (2012). 

Na preparação dos corpos-de-prova, as matérias-primas foram previamente moídas e 

homogeneizadas. Em seguida, foi adicionada água para que a umidade atingisse um teor de 

10%, em cujo controle foi utilizada uma balança com infravermelho (marca Gehaka, modelo 

BK 6000) e, também, com uma peneira de malha 8 ABNT (2.36 mm). Em sequência, as 



45 
 

matérias-primas foram completamente homogeneizadas, com o teor de umidade em 10%, 

após um período de 24 horas. Finalmente, utilizando-se do método de prensagem a seco num 

molde de dimensões 10.0 X 3.5 cm e com uma espessura mínima inferior a 7.5 mm foi 

aplicada uma pressão de 250 kgf/cm 2 numa prensa hidráulica, cujo controle foi obtido com    

a determinação da densidade aparente do corpo-de-prova úmido em  2g/cm 3 , com massa de 

50 g. 

 

 

 



46 
 

Figura 9 – Fotos da mina Pieroni. 

 

 

Fonte: Autor (2012). 

 

 

 

 



47 
 

Figura 10 – Fotos da mina Santa Amábile. 

 

 

Fonte: Autor (2012). 

 



48 
 

3.4. Determinação das Variáveis Físicas dos Corpos-de-Prova 

 

Para amostras secas foram calculadas as variáveis físicas Retração Linear de Secagem 

(RLS), Densidade de Prensagem (DP) e a Densidade Aparente dos Corpos-de-Prova Secos 

(DAS). Para as amostras sinterizadas foram calculadas as variáveis físicas Módulo de 

Resistência à Flexão (MRF), Retração Linear de Queima (RLQ), Absorção de Água (Abs), 

Carga de Ruptura (CR) e Perda ao Fogo (PF). Os ensaios para o cálculo das variáveis Abs , 

CR e MRF seguiram as especificações técnicas da norma NBR 13818 (ABNT, 1997), cujos 

valores são: 6 ?  Abs &amp;lt;10 (grupo BIIb), CR &gt; 500 N e MRF &gt; 18 MPa. 

Tanto a variável DP como a variável DAS foram calculadas pela fórmula 

Densidade = 
V

m
 ,           (3.1) 

onde m é a massa do corpo-de-prova, em g , e V é o volume do corpo-de-prova, em cm 3 . A 

unidade de densidade é g/cm 3 . 

Medindo-se com um paquímetro o comprimento, em mm, dos corpos-de-prova úmidos 

após a prensagem ( pL ) e o comprimento dos mesmos corpos-de-prova após a secagem ( sL ) 

em uma estufa mantida à temperatura constante de 110° C por 24 horas, a variável RLS foi 

calculada, em porcentagem, usando-se a expressão: 

.100*
p

ps

L

LL
RLS

?
?                      (3.2) 

Em seguida, os corpos-de-prova foram secados até o teor de umidade atingir um valor 

inferior a 1% e queimados       (forno de rolo de laboratório – marca Cifel) em um ciclo de 30 

minutos nas temperaturas de 1000° C, 1020° C, 1040° C e 1060° C. 

Para o cálculo do MRF, em MPa, os corpos-de-prova já prensados, secos e queimados, 

utilizou-se de um flexímetro (marca Nannetti, modelo FM/96) durante os ensaios. A fórmula 

usada foi  

,8066.9*
2

3
2
minbe

FL
MRF ?           (3.3) 

 



49 
 

onde F é a força de ruptura, em kgf (1 kgf = 9.9066 N – fator de correção das unidades na 

fórmula (3.3)), L é a distância entre as barras de apoio, em mm, b é a largura do corpo-de-

prova ao longo da ruptura após o ensaio, em mm, e mine  é a espessura mínima do corpo-de-

prova, também em mm. 

Analogamente, foram calculadas: 

,100*
1

12

m

mm
Abs

?
?            (3.4) 

onde Abs é a absorção de água, em porcentagem, 1m  é a massa seca, em g, e 2m  é a massa 

úmida ou saturada, em g ; 

,
b

FL
CR ?             (3.5) 

onde CR é a carga de ruptura, em N, F é a força de ruptura, em N, L é a distância entre as 

barras de apoio, em mm e b é a largura do corpo-de-prova ao longo da ruptura após o ensaio, 

em mm; 

,100*
s

sq

L

LL
RLQ

?
?           (3.6) 

onde RLQ é a retração linear de queima, em porcentagem, qL  é o comprimento do corpo-de-

prova queimado, em mm, e sL  é o comprimento do corpo-de-prova seco, em mm; e, 

finalmente 

,100*
s

sq

m

mm
PF

?
?            (3.7)  

onde PF é a perda ao fogo, em porcentagem, qm  é a massa do corpo-de-prova queimado, em 

g, e sm  é a massa do corpo-de-prova seco, em g. Neste caso, foi usada uma balança de 

precisão (mediu a massa com três casas decimais). 

Os dados dos ensaios realizados, de acordo com Rocha (2012), constam dos ANEXOS 

A (1000°C), B (1020°C), C (1040°C) e D (1060°C). No ANEXO A entre as minas e suas 

bancadas foram encontrados um total de 24 valores para as variáveis físicas RLQ, PF, Abs, 



50 
 

CR e MRF. Da mesma maneira, nos ANEXOS B, C e D as variáveis são DP, DAS, RLS, 

RLQ, PF, Abs, CR e MRF, sendo que o número de valores encontrados foram 25, 24 e 23, 

respectivamente. Nestes ANEXOS, foram calculadas também a amplitude, a média aritmética 

e o desvio padrão. 

 

3.5. Metodologia 

 

Em princípio, o método empregado neste trabalho será o dedutivo. 

A indústria cerâmica busca se estabelecer em um nicho de mercado e para isso precisa 

determinar para o seu produto um padrão de qualidade, dentro das normas técnicas brasileiras 

emanadas pela ABNT. Por outro lado, de acordo com Rocha (2012), o Brasil já atingiu o 

segundo lugar no mundo com a produção de pisos e revestimentos cerâmicos, mas ainda 

encontra-se em quinto lugar dentro do pequeno grupo de países exportadores, por isso há a 

esperança de que num futuro próximo o Brasil também se aproxime dos primeiros lugares. 

Em busca de novos mercados, as indústrias cerâmicas brasileiras terão que se preparar para 

enfrentar uma dura concorrência num mundo cada vez mais globalizado, competitivo e cada 

vez mais carente de produtos e serviços. Em função da sua política e dos seus interesses em 

como penetrar nestes novos mercados, cada indústria cerâmica, via de regra, deverá 

estabelecer um padrão de especificação diferente para cada um dos seus produtos. Em vista 

disso, optou-se neste trabalho de pesquisa por um padrão de saída (especificação) da rede 

neural (particular e independente dos valores especificados por qualquer indústria do PCSG) 

baseado nos valores próximos aos da média aritmética geral das médias aritméticas dos 

ANEXOS A (1000°C), B (1020°C), C (1040°C) e D (1060°C), que são aqueles que 

apresentam os dados completos dos ensaios dos corpos-de-prova do maior número de 

bancadas e de minas, que fazem parte da Tabela 4, a seguir: 

 



51 
 

Tabela 4 – Médias dos ANEXOS A, B, C e D, média geral e o padrão de produto estabelecido neste trabalho. 
MÉDIAS DP 

(g/cm 3 ) 

DAS 

(g/cm 3 ) 

RLS 

(%) 

RLQ 

(%) 

PF 

(%) 

Abs 

(%) 

CR 

(N) 

MRF 

(Mpa) 

ANEXO A - - - 3.7030 3.3355 10.2555 473.34 37.34 

ANEXO B 2.0288 1.8666 0.3078 5.2342 3.9364 7.3188 599.79 50.77 

ANEXO C 2.0241 1.8559 0.3052 6.1957 4.1189 5.8655 649.04 56.13 

ANEXO D 2.0272 1.8604 0.2926 6.6995 4.5172 4.5365 684.4 59.41 

         

Média 

Geral 

2.0267 1.8610 0.3019 5.4581 3.9770 6.9941 601.64 50.91 

         

PADRÃO 2 1.9 0.3 5.5 4 7 602 51 

Fonte: Elaborada pelo autor. 

 

É interessante observar que na Tabela 4, com exceção das variáveis físicas DP e DAS, 

as variáveis RLQ, PF, CR e MRF apresentam valores crescentes em função do aumento da 

temperatura, contrário ao das variáveis físicas RLS e Abs, que apresentaram valores 

decrescentes, tendência que será utilizada para reforçar a aplicação das redes neurais 

artificiais. 

Esta escolha garante uma independência com relação aos padrões especificados nas 

indústrias do PCSG. Como há um padrão pré-estabelecido, com relação à aplicação mediante 

a utilização das redes neurais artificiais, optou-se pela rede que apresenta um padrão (ou 

vetor) de saída desejado. A escolha recaiu então na rede MLP (Multilayer Perceptron) ou rede 

Perceptron de múltiplas camadas, conforme Haykin (2001). A vantagem desta escolha é a sua 

arquitetura, a qual permite que a rede neural seja montada com uma camada de entrada (ou 

padrão de entrada), uma camada intermediária ou oculta e uma camada de saída (ou padrão de 

saída) que corresponde ao já citado padrão de especificação. Com isto, a diferenciação entre 

uma rede e outra para encontrar o padrão de saída desejado fica por conta do número de 

neurônios da camada oculta. Além da arquitetura, pode-se dizer que outra vantagem é que a 

rede MLP permite a aplicação da Regra de Aprendizado Delta Generalizada (ou regra 

backpropagation, do original em inglês) ou Regra de Retropropagação do Erro, uma das mais 

poderosas regras de aprendizado das redes neurais (ABDI et al., 1999). Outra vantagem é a 

facilidade de aplicação desta rede dentro dos softwares conhecidos atualmente. 



52 
 

Para aplicar a regra de aprendizado backpropagation na rede MLP foi utilizado o 

software MATLAB® (MATrix LABoratory), da MathWorks, Inc., cujo programa detalhado 

consta do Neural Networks Toolbox User’s Guide. Neste trabalho, além da regra básica foram 

escolhidas outras três variações da mesma regra, dentre outras que constam do mesmo 

software, que são a Regra Backpropagation com Momento, a Regra Backpropagation 

Resiliente e a Regra Backpropagation de Levenberg-Marquadt, sendo que cada uma destas 

variações apresenta uma velocidade diferente de convergência. Inicialmente, foram 

padronizadas as tabelas para que as mesmas variáveis físicas fossem mostradas nas mesmas 

bancadas das mesmas minas e submetidas às mesmas temperaturas de queima. Dessa forma, 

com cinco variáveis físicas (RLQ, PF, Abs, CR e MRF) reuniram-se as tabelas de dados dos 

ANEXOS E (1000°C), F (1020°C), G (1040°C) e H (1060°C), as quais apresentam 16 linhas 

de amostras de dados, e com as oito variáveis físicas (DP, DAS, RLS, RLQ, PF, Abs, CR e 

MRF) apresentam-se as tabelas de dados dos ANEXOS I (1020°C), J (1040°C) e K (1060°C), 

nas quais constam 20 linhas de amostras de dados. Também, nestas tabelas, foram calculadas 

as respectivas amplitudes, médias aritméticas e o desvio padrão correspondente. Os dados que 

constam dos ANEXOS E, F, G, H, I, J e K são inseridos como o padrão (ou vetor) de entrada 

que, no MATLAB, é designado pela letra latina p (ANEXO L). 

Para empregar o MATLAB foram considerados os seguintes parâmetros de controle 

em função das quatro variações da regra de aprendizado backpropagation. São eles: show, 

epochs, lr, mc e goal. 

O parâmetro show significa que os resultados de saída do programa serão apresentados 

no gráfico correspondente ao erro (mse: erro médio quadrático) de acordo com um valor pré-

estabelecido (o valor utilizado foi 200, para visualização conveniente do gráfico); o parâmetro 

lr (learning rate, do original em inglês) é a taxa de aprendizado e que para dar estabilidade ao 

processo deve ser pequena, de acordo com sugestão inserida no software (foi usado lr = 0.05); 

o parâmetro mc é o do momento, para o qual o programa sugere um valor alto (neste caso, mc 

= 0.9). Tanto o parâmetro lr como o parâmetro mc assumem valores que pertencem ao 

intervalo de números reais [0,1]. Os parâmetros epochs e goal são parâmetros de parada do 

processo, isto é, independentemente de ter sido atingido ou não o ponto de convergência 

ótimo, eles finalizam o processo de execução do programa. No caso do epochs, ele significa 

quantas vezes o processo deve ser propagado (no sentido da camada de entrada para a de 

saída) e retropropagado (no sentido contrário) para se calcular o erro entre o valor obtido e o 



53 
 

valor desejado (iniciou-se com o valor 2000). E, finalmente, o parâmetro goal é o valor do 

erro médio quadrático (mse), que o software calcula mediante a aplicação da fórmula: 

])},21[({
*

1
MMsquaresummation

nn
mse ??            (3.8) 

onde n é o número de dados, M1 é a matriz correspondente ao padrão de saída esperado e M2 

é a matriz correspondente ao padrão de saída encontrado durante o processo. Em função dos 

dados das tabelas constantes dos ANEXOS E, F, G, H, I, J e K (três casas decimais no 

máximo), o valor do parâmetro goal foi fixado em 1e-5. 

Em seguida, foi estabelecido que a camada oculta de cada rede neural seria montada 

com um número que iria de 2 a 35 neurônios (N=2,3,4,...,35) ou N= 5,10,15,20,25,30,35, em 

alguns casos, e o processo, para cada número de neurônios, deveria ser aplicado em três testes 

diferentes para verificar a convergência. O número de testes de convergência foi colocado 

tendo em vista que é muito comum a superfície de resposta do erro (relacionado ao mse), 

além de apresentar um mínimo global, não necessariamente, indicando uma otimizada 

convergência, pode também apresentar vários mínimos locais (diferentes do mínimo global), 

por vezes intransponíveis durante a execução do processo, constituindo-se em verdadeiras 

armadilhas, antagônicas à convergência esperada. Dessa forma, verificado que nos três testes 

de convergência os erros, localizados na sua própria superfície de resposta de erro, ficaram 

presos nestas armadilhas, a aplicação do processo era encerrado no momento em que o 

número de neurônios da camada oculta atingisse o valor de N=35. Caso contrário, se entre os 

três testes fosse detectada alguma possibilidade de convergência na direção de um mínimo 

global, o número de testes poderia ser ampliado até para dez, e, da mesma maneira, o número 

de neurônios na camada oculta também poderia ser aumentado e, de fato, o foi alcançando o 

valor N=100, desde que se atingisse a estabilidade da rede neural; o mesmo poderia acontecer 

com o número de epochs que, em alguns casos, chegou a ser ampliado para 5000, 10000 ou 

até 20000. 

Como todos os dados são positivos, as funções de transferência usadas foram a 

logística (no MATLAB, logsig) entre a camada de entrada e a camada oculta e a linear (no 

MATLAB, purelin) entre a camada oculta e a camada de saída. Nestes programas, o software 

coloca como padrão de saída a letra latina t. Em tempo, ele usa como matriz dos pesos entre a 

camada de entrada e a camada oculta a sigla IW (no capítulo 2, a notação usada foi Z, 

conforme a Figura 8) e a matriz de limiar correspondente a notação b e como matriz dos pesos 



54 
 

entre a camada oculta e a camada de saída foi usada a sigla LW (novamente, no capítulo 2 a 

notação usada foi W, de acordo com a Figura 8) e a matriz de limiar correspondente a mesma 

notação b. O resultado da simulação é denotado pela letra latina a. 

 

Para a validação desta metodologia e consequentemente para garantir a efetiva 

aplicação das redes neurais artificiais à indústria de pisos e revestimentos cerâmicos do 

PCSG, a solução foi a de que encontrada, numa temperatura de queima (1000° C, 1020° C, 

1040° C e 1060° C) para as cinco variáveis físicas, ou nas temperaturas de queima de 1020° 

C, 1040° C e 1060° C, para as oito variáveis, um valor convergente compatível com o erro 

(mse) pré-estabelecido (goal), ou ainda, compatível com o padrão especificado originalmente 

determinado, as matrizes de pesos encontradas, LW e IW, junto com as matrizes de limiares, 

seriam então aplicadas às outras temperaturas de queima. Caso os resultados fossem 

conformes com o padrão de saída, ou ainda, dentro das especificações da norma NBR 13818 

(ABNT, 1997), a validação desta metodologia estaria comprovada.  

 

 



55 
 

4. RESULTADOS E DISCUSSÕES 

4.1. Introdução 

 

Em testes preliminares para  viabilizar se uma rede neural artificial e o algoritmo de 

aprendizado, contudo com uma padronização dos valores das tabelas de dados para que todos 

eles ficassem restritos ao intervalo real [0,1] (observado que os valores das tabelas são todos 

positivos e que a função logística assume somente valores positivos dentro do mesmo 

intervalo real), iniciou-se o processo de análise com a regra de aprendizado backpropagation, 

no seu formato básico mas, como não se conseguiu um resultado satisfatório, avançou-se com 

a inclusão das variações ao método até que fosse encontrado um resultado que pudesse 

garantir a validade do método e, por consequência, ficasse comprovada a hipótese deste 

trabalho. Além disto, o processo deve ser viável, isto é, que o trabalho de pesquisa por parte 

da indústria consuma pouco tempo, seja de fácil manipulação e que os resultados sejam 

rigorosos para que se atinja um excelente controle de qualidade dos pisos e revestimentos 

cerâmicos nas indústrias incluídas no PCSG. 

 

4.2. Regra de Aprendizado Backpropagation (Formato Básico) 

 

Escolhidas a rede neural artificial Perceptron de três camadas, a regra de aprendizado 

backpropagation e o software MATLAB 7.0, o processo de aplicação envolveu inicialmente a 

regra de aprendizado na sua forma básica (traingd) às tabelas de dados dos ANEXOS E, F, G, 

H, I, J e K. Para os dados do ANEXO E que envolve cinco variáveis físicas e temperatura de 

queima em 1000° C foram determinados os parâmetros show = 200, lr = 0.05, epochs = 2000 

e goal = 1e-5. A performance do programa que corresponde ao cálculo do erro médio 

quadrático (mse) e aparece como  valor inferior ao parâmetro goal foi encerrado após a 

realização de três testes, pois os resultados obtidos foram de que para N = 2 neurônios até N = 

29 neurônios na camada oculta não ocorreu nenhum indício de convergência e o mse ficou 

preso num mínimo local (armadilha) com um valor extremamente alto igual a 55090.8 . Com 

N = 30, 31 e 34 neurônios, além do valor  anterior, foi encontrado um segundo valor como 

mínimo local, também muito alto. Com N = 32, 33 e 35 neurônios foram encontrados até três 

mínimos locais diferentes, sempre com valores altíssimos, e, inclusive, uma tendência de 

máximo absoluto em completa divergência. 



56 
 

Para os dados do ANEXO F (cinco variáveis físicas) com temperatura de queima igual 

a 1020° C foram utilizados os mesmos parâmetros e as mesmas condições. Os resultados 

encontrados foram de que para N = 2 até N = 31 neurônios na camada oculta, o mse ficou 

preso no mesmo mínimo local do caso anterior (55090.8), da mesma forma que com N = 33, 

34 e 35 neurônios. Quando N = 32 neurônios ocorreu uma tendência de divergência (máximo 

absoluto). 

Em virtude destes resultados, para os dados das tabelas dos ANEXOS G e H, 

submetidos aos mesmos parâmetros e condições o processo foi aplicado somente para N = 5, 

10, 15, 20, 25, 30 e 35 neurônios na camada oculta e, conforme o que se esperava, em todos 

os casos, o mse (goal) ficou preso no mesmo mínimo local de 55090.8, com exceção da tabela 

do ANEXO H, onde quando N = 30 neurônios, ocorreu novamente um resultado divergente. 

Em seguida, para as tabelas de dados dos ANEXOS I, J e K, agora com oito variáveis 

físicas, mas com os mesmos parâmetros e condições, o processo foi iniciado com os dados da 

tabela do ANEXO I (temperatura de queima de 1020° C) e aplicados de N = 2 até N = 35 

neurônios na camada oculta, mas nada mudou, o mse ficou da mesma forma preso num 

mínimo local, agora com o valor igual a 38546.7, ainda muito alto, com uma única exceção 

quando N = 34 neurônios, onde ocorreu uma divergência. Com estes resultados, para os 

outros ANEXOS, o processo foi aplicado somente para N = 5, 10, 15, 20, 25, 30 e 35 

neurônios na camada oculta e todos indistintamente ficaram presos no mesmo mínimo local 

de 38546.7, com uma única exceção, para N = 30 neurônios do ANEXO J onde, novamente, 

ocorreu a divergência. 

Onde se concluiu que depois de efetuados todos os testes para as tabelas de dados dos 

ANEXOS E, F, G, H, I, J e K, a regra de aprendizado backpropagation no seu formato básico 

não permitiu a aplicação das redes neurais artificiais à indústria cerâmica, tendo em vista que 

em quase todos os casos o erro médio quadrático (mse) ficou sempre preso num mínimo local 

de valor muito alto. Houve alguns casos excepcionais, onde ao invés da esperada 

convergência viu-se a ocorrência da divergência do processo. Isto assim colocado, este 

processo foi abandonado e, em seu lugar, foi testado uma variação que, teoricamente acelera a 

convergência, que é o backpropagation com momento dentro do próprio MATLAB, 

conforme item a seguir. 

 



57 
 

4.3. Regra de Aprendizado Backpropagation com Momento (traingdm) 

 

O método seguinte aplicado (com convergência acelerada) foi o da regra de 

aprendizado backpropagation com momento e, da mesma forma que o software sugere uma 

taxa de aprendizado pequena (próxima de zero) para a estabilidade da rede neural, ele também 

sugere que o momento deve ser grande (próximo de um). O valor utilizado foi mc = 0.9. 

Assim, para a modelagem da rede neural os parâmetros utilizados foram show = 200,             

lr = 0.05, mc = 0.9, epochs = 2000 e goal (mse) = 1e-5, com três testes para verificar se há 

algum indício de que a rede neural converge. 

Para a tabela de dados do ANEXO E (5 variáveis, 1000° C) foram usados de N = 2 até 

N = 35 neurônios na camada oculta e, sem exceção, todos os testes ficaram presos no mesmo 

mínimo local e sempre igual a mse = 55090.8, como no item anterior deste capítulo. É 

interessante observar que no caso em que N = 34 e 35 o número de epochs ficou entre 440 e 

570 e da mesma maneira o processo ficou preso na armadilha do mínimo local em dois testes 

dos três previstos. 

Analogamente, para a tabela de dados do ANEXO F (5 variáveis, 1020° C) com N = 2, 

3, 4,..., 35, o resultado foi sempre o mesmo, isto é, mse = 55090.8, inclusive com o fato de 

que o número de epochs não atingia o máximo previsto de 2000; ficou entre 440 e 570. 

Com isto, para as tabelas de dados dos ANEXOS G (5 variáveis, 1040° C) e H (5 

variáveis, 1060° C), o número de neurônios da camada oculta foi pré-fixado em N = 5, 10, 15, 

20, 25, 30 e 35 e, novamente, não ocorreu nenhuma tendência de convergência. 

Para a tabela de dados do ANEXO I (8 variáveis, 1020° C), inicialmente, com N = 2, 

3, 4, ..., 35 neurônios na camada oculta não ocorreu nenhum indício de convergência em todos 

os testes. Todos eles ficaram presos no mesmo mínimo local igual a mse = 38546.7 e da 

mesma maneira, para N = 8, 17, 18, 19, 22, 23, 25, 26, 28, 29 e 32 neurônios o valor do erro 

foi atingido com valores entre 440 e 570 epochs, não atingindo o máximo pré-estabelecido de 

2000 epochs. 

Para as tabelas dos ANEXOS J (8 variáveis, 1040° C) e K (8 variáveis, 1060° C), o 

mínimo local (armadilha) atingido ficou em mse = 38546.7 quando N = 5, 10, 15, 20, 25, 30 e 

35 neurônios na camada oculta. No caso dos dados do ANEXO J e N = 10, 20, 30 e 35 

neurônios e dos dados do ANEXO K com N = 25 e 35 neurônios o mínimo local foi atingido 

quando o número de epochs ficou entre 440 e 570. 



58 
 

Desta forma, como no caso do item anterior, esta regra não apresentou nenhum indício 

de convergência. Em função destes resultados, fizeram-se necessários a aplicação de 

processos com convergência bem mais acelerada do que este processo com momento. Foram 

então selecionadas as regras de aprendizado backpropagation resiliente e de Levenberg-

Marquadt, variações da regra no formato básico, sempre dentro do MATLAB. Aqui, convém 

lembrar que Cintra (2003) aplicou a regra de Levenberg-Marquadt para o controle de teores 

de cobre e ouro no depósito de Chapada (GO); a rede se estabilizou com  N = 32 neurônios na 

camada oculta. 

 

4.4. Regra de Aprendizado Backpropagation Resiliente (trainrp) 

 

Para a aplicação desta regra de aprendizado, os parâmetros de controle são apenas três: 

show = 200, epochs = 2000 e goal(mse) = 1e-5 e, como sempre inicialmente, foram realizados 

três testes. 

Com relação à tabela de dados do ANEXO E (5 variáveis, 1000° C), iniciou-se o 

processo com N = 2 neurônios na camada oculta. Novamente, o processo ficou preso no 

mesmo mínimo local (armadilha) igual a 55090.8. Com N= 3 neurônios, o primeiro indício de 

convergência surgiu, apesar de ser muito lento. Por isto, o número de epochs foi ampliado 

para 5000 e o número de testes aumentado para 5. Para N = 4 neurônios, a tendência de 

convergência persistiu e com isso o número de epochs foi gradativamente sendo aumentado 

para 5000, 10000 e 20000, com o intuito de verificar a convergência do processo na direção 

do mínimo global. O número de testes permaneceu em 5. Mesmo assim, a convergência 

mostrou-se muito lenta. Os mesmos resultados se repetiram para N = 5 e 6 neurônios. 

Quando foi fixado N = 7 neurônios na camada oculta, o número de testes mantido em 

5, e em contrapartida o número de epochs sendo aumentado gradativamente para 5000, 10000 

e 20000, apesar de algumas falhas, houve convergência rápida com apenas 242 epochs 

(Figura 11). 

 



59 
 

Figura 11 – Resultado encontrado para os dados do ANEXO E com N = 7 e 242 epochs. 

0 50 100 150 200
10

-6

10
-4

10
-2

10
0

10
2

10
4

242 Epochs

T
ra

in
in

g
-B

lu
e
  

G
o
a
l-
B

la
c
k

Performance is 8.76838e-006, Goal is 1e-005

 
Fonte: MATLAB® 7.0. 

 

A Figura 11 acima mostra que o erro médio quadrático (mse), dentro da performance é 

de 8.76838e-6. A simulação obtida pela aplicação da regra de aprendizado resiliente é 

recuperada pelo comando 

a  = sim (net,p),            (4.1) 

onde net significa a rede, p é o padrão de entrada e matriz encontrada a é dada por 

a = [5.5008 4.0002 7.0064 601.9992 50.9986]. 

Para efeito de comparação, o padrão pré-estabelecido para estas cinco variáveis físicas 

foi fixado através da matriz: 

PADRÃO(5) = [5.5 4 7 602 51]. 

Designando por p1 a matriz do padrão de entrada referente aos dados da tabela do 

ANEXO E e, da mesma forma, p2 para os dados da tabela do ANEXO F, p3 para os dados da 

tabela do ANEXO G e p4 para os dados da tabela do ANEXO H e recuperando a matriz de 



60 
 

pesos IW, entre os neurônios da camada de entrada e os neurônios da camada oculta, através 

do comando 

celldisp(net.IW),           (4.2)  

a matriz de pesos LW, entre os neurônios da camada oculta e os neurônios da camada de 

saída, através do comando 

celldisp(net.LW) e           (4.3) 

as matrizes de limiares b1 e b2, ambas através do mesmo comando 

celldisp(net.b),           (4.4)  

pode-se aplicar os resultados das fórmulas dos comandos (4.2), (4.3) e (4.4) aos dados das 

tabelas dos outros ANEXO F, G e H com o auxílio das seguintes fórmulas, dentro das 

denominações definidas no MATLAB, 

 

IW * pi + b1 * ones (1,5),                   (4.5) 

Ai = logsig (IW *pi + b1 * ones(1,5)),                             (4.6) 

Ti = LW * Ai + b2,                     (4.7) 

i = 1, 2, 3 e 4,  

 

a matriz Ai indica a aplicação da função de transferência 

 

y = logsig (x),           (4.8) 

 

na forma matricial, e Ti, é a matriz dos padrões de saída. Neste caso, T1 é o resultado para os 

dados da tabela do ANEXO E, T2 referente ao ANEXO F, T3 referente ao ANEXO G e T4 

referente do ANEXO H. 

Aplicando-se as fórmulas (4.5), (4.6) e (4.7) às matrizes de pesos e de limiares 

encontradas e aos dados das tabelas dos ANEXOS E, F, G e H, chegam-se aos resultados que 

constam da Tabela 5. 

 



61 
 

Tabela 5 – As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS E, F, G e H,com N = 7 e 242 
epochs. 

Variáveis 

Físicas 

RLQ 

(%) 

PF 

(%) 

Abs 

(%) 

CR 

(N) 

MRF 

(MPa) 

      

a 5.5008 4.0002 7.0064 601.9992 50.9986 

      

T1 (1000°C) 5.4943 3.9915 6.9803 601.2047 50.8284 

T2 (1020°C) 6.4059 3.8566 3.6228 296.1997 21.5664 

T3 (1040°C) 6.3043 3.9856 4.4235 257.7497 12.6877 

T4 (1060°C) 5.3570 4.3453 6.5846 227.7777 -4.9783 

      

PADRÃO 5 5.5 4 7 602 51 

Fonte: Elaborado pelo autor. 

 

O valor a é o resultado da simulação da rede neural, com N =7 neurônios e 242 

epochs, feito pelo próprio software. O valor a e o valor encontrado quando a temperatura é de 

1000° C (T1), referente ao teste, estão bem próximos, mas não são os mesmos valores em 

virtude da propagação do erro em função dos truncamentos e arredondamentos, quando o 

processo é calculado diretamente. No caso desta Tabela, tem-se: 

e(RLQ) = a(RLQ) – T1(RLQ) = 5.5008 – 5.4943 = 0.0065. 

Analogamente, 

e(PF) = 4.0002 – 3.9915 = 0.0087; 

e(Abs) = 7.0064 – 6.9803 = 0.0261; 

e(CR) = 601.9992 – 601.2047 = 0.7945; e 

e(MRF) = 50.9986 – 50.8284 = 0.1702. 

 

É um erro que, infelizmente, é inerente ao processo e não há como evitá-lo. Este 

resultado não pode ser usado em virtude dos dados das linhas 3, 4 e 5 e das colunas 5 e 6, 

constantes da Tabela 5, onde apareceu até um valor negativo para a variável física MRF 

quando a temperatura de queima é de 1060° C (T4). É bom lembrar que a norma NBR 13818 

(ABNT, 1997) especifica que 6 ?  Abs  &amp;lt;10 , que CR &gt; 500N e que MRF &gt; 18MPa. 



62 
 

Continuando o processo, para N = 8 e 9 neurônios, os resultados repetiram uma 

convergência muito lenta e nada pode ser aproveitado. Com N = 10, 11,..., 22 neurônios, o 

número de testes foi aumentado para 10, enquanto que em alguns casos o número de epochs 

chegou a 20000, mas nenhum resultado pode ser usado. A partir de N = 23 neurônios na 

camada oculta o método apresentou novamente uma convergência rápida e com isso o número 

de epochs para os 10 testes foi fixado em 5000. A melhor solução ocorreu quando N = 34 

neurônios, com 164 epochs e performance (mse) = 4.75064e-7, cujos dados podem ser vistos 

na Figura 12 e constam da Tabela 6. 

Nesta Tabela 6, o resultado inconsistente é o valor da variável física Abs para a 

temperatura de queima em 1060° C, pois contrariando a tendência de queda, ultrapassou o 

valor da escala de classificação do grupo BIIb. É bom observar que para N = 35 neurônios, a 

convergência ocorreu com apenas 122 epochs, mas os resultados não foram convenientes, 

como pode ser visto na Tabela 7. Também, devem ser observados os erros médios quadráticos 

(mse). 

 

Figura 12 – Resultado encontrado para os dados do ANEXO E com N =34 e 164 epochs. 

0 20 40 60 80 100 120 140 160

10
-6

10
-4

10
-2

10
0

10
2

10
4

164 Epochs

T
ra

in
in

g
-B

lu
e
  
G

o
a
l-
B

la
c
k

Performance is 4.75064e-007, Goal is 1e-005

 
Fonte: MATLAB® 7.0. 

 



63 
 

Tabela 6 – As fórmulas (4.5), (4.6) e (4.7) aplicadas as dados dos ANEXOS E, F, G e H, com N = 34 e 164 
epochs. 

Variáveis 
Físicas 

RLQ 
(%) 

PF 
(%) 

Abs 
(%) 

CR 
(N) 

MRF 
(MPa) 

      
T1 (1000°C) 5.5129 4.0126 7.0096 602.3708 51.2143 

T2 (1020°C) 6.2948 3.5409 3.8028 513.9377 35.6558 

T3 (1040°C) 6.9630 3.5255 3.2918 539.5973 44.8043 

T4 (1060°C) 6.6550 3.6279 12.3915 550.7220 39.3721 

      
PADRÃO 5 5.5 4 7 602 51 

Fonte: Elaborada pelo autor. 

 

Tabela 7 – As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS E, F, G e H, com N = 35 e 122 
epochs. 

Variáveis 
Físicas 

RLQ 
(%) 

PF 
(%) 

Abs 
(%) 

CR 
(N) 

MRF 
(MPa) 

T1 (1000°C) 5.4958 3.9969  6.9857 601.9150 50.9923 

T2 (1020°C) 7.7770 4.2938 4.9043 446.7413 36.9043 

T3 (1040°C) 8.2346 4.2443 2.9948 452.1809 44.2455 

T4 (1060°C) 7.8101 4.6059 1.7728 480.7872 38.6263 

Fonte: Elaborada pelo autor. 

 

Em função dos resultados anteriores, a partir da tabela de dados do ANEXO F (cinco 

variáveis físicas e temperatura de queima de 1020° C), fixaram-se os parâmetros de controle 

do MATLAB em show = 200, epochs = 5000 e goal (mse) = 1e-5 e, também, o número de 

testes em 10. Com isto, para o ANEXO F, encontraram-se os dados da Tabela 8. 

Para analisar a estabilidade da rede, o número de neurônios na camada oculta foi 

sendo aumentado gradativamente e os resultados foram incluídos na                     Tabela 9       

(complemento da anterior). 

 



64 
 

Tabela 8 – Aplicação do método resiliente aos dados do ANEXO F. 
N Performance (mse) Observações 

2 Mínimo local (10 testes) 55090.8 

3 Mínimo local (10 testes) 55090.8 

4 1 teste convergente - 

5 1 teste convergente - 

6 2 testes convergentes - 

7 2 testes convergentes - 

8 2 testes convergentes - 

9 4 testes convergentes  - 

10 6 testes convergentes - 

11 6 testes convergentes - 

12 7 testes convergentes -  

13 7 testes convergentes - 

14 8 testes convergentes - 

15  10 testes convergentes Número de epochs muito alto (3997) 

16 8 testes convergentes - 

17 9 testes convergentes - 

18 9 testes convergentes - 

19  9 testes convergentes - 

20 10 testes convergentes Número de epochs muito alto 

21 10 testes convergentes Número de epochs muito alto (2259) 

22 10 testes convergentes Número de epochs muito alto 

23 10 testes convergentes Número de epochs muito alto 

24 10 testes convergentes  Número de epochs muito alto 

25 10 testes convergentes Número de epochs muito alto 

26  10 testes convergentes Número de epochs muito alto 

27 10 testes convergentes (88) 

28 10 testes convergentes Número de epochs muito alto 

Fonte: Elaborada pelo autor.  



65 
 

Tabela 9 - Complementação da Tabela 8. 
N Performance Observações 

29 10 testes convergentes Número de epochs muito alto 

30 10 testes convergentes Número de epochs muito alto 

31 10 testes convergentes Número de epochs muito alto 

32 10 testes convergentes Número de epochs muito alto 

33 9 testes convergentes (83) 

34 10 testes convergentes Número de epochs muito alto 

35 10 testes convergentes Número de epochs muito alto 

40 10 testes convergentes Número de epochs muito alto 

45 10 testes convergentes Número de epochs muito alto 

50 10 testes convergentes Número de epochs muito alto 

60 10 testes convergentes Número de epochs muito alto 

70 10 testes convergentes Número de epochs muito alto 

80 10 testes convergentes Número de epochs muito alto 

90 10 testes convergentes (60) 

100 10 testes convergentes Número de epochs muito alto 

Fonte: Elaborada pelo autor. 

 

Ficou bem evidente nas Tabelas 8 e 9 que a regra de aprendizado backpropagation 

resiliente é de convergência rápida, pois na maioria dos casos em 10 testes foram encontrados 

10 testes convergentes, apesar de que na maioria das vezes o número de epochs era muito alto 

(acima de 2259, no caso, pois quando N = 15 neurônios, o menor número de epochs foi de 

3997, para N =21 neurônios, 2259 epochs, para N = 24 neurônios, 2403 epochs e para N = 28 

neurônios, 3611 epochs). Quando foram aplicadas as fórmulas (4.5), (4.6) e (4.7) com estes 

dados os resultados não foram nem mesmo satisfatórios. Entretanto, quando N = 27 neurônios 

na camada oculta e com 88 epochs, quando N = 33 neurônios e 83 epochs e, finalmente, 

quando N =90 neurônios e 60 epochs apareceram as melhores soluções e elas constam das 

Tabelas 10, 11 e 12. As Figuras 13, 14 e 15 mostram os gráficos em correspondência com as 

Tabelas 10, 11 e 12, respectivamente. 



66 
 

Figura 13 – Resultado encontrado para os dados do ANEXO F, com N = 27 e 88 epochs. 

0 10 20 30 40 50 60 70 80
10

-6

10
-4

10
-2

10
0

10
2

10
4

88 Epochs

T
ra

in
in

g
-B

lu
e
  

G
o
a
l-
B

la
c
k

Performance is 3.81476e-006, Goal is 1e-005

 
Fonte: MATLAB® 7.0. 

 

Tabela 10 – As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS E, F, G e H, com N = 27 e 88 
epochs. 

Variáveis 

Físicas 

RLQ 

(%) 

PF 

(%) 

Abs 

(%) 

CR 

(N) 

MRF 

(MPa) 

T1 (1000°C) 3.8176 4.3852 9.8580 341.7696 20.7748 

T2 (1020°C) 5.4917 4.0012  6.9926 601.7553 50.8131 

T3 (1040°C) 7.2374 4.0660 10.9442 598.5219 55.0452 

T4 (1060°C) 7.0804 4.1405 19.0759 597.0924 55.0565 

      

PADRÃO 5 5.5 4 7 602 51 

Fonte: Elaborada pelo autor. 



67 
 

Figura 14 – Resultado encontrado para os dados do ANEXO F, com N = 33 e 83 epochs. 

0 10 20 30 40 50 60 70 80
10

-6

10
-4

10
-2

10
0

10
2

10
4

83 Epochs

T
ra

in
in

g
-B

lu
e
  

G
o
a
l-
B

la
c
k

Performance is 9.05677e-006, Goal is 1e-005

 
Fonte: MATLAB® 7.0. 

 

Tabela 11 – As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS E, F, G e H, com N = 33 e 83 
epochs. 

Variáveis 

Físicas 

RLQ 

(%) 

PF 

(%) 

Abs 

(%) 

CR 

(N) 

MRF 

(MPa) 

T1 (1000°C) 4.2713 4.2152 9.9552 492.0558 34.1350 

T2 (1020°C) 5.4974 3.9935 6.9875 601.9989 50.9171 

T3 (1040°C) 7.2452 4.0279 5.7282 607.0999 59.1992 

T4 (1060°C) 8.1126 4.0150 11.0738 610.0570 54.9349 

      

PADRÃO 5 5.5 4 7 602 51 

Fonte: Elaborada pelo autor. 

 



68 
 

Figura 15 – Resultado encontrado para os dados do ANEXO F, com n = 90 e 60 epochs. 

0 10 20 30 40 50 60
10

-6

10
-4

10
-2

10
0

10
2

10
4

60 Epochs

T
ra

in
in

g
-B

lu
e
  

G
o
a
l-
B

la
c
k

Performance is 9.26147e-006, Goal is 1e-005

 
Fonte: MATLAB® 7.0. 

 

Tabela 12 – As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS E, F, G e H, com N = 90 e 60 
epochs. 

Variáveis 

Físicas 

RLQ 

(%) 

PF 

(%) 

Abs 

(%) 

CR 

(N) 

MRF 

(MPa) 

T1 (1000°C) 4.1489 3.6117 9.0583 572.1501 33.9442 

T2 (1020°C) 5.4955 4.0015 7.0058 601.8821 50.9997 

T3 (1040°C) 5.7914 3.9837 4.1276 603.5703 63.6037 

T4 (1060°C) 6.2612 4.0810 1.3956 603.4972 73.9983 

      

PADRÃO 5 5.5 4 7 602 51 

Fonte: Elaborada pelo autor. 

 

Mesmo assim, fica evidente, que as Tabelas 10, 11 e 12 mostram valores que não 

atendem as especificações técnicas da ABNT, apesar de terem sido obtidos como sendo os 

melhores resultados dos testes. 



69 
 

Com relação aos dados da tabela do ANEXO G (5 variáveis físicas e temperatura de 

queima de 1040°C), foram mantidos os mesmos parâmetros de controle do MATLAB, ou 

seja, show = 200, epochs = 5000 e goal (mse) = 1e-5. O processo foi aplicado inicialmente 

para N = 5, 10, 15, 20, 25, 30 e 35 neurônios na camada oculta, cujos resultados são 

mostrados na Tabela 13. 

 

Tabela 13 – Aplicação do método resiliente aos dados do ANEXO G. 

N Performance (mse) Observações 
5 Mínimo local 55090.8 
10 Mínimo local 55090.8 
15 Mínimo local 55090.8 
20 6 testes convergentes Número de epochs muito alto 
25 10 testes convergentes Número de epochs muito alto 
30 9 testes convergentes Número de epochs muito alto 
35 10 testes convergentes Número de epochs muito alto 

Fonte: Elaborada pelo autor. 

 
Em virtude dos resultados nada satisfatórios mostrados pela Tabela 13, foi realizada 

uma complementação para testar a estabilidade da rede neural e os dados constam da Tabela 

14. 

 
Tabela 14 – Complementação dos dados da Tabela 13. 

N Performance (mse) Observações 

21 9 testes convergentes Número de epochs muito alto 

22 8 testes convergentes Número de epochs muito alto 

23 9 testes convergentes Número de epochs muito alto 

24 10 testes convergentes Número de epochs muito alto 

26 10 testes convergentes Número de epochs muito alto 

27 10 testes convergentes (85) 

28 8 testes convergentes Número de epochs muito alto 

29 10 testes convergentes Número de epochs muito alto 

31 9 testes convergentes Número de epochs muito alto 

32 10 testes convergentes Número de epochs muito alto 

33 10 testes convergentes (82) 

34 10 testes convergentes (73) 

Fonte: Elaborada pelo autor. 

 



70 
 

As melhores soluções aconteceram quando N = 27 neurônios com 85 epochs, quando 

N = 33 neurônios com 82 epochs e quando N = 34 com 73 epochs. Novamente, quando o 

número de epochs é muito alto, os resultados não permitem as suas aplicações no processo, 

por isso, os resultados anteriores foram considerados os melhores. As Tabelas 15, 16 e 17 

mostram os dados encontrados quando da aplicação das fórmulas (4.5), (4.6) e (4.7) às tabelas 

de dados dos ANEXOS E, F, G e H. Em correspondência aos dados destas Tabelas, são 

mostradas as Figuras 16, 17 e 18. 

 

Figura 16 - Resultado encontrado para os dados do ANEXO G, com N= 27 e 85 epochs. 

0 10 20 30 40 50 60 70 80
10

-6

10
-4

10
-2

10
0

10
2

10
4

85 Epochs

T
ra

in
in

g
-B

lu
e
  

G
o
a
l-
B

la
c
k

Performance is 3.51056e-006, Goal is 1e-005

 
Fonte: MATLAB® 7.0. 

 



71 
 

Tabela 15 - As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS E, F, G e H, com N = 27 e 85 
epochs. 

Variáveis 

Físicas 

RLQ 

(%) 

PF 

(%) 

Abs 

(%) 

CR 

(N) 

MRF 

(MPa) 

T1 (1000°C) 3.3854 4.5142 9.9596 580.4700 39.6761 

T2 (1020°C) 3.9784 4.0617 5.2503 596.7161 44.6927 

T3 (1040°C) 5.5046 4.0054 7.0062 601.9684 51.0574 

T4 (1060°C) 6.1106 4.0444 12.5082 603.4387 52.6749 

      

PADRÃO 5 5.5 4 7 602 51 

Fonte: Elaborada pelo autor. 

 

Figura 17 - Resultado encontrado para os dados do ANEXO G, com N = 33 e 82 epochs. 

0 10 20 30 40 50 60 70 80
10

-6

10
-4

10
-2

10
0

10
2

10
4

82 Epochs

T
ra

in
in

g
-B

lu
e
  

G
o
a
l-
B

la
c
k

Performance is 6.92398e-006, Goal is 1e-005

 
Fonte: MATLAB® 7.0. 

 



72 
 

Tabela 16 - As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS E, F, G e H, com N = 33 e 82 
epochs. 

Variáveis 

Físicas 

RLQ 

(%) 

PF 

(%) 

Abs 

(%) 

CR 

(N) 

MRF 

(MPa) 

T1 (1000°C) 5.3218 2.9787 29.6079 601.0522 16.5579 

T2 (1020°C) 9.3116 3.7019 17.2727 597.8778 29.0875 

T3 (1040°C) 5.4951 3.9957 6.9866 602.0411 50.6939 

T4 (1060°C) 0.4765 3.5444 7.9963 602.7652 93.1350 

      

PADRÃO 5 5.5 4 7 602 51 

Fonte: Elaborado pelo autor. 

 

Figura 18 - Resultado encontrado para os dados do ANEXO G, com N = 34 e 73 epochs. 

0 10 20 30 40 50 60 70
10

-6

10
-4

10
-2

10
0

10
2

10
4

73 Epochs

T
ra

in
in

g
-B

lu
e
  

G
o
a
l-
B

la
c
k

Performance is 7.39822e-006, Goal is 1e-005

 
Fonte: MATLAB® 7.0. 

 



73 
 

Tabela 17 - As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS E, F, G e H, com N = 34 e 73 
epochs. 

Variáveis 

Físicas 

RLQ 

(%) 

PF 

(%) 

Abs 

(%) 

CR 

(N) 

MRF 

(MPa) 

T1 (1000°C) 2.3421 3.1560 30.2824 469.8984 16.1915 

T2 (1020°C) 6.1257 3.7758 15.6994 599.6521 48.3919 

T3 (1040°C) 5.4791 4.0018 6.9984 602.0983 50.7668 

T4 (1060°C) 6.4976 4.0933 0.1437 601.9720 101.8092 

      

PADRÃO 5 5.5 4 7 602 51 

Fonte: Elaborado pelo autor. 

 

Dentre as Tabelas anteriores, é visível que o melhor resultado ocorreu com os dados 

da Tabela 15. Os dados das outras duas Tabelas estão, em alguns casos, bem afastados dos 

valores especificados em norma da ABNT. 

Neste momento, chega-se ao ANEXO H que exibe cinco variáveis físicas e 

temperatura de queima de 1060° C. Os parâmetros de controle permanecem com os mesmos 

valores, ou seja, show = 200, epochs = 5000 e goal (mse) = 1e-5. Da mesma maneira com      

N = 5, 10, 15, 20, 25, 30 e 35 foram efetuados 10 testes, conforme Tabela 18. 

 

Tabela 18 – Aplicação do método resiliente aos dados do ANEXO H. 
N Performance (mse) Observações 

5 Mínimo local 55090.8 

10 Mínimo local 55090.8 

15 4 testes convergentes Muito lenta 

20 4 testes convergentes Muito lenta 

25 5 testes convergentes Muito lenta 

30 8 testes convergentes Lenta 

35 7 testes convergentes Lenta 

Fonte: Elaborado pelo autor. 

 

Em função dos indícios de convergência do processo, a Tabela 18 foi complementada 

com a Tabela 19, a seguir. 



74 
 

Tabela 19 – Complementação dos dados da Tabela 18. 
N Performance (mse) Observações 

36 9 testes convergentes  

37 7 testes convergentes  

38 10 testes convergentes (94), (215) 

39 8 testes convergentes  

40 9 testes convergentes (118), (126), (127) 

Fonte: Elaborada pelo autor. 

 

Os resultados encontrados estão expostos nas Tabelas 20, 21, 22, 23, 24 e 25 e, em 

correspondência, os erros mínimos quadráticos (mse) junto às Figuras 19, 20, 21, 22, 23 e 24. 

 

Figura 19 - Resultado encontrado para os dados do ANEXO H, com N = 38 e 215 epochs. 

0 20 40 60 80 100 120 140 160 180 200
10

-6

10
-4

10
-2

10
0

10
2

10
4

215 Epochs

T
ra

in
in

g
-B

lu
e
  

G
o
a
l-
B

la
c
k

Performance is 7.12127e-006, Goal is 1e-005

 
Fonte: MATLAB® 7.0. 

 



75 
 

Tabela 20 - As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS E, F, G e H, com N = 38 e 215 
epochs. 

Variáveis 

Físicas 

RLQ 

(%) 

PF 

(%) 

Abs 

(%) 

CR 

(N) 

MRF 

(MPa) 

a 5.4978 4.0028 6.9992 602.0013 51.0046 

T1 (1000°C) -1.4555 3.4268 14.7742 15.5033 1.8843 

T2 (1020°C) 0.0025 4.1779 18.7575 596.9313 8.7448 

T3 (1040°C) 2.8703 4.3912 14.3653 596.4031 6.8872 

T4 (1060°C) 5.4825 3.9943 6.9943 601.5592 50.9025 

      

PADRÃO 5 5.5 4 7 602 51 

Fonte: Elaborada pelo autor. 

 

Figura 20 - Resultado encontrado para os dados do ANEXO H, com N = 38 e 94 epochs. 

0 10 20 30 40 50 60 70 80 90
10

-6

10
-4

10
-2

10
0

10
2

10
4

94 Epochs

T
ra

in
in

g
-B

lu
e
  

G
o
a
l-
B

la
c
k

Performance is 9.44677e-006, Goal is 1e-005

 
Fonte: MATLAB® 7.0. 

 



76 
 

Tabela 21 - As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS E, F, G e H, com N = 38 e 94 
epochs. 

Variáveis 

Físicas 

RLQ 

(%) 

PF 

(%) 

Abs 

(%) 

CR 

(N) 

MRF 

(MPa) 

a 5.5026 3.9951 7.0000 602.0000 51.0040 

T1 (1000°C) 3.5936 4.0055 6.4765 489.6407 21.5601 

T2 (1020°C) 4.7546 3.9940 6.1944 537.3637 32.5325 

T3 (1040°C) 5.4814 3.8803 5.0144 582.0974 44.3654 

T4 (1060°C) 5.5026 3.9953 6.9990 601.6696 50.9433 

      

PADRÃO 5 5.5 4 7 602 51 

Fonte: Elaborada pelo autor. 

 

Figura 21 - Resultado encontrado para os dados do ANEXO H, com N =40 e 127 epochs. 

0 20 40 60 80 100 120
10

-6

10
-4

10
-2

10
0

10
2

10
4

127 Epochs

T
ra

in
in

g
-B

lu
e
  

G
o
a
l-
B

la
c
k

Performance is 5.56055e-006, Goal is 1e-005

 
Fonte: MATLAB® 7.0. 

 



77 
 

Tabela 22 - As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS E, F, G e H, com N = 40 e 127 
epochs. 

Variáveis 

Físicas 

RLQ 

(%) 

PF 

(%) 

Abs 

(%) 

CR 

(N) 

MRF 

(MPa) 

a 5.4990 4.0050 7.0012 602.0003 51.0005 

T1 (1000°C) 4.1497 3.9542 6.8460 541.4200 20.0360 

T2 (1020°C) 4.7328 4.0004 4.6428 586.6600 32.9680 

T3 (1040°C) 5.6274 4.0113 5.6135 599.3200 45.1340 

T4 (1060°C) 5.5071 4.0079 7.0021 602.2700 51.2140 

      

PADRÃO 5 5.5 4 7 602 51 

Fonte: Elaborada pelo autor. 

 

Figura 22 - Resultado encontrado para os dados do ANEXO H, com N = 40 e 126 epochs. 

0 20 40 60 80 100 120
10

-6

10
-4

10
-2

10
0

10
2

10
4

126 Epochs

T
ra

in
in

g
-B

lu
e
  

G
o
a
l-
B

la
c
k

Performance is 5.23196e-006, Goal is 1e-005

 
Fonte: MATLAB® 7.0. 



78 
 

Tabela 23 - As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS E, F, G e H, com N = 40 e 126 
epochs. 

Variáveis 

Físicas 

RLQ 

(%) 

PF 

(%) 

Abs 

(%) 

CR 

(N) 

MRF 

(MPa) 

a 5.4997 4.0040 6.9970 601.9994 50.9991 

T1 (1000°C) 3.7022 3.8332 7.3212 599.5800 18.8490 

T2 (1020°C) 4.4276 3.9802 5.8040 602.0100 32.0970 

T3 (1040°C) 5.0879 3.9885 4.7899 602.0100 35.6620 

T4 (1060°C) 5.5001 4.0043 6.9975 602.0000 51.1200 

      

PADRÃO 5 5.5 4 7 602 51 

Fonte: Elaborada pelo autor. 

 

Figura 23 - Resultado encontrado para os dados do ANEXO H, com N = 40 e 118 epochs. 

0 20 40 60 80 100
10

-6

10
-4

10
-2

10
0

10
2

10
4

118 Epochs

T
ra

in
in

g
-B

lu
e
  

G
o
a
l-
B

la
c
k

Performance is 4.02038e-006, Goal is 1e-005

 
Fonte: MATLAB® 7.0. 



79 
 

Tabela 24 - As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS E, F, G e H, com N = 40 e 118 
epochs. 

Variáveis 

Físicas 

RLQ 

(%) 

PF 

(%) 

Abs 

(%) 

CR 

(N) 

MRF 

(MPa) 

a 5.5026 3.9968 6.9996 602.0003 51.0016 

T1 (1000°C) 6.0240 4.6738 8.0256 433.9165 29.3281 

T2 (1020°C) 6.1732 4.1264 3.6558 314.9238 9.6733 

T3 (1040°C) 6.3421 3.8372 3.2981 556.7295 35.5169 

T4 (1060°C) 5.4954 4.0008 7.0008 602.1945 50.9625 

      

PADRÃO 5 5.5 4 7 602 51 

Fonte: Elaborada pelo autor. 

 

Figura 24 - Resultado encontrado para os dados do ANEXO H, com N = 40 e 78 epochs.  

0 10 20 30 40 50 60 70
10

-6

10
-4

10
-2

10
0

10
2

10
4

78 Epochs

T
ra

in
in

g
-B

lu
e
  

G
o
a
l-
B

la
c
k

Performance is 8.99296e-006, Goal is 1e-005

 
Fonte: MATLAB® 7.0. 

 

 



80 
 

Tabela 25 - As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS E, F, G e H, com N = 40 e 78 
epochs. 

Variáveis 

Físicas 

RLQ 

(%) 

PF 

(%) 

Abs 

(%) 

CR 

(N) 

MRF 

(MPa) 

a 5.5063 4.0001 6.9978 601.9992 50.9997 

T1 (1000°C) 4.7132 4.3448 5.3693 547.1919 21.5839 

T2 (1020°C) 5.2488 4.0439 3.4622 502.2522 25.9255 

T3 (1040°C) 5.7853 4.0347 4.0992 585.9482 41.8421 

T4 (1060°C) 5.5167 4.0045 7.0017 602.0606 51.0640 

      

PADRÃO 5 5.5 4 7 602 51 

Fonte: Elaborada pelo autor. 

 

Em resumo, com cinco variáveis físicas, como constam das tabelas de dados dos 

ANEXOS E, F, G e H, os melhores resultados ocorreram para a temperatura de queima de 

1000° C, Tabela 6 (Figura 12), quando N = 34 neurônios  com 164 epochs, entretanto, três 

valores da variável Abs ficaram fora dos limites preconizados pela ABNT (6 ?  Abs &amp;lt;10). Na  

temperatura de queima de 1020° C , Tabela 11 (Figura 14), a melhor solução apresentada pela 

rede neural com o método resiliente surgiu quando N = 33 e 83 epochs, apesar de que dois 

valores da variável Abs ficaram fora dos limites citado anteriormente e de que um valor da 

variável CR ficou abaixo de 500N. Com a temperatura de queima em 1040° C, Tabela 15 

(Figura 17), encontraram-se somente dois valores da variável Abs fora dos limites já citado e, 

neste caso, pode-se dizer que a resposta foi relativamente boa porque as variáveis CR e MRF 

encontradas, ambas, estão na ordem crescente. E, finalmente, com a temperatura de queima 

em 1060° C, Tabela 22 (Figura 21), ocorreu a melhor de todas as soluções quando N = 40 

neurônios e 126 epochs, tendo em vista que os desvios entre a simulação a e a resposta 

fornecida pela rede T4 foram muito pequenos, as respostas da rede para a variável Abs 

apareceram no sentido decrescente, com exceção do último dado em T4, (os valores da 

variável ficaram bem próximos), enquanto que os valores da variável MRF, apesar de estarem 

afastados, da mesma forma que a variável CR se apresentaram dentro dos limites impostos 

pela norma NBR 13818 (ABNT, 1997) e no sentido crescente. Com estes resultados pode-se 

concluir que a melhor resposta da rede neural artificial, com a regra de aprendizado 

backpropagation resiliente, para as cinco variáveis RLQ, PF, Abs, CR e MRF, foi obtida 

quando a temperatura de queima foi de 1060° C, a maior de todas, o que valida o processo 

para todos os anexos com cinco variáveis. 



81 
 

Isto colocado, em sequência continuou-se com a aplicação do processo para as tabelas 

de dados dos ANEXOS I, J e K, agora com oito variáveis físicas, que são: DP, DAS, RLS, 

RLQ, PF, Abs, CR e MRF. As temperaturas de sinterização são três: 1020° C, 1040° C e 

1060° C, respectivamente, para cada um dos anexos. 

Pelos indícios de convergência do processo resiliente nos quatro casos já tratados, os 

parâmetros de controle continuam com os mesmos valores, ou seja, show = 200, epochs = 

5000 e goal (mse) = 1e-5. Continuam, também, os testes num total de 10 e o número de 

neurônios na camada oculta para a temperatura de queima de 1020° C será N = 2, 3, 4,..., 35 e 

para as outras duas temperaturas de queima, a análise será feita, inicialmente, com N = 5, 10, 

15, 20, 25, 30 e 35 neurônios. 

Começando com a tabela de dados do ANEXO I, a aplicação do método resiliente 

forneceu os dados da Tabela 26. 

 



82 
 

Tabela 26 – Aplicação do método resiliente aos dados do ANEXO I. 
N Performance (mse) Observações 
2 Vários mínimos locais Desiguais 
3 1 teste convergente - 
4 Vários mínimos locais Desiguais 
5 Vários mínimos locais Desiguais 
6 Vários mínimos locais Desiguais 
7 Vários mínimos locais Desiguais 
8 1 teste convergente - 
9 2 testes convergentes - 

10 Vários mínimos locais Desiguais 
11 Vários mínimos locais Desiguais 
12 Vários mínimos locais Desiguais 
13 1 teste convergente - 
14 2 testes convergentes - 
15 2 testes convergentes - 
16 Vários mínimos locais Desiguais 
17 Vários mínimos locais Desiguais 
18 4 testes convergentes - 
19 2 testes convergentes Número de epochs muito alto 
20 3 testes convergentes Número de epochs muito alto 
21 3 testes convergentes Número de epochs muito alto 
22 2 testes convergentes Número de epochs muito alto 
23 3 testes convergentes - 
24 Vários mínimos locais Desiguais 
25 3 testes convergentes - 
26 3 testes convergentes - 
27 4 testes convergentes - 
28 3 testes convergentes - 
29 2 testes convergentes - 
30 6 testes convergentes - 
31 3 testes convergentes - 
32 3 testes convergentes - 
33 5 testes convergentes - 
34 3 testes convergentes - 
35 4 testes convergentes - 

Fonte: Elaborada pelo autor. 

 

Há convergência, mas com muitas falhas e com um número de epochs muito alto, por 

isto esta Tabela foi complementada com a seguinte Tabela 27. 

 

 



83 
 

Tabela 27 – Complementação dos dados da Tabela 26. 
N Performance (mse)  Observações 

36  6 testes convergentes Número de epochs muito alto 

37 5 testes convergentes Número de epochs muito alto 

38 4 testes convergentes Número de epochs muito alto 

39 6 testes convergentes Número de epochs muito alto 

40 4 testes convergentes Número de epochs muito alto 

41 5 testes convergentes Número de epochs muito alto 

42 6 testes convergentes Número de epochs muito alto 

43 5 testes convergentes Número de epochs muito alto 

44 7 testes convergentes Número de epochs muito alto 

45 5 testes convergentes Número de epochs muito alto 

50 5 testes convergentes Número de epochs muito alto 

55 5 testes convergentes Número de epochs muito alto 

56 7 testes convergentes (64) 

57 6 testes convergentes Número de epochs muito alto 

58 8 testes convergentes (158) 

59 8 testes convergentes Número de epochs muito alto 

60 10 testes convergentes (263), (269) 

61 8 testes convergentes Número de epochs muito alto 

62 6 testes convergentes Número de epochs muito alto 

Fonte: Elaborado pelo autor. 

 

Foram selecionados os melhores resultados como sendo aqueles que ocorreram 

quando N = 56 neurônios na camada oculta com 64 epochs, quando N = 58 neurônios com 

158 epochs e quando N = 60 neurônios com 263 e 269 epochs. Eles constituem as Tabelas 28, 

29, 30 e 31. As fórmulas (4.5), (4.6) e (4.7) podem ser usadas agora com i = 1, 2 e 3. Em 

sequência, as Figuras 25, 26, 27 e 28 correspondentes aos dados das Tabelas 28, 29, 30 e 31. 

 



84 
 

Figura 25 - Resultado encontrado para os dados do ANEXO I, com N = 56 e 64 epochs. 

0 10 20 30 40 50 60
10

-6

10
-4

10
-2

10
0

10
2

10
4

64 Epochs

T
ra

in
in

g
-B

lu
e
  
G

o
a
l-
B

la
c
k

Performance is 9.04311e-006, Goal is 1e-005

 
Fonte: MATLAB® 7.0. 

 

Tabela 28 - As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N = 56 e 64 epochs. 
Variáveis 

Físicas 

DP 

(g/cm 3 ) 

DAS 

(g/cm 3 ) 

RLS 

(%) 

RLQ 

(%) 

PF 

(%) 

Abs 

(%) 

CR 

(N) 

MRF 

(MPa) 

a 1.9962 1.9063 0.3006 5.4959 4.0002 7.0001 601.9990 51.0000 

T1 (1020°C) 2.0005 1.9102 0.3007 5.5018 4.0099 7.0311 601.7692 51.1454 

T2 (1040°C) 1.9994 1.9104 0.3460 6.8791 4.1394 5.0437 605.7800 59.1417 

T3 (1060°C) 2.0037 1.9147 0.2936 7.4347 4.1704 2.2264 607.8878 64.8783 

         

PADRÃO 8 2 1.9 0.3 5.5 4 7 602 51 

Fonte: Elaborado pelo autor. 

 



85 
 

Figura 26 - Resultado encontrado para os dados do ANEXO I, com N = 58 e 158 epochs. 

0 50 100 150
10

-6

10
-4

10
-2

10
0

10
2

10
4

158 Epochs

T
ra

in
in

g
-B

lu
e
  

G
o
a
l-
B

la
c
k

Performance is 9.70568e-006, Goal is 1e-005

 
Fonte: MATLAB® 7.0. 

 

Tabela 29 - As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N = 58 e 158 
epochs. 

Variáveis 

Físicas 

DP 

(g/cm 3 ) 

DAS 

(g/cm 3 ) 

RLS 

(%) 

RLQ 

(%) 

PF 

(%) 

Abs 

(%) 

CR 

(N) 

MRF 

(MPa) 

a 2.0052 1.8937 0.3000 5.4980 4.0023 6.9986 602.0000 51.0002 

T1 (1020°C) 2.0062 1.8947 0.3001 5.5012 4.0022 7.0027 601.9548 51.0176 

T2 (1040°C) 2.0100 1.9111 0.3287 6.9740 4.0842 4.7241 601.9124 62.5422 

T3 (1060°C) 2.0185 1.9173 0.2894 7.5741 4.0828 3.0673 583.6256 68.3699 

         

PADRÃO 8 2 1.9 0.3 5.5 4 7 602 51 

Fonte: Elaborada pelo autor. 



86 
 

Figura 27 - Resultado encontrado para os dados do ANEXO I, com N = 60 e 263 epochs.  

0 50 100 150 200 250
10

-6

10
-4

10
-2

10
0

10
2

10
4

263 Epochs

T
ra

in
in

g
-B

lu
e
  

G
o
a
l-
B

la
c
k

Performance is 9.91424e-006, Goal is 1e-005

 
Fonte: MATLAB® 7.0. 

 

Tabela 30 - As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N = 60 e 263 
epochs. 

Variáveis 

Físicas 

DP 

(g/cm 3 ) 

DAS 

(g/cm 3 ) 

RLS 

(%) 

RLQ 

(%) 

PF 

(%) 

Abs 

(%) 

CR 

(N) 

MRF 

(MPa) 

a 2.0070 1.8947 0.3004 5.4988 3.9994 7.0000 602.0000 51.0003 

T1 (1020°C) 1.9989 1.8872 0.2995 5.4776 3.9852 6.9785 601.8094 50.7172 

T2 (1040°C) 1.9983 1.8591 0.2901 5.9603 3.8205 5.9671 579.4804 64.9740 

T3 (1060°C) 2.0018 1.8705 0.3239 5.8842 4.0053 4.2970 571.7003 85.8344 

         

PADRÃO 8 2 1.9 0.3 5.5 4 7 602 51 

Fonte: Elaborada pelo autor. 

 



87 
 

Figura 28 - Resultado encontrado para os dados do ANEXO I, com N = 60 e 269 epochs. 

0 50 100 150 200 250
10

-6

10
-4

10
-2

10
0

10
2

10
4

269 Epochs

T
ra

in
in

g
-B

lu
e
  

G
o
a
l-
B

la
c
k

Performance is 9.95808e-006, Goal is 1e-005

 
Fonte: MATLAB® 7.0. 

 

Tabela 31 - As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N = 60 e 269 
epochs. 

Variáveis 

Físicas 

DP 

(g/cm 3 ) 

DAS 

(g/cm 3 ) 

RLS 

(%) 

RLQ 

(%) 

PF 

(%) 

Abs 

(%) 

CR 

(N) 

MRF 

(MPa) 

a 2.0066 1.8940 0.3002 5.5000 4.0000 6.9998 602.0000 51.0001 

T1 (1020°C) 2.0054 1.8928 0.2997 5.4953 3.9968 6.9973 601.5843 50.9632 

T2 (1040°C) 2.0059 1.8857 0.2903 6.7786 3.9716 3.9624 600.7590 67.0975 

T3 (1060°C) 2.0107 1.8934 0.3022 7.6396 4.1245 1.8284 589.5212 99.2379 

         

PADRÃO 8 2 1.9 0.3 5.5 4 7 602 51 

Fonte: Elaborada pelo autor. 

 

Com relação aos dados da tabela do ANEXO J (8 variáveis físicas e temperatura de 

queima de 1040° C), continuou-se com show = 200, epochs = 5000, goal (mse) = 1e-5, 10 

testes e N = 5, 10, 15, 20, 25, 30 e 35, onde foram encontrados os valores da Tabela 32. 

 



88 
 

Tabela 32 – Aplicação do método resiliente aos dados do ANEXO J. 
N  Performance (mse) Observações 

5  1 teste convergente - 

10 1 teste convergente - 

15 Vários mínimos locais Desiguais 

20 1 teste convergente - 

25 6 testes convergentes - 

30 2 testes convergentes - 

35 3 testes convergentes - 

Fonte: Elaborada pelo autor. 

 

Houve necessidade de complementação devido aos indícios de convergência e os 

dados constam da Tabela 33. 

 

Tabela 33 – Complementação dos dados da Tabela 32. 
N  Performance (mse) Observações 

40 6 testes convergentes - 

45 6 testes convergentes - 

50 6 testes convergentes - 

55 8 testes convergentes - 

60 7 testes convergentes (80) 

61 7 testes convergentes - 

62 10 testes convergentes (74), (81) 

63 9 testes convergentes (66) 

64 4 testes convergentes - 

65 9 testes convergentes (75), (167) 

66 8 testes convergentes - 

67 9 testes convergentes - 

Fonte; Elaborada pelo autor. 

 

Os resultados foram compensadores quando N = 60 neurônios na camada oculta e com 

80 epochs; analogamente, para N = 62 neurônios com 74 e 81 epochs, N = 63 neurônios com 

66 epochs e N= 65 neurônios com 75 e 167 epochs. Eles constam das Tabelas 34, 35, 36, 37, 

38 e 39, enquanto as performances podem ser vistas através das Figuras 29, 30, 31, 32, 33 e 

34. 

 

 

 



89 
 

Figura 29 - Resultado encontrado para os dados do ANEXO J, com N = 60 e 80 epochs. 

0 10 20 30 40 50 60 70 80
10

-6

10
-4

10
-2

10
0

10
2

10
4

80 Epochs

T
ra

in
in

g
-B

lu
e
  

G
o
a
l-
B

la
c
k

Performance is 9.49536e-006, Goal is 1e-005

 
Fonte: MATLAB® 7.0. 

 

Tabela 34 - As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N = 60 e 80 epochs. 
Variáveis 

Físicas 

DP 

(g/cm 3 ) 

DAS 

(g/cm 3 ) 

RLS 

(%) 

RLQ 

(%) 

PF 

(%) 

Abs 

(%) 

CR 

(N) 

MRF 

(MPa) 

a 2.0066 1.8949 0.3005 5.5014 3.9990 6.9985 601.9999 51.0001 

T1 (1020°C) 2.0082 1.9004 0.3111 4.2524 3.9722 11.6430 601.6900 43.3420 

T2 (1040°C) 2.0072 1.8954 0.3010 5.5052 3.9991 6.9931 602.0100 51.0660 

T3 (1060°C) 2.0118 1.9005 0.2938 6.6704 4.0913 2.7039 601.0700 59.7200 

         

PADRÃO 8 2 1.9 0.3 5.5 4 7 602 51 

Fonte: Elaborada pelo autor. 



90 
 

Figura 30 - Resultado encontrado para os dados do ANEXO J, com N = 62 e 74 epochs. 

0 10 20 30 40 50 60 70
10

-6

10
-4

10
-2

10
0

10
2

10
4

74 Epochs

T
ra

in
in

g
-B

lu
e
  

G
o
a
l-
B

la
c
k

Performance is 9.99068e-006, Goal is 1e-005

 
Fonte: MATLAB® 7.0. 

 

Tabela 35 - As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N = 62 e 74 epochs. 
Variáveis 

Físicas 

DP 

(g/cm 3 ) 

DAS 

(g/cm 3 ) 

RLS 

(%) 

RLQ 

(%) 

PF 

(%) 

Abs 

(%) 

CR 

(N) 

MRF 

(MPa) 

a 2.0068 1.8965 0.3011 5.4992 3.9963 7.0025 601.9999 51.0001 

T1 (1020°C) 2.0106 1.9011 0.3219 4.4838 4.0869 6.8584 596.5225 43.1884 

T2 (1040°C) 2.0102 1.8997 0.3019 5.5122 4.0058 7.0106 602.8015 51.2447 

T3 (1060°C) 2.0190 1.9102 0.3302 6.0188 4.0890 4.1550 562.3329 64.5364 

         

PADRÃO 8 2 1.9 0.3 5.5 4 7 602 51 

Fonte: Elaborada pelo autor. 



91 
 

Figura 31 - Resultado encontrado para os dados do ANEXO J, com N = 62 e 81 epochs. 

0 10 20 30 40 50 60 70 80
10

-6

10
-4

10
-2

10
0

10
2

10
4

81 Epochs

T
ra

in
in

g
-B

lu
e
  

G
o
a
l-
B

la
c
k

Performance is 9.00362e-006, Goal is 1e-005

 
Fonte: MATLAB® 7.0. 

 

Tabela 36 - As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N = 62 e 81 epochs. 
Variáveis 

Físicas 

DP 

(g/cm 3 ) 

DAS 

(g/cm 3 ) 

RLS 

(%) 

RLQ 

(%) 

PF 

(%) 

Abs 

(%) 

CR 

(N) 

MRF 

(MPa) 

a 2.0016 1.8980 0.2996 5.4975 4.0054 7.0054 601.9999 51.0001 

T1 (1020°C) 2.0082 1.9035 0.3304 4.0838 3.9501 9.7248 599.7006 44.4462 

T2 (1040°C) 2.0090 1.9049 0.3004 5.5243 4.0201 7.0245 602.0086 51.4463 

T3 (1060°C) 2.0149 1.9137 0.3390 6.3542 4.0528 4.2268 602.4022 60.6326 

         

PADRÃO 8 2 1.9 0.3 5.5 4 7 602 51 

Fonte: Elaborada pelo autor. 

 



92 
 

Figura 32 - Resultado encontrado para os dados do ANEXO J, com N = 63 e 66 epochs. 

0 10 20 30 40 50 60
10

-6

10
-4

10
-2

10
0

10
2

10
4

66 Epochs

T
ra

in
in

g
-B

lu
e
  

G
o
a
l-
B

la
c
k

Performance is 4.12249e-006, Goal is 1e-005

 
Fonte: MATLAB® 7.0. 

 

Tabela 37 - As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N = 63 e 66 epochs. 
Variáveis 

Físicas 

DP 

(g/cm 3 ) 

DAS 

(g/cm 3 ) 

RLS 

(%) 

RLQ 

(%) 

PF 

(%) 

Abs 

(%) 

CR 

(N) 

MRF 

(MPa) 

a 2.0025 1.9001 0.3034 5.5032 3.9981 6.9986 602.0000 50.9999 

T1 (1020°C) 2.0010 1.9005 0.3052 3.8416 3.9267 10.8099 601.7305 41.6519 

T2 (1040°C) 2.0013 1.8990 0.3032 5.5010 3.9936 6.9879 602.0000 50.9413 

T3 (1060°C) 2.0037 1.9022 0.3006 6.6933 4.0041 3.6489 595.6186  64.3518 

         

PADRÃO 8 2 1.9 0.3 5.5 4 7 602 51 

Fonte: Elaborada pelo autor. 

 



93 
 

Figura 33 - Resultado encontrado para os dados do ANEXO J, com N = 65 e 75 epochs. 

0 10 20 30 40 50 60 70
10

-6

10
-4

10
-2

10
0

10
2

10
4

75 Epochs

T
ra

in
in

g
-B

lu
e
  

G
o
a
l-
B

la
c
k

Performance is 7.93299e-006, Goal is 1e-005

 
Fonte: MATLAB® 7.0. 

 

Tabela 38 - As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N = 65 e 75 epochs. 
Variáveis 

Físicas 

DP 

(g/cm 3 ) 

DAS 

(g/cm 3 ) 

RLS 

(%) 

RLQ 

(%) 

PF 

(%) 

Abs 

(%) 

CR 

(N) 

MRF 

(MPa) 

a 1.9999 1.9029 0.2991 5.5055 3.9976 7.0041 601.9993 50.9999 

T1 (1020°C) 2.0043 1.9150 0.3395 3.8566 4.0272 11.3280 582.1600 39.4020 

T2 (1040°C) 1.9998 1.9028 0.2988 5.5059 3.9965 7.0027 603.2200 50.9320 

T3 (1060°C) 2.0108 1.9132 0.3398 6.9754 4.0909 1.5772 618.0100 86.4140 

         

PADRÃO 8 2 1.9 0.3 5.5 4 7 602 51 

Fonte: Elaborada pelo autor. 

 



94 
 

Figura 34 - Resultado encontrado para os dados do ANEXO J, com N = 65 e 167 epochs. 

0 20 40 60 80 100 120 140 160
10

-6

10
-4

10
-2

10
0

10
2

10
4

167 Epochs

T
ra

in
in

g
-B

lu
e
  

G
o
a
l-
B

la
c
k

Performance is 9.96187e-006, Goal is 1e-005

 
Fonte : MATLAB® 7.0. 

 

Tabela 39 - As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N = 65 e 167 
epochs. 

Variáveis 

Físicas 

DP 

(g/cm 3 ) 

DAS 

(g/cm 3 ) 

RLS 

(%) 

RLQ 

(%) 

PF 

(%) 

Abs 

(%) 

CR 

(N) 

MRF 

(MPa) 

a 1.9932 1.9056 0.2998 5.5001 4.0010 7.0002 602.0000 51.0000 

T1 (1020°C) 1.9830 1.8752 0.3076 3.8657 3.6533 10.5070 599.0200 42.6860 

T2 (1040°C) 1.9926 1.9052 0.2999 5.4961 4.0017 7.0030 602.6100 50.9790 

T3 (1060°C) 2.0014 1.9084 0.2950 7.4494 3.7336 3.5958 578.0500 54.4640 

         

PADRÃO 8 2 1.9 0.3 5.5 4 7 602 51 

Fonte: Elaborada pelo autor. 

 

Finalmente, para os dados da tabela do ANEXO K (8 variáveis físicas e temperatura 

de sinterização de 1060° C), usaram-se os mesmos valores show = 200, epochs = 5000 e goal 

(mse) = 1e-5, 10 testes e N = 5, 10, 15, 20, 25, 30 e 35. Quando da aplicação do método 

resiliente foram encontrados os valores que constam da Tabela 40. 

 



95 
 

Tabela 40 – Aplicação do método resiliente aos dados do ANEXO K. 
N Performance (mse)  Observações 

5 Vários mínimos locais Desiguais 

10 1 teste convergente - 

15 3 testes convergentes - 

20 2 testes convergentes - 

25 4 testes convergentes - 

30 4 testes convergentes - 

35 8 testes convergentes - 

Fonte: Elaborada pelo autor. 

 

Em função dos indícios de convergência, houve a necessidade de complementação, 

conforme TABELA 41. 

 

Tabela 41 – Complementação dos dados da Tabela 40. 
N Performance (mse) Observações 

40 6 testes convergentes - 

45 6 testes convergentes  - 

50 7 testes convergentes - 

55 8 testes convergentes (76), (92) 

60 7 testes convergentes - 

65 9 testes convergentes (65) 

66 7 testes convergentes - 

67 10 testes convergentes (69), (97) 

68 9 testes convergentes - 

69 9 testes convergentes - 

70  10 testes convergentes (81) 

71 8 testes convergentes (60) 

72 9 testes convergentes - 

73 9 testes convergentes - 

74 9 testes convergentes - 

Fonte: Elaborada pelo autor. 

 



96 
 

Os resultados mostraram-se úteis quando N = 55 neurônios na camada oculta, com 76 

e 92 epochs, quando N = 65 neurônios com 65 epochs, quando N = 67 neurônios com 69 e 97 

epochs, quando N = 70 neurônios com 81 epochs e quando N = 71 neurônios com 60 epochs. 

Nas observações, espaço reservado nestas tabelas, nada foi colocado porque o número de 

epochs era muito alto; o mesmo aconteceu em todas as tabelas anteriores. Estas soluções da 

rede neural constam das Tabelas 42, 43, 44, 45, 46, 47 e 48  em correspondência com as 

Figuras 35, 36, 37, 38, 39, 40 e 41. 

 

Figura 35 - Resultado encontrado para os dados do ANEXO K, com N = 55 e 76 epochs. 

0 10 20 30 40 50 60 70
10

-6

10
-4

10
-2

10
0

10
2

10
4

76 Epochs

T
ra

in
in

g
-B

lu
e
  

G
o
a
l-
B

la
c
k

Performance is 9.17852e-006, Goal is 1e-005

 
Fonte: MATLAB® 7.0. 

 



97 
 

Tabela 42 - As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N = 55 e 76 epochs. 
Variáveis 

Físicas 

DP 

(g/cm 3 ) 

DAS 

(g/cm 3 ) 

RLS 

(%) 

RLQ 

(%) 

PF 

(%) 

Abs 

(%) 

CR 

(N) 

MRF 

(MPa) 

a 1.9946 1.9045 0.2990 5.4992 4.0043 6.9981 601.9998 51.0001 

T1 (1020°C) 1.9897 1.8735 0.3225 3.2483 3.9584 9.6778 599.8834 35.1371 

T2 (1040°C) 1.9910 1.8985 0.3356 4.2641 3.9230 8.7417 601.8029 40.2199 

T3 (1060°C) 1.9939 1.9039 0.2985 5.4968 3.9999 6.9905 601.9747 51.0167 

         

PADRÃO 8 2 1.9 0.3 5.5 4 7 602 51 

Fonte: Elaborada pelo autor. 

 

Figura 36 - Resultado encontrado para os dados do ANEXO K, com N = 55 e 92 epochs. 

0 10 20 30 40 50 60 70 80 90
10

-6

10
-4

10
-2

10
0

10
2

10
4

92 Epochs

T
ra

in
in

g
-B

lu
e
  

G
o
a
l-
B

la
c
k

Performance is 9.36375e-006, Goal is 1e-005

 
Fonte: MATLAB® 7.0. 



98 
 

Tabela 43 - As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N = 55 e 92 epochs. 
Variáveis 

Físicas 

DP 

(g/cm 3 ) 

DAS 

(g/cm 3 ) 

RLS 

(%) 

RLQ 

(%) 

PF 

(%) 

Abs 

(%) 

CR 

(N) 

MRF 

(MPa) 

a 1.9957 1.9063 0.2975 5.5018 3.9977 7.0009 602.0000 50.9977 

T1 (1020°C) 1.9568 1.8129 0.3608 1.5532 3.9402 11.1489 545.1705 -10.0082 

T2 (1040°C) 1.9731 1.8988 0.3144 2.9577 4.0003 8.8076 473.5459 -13.1180 

T3 (1060°C) 1.9936 1.9045 0.2973 5.4953 3.9947 6.9984 601.9981 50.6856 

         

PADRÃO 8 2 1.9 0.3 5.5 4 7 602 51 

Fonte: Elaborada pelo autor. 

 

Figura 37 - Resultado encontrado para os dados do ANEXO K, com N = 65 e 65 epochs. 

0 10 20 30 40 50 60
10

-6

10
-4

10
-2

10
0

10
2

10
4

65 Epochs

T
ra

in
in

g
-B

lu
e
  

G
o
a
l-
B

la
c
k

Performance is 3.27321e-006, Goal is 1e-005

 
Fonte: MATLAB® 7.0. 



99 
 

Tabela 44 - As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N = 65 e 65 epochs. 
Variáveis 

Físicas 

DP 

(g/cm 3 ) 

DAS 

(g/cm 3 ) 

RLS 

(%) 

RLQ 

(%) 

PF 

(%) 

Abs 

(%) 

CR 

(N) 

MRF 

(MPa) 

a 1.9977 1.9005 0.2997 5.4997 4.0007 7.0004 602.0045 50.9999 

T1 (1020°C) 1.9878 1.8885 0.3090 3.5373 4.0022 5.8352 588.9179 39.2767 

T2 (1040°C) 1.9914 1.8932 0.4073 4.7389 4.0610 6.1922 599.2251 44.6545 

T3 (1060°C) 1.9972 1.9001 0.2994 5.4961 3.9981 6.9995 602.4032 50.9507 

         

PADRÃO 8 2 1.9 0.3 5.5 4 7 602 51 

Fonte: Elaborada pelo autor. 

 

Figura 38 - Resultado encontrado para os dados do ANEXO K, com N = 67 e 69 epochs. 

0 10 20 30 40 50 60
10

-6

10
-4

10
-2

10
0

10
2

10
4

69 Epochs

T
ra

in
in

g
-B

lu
e
  

G
o
a
l-
B

la
c
k

Performance is 4.16672e-006, Goal is 1e-005

 
Fonte: MATLAB® 7.0. 

 



100 
 

Tabela 45 - As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N = 67 e 69 epochs. 
Variáveis 

Físicas 

DP 

(g/cm 3 ) 

DAS 

(g/cm 3 ) 

RLS 

(%) 

RLQ 

(%) 

PF 

(%) 

Abs 

(%) 

CR 

(N) 

MRF 

(MPa) 

a 2.0043 1.8998 0.3028 5.5011 3.9989 6.9981 602.0006 50.9998 

T1 (1020°C) 1.9987 1.8922 0.3055 3.2714 3.9207 12.0186 591.9261 38.5242 

T2 (1040°C) 2.0070 1.8964 0.3493 4.1951 4.0123 9.9698 600.2105 45.7710 

T3 (1060°C) 2.0045 1.9000 0.3028 5.4998 3.9985 7.0007 601.8002 50.9753 

         

PADRÃO 8 2 1.9 0.3 5.5 4 7 602 51 

Fonte: Elaborada pelo autor. 

 

Figura 39 - Resultado encontrado para os dados do ANEXO K, com N = 67 e 97 epochs. 

0 10 20 30 40 50 60 70 80 90
10

-6

10
-4

10
-2

10
0

10
2

10
4

97 Epochs

T
ra

in
in

g
-B

lu
e
  

G
o
a
l-
B

la
c
k

Performance is 9.06144e-006, Goal is 1e-005

 
Fonte: MATLAB® 7.0. 

 



101 
 

Tabela 46 - As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N = 67 e 97 epochs. 
Variáveis 

Físicas 

DP 

(g/cm 3 ) 

DAS 

(g/cm 3 ) 

RLS 

(%) 

RLQ 

(%) 

PF 

(%) 

Abs 

(%) 

CR 

(N) 

MRF 

(MPa) 

a 1.9935 1.9052 0.2998 5.5002 4.0008 7.0014 602.0000 51.0000 

T1 (1020°C) 1.9906 1.8988 0.2768 6.1225 3.9056 7.2003 574.5857 55.2187 

T2 (1040°C) 1.9943 1.9087 0.3497 6.2407 4.0325 7.6008 597.0215 59.8810 

T3 (1060°C) 1.9947 1.9063 0.2999 5.019 4.0016 7.0037 602.3042 51.0358 

         

PADRÃO 8 2 1.9 0.3 5.5 4 7 602 51 

Fonte: Elaborada pelo autor. 

 

Figura 40 - Resultado encontrado para os dados do ANEXO K, com N = 70 e 81 epochs. 

0 10 20 30 40 50 60 70 80
10

-6

10
-4

10
-2

10
0

10
2

10
4

81 Epochs

T
ra

in
in

g
-B

lu
e
  

G
o
a
l-
B

la
c
k

Performance is 9.61698e-006, Goal is 1e-005

 
Fonte: MATLAB® 7.0. 



102 
 

Tabela 47 - As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N = 70 e 81 epochs. 
Variáveis 

Físicas 

DP 

(g/cm 3 ) 

DAS 

(g/cm 3 ) 

RLS 

(%) 

RLQ 

(%) 

PF 

(%) 

Abs 

(%) 

CR 

(N) 

MRF 

(MPa) 

a 1.9938 1.9062 0.2994 5.5002 4.0000 6.9999 602.0000 51.0000 

T1 (1020°C) 1.9875 1.8856 0.2838 4.8097 3.9479 6.5638 596.6497 39.0613 

T2 (1040°C) 1.9872 1.8999 0.2849 5.5000 3.9549 6.6073 600.6902 45.8705 

T3 (1060°C) 1.9931 1.9055 0.2986 5.4944 3.9982 7.0027 602.0094 50.8983 

         

PADRÃO 8 2 1.9 0.3 5.5 4 7 602 51 

Fonte: Elaborada pelo autor. 

 

Figura 41 - Resultado encontrado para os dados do ANEXO K, com N = 71 e 60 epochs. 

0 10 20 30 40 50 60
10

-6

10
-4

10
-2

10
0

10
2

10
4

60 Epochs

T
ra

in
in

g
-B

lu
e
  

G
o
a
l-
B

la
c
k

Performance is 9.06948e-006, Goal is 1e-005

 
Fonte: MATLAB® 7.0. 

 

 



103 
 

Tabela 48 - As fórmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N = 71 e 60 epochs. 
Variáveis 

Físicas 

DP 

(g/cm 3 ) 

DAS 

(g/cm 3 ) 

RLS 

(%) 

RLQ 

(%) 

PF 

(%) 

Abs 

(%) 

CR 

(N) 

MRF 

(MPa) 

a 1.9971 1.8997 0.3018 5.4945 4.0016 6.9947 602.0001 50.9999 

T1 (1020°C) 1.9751 1.8621 0.3109 3.6305 4.1029 5.4755 601.6643 35.2209 

T2 (1040°C) 1.9810 1.8857 0.3811 4.7575 4.1775 6.5362 601.7453 44.0534 

T3 (1060°C) 1.9960 1.8988 0.3015 5.4909 4.0006 6.9935 602.0735 50.9426 

         

PADRÃO 8 2 1.9 0.3 5.5 4 7 602 51 

Fonte: Elaborada pelo autor. 

 

Resumindo, os resultados para as tabelas de dados dos ANEXOS I, J e K que melhor 

corresponderam, às aplicações da regra de aprendizado backpropagation resiliente, foram 

para a temperatura de queima de 1020° C, os dados constantes da Tabela 30 (Figura 27), para 

a temperatura de queima de 1040° C, os dados que constam da Tabela 36 (Figura 31) e para a 

temperatura de queima de 1060° C, os dados apresentados na Tabela 47 (Figura 40). Destas 

três soluções, o melhor resultado surgiu para os dados correspondentes à temperatura de 

queima de 1060° C do ANEXO K e, novamente, para a maior temperatura de sinterização e 

demonstra definitivamente a validação do processo. 

 

4.5. Regra de Aprendizado Backpropagation de Levenberg-Marquadt (trainlm) 

 

Na aplicação da regra de aprendizado backpropagation de Levenberg-Marquadt os 

parâmetros de controle do MATLAB são apenas três: show = 200, epochs = 5000 e goal = 1e-

5, com no caso do método anterior. Foram realizados 10 testes para verificar a estabilidade e a 

convergência do método. Iniciou-se o processo com o número de neurônios na camada oculta, 

N = 2, 3,..., 25, acrescidos dos valores N = 30 e 35, para os dados da tabela do ANEXO E, 

onde as variáveis físicas são cinco e a temperatura de queima de 1000° C. 

Apesar da grande expectativa neste processo ele apresentou em todos os valores de N 

uma convergência extremamente lenta, exigindo, no mínimo 40000 epochs para se chegar a 

um valor satisfatório, além de demandar um tempo razoável de utilização em um computador 

pessoal. Para se mostrar uma estimativa da lentidão da convergência, quando N = 22 

neurônios, a cada 200 epochs a queda no valor do mse (erro médio quadrático) era da ordem 



104 
 

de 2 em 44000 e quando N = 23 neurônios, com 200 epochs o valor do mse era de 51064.8, 

enquanto que com 5000 epochs, o valor do mse era ligeiramente menor e igual a 51059.1, 

mostrando que, neste caso, o número de epochs, para se alcançar um mínimo global na 

vizinhança de zero, deveria ser de ordem aproximada de 43000000 (quarenta e três milhões!). 

Outra situação inesperada quando da aplicação deste processo foi o fato de que em 

algumas passagens as matrizes de pesos ou de limiares alcançaram valores muito próximos de 

atingirem determinante nulo, o que leva a resultados não confiáveis, ou ainda, valores ruins de 

escala. 

Para as tabelas de dados dos ANEXOS F, G, H, I, J e K, os dez testes foram aplicados 

com N = 5, 10, 15, 20, 25, 30 e 35 neurônios na camada oculta e observou-se em todos os 

casos a persistência da convergência extremamente lenta, o que não permite uma aplicação 

rápida deste processo à indústria de pisos e revestimentos cerâmicos do PCSG. Dessa forma, 

este método foi abandonado. Para a sua aplicação os dados do padrão de entrada devem ser 

normalizados de maneira que eles fiquem contidos dentro do intervalo real [0,1]. Como, no 

entanto, o método resiliente atendeu ao objetivo geral deste trabalho e mostrou também que a 

hipótese, integrante deste trabalho enunciada no capítulo 1, é verdadeira, este trabalho foi 

encerrado, não obstante, sabendo que além destes métodos, o próprio software MATLAB 

apresenta vários outros métodos, variações do método backpropagation.  

 

4.6. Conclusões  

 

O objetivo geral deste trabalho foi alcançado na medida em que foi encontrada a rede 

neural artificial direta de três camadas chamada perceptron (rede MLP: Multilayer 

Perceptron, do original em inglês), sendo uma camada de entrada (padrão de entrada), uma 

camada intermediária ou oculta e uma camada de saída (ou padrão de saída). No caso, o 

padrão de entrada (pi) foram todas as tabelas de dados dos ANEXOS E, F, G, H, I, J e K,  o 

padrão de saída para as cinco variáveis físicas foi fixado pelo vetor linha 

PADRÃO (5) = [5.5 4 7 602 51], 

e o padrão de saída para as oito variáveis físicas foi estabelecido por outro vetor linha  

PADRÃO (8) = [2 1.9 0.3 5.5 4 7 602 51]. 



105 
 

A diferenciação entre uma rede e outra foi condicionada ao número de neurônios da 

camada oculta que, na maioria dos casos, era de N = 2, 3, 4,..., 35 ou, somente, os múltiplos 

de 5. Em alguns casos,  atingiu o valor N = 100 na busca pela estabilidade e pela 

convergência. 

Dentre quatro variações da regra de aprendizado delta generalizada (ou simplesmente 

backpropagation), todas incluídas no MATLAB, aquela que atendeu ao objetivo geral foi a 

regra resiliente. Com a rede MLP e a regra de aprendizado backpropagation resiliente, o 

objetivo geral foi alcançado. 

Com relação à hipótese deste trabalho, resta apresentar as matrizes de pesos e de 

limiares encontradas, as quais permitem  a aplicação das redes neurais artificiais para um 

controle mais rigoroso da qualidade dos produtos industriais dos pisos e revestimentos 

cerâmicos das indústrias do PCSG. Esta é uma das grandes vantagens da aplicação das redes 

neurais, pois leva em consideração que o padrão de saída pode ser determinado com bastante 

precisão. Os melhores resultados foram alcançados tanto para as cinco variáveis físicas como 

para as oito variáveis sempre na maior temperatura de queima, isto é, 1060° C. A Tabela 23 e 

em correspondência a Figura 22 mostram os dados para cinco variáveis. Nesta Tabela, as 

variáveis RLQ, PF, CR e MRF são crescentes, com uma ligeira igualdade da variável CR, 

enquanto a variável Abs é decrescente com exceção daquele dado referente à temperatura de 

queima de 1060° C. 

A matriz de peso IW é de ordem 40 X 16, por isso ela foi dividida em blocos. Assim, 

 ?
?

?
?
?

?
?

43

21

BB

BB
IW ,           (4.9) 

onde cada bloco Bi, i = 1, 2, 3 e 4, é de ordem 20 X 8. As outras matrizes LW, b1 e b2 são de 

mais fácil manipulação e não necessitam da divisão em blocos. Os blocos B1, B2, B3 e B4 

constam, respectivamente, das Tabelas 49, 50, 51 e 52. 



106 
 

Tabela 49 – Bloco B1 da matriz IW, fórmula (4.9). 
2.4010 2.4007 2.3999 2.3982 2.4052 2.4007 0.4302 2.4034 

5.0451 5.0454 5.0428 5.0438 5.0399 5.0452 0.8628 5.0402 

0.3729 1.9929 1.9954 1.9946 0.3751 0.3710 0.3761 0.3778 

-0.0153 -0.0145 -0.0180 -0.0163 -0.0139 -0.0143 -0.0152 -0.0135 

-2.5245 -2.5292 -2.5264 -2.5276 -2.5261 -2.5280 -2.5291 -2.5279 

-5.5401 -5.5385 -5.5382 -5.5433 -5.5424 -5.5426 -5.5385 -5.5425 

1.9493 1.8100 1.8130 1.8107 1.9526 1.9553 1.8105 1.9526 

-2.2676 -2.2696 -2.2669 -2.2695 -2.2675 -2.2719 -2.2729 -2.2719 

1.7399 1.6031 1.6024 1.5999 1.7405 1.7464 1.6008 1.7460 

-0.5408 -0.0603 -0.0220 -0.0278 -0.1075 -0.3273 -0.0228 -0.4687 

-0.0074 -0.0021 -0.0025 -0.0045 -0.0043 -0.0015 -0.0054 -0.0213 

-4.9680 -4.8266 -4.8222 -4.8291 -4.9740 -4.9699 04.8268 -4.9675 

0.2961 1.5459 1.5437 1.5459 0.2943 1.5494 1.5416 0.2966 

0.3636 0.3674 2.1730 2.1679 0.3634 0.3654 2.1749 0.3661 

-0.4748 -0.0630 -0.0266 -0.0252 -0.1046 -0.3166 -0.0174 -0.4672 

4.8215 1.0123 1.0118 1.0086 4.9629 4.8209 1.0135 4.8235 

0.0048 -0.0009 0.0027 0.0038 -0.0003 -0.0037 0.0033 -0.0016 

-3.2759 -0.3557 -0.3512 -0.3544 -3.1361 -3.1315 -0.3529 -3.2741 

5.5207 5.5170 5.5141 5.5202 5.5166 5.5187 0.9450 5.5171 

-0.5194 -2.8481 -2.8479 -2.8487 -0.5232 -0.5192 -2.8516 -0.5254 

Fonte: Elaborada pelo autor. 



107 
 

Tabela 50 – Bloco B2 da matriz IW, fórmula (4.9). 
2.3994 2.4019 0.4275 2.3991 2.4006 0.4339 2.3997 0.4319 

5.0425 5.0411 0.8651 5.0450 5.0453 0.8585 5.0405 0.8646 

0.3710 0.3729 1.9963 0.3766 0.3711 1.9979 0.3723 1.9981 

-0.0174 -0.0189 -0.0194 -0.0175 -0.0128 -0.0139 -0.0186 -0.0179 

-2.5305 -2.5266 -2.5232 -2.5308 -2.5292 -2.5246 -2.5283 -2.5293 

-5.5441 -5.5438 -0.8338 -5.5426 -5.5453 -0.8364 -5.5384 -0.8306 

1.9536 1.9516 1.8124 1.9544 1.8055 0.1719 1.8057 1.8093 

-2.2715 -2.2716 -2.2673 -2.2719 -2.2721 -2.2732 -2.2708 -2.2723 

1.7458 1.7428 1.6038 1.7455 1.7433 1.5977 1.7415 1.5974 

-0.4738 -0.1026 -0.0191 -0.0114 0.0790 0.1872 0.0327 -0.0179 

-0.0144 -0.0495 -0.0064 -0.0039 -0.0039 -0.0170 -0.0078 -0.0081 

-4.9719 -4.9741 -0.6902 -4.8265 -4.8285 -0.6905 -4.8299 -0.6903 

0.2980 0.2978 1.5482 0.2940 0.2920 1.5411 1.5462 1.5444 

0.3675 0.3635 2.1704 0.3686 0.3689 2.1708 0.3650 2.1725 

-0.4733 -0.1108 -0.0193 -0.0090 -0.0661 0.1890 0.0782 -0.0166 

4.8229 4.9648 1.0531 4.9613 4.9671 1.0487 4.9619 1.0491 

-0.0030 -0.0038 -0.0030 -0.0044 0.0014 -0.0027 -0.0006 0.0039 

-3.2755 -3.1301 -0.3513 -0.4003 -0.3512 -0.3566 -0.3575 -0.3513 

5.5197 5.5204 0.9435 5.5198 5.5177 0.9433 5.5183 0.9477 

-0.5254 -0.5255 -2.8488 -0.5191 -0.5184 -2.8466 -2.8487 -2.8474 

Fonte: Elaborada pelo autor. 



108 
 

Tabela 51 - Bloco B3 da matriz IW, fórmula (4.9). 
1.5436 1.5428 1.5451 1.5490 1.5429 1.5434 1.5481 1.5449 

-2.6653 -0.5142 -0.5189 -0.5149 -2.6640 -2.6638 -0.5133 -2.6700 

0.3491 0.3559 0.3504 0.3522 0.3556 0.3507 0.3552 0.3561 

-1.7977 -0.3644 -0.3675 -0.3635 -1.7999 -1.7943 -0.3695 -1.7984 

1.4411 1.5799 1.5831 1.5843 1.4409 1.4376 1.5846 1.4404 

-2.6065 -2.6068 -2.6012 -2.6008 -2.5992 -2.6039 -2.6021 -2.6032 

-2.8453 -2.8458 -2.8507 -2.8473 -2.8500 -2.8507 -2.8465 -2.8492 

2.1115 2.1092 2.1106 2.1091 2.1133 2.1136 2.1091 2.1122 

2.1129 2.1095 2.1149 2.1078 2.1115 2.1131 2.1084 2.1135 

5.9267 0.8535 0.8523 0.8535 5.7781 5.9222 0.8590 5.9261 

-3.5899 -3.5904 -3.4528 -3.4495 -3.5894 -3.5879 -3.4460 -3.5876 

2.9366 2.9402 2.9357 2.9375 2.9405 2.9425 2.8013 2.9372 

0.3628 0.0203 -0.0470 -0.0160 0.3748 0.2588 -0.0523 0.2883 

0.2292 -0.1327 -0.1281 -0.1324 0.0889 0.2265 -0.1772 0.2292 

-0.0088 -0.0082 -0.0075 -0.0098 0.0159 -0.0095 -0.0075 -0.0030 

6.4154 0.9455 0.9440 0.9410 6.2712 6.4123 0.9378 6.4186 

0.0009 -0.0011 -0.0028 0.0001 0.0042 -0.0011 -0.0017 -0.0025 

-0.6008 0.0825 0.1032 0.1059 -0.3547 -0.3710 0.2448 -0.3691 

-0.9250 -0.9287 -0.9236 -0.9263 -0.9287 -0.9255 -0.9302 -0.9235 

0.1192 -0.0668 -0.0962 -0.0678 -0.0212 0.0645 -0.0967 1.8221 

Fonte: Elaborada pelo autor. 



109 
 

Tabela 52 – Bloco B4 da matriz IW, fórmula (4.9). 
1.5443 1.5490 1.5492 1.5483 1.5454 1.5460 1.5440 1.5475 

-2.6657 -2.6667 -0.4795 -2.6667 -0.5174 -0.4821 -2.6682 -0.4848 

0.3541 0.3536 0.3532 0.3527 0.3525 0.0599 0.3516 0.0576 

-1.7985 -1.8008 -0.3313 -1.7941 -0.3655 -0.3299 -1.7969 -0.3345 

1.4354 1.4384 1.5809 1.4359 1.4409 1.5781 1.4397 1.5788 

-2.6046 -2.6040 -2.5982 -2.6034 -2.6024 -2.6003 -2.6039 -2.6052 

-0.5249 -2.8464 -2.8472 -2.8478 -2.8450 -2.8455 -2.8517 -2.8502 

2.1139 2.1100 2.1148 2.1072 2.1147 2.1115 2.1129 2.1093 

2.1096 2.1094 2.1094 2.1136 2.1104 2.1143 2.1082 2.1142 

5.9231 5.7805 0.8171 5.7807 5.7805 0.8181 5.7816 0.8138 

-3.5928 -3.5909 -3.4493 -3.5927 -3.5881 -3.4459 -3.5908 -3.4444 

2.9413 2.9398 0.3829 2.9394 2.9386 0.3787 2.9405 0.3790 

0.2875 0.2428 -0.2852 0.3118 0.1188 -0.0872 0.0907 -0.2876 

0.2368 0.0831 -0.1715 0.0871 -0.1317 -0.1790 -0.1325 -0.1717 

-0.0018 0.0170 -0.0066 0.0196 -0.0071 0.1382 -0.0028 -0.0041 

6.4135 6.2756 0.9012 6.2740 6.2715 0.8998 6.2720 0.8970 

0.0064 -0.0023 -0.0008 0.0027 0.0030 0.0071 -0.0008 0.0024 

-0.3735 -0.3588 0.1458 -0.2037 -0.1372 0.1530 -0.1427 0.1444 

-0.9246 -0.9250 -0.9273 -0.9288 -0.9295 -0.9312 -0.9240 -0.9252 

1.8224 0.2247 -0.1825 -0.0024 -0.0262 -0.2320 -0.0280 -0.1754 

Fonte: Elaborada pelo autor. 

 

A seguir, são fornecidas as matrizes LW (vetor linha), b1 (vetor coluna) e b2 (vetor linha), 

 

LW =[10.6507 -1.6113 -8.0526 -12.1897 1.0834 4.6068 1.5350 0.7478 -8.2536 1.5361           

-9.6545 5.2655 -8.0779 1.6610 1.4393 2.5157 -1.3052 3.5816 -2.5204 1.0854 -8.4389 1.0781 

0.5190 0.5594 6.4596 1.0913 1.0803 -7.5189 -7.5213 -0.1267 2.2879 -9.5623 -5.9951 -2.4220 

575.5548 1.6027 4.5744 2.8892 1.2824 -1.0867], 

 

Tb1  = [11.1182 -6.7321 3.3964 10.0635 -11.8536 -4.5271 4.4333 2.6438 4.4016 -0.0967 

1.8429 -3.1783 3.5161 5.1964 -0.1652 -0.8158 -3.9578 -1.4680 -7.6699  -4.6255 -2.6878         

-3.9224 -4.0715 -1.5730 -1.3933 -2.4638 -4.5618 0.9091 2.2839 2.6582 -6.5349 3.7130 

5.6143 4.4312 -8.6249 2.7774 1.7058 -3.0905 -3.5703 1.6306],  



110 
 

e b2 = [65.7042 65.7042 65.7042 65.7042 65.7042], 

 

onde o símbolo T indica matriz transposta. 

 

Para a aplicação são usadas as fórmulas (4.5), (4.6) e (4.7), repetidas abaixo: 

IW * pi + b1 * ones(1,5), 

Ai = logsig(IW * pi + b1 * ones(1,5)), 

Ti = LW * Ai + b2, 

onde i = 1, 2, 3 e 4, pi é o padrão de entrada, ones(1,5) = [1 1 1 1 1], Ti é o padrão de saída e 

b2 é a matriz de limiar, de ordem 1 X 5, com o valor encontrado 65.7042 repetido 5 vezes. 

 

Com relação à mesma temperatura de queima de 1060° C, agora com oito variáveis, a 

solução foi decidida entre dois resultados, aqueles fornecidos pelas Tabelas 46 e 47. Para isso, 

foi calculado o desvio entre o valor mostrado pela Tabela, designado por vt e o valor do 

padrão de resposta esperado, PADRÃO (8), ou de forma simplificada P(8). Em valor 

absoluto, o desvio é obtido da expressão. 

d = | vt – P(8) |,                     (4.10)  

e, transformado em porcentagem, pela fórmula 

D = .100*
)8(P

d
                     (4.11) 

Os resultados foram incluídos nas Tabelas 53 e 54. 

 

Tabela 53 – Cálculo dos desvios, D(%), para os valores da Tabela 46. 
D(%) DP DAS RLS RLQ PF  Abs CR MRF 

1020 0.47 0.06 7.73 11.32 2.36 2.86 4.55 8.27 

1040 0.29 0.46 16.57 13.47 0.81 8.58 0.83 17.41 

1060 0.27 033 0.03 0.04 0.04 0.05 0.05 0.07 

Fonte: Elaborada pelo autor. 

 



111 
 

Tabela 54 – Cálculo dos desvios, D(%), para os valores da Tabela 47. 
D(%) DP  DAS RLS RLQ PF Abs CR MRF 

1020 0.63 0.76 5.40 12.55 1.30 6.23 0.89 23.41 

1040 0.64 0.01 5.03 0 1.13 5.61 0.22 10.06 

1060 0.35 0.29 0.47 0.10 0.05 0.04 0 0.20 

Fonte: Elaborada pelo autor. 
 

Como a norma NBR 13818 (ABNT, 1997) especifica as variáveis físicas Abs, CR e 

MRF, a escolha recaiu sobre os dados da Tabela 47 combinado com os dados da Tabela 54, 

porque esta Tabela suplantou em seis dos nove resultados os dados da Tabela 53, com desvios 

menores. Convém observar que a Tabela 54 apresentou somente um valor, D = 23.41%, 

relativamente alto com relação aos outros valores encontrados. 

Com isto colocado, a matriz de peso IW ficou sendo de ordem 70 X 20 e, também, foi 

dividida em blocos, conforme a matriz abaixo: 

,

65

43

21

?
?
?

?

?

?
?
?

?

?
?

BB

BB

BB

IW                      (4.12) 

onde B1 e B2 são de ordem 20 X 10, enquanto B3, B4, B5 e B6 são de ordem 25 X 10. As 

outras matrizes não necessitam da divisão em blocos. Os blocos B1, B2, B3, B4, B5 e B6 

constam, respectivamente, das Tabelas 55, 56, 57, 58, 59 e 60. 

 



112 
 

Tabela 55 – Bloco B1 da matriz IW, fórmula (4.12). 
0.0404 0.0204 0.0106 0.0021 0.0220 0.0141 -0.0058 0.0436 0.0108 0.0429 

-0.0594 -0.0569 -0.0556 -0.0629 -0.0551 -0.0792 -0.0771 -0.0640 -0.0750 -0.0743 

0.3525 0.3484 0.3494 0.2949 0.3544 0.3163 0.3063 0.3538 0.3063 0.3067 

-0.0578 -0.0565 -0.0566 -0.0560 -0.0564 -0.0812 -0.0771 -0.0622 -0.0747 -0.0749 

0.0317 0.0322 0.0312 0.0258 0.0272 0.0246 0.0250 0.0293 0.0263 0.0268 

-0.2090 -0.3408 -0.3393 -0.1198 -0.3426 -0.4490 -0.1168 -0.2099 -0.6812 -0.2055 

-0.5425 0.0348 0.0324 -0.0047 0.0353 -1.0021 -0.0036 -0.4455 -0.0087 -0.7400 

0.0009 -0.0018 -0.0037 -0.0097 -0.0030 -0.0041 -0.0036 0.0008 -0.0012 -0.0037 

-0.0570 -0.623 -0.0542 -0.0561 -0.0545 -0.0768 -0.0783 -0.0628 -0.0794 -0.0786 

0.0015 -0.0006 0.0041 0.0040 0.0001 0.0006 -0.0017 0.0009 0.0025 0.0029 

-0.0581 -0.0591 -0.0550 -0.0156 -0.0557 -0.0952 -0.1083 -0.0605 -0.1129 -0.0794 

0.0483 0.0714 0.0704 0.0289 0.0690 0.0467 0.0407 0.0422 0.0713 0.0495 

0.0022 0.0021 0.0030 -0.0087 0.0005 -0.0010 0.0031 -0.0015 0.0023 0.0006 

0.4360 -0.0434 -0.0414 -0.0343 -0.0397 0.0546 -0.1238 0.3675 -0.0923 0.3569 

-0.0573 -0.0572 -0.0590 -0.0572 -0.0543 -0.0786 -0.0775 -0.0589 -0.0730 -0.0769 

0.0505 0.0239 0.0198 -0.1151 0.0222 0.0269 0.0189 0.0331 0.0232 0.0512 

0.1041 0.1493 0.1481 0.0937 0.0791 0.0987 0.0527 0.0971 0.1184 0.0941 

0.5310 0.1815 -0.0512 -0.0391 -0.0513 0.1170 -0.0284 0.3094 0.0005 0.3092 

-2.7032 -0.2043 -0.2061 -0.0889 -0.2059 -0.3154 -0.0955 -2.7081 -0.5518 -2.7056 

0.0031 0.0008 -0.0001 0.0048 0.0019 -0.0015 0.0021 -0.0023 0.0004 0.0023 

Fonte: Elaborada pelo autor. 



113 
 

Tabela 56 – Bloco B2 da matriz IW, fórmula (4.12). 

0.0440 0.0196 -0.0081 0.0407 0.0182 -0.0081 0.0167 0.0025 0.0040 -0.0106 

-0.0770 -0.0772 -0.0629 -0.0584 -0.0643 -0.1217 -0.0584 -0.0724 -0.1127 -0.1029 

0.2809 0.3078 0.3539 0.3486 0.3540 0.1847 0.3478 0.3481 0.2586 0.1555 

-0.0756 -0.0785 -0.0601 -0.0628 -0.0644 -0.1224 -0.0595 -0.0736 -0.1091 -0.1093 

0.0230 0.0235 0.0230 0.0279 0.0291 0.0103 0.0279 0.0234 0.0127 0.0904 

-0.2049 -0.3415 -0.1136 -0.3411 -0.3386 -0.1198 -0.3378 -0.1148 -0.1183 -0.8532 

-0.4486 -1.0019 -0.0039 0.0297 0.0325 -0.0018 0.0306 -0.0022 -0.0062 -0.1491 

0 -0.0037 -0.0034 -0.0027 -0.0020 0.0036 0.0008 0.0072 0.0039 -0.0013 

-0.0775 -0.0755 -0.0727 -0.0611 -0.0614 -0.1232 -0.0572 -0.0754 -0.1093 -0.1207 

0.0017 -0.0007 0.0045 -0.0011 -0.0016 -0.0016 0.0041 0.0033 0.0003 -0.0009 

-0.0754 -0.0780 -0.0945 -0.0578 0.0568 -0.1478 -0.0605 -0.1094 -0.1323 -0.0182 

0.0478 0.0469 0.0426 0.0435 0.0451 0.0452 0.0747 0.0414 0.0362 0.0167 

-0.0008 0.0007 0.0025 0.0023 -0.0024 -0.0002 -0.0037 0.0032 -0.0115 -0.0139 

0.3824 0.2023 -0.0322 0.2530 -0.0442 -0.0158 -0.1059 -0.0311 -0.0197 -0.0075 

-0.0757 -0.0783 -0.0597 -0.0603 -0.0650 -0.1252 -0.0588 -0.0781 -0.1093 -0.1081 

0.2251 0.0192 -0.1166 -0.0124 0.0226 -0.1207 0.0207 -0.1152 -0.1109 -0.0779 

0.0995 0.0940 0.1032 0.1181 0.1211 0.0971 0.1213 0.0990 0.1047 0.0919 

0.3121 0.1681 -0.0346 0.1664 0.1705 -0.0391 0.1677 -0.0404 -0.0355 -0.0270 

-2.7048 -0.2034 -0.0922 -0.2034 -0.2036 -0.0923 -0.2026 -0.0972 -0.0944 -0.7171 

0.0029 0.0025 0.0037 -0.0016 -0.0019 -0.0030 0.0007 0.0003 0.0050 -0.0004 

Fonte: Elaborada pelo autor 



114 
 

Tabela 57 – Bloco B3 da matriz IW, fórmula (4.12). 
0.0677 0.0286 0.0297 -0.0374 0.0253 -0.0654 -0.0574 0.0725 -0.0602 0.0718 

0.0028 0.0003 0.0029 -0.0036 0.0007 0.0051 -0.0157 -0.0090 0.0025 0.0022 

-0.1352 -0.1920 -0.1937 -0.0416 -0.1928 -0.1350 -0.1579 -0.1355 -0.1951 -0.1320 

-0.1710 -0.1838 -0.1863 -0.0498 -0.1831 -0.1663 -0.1934 -0.1714 -0.1945 -0.1698 

-0.0270 -0.0277 -0.0328 0.0088 -0.0309 -0.0288 0.0163 -0.0267 -0.0289 -0.0253 

0.0333 0.0166 0.0149 0.0071 0.0162 0.0156 0.0004 0.0273 0.0122 0.0275 

0.0874 0.0694 0.0689 0.0120 0.0660 -0.1259 0.0274 0.0875 0.0316 0.0917 

-0.2085 -0.3440 -0.3390 -0.1147 -0.3408 -0.4480 -0.1144 -0.2087 -0.1072 -0.2092 

0.0044 0.0018 0.0018 -0.0068 0.0026 0.0013 -0.0002 -0.0003 0.0024 0.0021 

0.0008 0.0019 0.0037 0.0030 0.0022 -0.0022 -0.0003 0.0036 -0.0016 -0.0017 

0.3863 -0.0416 -0.0387 -0.0260 -0.0405 0.0597 -0.0303 0.3818 -0.1332 0.3588 

0.2928 0.2937 0.2963 0.2106 0.2439 0.2944 0.2457 0.2971 0.2937 0.2933 

-0.0288 -0.0254 -0.0196 -0.0200 -0.0228 -0.0273 -0.0285 -0.0262 -0.0244 -0.0289 

-0.0765 -0.0276 -0.0243 -0.0550 -0.0290 0.0384 -0.0442 -0.0754 -0.0479 0.0280 

-0.0619 -0.6360 -0.0538 -0.0590 -0.0595 -0.0772 -0.0749 -0.0611 -0.0756 -0.0774 

0.1078 0.1478 0.1439 0.0997 0.0788 0.0983 0.0508 0.0984 0.1238 0.0978 

-0.0371 -0.0247 -0.0283 -0.0175 -0.0239 -0.0416 -0.0222 -0.0351 -0.0252 -0.0359 

-0.0562 -0.0541 -0.0532 -0.1135 -0.0584 -0.0666 -0.1189 -0.0550 -0.0620 -0.0605 

0.3238 0.1774 0.1781 0.0256 0.1827 0.1857 0.1016 0.3189 0.0246 0.3644 

0.0125 0.0014 -0.0285 -0.0202 -0.0228 0.0120 -0.0242 0.0072 -0.0273 0.0126 

0.0752 0.0718 0.0697 0.0281 0.0763 0.0544 0.0174 0.0716 0.0432 0.0642 

-0.0598 -0.0576 -0.0610 -0.0553 -0.0564 -0.0780 -0.0750 -0.0621 -0.0752 -0.0762 

-0.0001 0.0022 0.0038 -0.0022 0.0013 -0.0026 -0.0019 0.0011 0.0026 -0.0018 

-0.0454 -0.0119 -0.0145 -0.0063 -0.0122 -0.0161 0.0111 -0.0408 -0.0133 -0.0414 

0.3129 0.2564 0.2590 0.0766 0.2625 2.7096 0.0739 0.3121 0.4481 0.3139 

Fonte: Elaborada pelo autor. 



115 
 

Tabela 58 – Bloco B4 da matriz IW, fórmula (4.12). 
0.0702 -0.0615 -0.0567 -0.0655 -0.0622 -0.0580 -0.0674 -0.0585 -0.0714 -0.0260 

0.0010 0.0023 -0.0131 0.0061 0.0065 -0.0017 0.0050 -0.0020 -0.0034 0.0022 

-0.1347 -0.1316 -0.1567 -0.1350 -0.1358 -0.1578 -0.1337 -0.1621 -0.1476 -0.0392 

-0.1676 -0.1678 -0.1807 -0.1703 -0.1699 -0.1908 -0.1650 -0.0587 -0.0486 -0.0325 

-0.0256 -0.0265 0.0159 -0.0253 -0.0259 0.0181 -0.0282 0.0070 0.0107 0.0029 

0.0228 0.0263 0.0058 0.0232 0.0116 0.0055 0.0125 0.0048 0.0065 -0.0009 

0.0003 0.0031 0.0278 0.0368 -0.1229 -0.0650 -0.1266 0.0270 -0.0697 -0.0915 

-0.2083 -0.3385 -0.1195 -0.3437 -0.3387 -0.1128 -0.3400 -0.1151 -0.1185 -0.8537 

0.0014 0 0.0026 0.0005 0.0039 0.0045 -0.0013 0.0014 0.0008 0.0050 

-0.0009 -0.0023 0.0004 0.0008 -0.0038 0.0030 0.0021 0.0014 0.0044 0.0034 

0.0767 0.3591 -0.0288 0.3602 -0.0779 -0.0247 -0.0770 -0.0333 -0.0166 -0.0074 

0.2964 0.2954 0.2115 0.2988 0.2939 0.2133 0.2957 0.2079 0.2096 1.1022 

-0.0307 -0.0300 -0.0226 -0.0250 -0.0290 -0.0633 -0.0229 -0.0228 -0.0595 -0.4179 

0.0264 0.0249 -0.0611 -0.0744 -0.1337 -0.0200 -0.0826 -0.0609 -0.0287 -0.0489 

-0.0777 -0.0752 -0.0766 -0.0573 -0.0596 -0.1206 -0.0584 -0.0728 -0.1082 -0.1223 

0.1017 0.1018 0.1024 0.1251 0.1193 0.0997 0.1236 0.1037 0.1046 0.0987 

-0.0024 -0.0049 -0.0246 -0.0369 -0.0365 -0.0257 -0.0379 -0.0236 -0.0205 -0.0190 

-0.0608 -0.0651 -0.1019 -0.0550 -0.0566 -0.1805 -0.0558 -0.1028 -0.1458 -0.1586 

0.3455 0.1749 0.0314 0.3618 0.1864 -0.0747 0.1826 0.0271 -0.0759 -0.0864 

0.0123 0.0082 -0.0236 0.0085 0.0095 -0.0276 0.0123 -0.0272 -0.0194 -0.0061 

0.0594 0.0560 0.0155 0.0752 0.0644 0.0210 0.0771 0.0266 0.0280 0.0160 

-0.0753 -0.0761 -0.0749 -0.0606 -0.0591 -0.1188 -0.0579 -0.0732 -0.1134 -0.1222 

0.0023 0.0004 0.0021 -0.0014 -0.0024 0.0036 0.0021 -0.0002 0.0044 0.0051 

-0.0454 -0.0193 0.0100 -0.0099 -0.0142 0.0121 -0.0155 0.0061 -0.0001 0.0052 

0.3146 0.3119 0.0746 0.3164 0.2570 0.0799 0.3145 0.0766 0.0769 06128 

Fonte: Elaborada pelo autor. 



116 
 

Tabela 59 – Bloco B5 da matriz IW, fórmula (4.12). 
-0.0007 0.0013 -0.0008 -0.0014 0.0016 0.0012 -0.0008 -0.0035 -0.0027 0.0002 

0.0275 0.0320 0.0345 0.0211 0.0316 0.0307 0.0404 0.0304 0.0317 0.0302 

0.0283 0.0136 0.0153 0.0043 0.0132 0.0111 0.0003 0.0232 0.0143 0.0245 

-0.0311 0.0060 0.0094 0.0059 0.0085 -0.0203 0.0084 -0.0325 0.0107 -0.0279 

-0.0377 -0.0276 -0.0267 -0.0183 -0.0217 -0.0247 -0.0195 -0.0361 -0.0215 -0.0388 

-0.0157 0.0148 0.0199 0.0245 0.0243 -0.0104 0.0241 -0.0104 0.0131 -0.0138 

0.4300 -0.0276 -0.0282 -0.0049 -0.0315 0.0989 -0.0164 0.3900 -0.0260 0.3657 

0.0024 0.0026 0.0023 0.0031 0.0026 0.0016 -0.0022 -0.0009 -0.0007 -0.0029 

-0.0375 -0.0254 -0.0268 -0.0237 -0.0218 -0.0250 -0.0187 -0.0374 -0.0262 -0.0376 

0.0195 0.0212 0.0208 0.0194 0.0195 0.0153 0.0213 0.0180 0.0168 -0.0499 

-0.0381 -0.0222 -0.0209 -0.0211 -0.0219 -0.0225 -0.0255 -0.0403 -0.0225 -0.0401 

0.0401 -0.0207 -0.0184 -0.0403 -0.0188 0.0441 -0.0222 0.0446 -0.0180 0.0969 

-0.0366 -0.0216 -0.0238 -0.0183 -0.0266 -0.0233 -0.0206 -0.0370 -0.0239 -0.0349 

-0.0599 -0.0596 -0.0563 -0.0561 -0.0576 -0.0799 -0.0797 -0.0588 -0.0732 -0.0767 

0.0221 0.0272 0.0690 -0.0237 -0.0335 0.0226 -0.0194 0.0250 0.0733 0.0262 

-0.0440 -0.0748 -0.0993 -0.0787 -0.0994 -0.0493 -0.1000 -0.0483 -0.0983 -0.0393 

-0.0011 -0.0026 -0.0023 0.0055 0.0003 -0.0002 0.0026 -0.0031 0.0040 0.0009 

0.0033 0.0019 -0.0030 0.0072 -0.0030 -0.0002 -0.0011 -0.0022 -0.0025 0.0011 

0.0009 -0.0048 -0.0017 0.0042 -0.0028 -0.0051 -0.0005 -0.0007 0.0008 -0.0049 

0.0080 0.0097 0.0057 0.0013 0.0071 0.0121 0.0129 0.0121 0.0077 0.0102 

0.3624 0.2319 0.2293 -0.1034 0.2351 -0.2296 -0.1022 0.0106 -0.0712 0.0120 

0.0304 0.0304 0.0385 0.2377 0.0386 0.0230 0.0262 0.0290 0.0220 0.0214 

0.1020 -0.0267 -0.0645 -0.0621 -0.0682 -0.0185 -0.3610 0.1100 0.0529 0.1142 

0.0022 -0.0017 0.0016 -0.0064 0.0008 -0.0039 -0.0030 0.0016 0.0015 -0.0022 

-0.0344 -0.0035 -0.0060 0.0067 -0.0046 0.0003 0.0161 -0.0257 -0.0035 -0.0307 

Fonte: Elaborada pelo autor. 



117 
 

Tabela 60 – Bloco B6 da matriz IW, fórmula (4.12). 
-0.0026 -0.0018 -0.0005 -0.0021 -0.0031 -0.0047 0.0018 0.0060 0.0118 -0.0118 

0.0266 0.0258 0.0399 0.0310 0.0330 0.0395 0.0266 0.0274 0.0255 0.0001 

0.0240 0.0250 0.0038 0.0257 0.0165 0.0112 0.0265 0.0092 0.0060 0.0041 

-0.0285 -0.0271 0.0345 -0.0167 0.0093 0.0315 0.0053 0.0348 0.0322 0.0164 

-0.0374 -0.0395 -0.0220 -0.0354 -0.0230 -0.0213 -0.0252 -0.0214 -0.0247 -0.0191 

0.0106 -0.0124 0.0195 0.0099 -0.0137 0.0228 -0.0116 0.0241 0.0232 0.0208 

0.3676 0.3660 -0.0151 -0.0861 -0.1285 -0.0143 0.0041 -0.0196 -0.0033 -0.0049 

-0.0007 0.0005 0.0003 -0.0026 0.0028 0.0026 0.0028 -0.0018 -0.0017 -0.0462 

-0.0355 -0.0385 -0.0232 -0.0370 -0.0255 -0.0217 -0.0219 -0.0260 -0.0173 -0.0164 

0.0142 -0.0522 0.0191 0.0155 0.0182 0.0153 0.0189 0.0094 0.0106 0.0060 

-0.0412 -0.0372 -0.0258 -0.0228 -0.0268 -0.0263 -0.0250 -0.0219 -0.0225 0.0080 

0.0968 0.1019 -0.0397 0.0093 -0.0220 -0.0457 -0.0189 -0.0412 -0.0406 -0.2023 

-0.0359 -0.0369 -0.0218 -0.0361 -0.0263 -0.0243 -0.0282 -0.0212 -0.0203 -0.0225 

-0.0755 -0.0757 -0.0732 -0.0596 -0.0643 -0.1195 -0.0569 -0.0776 -0.1097 -0.1062 

0.0221 0.0236 -0.0386 0.0243 0.0265 -0.0373 0.0210 -0.0211 -0.0230 -0.1906 

-0.0534 -0.0378 -0.0922 -0.0496 -0.0462 -0.1021 -0.0460 -0.0921 -0.0840 -0.0283 

0.0036 0.0015 0.0011 0.0024 0.0009 0.0011 0.0011 0.0036 -0.0006 0.0056 

0.0007 0.0008 -0.0007 -0.0010 -0.0029 -0.0038 0.0002 -0.0014 0.0062 0.0120 

-0.0039 -0.0035 -0.0051 -0.0022 -0.0021 0.0018 -0.0053 -0.0025 0.0088 0.0331 

0.0063 0.0101 0.0082 0.0073 0.0080 0.0115 0.0121 0.0102 0.0006 0.0310 

0.0104 -0.1215 -0.0804 0.2307 -0.1201 -0.0785 0.2287 -0.1002 -0.1028 -0.7607 

0.0221 0.0221 0.0326 0.0330 0.0269 0.0241 0.0273 0.0281 0.0203 0.1192 

0.1150 -0.0286 -0.0573 0.1116 -0.0314 -0.0621 -0.0272 -0.0620 -0.0587 -0.5267 

-0.0023 -0.0029 0.0011 -0.0009 -0.0027 -0.0022 0.0020 0.0016 -0.0023 -0.0015 

-0.0360 -0.0360 0.0148 -0.0046 -0.0050 0.0181 -0.0056 -0.0118 0.0045 0.0208 

Fonte: Elaborada pelo autor. 

 

A seguir, as outras matrizes,  

LW = [-1.2516 -27.0481 -0.0985 -27.1744 -0.5460 0.8034 -3.7941 -6.5422  -27.6216 95.9035 

-26.8871 1.8888 -0.4168 -2.5810 -26.8337 2.3459 -0.7838 -0.3164 0.5908 96.6923 0.5864      

-9.0630 -0.9755 -0.8091 0.9583 -0.7615 -0.0532 0.7978 95.9943 96.7776 -1.6576 -0.8905      

-15.0051 0.6144 -27.0444 -0.1060 1.1710 -0.4133 0.1325 -0.6060 -3.7948 -27.3438 96.1249 

0.8995 -3.3421 0.8964 1.9142 -0.5593 2.1019 1.5406 2.1179 -3.3831 -5.0903 1.3298 2.4226  

-1.1270 2.4355 1.4058 -27.3181 2.4678 -0.4543 96.5362 5.0686 -7.0072 2.2331 -0.8964 

0.3360 0.2627 -0.4157 1.2539], 



118 
 

Tb1  = [2.4410 -7.9358 -5.0742 -7.2658 2.6508 0.4874 1.5836 1.5013 -8.6979 -3.0517 -5.6962 

-2.7558 -7.1611 -0.6151 -6.5599 -0.5943 -5.7552 -0.5321 -2.0322 -3.5029 -0.0488 0.7850 

3.2023 2.8671 -1.4401 4.0182 1.3002 -0.0167 -4.8233 -3.3370 -0.9908 4.8597 -2.2629           

-0.2202 -6.6958 -5.9316 -4.0406 -5.0494 1.4031 2.4255 1.7494 -7.9289 -4.0354 -2.3067         

-0.4935 1.8228 -1.2622 4.6173 -4.3329 -3.9924 -3.3867 0.5023 8.6083 -3.5537 -1.9742          

-2.7788 -2.9090 -4.6417 -8.9476 -3.6206 3.5346 -4.6819 10.6586 2.2981 -4.3918 1.0575 

2.5938 1.5923 2.6691 -4.0129] e 

 

b2 = [22.2270 22.2270 22.2270 22.2270 22.2270 22.2270 22.2270 22.2270],  

onde o símbolo T indica  matriz transposta. 

Para a aplicação da rede neural artificial aos dados das tabelas dos ANEXOS I, J e K 

são usadas as mesmas fórmulas (4.5), (4.6) e (4.7), agora com i = 1, 2 e 3, b2 é a matriz de 

limiar, de ordem 1 X 8, com o valor encontrado 22.2270 repetido 8 vezes e, por fim,  a matriz 

ones(1,8) = [1 1 1 1 1 1 1 1]. 

Com estes resultados fica provada a hipótese formulada no capítulo introdutório deste 

trabalho, isto é, ¨é possível aplicar as redes neurais artificiais na indústria de pisos e 

revestimentos cerâmicos¨. 

É importante ressaltar que o processo de aplicação das RNAs foi também validado na 

medida em que as matrizes encontradas para a temperatura de sinterização de 1060° C, tanto 

para cinco como para oito variáveis, puderam ser aplicadas às outras temperaturas de queima 

(1000° C para 5 variáveis, 1020° C e 1040° C para 5 e 8 variáveis).  

Pode-se concluir, então, que a estabilidade e a convergência não ocorreram quando o 

número de neurônios na camada oculta é grande e nem mesmo quando o número de iterações 

(epochs) é muito elevado. Ocorreu quando houve uma relação ótima entre os dois números, 

na temperatura de sinterização de 1060° C. Ainda, levando em consideração que uma rede 

neural é um conceito desenvolvido para operar com grande volume de dados, observou-se que 

a melhor solução foi encontrada quando, além da temperatura mencionada, o número de 

variáveis atingiu o número de oito e quando o número de linhas de dados era de vinte, de 

acordo com as tabelas dos ANEXOS I, J e K. 

Após ter contratado os fornecedores de argilas e de outros insumos e definido o 

produto com as especificações técnicas das variáveis de controle, o industrial interessado em 



119 
 

aplicar as RNAs, para se estabelecer em algum nicho de mercado, deve proceder de acordo 

com as seguintes etapas: 

1 – Recolher amostras das minas e bancadas de argilas; 

2 – Confeccionar os corpos-de-prova; 

3 – Testar em laboratório os corpos-de-prova, para encontrar os dados amostrais das          

variáveis de controle; 

4 – Elaborar a tabela (ou tabelas) com os dados amostrais, que será definida como o padrão de 

entrada das RNAs; e 

5 – Aplicar as RNAs. 

 Para a aplicação das RNAs, seguem-se as seguintes etapas: 

1 – Disponibilizar um software (MATLAB); 

2 – Definir a rede (MLP de três camadas); 

3 – Definir a regra de aprendizado da rede (regra delta generalizada ou backpropagation com 

quatro variações); e 

4 – Estabelecer as funções de transferência da rede (logística e linear). 

 Entre parênteses foram colocados o software e os dados utilizados neste trabalho. O 

programa usado para o backpropagation resiliente (trainrp) consiste dos seguintes passos: 

1. p = [2.058 ... 61.3; 2.006 ... 59.7; ... ; 1.984 ... 22.9]; 

2. t = [2  1.9  0.3  5.5  4  7  602  51]; 

3. net = newff(minmax(p),[3,1],{‘logsig’,’purelin’},’trainrp’); 

4. net.trainParam.show = 200; 

5. net.trainParam.epochs = 5000; 

6. net.trainParam.goal = 1e-5; 

7. [net,tr] = train(net,p,t); 

      Realizados os testes e encontrados os melhores valores de parada (epochs ou goal), 

indicando que houve convergência, o comando a = sim(net,p) fornece a simulação executada 

pelo programa. 



120 
 

 Mais importante, os comandos celldisp(net.IW), celldisp(net.LW) e celldisp(net.b)   

recuperam as matrizes de pesos IW e LW e as matrizes de limiares b(duas). Aplicando as 

fórmulas (4.5), (4.6) e (4.7), p. 60, podem ser encontrados os valores estimados para o 

produto industrial, os quais se apresentam aproximadamente iguais ao padrão de saída da 

rede, designado por t no programa. Convém observar que a matriz p corresponde aos dados 

amostrais do ANEXO I.  

  

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 



121 
 

5. CONSIDERAÇÕES FINAIS 
 

Ao concluir este trabalho é muito importante reforçar alguns aspectos quando da 

aplicação de uma rede neural artificial. A ideia da aplicação do conceito das redes neurais 

artificiais às indústrias de pisos e revestimentos cerâmicos do PCSG mostrou-se bastante 

promissora tendo em vista que, mesmo operando com uma pequena quantidade de dados, 

contrariando a sua proposta original de trabalhar com grande volume de dados em problemas 

complexos, as redes forneceram soluções com pequenos desvios com relação ao padrão de 

saída previamente estabelecido. Mais ainda, a escolha das minas para fornecer as amostras 

recaiu sobre aquelas que apresentaram as maiores diferenciações nas argilas e representavam 

toda a coluna estratigráfica da Formação Corumbataí. Por isso, mesmo diferentes e com dados 

amostrais que variavam em média de 0.3 a 602, todos os dados dos ANEXOS A, B, C e D 

foram usados desde que pertencessem às mesmas variáveis físicas e às mesmas temperaturas 

de queima. Estes dados também não interferiram nas soluções encontradas. 

Provavelmente, uma indústria cerâmica não utiliza argilas tão diferentes para a sua 

linha de produção. Com valores mais homogêneos é possível estabelecer um padrão de 

qualidade bem mais rigoroso, que pode se constituir no padrão de saída pré-estabelecido para 

o funcionamento de uma rede neural.  

Este trabalho pode ser ampliado, futuramente, incluindo também outras variáveis, 

além das variáveis físicas aqui usadas, que são as variáveis químicas e mineralógicas. Isto 

possibilita  um melhor desempenho por parte das redes neurais. 

É bom reforçar também que as redes neurais artificiais apresentam outras arquiteturas 

e também outras regras de aprendizado. Elas podem ser operadas isoladamente ou associadas 

a outro conceito ou método, em especial, daqueles que já fazem parte do que é concebido 

como Inteligência Artificial. 

Por outro lado, é também igualmente importante ressaltar que com o inevitável 

aumento da população humana, ocorrerá uma pressão cada vez maior para a ocupação dos 

espaços ainda inexplorados na face deste planeta Terra. A humanidade em busca por água, 

por alimentos e por outros recursos naturais, forçosamente provocará um aumento dos 

nefastos impactos causados ao meio ambiente. Especificamente, no caso das mineradoras e da 

industrialização correspondente, Poletto (2007) e AMBIENTE BRASIL (2009), sugerem 

alternativas no sentido de se diminuir o consumo e de se aumentar a reciclagem e, no caso das 

minas, de se promover a revegetação e a reversão das áreas degradadas em terras produtivas. 



122 
 

No caso da revegetação, como um trabalho para o futuro, poder-se-ia sugerir a aplicação de 

uma rede Kohonen, as quais não exibem uma camada de saída, contudo os neurônios da 

camada de entrada competem entre si e o vencedor leva tudo (ou vencedores). Com esta rede 

poderiam ser resolvidos alguns problemas relativos à flora, promovendo uma competição 

controlada biologicamente entre as espécies vegetais para que aquelas que estiverem melhores 

adaptadas ao local possam se estabelecer. 



123 
 

REFERÊNCIA BIBLIOGRÁFICA 

 

ABDI, H.; VALENTIN, D.; EDELMAN, B. Neural networks. Sage University Papers Series 

on Quantitative Applications in the Social Sciences, 07 – 124. Thousand Oaks, CA, 1999. 

AMBIENTE Brasil. Disponível em: http://www.ambientebrasil.com.br. Acesso em: 11 nov. 

2009. 

ASSOCIAÇÃO BRASILEIRA DE NORMAS TÉCNICAS. NBR 13818: placas cerâmicas 

para revestimentos: especificação e métodos de ensaio. Rio de Janeiro, 1997. 78 p. 

ASSOCIAÇÃO PAULISTA DAS CERÂMICAS DE REVESTIMENTOS – ASPACER. 

Disponível em: http://www.aspacer.com.br. Acesso em: 10 fev. 2012. 

AZEVEDO, F. M.; BRASIL, L.; OLIVEIRA, R. C. L. Redes neurais com aplicações em 

controle e em sistemas especialistas. Florianópolis: Bookstore, 2000. 

BRAGA, R. L.; CARVALHO, A. P. L.; LURDEMIR, T. B. Redes neurais artificiais: teoria e 

aplicações. 2. ed. Rio de Janeiro: Livros Técnicos, 2007. 

CAMPANHA, J. R. Determinação dos parâmetros de ordem de redes neurais pelo método 

dos cumulantes. 1994. 129p. Tese (Doutorado em Engenharia Elétrica) – Faculdade de 

Engenharia Elétrica e de Computação, Universidade Estadual de Campinas, Campinas, 1994. 

CHRISTOFOLETTI, S. R. Estudo mineralógico, químico e textural das rochas sedimentares 

da Formação Corumbataí “Jazida Cruzeiro”, e suas implicações nos produtos e processos 

cerâmicos. 1999. 120p. Tese (Doutorado em Geociências) – Instituto de Geociências e 

Ciências Exatas, Universidade Estadual Paulista, Rio Claro, 1999. 

CHRISTOFOLETTI, S. R.; MORENO, M. M. T. Características das rochas da Formação 

Corumbataí utilizadas na indústria de revestimento cerâmico. Geociências, São Paulo, v. 23, 

n. 1/2, p.79-88, 2004. 

CHRISTOFOLETTI, S. R.; THOMAZELLA, H. R.; MORENO, M. M. T.; MASSON, M. R. 

Utilização da análise estatística multivariada no tratamento de dados aplicados às matérias-

primas cerâmicas. Revista do Instituto Geológico, São Paulo, v. 26, n. 1/2, p. 19-29, 2005. 



124 
 

CINTRA, E. C. Aplicação de redes neurais no controle de teores de cobre e ouro no depósito 

de Chapada (GO). 2003. 170f. Tese (Doutorado em Geologia) – Instituto de Geociências e 

Ciências Exatas, Universidade Estadual Paulista, Rio Claro, 2003. 

CORREIA, S. L.; TOMAZI, F. C.; FOLGUERAS, M. V. Influência da composição 

mineralógica e temperatura de queima nas propriedades tecnológicas de massas cerâmicas. In: 

CONGRESSO BRASILEIRO DE CERÂMICA, 51., 2007, Salvador. Anais ..., Salvador, 

2007, p. 12. 

DAMÁSIO, A. R. O erro de Descarte: emoção, razão e o cérebro humano. Traduzido por 

Dora Vicente e Georgina Segurado. Revisão Técnica de Carmem S. da Costa e Ana Maria 

Barbosa. São Paulo: Companhia das Letras, 1996. 

FERRO, L. Modelo do bulbo olfativo baseado em redes neurais recorrentes. 2007. 97f. 

Dissertação (Mestrado em Física Aplicada) – Instituto de Geociências e Ciências Exatas, 

Universidade Estadual Paulista, Rio Claro, 2007. 

ECCLES, J. C. The physiology of nerve cells. John Hopkins University Press, [S.l.], 1957 

GAIDZINSKI, R. Fatores envolvidos no sazonamento e suas implicações nas propriedades 

de argilas para a Indústria Cerâmica. 2006. 179p. Tese (Doutorado em Ciências, em 

Engenharia Metalúrgica e de Materiais) – Engenharia Metalúrgica e de Materiais, COPPE, 

Universidade Federal do Rio de Janeiro, Rio de Janeiro, 2006. 

GROSSBERG, S. Adaptive pattern classification and universal recording. Biological 

Cybernetics, [S.l.], v. 23, n. 1, p. 121-134, 1976. 

GROSSBERG, S. How does the brain build a cognitive code? Psychological Review, [S.l.], 

v.89, n. 1, p. 529-572, 1980. 

HAYKIN, S. Redes neurais: princípios e prática. Tradução de Paulo Martins Engel. 2. ed. 

Porto Alegre: Bookman, 2001. 

HEBB, D. The organization of behavior: a neurophysiological theory. New York: John Wiley 

&amp;amp; Sons, 1949. 



125 
 

HOPFIELD, J. J. Neural networks and physical systems with emergent collective 

computational abilities. Proceedings of the National Academy of Sciences, [S.l.], v. 79, p. 

2554-2558, 1982. 

KOHONEN, T. Clustering, taxonomy and topological maps of pattern: pattern recognition. 

[S.l.], [s.n.], 1982. 

KOVÁCS, Z. L. Redes neurais artificiais: fundamentos e aplicações: um texto básico. 3. ed. 

São Paulo: Livraria da Física, 2002. 

LE CUN, Y. A learning procedure for asymmetric threshold network. Proceedings of 

Cognitiva, [S.l.], n. 85, p. 599-604, 1985. 

McCULLOCH, W.; PITTS, W. A logical calculus of the ideas immanent in nervous activity. 

Bulletin of Mathematical Biophysics, [S.l.], v. 5, n.1, p. 115-133, 1943. 

MINSKY, M.; PAPERT, S. Perceptrons: an introduction to computational geometry. 

Cambridge: The MIT Press, 1969. 

MORENO, M. M. T.; BARTOLOMEU, D.; LIMA, R. H. C. Análise do comportamento de 

queima de argilas e formulações para revestimento cerâmico. Cerâmica, [S.l.], n.55, p.286-

295, 2009. 

NUSSENZVEIG, H. M. (Org.). Complexidade e caos. Rio de Janeiro: UFRJ/COPEA, 1999. 

PARKER, D. Learning logic. Invention report, Stanford University, File 1, Office 

Technology Licensing, 1982. 

POLETTO, E. R. A inovação tecnológica e a utilização de tecnologias ambientais como fator 

de diminuição de impactos ambientais na indústria cerâmica: o caso do APL de pisos e 

revestimentos cerâmicos de Santa Gertrudes (SP). Geografia, Londrina, v. 16, n. 2, p. 25-47, 

2007. 

POLETTO, E. R. O desenvolvimento territorial e a utilização dos recursos naturais: a 

mineração de argila no APL de pisos e revestimentos cerâmicos de Santa Gertrudes (SP). In: 

FÓRUM AMBIENTAL DA ALTA PAULISTA, 2007, [S.l.]. Anais..., v. III, 2007b, p. 19. 



126 
 

PRADO, A. C. A. Placas cerâmicas para revestimento de baixa absorção de água e 

estabilidade dimensional confeccionadas por moagem a seco usando o material da Formação 

Corumbataí. 2007. 203f. Tese (Doutorado em Geologia Regional) – Instituto de Geociências 

e Ciências Exatas, Universidade Estadual Paulista, Rio Claro, 2007. 

PRADO, A. C. A.; ZANARDO, A.; MORENO, M. M. T.; MENEGAZZO, A. P. M. Redução 

da susceptibilidade à deformação piroplástica das argilas do Pólo Cerâmico de Santa 

Gertrudes através da adição de matérias-primas. Cerâmica, [S.l.], n. 54, p. 7-20, 2008. 

ROCHA, R. R. Propriedades químico-mineralógicas de rochas da Formação Corumbataí: 

aplicação na classificação de produtos. 2012. 203p. Tese (Doutorado em Geologia Regional) 

– Instituto de Geociências e Ciências Exatas, Universidade Estadual Paulista, Rio Claro, 

2012. 

ROSENBLATT, F. The perceptron: a probabilistic model for information storage and 

organization in the brain. Psychological Review, [S.l.], v. 65, n. 1, p. 386-408, 1958. 

RUMELHART, D. E.; HINTON, G. E.; McCLELAND, J. L. A general framework for 

parallel distributed processing. Cambridge: The MIT Press, 1986. 

VON DER MARLSBURG, C. Self-organizing of orientation sensitive cells in the striated 

cortex. Kibernetic, [S.l.], v. 14, n. 1, p. 66-82, 1973. 

WASSERMAN, P. D. Neural computing: theory and practice. New York: Van Nostrand 

Reinhold, 1989. 

WERBOS, P. Beyond regression: new tools for prediction and analysis in the behavioral 

sciences. 1974. Ph. D. Thesis. Harvard University, Cambridge, 1974. 

WIDROW, B.; HOFF, M. Adaptive switching circuits. In: IRE WESCON CONVENTION 

RECORD, 1960, New York, Neurocomputing, p. 96-107. 

 

 

 



127 
 

BIBLIOGRAFIA COMPLEMENTAR 

 

AIHARA, K.; TAKABE, T.; TOYODA, M. Chaotic neural networks. Phys. Lett. A, n. 144, p. 

333-340, [S.l.], 1990. 

BAIRD, B.; HIRSH, M. W.; ECKMAN, F. A neural network associative memory for 

handwritten character recognition using multiple Chua attractors. IEEE Transactions on 

Circuits and Systems II: analog &amp;amp; digital signal processing, 40, p. 667-674, 1993. 

BARROS DE ANDRADE, L. Mapeamento do potencial mineral para níquel e ouro no 

Cinturão Metassedimentar Nova Brasilândia – Rondônia por meio de lógica nebulosa (fuzzy) 

e redes neurais artificiais. 2008. Dissertação (Mestrado em geociências) – Instituto de 

Geociências, Universidade Estadual de Campinas, Campinas, 2008. 

BIONDI NETO, L.; SIEIRA, A. C. C.; DANZIGER, B. R.; DA SILVA, J. G. S. Neuro - 

CPT: classificação de solos usando redes neurais artificiais. Engevista, Rio de Janeiro, v. 8, n. 

1, p.37-48, 2006. 

BISHOP, C. M. Neural networks for pattern recognition. New York: Oxford University Press 

Inc., 1995. 

BRASIL. Ministério das Minas e Energia. Secretaria de Geologia, Mineração e 

Transformação Mineral. Anuário estatístico 2009. Disponível em: http://www.mme.gov.br. 

Acesso em: 17 jan. 2010. 

BURDEN, R. L.; FAIRES, J. D. Análise numérica. Tradução de All Taskes. Revisão Técnica 

de Helena Castro. São Paulo: Cengage Learning, 2008. 

CABRAL JÚNIOR, M.; DEL MONTE, E.; MOTTA, J. F. M.; SINTONI, A.; SUSLICK, S.  

Arranjos produtivos minero-cerâmicos e o desenvolvimento econômico: caso do APL de 

Socorro – SP. Cerâmica Industrial, [S.l.], n.11(2), p.1-6, 2006. 

CAMPANHA, J. R.; TANCREDO, A. Um modelo físico para redes neurais. Cad. Cat. Ens. 

Fis., Rio Claro, v. 8, n.1. p.56-63, 1991. 



128 
 

CESSAC, B. Increase in complexity in random neural networks. J. Phys. I France 5, [S.l.], p. 

409-432, 1995. 

CHAPMAN, S. J. Programação em MATLAB® para engenheiros. Tradução de Flávio Soares 

Correa da Silva. São Paulo: Pioneira Thomson Learning. 2003. 

CHUTCHFIELD, J. et al. Power spectral analysis of a dynamical system. Physics Letters, 

[S.l.], v. 76A, n. 1, p. 1-4, 1980. 

CORREIA, S. L.; HOTZA, D.; SEGADÃES, A. M. Otimização da resistência mecânica de 

corpos cerâmicos em função das matérias-primas e restrições de propriedades tecnológicas. 

Cerâmica, [S.l.], n. 51, p.230-238, 2005. 

CRISANTI, A.; SOMPOLINSKY, H. Dynamics of spins systems with randomly asymmetric 

bonds ising spins and Glauber dynamics. Physical Review A, [S.l.], v. 37, n. 12, p. 4865-4874, 

1988. 

EKELAND, I. O cálculo e o imprevisto. Tradução de Maria Clara Constantino. Revisão 

Técnica de Hugo Vicente Capelato. São Paulo: Martins Fontes, 1987. 

GORINI, A. P. F.; CORREA, A. R. Cerâmica para revestimento. Cerâmica, Rio de Janeiro, n. 

10, p. 201-252, 1999. 

GROSSBERG, S. A theory of visual coding, memory and development: formal theories of 

visual perception. New York: John Wiley &amp;amp; Sons, 1978. 

GUTFREUND, H.; REGER, J. D.; YOUNG, A. P. The nature of attractors in an asymmetric 

spin glass with deterministic dynamics. J. Phys. A. Math. Gen., [S.l.], n. 1, p. 2775-2797, 

1988. 

HANSELMAN, D.; LITTLEFIELD, B. Matlab® 6: curso completo. Tradução de Cláudia 

Sant’Ana Martins. Revisão Técnica de Alberto Saa, Francisco A. M. Gomes e M. Aparecida 

Diniz Ehrhardt. São Paulo: Prentice Hall, 2003. 

HIRSCH, M. W.; BAIRD, B. Computing with dynamic attractors in neural networks. 

Biosystems, [S.l.], n. 34, p. 173-195, 1995. 



129 
 

IFEACHOR, E. C.; JERVIS, B. W. Digital signal processing: a practical approach. New 

York: Wesley Publishing Company, 1993. 

KLOTZ, A.; BRÄUER, K. A small-size neural networks and fuzzy logic: basic concepts and 

applications. The Institute of Electrical and Eletronics Engineers (IEEE) Press, Inc., New 

York, 1996. 

KOHONEN, T. Self organization and associative memory. 1st. ed. Berlin: Springer-Verlag, 

1984. 

LANDIM, P. M. B. O grupo Passa Dois na Bacia do Rio Corumbataí. São Paulo: DNPM – 

DGM, 1970. Boletim 252. 

LAU, C. (Ed.). Neural networks: theoretical foundations and analysis. New York: The 

Institute of Electrical and Eletronics Engineers (IEEE) Press, Inc., 1992. 

LI, Z.; DAYAN, P. Computational differences between asymmetrical and symmetrical 

networks. Network: Comput. Neural Syst., [S.l.], n.10, p. 59-77, 1999. 

MASSON, M. R. Caracterização de jazidas usando a garantia de qualidade de matérias-

primas para indústria cerâmica de revestimento. 2002. 267f. Tese (Doutorado em Geologia) 

– Instituto de Geociências e Ciências Exatas, Universidade Estadual Paulista, Rio Claro, 

2002. 

MIRZAI, A. R. (Org.). Artificial intelligence: concepts and applications in engineering. 

Cambridge: The MIT Press, [19-]. 

MORETTIN, P. A. Análise harmônica de processos estocásticos. In: COLÓQUIO 

BRASILEIRO DE MATEMÁTICA, 12., 1979, Poços de Caldas. Anais..., Poços de Caldas, 

1979. 

MOTTA, J. F. M. As matérias-primas e o estudo de três casos de rochas fundentes. 2000. 

208f. Tese (Doutorado em Geologia) – Instituto de Geociências e Ciências Exatas, 

Universidade Estadual Paulista, Rio Claro, 2000. 

MOTTA, J. F. M. et al. Características do pólo de revestimentos cerâmicos de Santa 

Gertrudes – SP: com ênfase na produção de argilas. Cerâmica Industrial, São Paulo, v. 9, n.1, 

p. 7-13, 2004. 



130 
 

NOSE FILHO, K.; LOTUFO, A. D. P.; LOPES, M. L. M. Utilização de redes neurais 

artificiais e redes neuro fuzzy para previsão de cargas elétricas. In: BRAZILIAN 

CONFERENCE ON DYNAMICS, CONTROL AND APPLICATIONS, 7., 2008, Presidente 

Prudente. Anais…, Presidente Prudente, 2008, p.6. 

OLIVEIRA, M. C.; MAGANHA, M. F. B. Guia técnico ambiental da indústria cerâmica 

branca e de revestimento. São Paulo: CETESB, série P + L, 2006, p.90. 

OSLER, T. J. A quick look at Liapunov space. New Jersey Mathematics and Computer 

Education, New Jersey, v. 28, n. 2, p. 183-197, [19-]. 

PANDORFI, H.; SILVA, I. J. O.; SARNIGHAUSEN, V. C. R.; VIEIRA, F. M. C.; 

NASCIMENTO, S. T.; GUISELINI, C. Uso de redes neurais artificiais para predição de 

índices zootécnicos nas fases de gestação e maternidade na suinocultura. Revista Brasileira de 

Zootecnia, [S.l.], v. 40, n. 3, p. 676-681, 2011. 

PARK, J.; SANDBERG, W. Universal approximation using radial basis functions. Neural 

Computation, [S.l.], [s.n.], n. 3, 1991. 

POGGIO, T.; GIROSI, F. Networks for approximation and learning. Proceedings of the 

Electrical and Eletronics Engineers, Detroit, v. 78, p. 1481-1497, 1990. 

POGGIO, T.; GIROSI, F. Regularization algorithms for learning that are equivalent to 

multilayer networks. Science, [S.l.], v, 247, n. 1, p. 978-982, [19-]. 

PORWAL, A.; CARRANZA, E. J. M.; HALE, M. Artificial neural networks for mineral-

potential mapping: a case study from Aravalli Province, Western India. Natural Resources 

Research, [S.l.], 12(3), p. 155-171, 2003. 

RENALS, S.; ROHWER, R. A study of network dynamics. Journal of Statistical Physics, 

[S.l.], v. 58, n. 5/6, 1990. 

RUMELHART, D. E.; HINTON, G. E.; WILLIANS, R. J. Learning internal representations 

by error propagation. Parallel Distributed Processing, Cambridge, MIT Press, v. 1, p. 319-

362, 1986b. 



131 
 

SHAFFER, W. M. Can nonlinear dynamics elucidate mechanisms in ecology and 

epidemiology? IMA Journal of Mathematics Applied in Medicine and Biology, [S.l.], n. 2, p. 

221-252, 1985. 

SHERRINGTON, D. Magnets, microchips and memories: from spin glasses to the brain, 

[S.l.], p.319-330, [19-]. 

VIEIRA, V. M. Redes neurais artificiais: uma aplicação em petrofísica e estudo dos efeitos 

de estímulos persistentes. 2007. 73p. Dissertação (Mestrado em Física) – Instituto de Física, 

Universidade Federal de Alagoas, Maceió, 2007. 

WANDRESEN, R.; MITISHITA, E. A.; DE ANDRADE, J. B. Identificação de pontos de 

apoio pré-sinalizados com o uso de redes neurais artificiais e correlação. Bol. Ciênc. Geod., 

sec. Artigos, Curitiba, v. 9, n. 2, p. 179-198, 2003. 

WANG, X. Chaos-based learning. Complex Systems, [S.l.], n . 5, p. 359-370, 1991. 

WIDROW, B. Generalization and information storage in networks of adaline neurons. Self-

organizing Systems. Washington, Spartan Books, p. 435-461, 1962. 

 

 

 

 

 

 

 

 

 

 

 

 

 

 



132 
 

 

 

 

 

 

 

 

 

 

 

 

 

ANEXOS 



133 
 

Anexo A - Variáveis físicas obtidas na temperatura de queima de 1000°C 
 

AMOSTRA RLQ (%) PF (%)  Abs (%)  CR(N) MRF (MPa) 
CF1 B1 5.501 3.505 5.724 662.3 55.5 
CF1 B2 5.504 4.945 6.625 537.8 47.0 
CF1 B3 5.517 5.229 6.449 226.5 20.1 
CF2 B2 5.473 5.682 8.583 619.3 49.8 
CR1 B3 2.720 2.575 11.032 464.7 36.5 
CR2 B1 3.838 2.605 8.955 520.1 41.7 
CR2 B2 4.085 2.666 8.810 596.7 48.2 
CR2 B3 3.152 3.259 10.964 438.6 35.3 
PG B1 3.545 2.093 10.179 575.3 42.8 
PG B2 3.093 1.972 10.432 498.1 37.8 
PG B3 2.232 1.988 13.460 478.4 34.3 
PG B4 4.159 5.055 11.884 561.4 45.0 
PT1 B1 2.613 2.768 11.295 336.3 25.5 
PT1 B2 4.483 3.260 8.337 587.0 48.5 
PT1 B3 3.909 2.970 8.932 548.8 43.6 
PT1 B4 2.966 2.975 10.656 468.2 36.9 
PT2 B1 2.170 2.449 12.417 288.5 20.7 
PT2 B2 3.459 2.565 10.135 506.3 39.4 
PT2 B3 3.976 2.933 9.804 278.3 21.4 
PT3 B1 2.452 5.051 18.466 306.6 20.4 
PI B1 2.012 1.997 14.113 343.7 24.0 
PI B2 2.929 2.775 10.007 409.5 31.6 
PI B3 5.335 4.001 6.934 499.0 42.7 
PI B4 3.750 4.735 11.938 608.7 47.5 

      
Amplitude 3.505 3.710 12.742 435.8 35.4 

Média 3.7030 3.3355 10.2555 473.34 37.34 
Desvio 
Padrão 

1.13556 1.16162 2.77118 121.216 10.600 

 
 
 



134 
 

Anexo B – Variáveis físicas obtidas na temperatura de queima de 1020°C 
 

AMOSTRA DP 
(g/cm 3 ) 

DAS 
(g/cm 3 ) 

RLS 
(%) 

RLQ 
(%) 

PF (%) Abs 
(%) 

CR (N) MRF 
(MPa) 

CF1 B1 2.058 1.985 0.381 7.121 3.506 3.202 677.4 61.3 
CF1 B2 2.006 1.863 0.668 6.722 5.261 4.843 673.3 59.7 
CF1 B3 1.973 1.848 0.794 6.630 5.577 5.567 589.1 52.5 
CF2 B1 2.008 1.867 0.500 2.246 7.598 13.863 428.0 33.6 
CF2 B2 2.013 1.846 0.851 6.854 5.650 5.423 766.8 69.8 
CR1 B3 2.076 1.887 0.094 5.579 2.752 5.572 602.4 51.2 
CR1 B4 2.119 1.935 0.136 1.398 3.993 12.527 445.9 34.8 
CR2 B1 2.071 1.893 0.172 5.900 2.965 4.669 588.1 49.4 
CR2 B2 2.088 1.897 0.279 6.316 3.153 4.269 643.7 57.6 
CR2 B3 2.073 1.897 0.207 5.031 3.644 7.218 566.0 47.3 
PG B1 2.022 1.845 0.173 6.298 2.393 4.905 695.6 58.3 
PG B2 2.044 1.853 0.159 5.942 2.138 5.183 681.1 56.7 
PG B3 1.928 1.751 0.118 5.451 2.346 9.251 642.1 46.1 
PG B4 1.991 1.823 0.344 5.674 5.642 9.374 593.3 50.3 
PT1 B2 2.017 1.864 0.381 6.913 3.060 3.757 684.3 60.2 
PT1 B3 2.081 1.888 0.299 6.331 3.001 3.998 646.7 57.9 

PT1 B4  2.052 1.893 0.359 5.257 3.124 6.283 612.2 52.0  

PT2 B2 2.045 1.864 0.146  5.994 2.625 4.665 661.7 59.0 
PT2 B3 2.005 1.850 0.275 6.192 2.952 4.972 606.8 50.9 
PI B1 1.963 1.800 0.056 4.130 2.318 9.486 515.2 42.9 
PI B2 2.050 1.873 0.239 4.332 3.265 7.044 384.6 29.9 
PI B3 2.003 1.871 0.548 6.988 4.256 3.897 715.3 64.1 
PI B4 1.999 1.850 0.317 5.027 5.407 9.154 768.9 65.3 
TU B2 2.051 1.906 0.118 1.724 5.742 15.129 478.2 35.6 
TU B3 1.984 1.816 0.082 0.806 6.041 18.718 328.1 22.9 

         
Amplitude 0.191 0.234 0.795 6.315 5.460 15.516 440.8 46.9 

Média 2.0288 1.8666 0.3078 5.2342 3.9364 7.3188 599.79 50.77 
Desvio 
Padrão 

0.04264 0.04504 0.21738 1.82845 1.48738 4.00806 113.806 11.872

 
 
 



135 
 

Anexo C – Variáveis físicas obtidas na temperatura de queima de 1040°C 
 

AMOSTRA DP 
(g/cm 3 ) 

DAS 
(g/cm 3 ) 

RLS 
(%) 

RLQ 
(%)  

PF (%) Abs 
(%) 

CR (N) MRF 
(MPa) 

CF1 B1 2.063 1.896 0.357 7.969 3.480 1.421 727.2 67.0 
CF1 B2 2.018 1.873 0.674 7.497 5.227 3.147 766.4 69.3 
CF1 B3 1.981 1.857 0.825 7.451 5.626 4.014 609.7 56.1 
CF2 B1  1.985 1.849 0.501 3.511 7.558 12.135 478.6 37.4 
CF2 B2 2.008 1.852 0.928 8.184 6.338 3.629 763.0 69.5 
CR1 B3 2.075 1.885 0.098 7.104 2.735 2.437 738.9 64.2 
CR1 B4 2.114 1.930 0.138 2.974 4.045 9.689 575.4 47.1 
CR1 B5 2.064 1.853 0.288 0.225 3.253 16.795 134.3 9.8 
CR2 B1 2.071 1.892 0.176 6.755 3.094 2.354 651.3 54.4 
CR2 B2 2.083 1.892 0.341 7.491 3.052 1.798 743.2 67.4 
CR2 B3 2.076 1.898 0.209 6.689 3.682 4.067 629.1 56.4 
PG B1 2.019 1.839 0.114 8.195 2.454 1.758 751.0 66.3 
PG B2 2.030 1.854 0.136 7.601 2.170 2.134 792.5 70.3 
PG B3 1.951 1.781 0.094 7.974 2.247 4.057 771.5 63.3 
PG B4 2.006 1.813 0.360 6.916 5.280 7.096 669.4 59.7 
PT1 B3 2.073 1.892 0.297 7.743 3.166 1.435 739.4 66.4 
PT1 B4 2.046 1.887 0.349 6.969 3.263 2.794 725.4 64.6 
PT2 B3 2.006 1.848 0.267 7.743 2.980 2.252 635.1 57.3 
PT3 B1 1.899 1.745 0.056 4.196 5.238 14.211 641.7 48.8 
PI B1 1.967 1.800 0.062 5.934 2.386 6.039 576.3 45.5 
PI B3 2.003 1.870 0.514 8.078 4.455 1.959 752.8 68.2 
PI B4  1.998 1.843 0.317 6.109 5.091 6.761 765.8 67.4 
TU B2  2.051 1.875 0.098 3.487 5.839 12.025 558.8 43.5 
TU B3 1.992 1.818 0.126 1.901 6.194 16.765 380.2 27.3 

         
Amplitude 0.215 0.185 0.872 7.970 5.388 15.374 658.2 60.5 

Média 2.0241 1.8559 0.3052 6.1957 4.1189 5.8655 649.04 56.13 
Desvio 
Padrão 

0.04958 0.04189 0.23681 2.23705 1.51295 4.99129 150.641 15.067

 
 
 



136 
 

Anexo D – Variáveis físicas obtidas na temperatura de queima de 1060°C. 
 

AMOSTRA  DP 
(g/cm 3 ) 

DAS 
(g/cm 3  

RLS 
(%) 

RLQ 
(%)  

PF (%) Abs 
(%) 

CR (N) MRF 
(MPa) 

CF1 B1 2.065 1.910 0.419 8.167 3.733 0.202 760.9 70.6 
CF1 B2 2.029 1.881 0.658 8.231 5.334 1.641 738.7 69.0 
CF1 B3 2.018 1.879 0.753 8.047 5.366 2.001 643.7 60.8 
CF2 B1 2.018 1.879 0.508 5.024 7.720 7.531 556.4 46.5 
CF2 B2 2.013 1.861 0.886 8.885 6.179 2.195 775.8 65.8 
CR1 B3 2.082 1.890 0.086 7.202 2.776 0.360 745.6 64.2 
CR1 B4 2.126 1.944 0.170 6.131 4.093 2.689 764.3 69.3 
CR1 B5 2.041 1.851 0.193 0.742 3.096 15.557 172.5 12.5 
CR2 B2 2.086 1.896 0.287 7.115 3.115 0.210 744.1 54.8 
CR2 B3 2.083 1.909 0.219 7.619 3.942 1.811 706.0 65.8 
PG B1 2.022 1.848 0.148 8.675 2.582 0.154 786.3 69.3 
PG B2 2.040 1.855 0.146 7.785 2.261 0.138 926.6 82.2 
PG B3 1.960 1.786 0.122 9.527 2.162 0.229 929.3 79.6 
PG B4 2.005 1.836 0.342 8.043 5.605 4.475 723.0 66.8 
PT1 B4 2.047 1.895 0.359 7.556 3.287 0.581 818.3 73.0 
PT2 B3 2.006 1.853 0.283 8.275 3.033 0.741 720.4 65.3 
PT3 B1 1.940 1.747 0.038 5.172 5.289 12.070 677.7 53.1 
PI B1 1.965 1.802 0.042 7.012 2.505 3.976 598.1 49.3 
PI B3 2.009 1.865 0.482 8.585 4.213 0.927 739.4 68.7 
PI B4 2.007 1.851 0.328 7.087 5.507 4.890 785.4 69.0 
TU B1 2.032 1.860 0.068 0.106 10.146 21.738 247.6 17.0 
TU B2 2.047 1.873 0.110 6.162 5.790 5.814 724.5 60.2 
TU B3 1.984 1.819 0.082 2.941 6.161 14.410 456.5 33.6 

         
Amplitude 0.186 0.197 0.848 9.421 7.984 21.600 756.8 69.7 

Média 2.0272 1.8604 0.2926 6.6995 4.5172 4.5365 684.4 59.41 
Desvio 
Padrão 

0.04362 0.04316 0.23398 2.45970 1.94883 5.91185 181.683 17.703

 
 

 



137 
 

Anexo E - Variáveis físicas obtidas na temperatura de queima de 1000°C. (Minas CF, CR, PG, PT e PI) 
 
AMOSTRA RLQ (%) PF (%)  Abs (%) CR (N) MRF (MPa) 

CF1 B1 5.501 3.505 5.724 662.3 55.5 

CF1 B2 5.504 4.945 6.625 537.8 47.0 

CF1 B3 5.517 5.229 6.449 226.5 20.1 

CF2 B2 5.473 5.682 8.583 619.3 49.8 

CR1 B3 2.720 2.575 11.032 464.7 36.5 

CR2 B2 4.085 2.666 8.810 596.7 48.2 

CR2 B3 3.152 3.259 10.964 438.6 35.3 

PG B1 3.545 2.093 10.179 575.3 42.8 

PG B2 3.093 1.972 10.432 498.1 37.8 

PG B3 2.232 1.988 13.460 478.4 34.3 

PG B4 4.159 5.055 11.884 561.4 45.0 

PT1 B4 2.966 2.975 10.656 468.2 36.9 

PT2 B2 3.459 2.565 10.135 506.3 39.4 

PI B1 2.012 1.997 14.113 343.7 24.0 

PI B3 5.335 4.001 6.934 499.0 42.7 

PI B4 3.750 4.735 11.938 608.7 47.5 

      

Amplitude 3.505 3.710 8.389 435.8 35.4 

Média 3.9064 3.4526 9.8699 505.31 40.18 

Desvio 

Padrão 

1.22611 0.49096 2.49924 108.912 9.245 

 

 

 



138 
 

Anexo F – Variáveis físicas obtidas na temperatura de queima de 1020°C. (Minas CF, CR, PG, PT e PI) 
 

AMOSTRA RLQ (%) PF (%) Abs (%) CR (N) MRF (MPa) 

CF1 B1 7.121 3.506 3.202 677.4 61.3 

CF1 B2 6.722 5.261 4.843 673.3 59.7 

CF1 B3 6.630 5.577 5.567 589.1 52.5 

CF2 B2 6.854 5.650 5.423 766.8 69.8 

CR1 B3 5.579 2.752 5.572 602.4 51.2 

CR2 B2 6.316 3.153 4.269 643.7 57.6 

CR2 B3 5.031 3.644 7.218 566.0 47.3 

PG B1 6.298 2.393 4.905 695.6 58.3 

PG B2 5.942 2.138 5.183 681.1 56.7 

PG B3 5.451 2.346 9.251 642.1 46.1 

PG B4 5.674 5.642 9.374 593.3 50.3 

PT1 B4 5.257 3.124 6.283 612.2 52.0 

PT2 B3 6.192 2.952 4.972 606.8 50.9 

PI B1 4.130 2.318 9.486 515.2 42.9 

PI B3 6.988 4.256 3.897 715.3 64.1 

PI B4 5.027 5.407 9.154 768.9 65.3 

      

Amplitude 2.991 3.512 6.284 253.7 26.9 

Média 5.9508 3.7574 6.1624 646.83 55.38 

Desvio 

Padrão 

0.84024 1.33480 2.07216 70.255 7.490 

 

 

 



139 
 

Anexo G – Variáveis físicas obtidas na temperatura de queima de 1040°C. (Minas CF, CR, PG, PT e PI) 
 

AMOSTRA RLQ (%) PF (%) Abs (%) CR (N) MRF (MPa) 

CF1 B1 7.969 3.480 1.421 727.2 67.0 

CF1 B2 7.497 5.227 3.147 766.4 69.3 

CF1 B3 7.451 5.626 4.014 609.7 56.1 

CF2 B2 8.184 6.338 3.629 763.0 69.5 

CR1 B3 7.104 2.735 2.437 738.9 64.2 

CR2 B2 7.491 3.052 1.798 743.2 67.4 

CR2 B3 6.689 3.682 4.067 629.1 56.4 

PG B1 8.195 2.454 1.758 751.0 66.3 

PG B2 7.601 2.170 2.134 792.5 70.3 

PG B3 7.974 2.247 4.057 771.5 63.3 

PG B4 6.916 5.280 7.096 669.4 59.7 

PT1 B4 6.969 3.263 2.794 725.4 64.6 

PT2 B3 7.743 2.980 2.252 635.1 57.3 

PI B1 5.934 2.386 6.039 576.3 45.5 

PI B2 8.078 4.455 1.959 752.8 68.2 

PI B4 6.109 5.091 6.761 765.8 67.4 

      

Amplitude 2.261 4.168 5.675 216.2 24.0 

Média 7.3690 3.7791 3.4602 713.58 63.28 

Desvio 

Padrão 

0.70007 1.35970 1.80192 66.937 6.691 

 

 

 



140 
 

Anexo H – Variáveis físicas obtidas na temperatura de queima de 1060°C. (Minas CF, CR, PG, PT e PI) 
 

AMOSTRA RLQ (%) PF (%) Abs (%) CR (N) MRF (MPa) 

CF1 B1 8.167 3.733 0.202 760.9 70.6 

CF1 B2 8.231 5.334 1.641 738.7 69.0 

CF1 B3 8.047 5.366 2.001 643.7 60.8 

CF2 B2 8.885 6.179 2.195 775.8 65.8 

CR1 B3 7.202 2.776 0.360 745.6 64.2 

CR2 B2 7.115 3.115 0.210 744.1 54.8 

CR2 B3 7.619 3.942 1.811 706.0 65.8 

PG B1 8.675 2.582 0.154 786.3 69.3 

PG B2 7.785 2.261 0.138 926.6 82.2 

PG B3 9.527 2.162 0.229 929.3 79.6 

PG B4 8.043 5.605 4.475 723.0 66.8 

PT1 B4 7.556 3.287  0.581 818.3 73.0 

PT2 B3 8.275 3.033 0.741 720.4 65.3 

PI B1 7.012 2.505 3.976 598.1 49.3 

PI B3 8.585 4.213 0.927 739.4 68.7 

PI B4 7.087 5.507 4.890 785.4 69.0 

      

Amplitude 2.515 4.017 4.752 331.2 32.9 

Média 7.9882 3.8500 1.5332 758.85 67.14 

Desvio 

Padrão 

0.71600 1.35089 1.61216 84.975 8.037 

 

 



141 
 

Anexo I – Variáveis físicas obtidas na temperatura de queima de 1020°C. (Minas CF, CR, PG, PT, PI e 
TU) 

 

AMOSTRA DP 

(g/cm 3 ) 

DAS 

(g/cm 3 ) 

RLS 

(%) 

RLQ 

(%) 

PF (%) ABS 

(%) 

CR (N) MRF 

(MPa) 

CF1 B1 2.058 1.985 0.381 7.121 3.506 3.202 677.4 61.3 

CF1 B2 2.006 1.863 0.668 6.722 5.261 4.843 673.3 59.7 

CF1 B3 1.973 1.848 0.794 6.630 5.577 5.567 589.1 52.5 

CF2 B1 2.008 1.867 0.500 2.246 7.598 13.863 428.0 33.6 

CF2 B2 2.013 1.846 0.851 6.854 5.650 5.423 766.8 69.8 

CR1 B3 2.076 1.887 0.094 5.579 2.752 5.572 602.4 51.2 

CR1 B4 2.119 1.935 0.136 1.398 3.993 12.527 445.9 34.8 

CR2 B2 2.088 1.897 0.279 6.316 3.153 4.269 643.7 57.6 

CR2 B3 2.073 1.897 0.207 5.031 3.644 7.218 566.0 47.3 

PG B1 2.022 1.845 0.173 6.298 2.393 4.905 695.6 58.3 

PG B2 2.044 1.853 0.159 5.942 2.138 5.183 681.1 56.7 

PG B3 1.928 1.751 0.118 5.451 2.346 9.251 642.1 46.1 

PG B4 1.991 1.823 0.344 5.674 5.642 9.374 593.3 50.3 

PT1 B4 2.052 1.893 0.359 5.257 3.124 6.283 612.2 52.0 

PT2 B3 2.005 1.850 0.275 6.192 2.952 4.972 606.8 50.9 

PI B1 1.963 1.800 0.056 4.130 2.318 9.486 515.2 42.9 

PI B3 2.003 1.871 0.548 6.988 4.256 3.897 715.3 64.1 

PI B4 1.999 1.850 0.317 5.027 5.407 9.154 768.9 65.3 

TU B2 2.051 1.906 0.118 1.724 5.742 15.129 478.2 35.6 

TU B3 1.984 1.816 0.082 0.806 6.041 18.718 328.1 22.9 

         

Amplitude 0.191 0.234 0.795 6.315 5.460 15.516 440.8 46.9 

Média 2.0228 1.8642 0.3230 5.0693 4.1747 7.9418 601.47 50.65 

Desvio 

Padrão 

0.04681 0.04992 0.23783 1.97134 1.57618 4.22980 114.976 12.003

 

 
 



142 
 

Anexo J – Variáveis físicas obtidas na temperatura de queima de 1040°C. (Minas CF, CR, PG, PT, PI e 
TU) 

 

AMOSTRA  DP 
(g/cm 3 ) 

DAS 
(g/cm 3 ) 

RLS 
(%) 

RLQ 
(%) 

PF (%) Abs 
(%) 

CR (N) MRF 
(MPa) 

CF1 B1 2.063 1.896 0.357 7.969 3.480 1.421 727.2 67.0 
CF1 B2 2.018 1.873 0.674 7.497 5.227 3.147 766.4 69.3 
CF1 B3 1.981 1.857 0.825 7.451 5.626 4.014 609.7 56.1 
CF2 B1 1.985 1.849 0.501 3.511 7.558 12.135 478.6 37.4 
CF2 B2 2.008 1.852 0.928 8.184 6.338 3.629 763.0 69.5 
CR1 B3 2.075 1.885 0.098 7.104 2.735 2.437 738.9 64.2 
CR1 B4 2.114 1.930 0.138 2.974 4.045 9.689 575.4 47.1 
CR2 B2 2.083 1.892 0.341 7.491 3.052 1.798 743.2 67.4 
CR2 B3 2.076 1.898 0.209 6.689 3.682 4.067 629.1 56.4 
PG B1 2.019 1.839 0.114 8.195 2.454 1.758 751.0 66.3 
PG B2 2.030 1.854 0.136 7.601 2.170 2.134 798.5 70.3 
PG B3 1.951 1.781 0.094 7.974 2.247 4.057 771.5 63.3 
PG B4 2.006 1.813 0.360 6.916 5.280  7.096 669.4 59.7 
PT1 B4 2.046 1.887 0.349 6.969 3.263 2.794 725.4 64.6 
PT2 B3 2.006 1.848 0.267 7.743 2.980 2.252 635.1  57.3 
PI B1 1.967 1.800 0.062 5.934 2.386 6.039 576.3 45.5 
PI B3 2.003 1.870 0.514 8.078 4.455 1.959 752.8 68.2 
PI B4 1.998 1.843 0.317 6.109 5.091 6.761 765.8 67.4 
TU B2 2.051 1.875 0.098 3.487 5.839 12.025 558.8 43.5 
TU B3 1.992 1.818 0.126 1.901 6.194 16.765 380.2 27.3 

         
Amplitude 0.163 0.149 0.866 6.294 5.388 15.344 418.3 43.0 

Média 2.0236 1.8580 0.3254 6.4889 4.2051 5.2989 670.82 58.39 
Desvio 
Padrão 

0.04274 0.03646 0.25157 1.93347 1.83622 4.26439 110.377 11.944

 
 
 
 

 
 



143 
 

Anexo K – Variáveis físicas obtidas na temperatura de queima de 1060°C. (Minas CF, CR, PG, PT, PI e 
TU) 

 

AMOSTRA DP 

(g/cm 3 ) 

DAS 

(g/cm 3 ) 

RLS 

(%) 

RLQ 

(%)  

PF (%) Abs 

(%) 

CR (N) MRF 

(MPa) 

CF1 B1 2.065 1.910 0.419 8.167 3.733 0.202 760.9 70.6 

CF1 B2 2.029 1.881 0.658 8.231 5.334 1.641 738.7 69.0 

CF1 B3 2.018 1.879 0.753 8.047 5.366 2.001 643.7 60.8 

CF2 B1 2.018 1.879 0.508 5.024 7.720 7.531 556.4 46.5 

CF2 B2 2.013 1.861 0.886 8.885 6.179 2.195 775.8 65.8 

CR1 B3 2.082 1.890 0.086 7.202 2.776 0.360 745.6 64.2 

CR1 B4 2.126 1.944 0.170 6.131 4.093 2.689 764.3 69.3 

CR2 B2 2.086 1.896 0.287 7.115 3.115 0.210 744.1 54.8 

CR2 B3 2.083 1.909 0.219 7.619 3.942 1.811 706.0 65.8 

PG B1 2.022 1.848 0.148 8.675 2.582 0.154 786.3 69.3 

PG B2 2.040 1.855 0.146 7.785 2.261 0.138 926.6 82.2 

PG B3 1.960 1.786 0.122 9.527 2.162 0.229 929.3 79.6 

PG B4 2.005 1.836 0.342 8.043 5.605 4.475 723.0 66.8 

PT1 B4 2.047 1.895 0.359 7.556 3.287 0.581 818.3 73.0 

PT2 B3 2.006 1.853 0.283 8.275 3.033 0.741 720.4 65.3 

PI B1 1.965 1.802 0.042 7.012 2.505 3.976 598.1 49.3 

PI B3 2.009 1.865 0.482 8.585 4.213 0.927 739.4 68.7 

PI B4 2.007 1.851 0.328 7.087 5.507 4.890 785.4 69.0 

TU B2 2.047 1.873 0.110 6.162 5.790 5.814 724.5 60.2 

TU B3 1.984 1.819 0.082 2.941 6.161 14.410 456.5 33.6 

         

Amplitude 0.166 0.158 0.844 6.586 5.558 14.272 472.8 48.6 

Média 2.0306 1.8666 0.3215 7.4035 4.2682 2.7488 732.17 64.19 

Desvio 

Padrão 

0.04219 0.03788 0.23648 1.48494 1.58971 3.48596 109.468 7.995 

 

 

 

 


	CAPA
	FICHA CATALOGRÁFICA
	COMISSÃO EXAMINADORA
	DEDICATÓRIA
	AGRADECIMENTOS
	RESUMO
	ABSTRACT
	LISTA DE FIGURAS
	LISTA DE TABELAS
	LISTA DE ANEXOS
	LISTA DE ABREVIATURAS E SIGLAS
	SUMÁRIO
	1. INTRODUÇÃO
	1.1. Objetivo Geral e Hipótese
	1.2. Estrutura

	2. REVISÃO BIBLIOGRÁFICA
	2.1. O Neurônio Biológico Natural
	2.2. As Redes Neurais Artificiais
	2.3. Breve Histórico sobre as Redes Neurais Artificiais
	2.4. Regra Delta e Regra Delta Generalizada

	3. MATERIAIS E MÉTODOS
	3.1. O Pólo Cerâmico de Santa Gertrudes
	3.2. Caracterização das Argilas e das Cerâmicas
	3.3. Os Corpos-de-Prova
	3.4. Determinação das Variáveis Físicas dos Corpos-de-Prova
	3.5. Metodologia

	4. RESULTADOS E DISCUSSÕES
	4.1. Introdução
	4.2. Regra de Aprendizado Backpropagation (Formato Básico)
	4.3. Regra de Aprendizado Backpropagation com Momento (traingdm)
	4.4. Regra de Aprendizado Backpropagation Resiliente (trainrp)
	4.5. Regra de Aprendizado Backpropagation de Levenberg-Marquadt (trainlm)
	4.6. Conclusões

	5. CONSIDERAÇÕES FINAIS
	REFERÊNCIA BIBLIOGRÁFICA
	BIBLIOGRAFIA COMPLEMENTAR
	ANEXOS



</field>
	</doc>
</add>