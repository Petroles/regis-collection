<?xml version="1.0" encoding="utf-8"?>
<add>
	<doc>
		<field name="docid">BR-TU.07116</field>
		<field name="filename">11756_Mesquita_MarcosEduardoRibeirodoValle_M.pdf</field>
		<field name="filetype">PDF</field>
		<field name="text">Um Estudo Comparativo em Memórias Associativas com Enfase em Memorias Associativas Morfologicas
Autor: Marcos Eduardo Ribeiro do Valle Mesquita
Orientador: Prof. Dr. Peter Sussner
Campinas, SP
Agosto/2005
Este exemplar corresponde à redação final da dissertação devidamente corrigida e defendida por Marcos Eduardo Ribeiro do Valle Mesquita e aprovada pela comissao comissao julgadora.
Campinas, 24 de Agosto de 2005
Prof. Dr. Peter Sussner
Banca Examinadora
1.	Alvaro Rodolfo De Pierro, Dr. ....
2.	Emanuel Pimentel Barbosa, Dr. ...
3.	Fernando Gomide, Dr. ......
4.	Peter Sussner, Dr. ........
.... DMA/IMECC/Unicamp
....DE/IMECC/Unicamp
.... DCA/FEEC/Unicamp
.... DMA/IMECC/Unicamp
Dissertacao apresentada ao Instituto de Matematica, Estatística e Computacao Científica, UNICAMP, como requisito parcial para obtencao do Título de Mestre em Matematica Aplicada.
FICHA CATALOGRÁFICA ELABORADA PELA BIBLIOTECA DO IMECC DA UNICAMP
Bibliotecário: Maria Júlia Milani Rodrigues - CRB8a / 2116
Mesquita, Marcos Eduardo Ribeiro do Valle
M562e Um estudo comparativo em memórias associativas com ênfase em memórias associativas morfológicas / Marcos Eduardo Ribeiro do Valle -- Campinas, [S.P. :s.n.], 2005.
Orientador : Peter Sussner
Dissertação (mestrado) - Universidade Estadual de Campinas, Instituto de Matemática, Estatística e Computação Científica.
1. Redes neurais (Computação). 2. Morfologia matemática. 3. Sistemas de memória de computadores. I. Sussner, Peter. II. Universidade Estadual de Campinas. Instituto de Matemática, Estatística e Computação Científica. III. Título.
Título em inglês: A comparative study on associative memories with emphasis on morphological associative memories
Palavras-chave em inglês (Keywords): 1. Neural networks (Computer science). 2. Mathematical morphology. 3. Computer memory systems.
Área de concentração: Matemática Aplicada
Titulação: Mestre em Matemática Aplicada
Banca examinadora: Prof. Dr. Peter Sussner (IMECC-UNICAMP)
Prof. Dr. Álvaro Rodolfo de Pierro (IMECC-UNICAMP)
Prof. Dr. Emanuel Pimentel Barbosa (IMECC-UNICAMP)
Prof. Dr. Fernando Antônio Campos Gomide (FEEC-UNICAMP)
Data da defesa: 24/08/2005
Dissertação de Mestrado defendida em 24 de agosto de 2005 e aprovada
Pela Banca Examinadora composta pelos Profs. Drs.
ProE (a). Dr (a). PETER SUSSNER
Prof. (a). Dr (a). FERNANDO ANTONIO CAMPOS GOMIDE
Resumo
Memorias associativas neurais são modelos do fenómeno biológico que permite o armazenamento de padrões e a recordacao destes após a apresentacao de uma versao ruidosa ou incompleta de um padrao armazenado. Existem varios modelos de memórias associativas neurais na literatura, entretanto, existem poucos trabalhos comparando as varias propostas. Nesta dissertacao comparamos sistematicamente o desempenho dos modelos mais influentes de memórias associativas neurais encontrados na literatura. Esta comparacao esta baseada nos seguintes criterios: capacidade de armazenamento, distribuicao da informacao nos pesos sinapticos, raio da bacia de atracao, memórias espurias e esforço computacional. Especial enfase e dado para as memórias associativas morfológicas cuja fundamentacao matematica encontra-se na morfologia matematica e na algebra de imagens.
Palavras-chave: Redes Neurais, Morfologia Matematica, Sistemas de Memória de Computadores.
Abstract
Associative neural memories are models of biological phenomena that allow for the storage of pattern associations and the retrieval of the desired output pattern upon presentation of a possibly noisy or incomplete version of an input pattern. There are several models of neural associative memories in the literature, however, there are few works relating them. In this thesis, we present a systematic comparison of the performances of some of the most widely known models of neural associative memories. This comparison is based on the following criteria: storage capacity, distribution of the information over the synaptic weights, basin of attraction, number of spurious memories, and computational effort. The thesis places a special emphasis on morphological associative memories whose mathematical foundations lie in mathematical morphology and image algebra.
Keywords: Neural Networks, Mathematical Morphology, Computer Memory Systems.
A Margarida Ribeiro do Valle e Mercedes Mesquita Silva
Agradecimentos
Aos meus pais, Antonio Marcos de Mesquita Silva e Ana LUcia Ribeiro do Valle Silva, pela vida e educação que me deram.
A minha irma, Ana Elisa do Valle Mesquita, e a minha noiva, Luciana Maria Ricci, pelo apoio durante esta jornada.
Ao meu orientador, Peter Sussner, sou grato pela orientacao.
A minha família e a todos os meus colegas da Unicamp e Sao Joao da Boa Vista. Em particular para Marcio, Rangel, Gustavo, Roberto, Jorge, Renata, Andre, David, Ederson, Mateus, Homero, Denilson e Angela. As boas amizades permanecem!
A todos os professores que contribuíram para a minha formacao na Unicamp.
A FAPESP, pelo apoio financeiro.
Sumário
1	Introdução	1
1.1	Contexto Histórico............................................................. 2
1.1.1	Redes Neurais Artificias................................................ 2
1.1.2	Memórias Associativas Neurais........................................... 4
1.1.3	Morfologia Matematica e Algebra de Imagens ............................. 5
1.2	Objetivos e Organizacao da	Dissertacao......................................... 6
2	Conceitos Bãsicos de Redes Neurais	9
2.1	Introducao .................................................................... 9
2.2	Modelos Neurais................................................................ 9
2.2.1	Modelo Neural Ciassico................................................. 10
2.2.2	Modelo Neural Morfológico.............................................. 11
2.3	Arquiteturas de Redes Neurais................................................. 12
2.4	Aprendizagem ................................................................. 15
2.4.1	Aprendizado Supervisionado............................................. 15
3	Conceitos Bãsicos de Memorias Associativas Neurais	17
3.1	Formulacao Matematica, Armazenamento e Associacao............................. 17
3.2	Classificacao das Memórias Associativas Neurais............................... 18
3.3	Características para um Bom Desempenho ....................................... 20
4	Memorias Associativas Lineares	21
4.1	Armazenamento por Correlacao.................................................. 21
4.1.1	Armazenamento por Correlacao Auto-associativo Bipolar.................. 23
4.2	Armazenamento por Projecao ................................................... 24
5	Memorias Associativas Dinamicas	29
5.1	Memória Associativa de Hopfield Discreta ..................................... 29
5.1.1	Arquitetura da Rede.................................................... 29
5.1.2	Aprendizado ........................................................... 30
5.1.3	Convergencia........................................................... 30
5.2	Memória Associativa Bidirecional.............................................. 33
5.2.1	Arquitetura ........................................................... 33
5.2.2	Aprendizado ........................................................... 33
5.2.3	Convergencia............................................................ 34
5.3	Memoria Associativa de Personnaz............................................... 38
5.3.1	Arquitetura da Rede..................................................... 38
5.3.2	Aprendizado ............................................................ 38
5.3.3	Convergencia............................................................ 39
5.4	Memória Associativa de Kanter-Sompolinsky...................................... 39
5.4.1	Arquitetura da Rede..................................................... 40
5.4.2	Aprendizado............................................................. 40
5.4.3	Convergencia............................................................ 40
5.5	Memoria Associativa Bidirecional Assimetrica .................................. 40
5.5.1	Arquitetura ............................................................ 41
5.5.2	Aprendizado ............................................................ 41
5.6	Memoria Associativa com Capacidade Exponencial................................. 42
5.6.1	Arquitetura............................................................. 42
5.6.2	Aprendizado ............................................................ 43
5.6.3	Convergencia............................................................ 44
5.7	Memoria Associativa Bidirecional com Capacidade Exponencial.................... 44
5.7.1	Arquitetura............................................................. 44
5.7.2	Aprendizado ............................................................ 46
5.7.3	Convergencia............................................................ 46
5.8	Modelo do Estado Cerebral numa Caixa (BSB) .................................... 46
5.8.1	Arquitetura ............................................................ 46
5.8.2	Aprendizado ............................................................ 47
5.8.3	Convergencia............................................................ 47
6	Memorias Associativas Morfológicas	49
6.1	Memorias Associativas Morfologicas Heteroassociativas.......................... 49
6.1.1	Aprendizado ............................................................ 50
6.2	Memorias Auto-Associativas Morfologicas ....................................... 58
6.3	Memorias Auto-associativas Morfologicas Binarias............................... 61
6.4	Memorias Associativas Morfologicas de Duas Camadas ............................ 63
6.4.1	Arquitetura............................................................. 63
6.4.2	Aprendizado ............................................................ 65
7	Desempenho das Memorias Associativas Binarias	67
7.1	Capacidade de Armazenamento ................................................... 67
7.1.1	Memoria Associativa de Hopfield......................................... 69
7.1.2	Memoria Associativa Bidirecional........................................ 70
7.1.3	Memoria Associativa de Personnaz ....................................... 70
7.1.4	Memoria Associativa de Kanter-Sompolinsky............................... 71
7.1.5	Memoria Associativa Bidirecional Assimetrica............................ 71
7.1.6	Memoria Associativa com Capacidade Exponencial.......................... 71
7.1.7	Memoria Associativa Bidirecional com Capacidade Exponencial............. 71
7.1.8	Memorias Associativa Morfologicas....................................... 72
7.1.9	Memória Associativa Morfológica de Duas Camadas...................... 72
7.2	Distribuicao da Informacao ................................................ 72
7.3	Raio de Atracao............................................................ 73
7.4	Memórias EspUrias.......................................................... 77
7.5	Esforço Computacional...................................................... 78
7.5.1	NUmero de Operacoes na Fase de Armazenamento......................... 80
7.5.2	NUmero de Operacoes por Iteracao na Fase de Recordacao............... 81
7.5.3	NUmero de Iteracoes na	Fase de Recordacao ........................... 82
8	Conclusão	85
Referencias bibliográficas	88
Capítulo 1
Introdução
A primeira pergunta que surge em nossa mente e, o que e uma memoria? A resposta nao e simples e nao e nosso objetivo discutir este assunto, mas podemos dizer que uma memoria e, simplificada-mente, um sistema que possui tres funcóes ou etapas: 1 - Registro, processo pelo qual armazenamos informacao; 2 - Preservação, para garantir que a informacao esta intacta; 3 - Recordacão, processo pelo qual uma informacao e recuperada [12].
Existem varios tipos de memória. Por exemplo, quando escrevemos o numero de um telefone num papel, estamos usando o papel como memoria. Depois poderemos ler e usar este nilmero.
Sempre que registramos informacoes na memória, precisamos de uma chave ou algo que permita recuperar o conteúdo armazenado. Por exemplo, quando deixamos uma bolsa num guarda-volumes, pegamos um ticket indicando o compartimento onde ela ficara. Aqui, o ticket nao possui nenhuma relacao com o conteiúdo da bolsa e podemos dizer que o endereco (ticket) e apenas um símbolo abstrato da memória onde a entidade (bolsa) esta, e mais, este endereco nao possui nenhuma relacao com o conteudo armazenado. Este tipo de memória e muito usado nos computadores digitais (memória RAM ou ROM). Em muitos casos este tipo de memória apresenta-se eficiente, entretanto possui uma serie de limitacóes. Por exemplo, o que acontecera se perdermos o ticket?
Suponha que perdemos o pequeno ticket. Teremos que usar um procedimento diferente para recuperar nossa bolsa. Evidentemente procuraremos o responsavel e passaremos informacoes parciais, mas suficientes, sobre a bolsa e seu conteudo. E comum encontrarmos bolsas semelhantes, mas o conteudo geralmente e diferente e uma descricao parcial dele e suficiente para que o responsavel possa identifica-la, e provavelmente concordara em devolve-la. Neste exemplo, podemos dizer que o endereco (descricao parcial do que tem na bolsa) e igual ao conteudo e dizemos que esta e uma memória auto-associativa (tambem conhecida por memória enderecada por conteudo), um caso particular das memórias associativas.
Uma memória associativa poderia recuperar um item da memória a partir de informacoes parciais. Por exemplo, se um item armazenado na memória e “J.J. Hopfield &amp;amp; D.W. Tank, Biological Cybernetics 52, 141-152 (1985)”. A entrada “&amp;amp; Tank (1985)”poderia ser suficiente para recuperarmos a informacao completa. Alem disso, uma memória associativa ideal poderia trabalhar com ruídos (ou erros) e recuperar esta referencia mesmo a partir de entradas incorretas como “&amp;amp; Rank, (1985)”. Nos computadores digitais, apenas formas relativamente simples de memória associativa tem sido implementadas em hardware. A maioria dos recursos para tolerancia a ruído no acesso da informacao sao implementados via software [40]. Esta e uma das razões para o estudo das memórias associativas.
As memórias associativas encontram aplicações em vários ramos da ciência. Por exemplo, Zhang et. al. utilizaram um modelo de memoria associativa para reconhecimento e classificacao de padroes [111, 112]. A metodologia para classificacao de padroes baseada em memorias associativas tambem foi aplicada em problemas de deteccao de falha em motores [54], seguranca de rede [110] e aprendizado de linguagem natural [29]. Hopfield mostrou que seu modelo de memoria associativa pode ser usado para resolver problemas de otimizacao, como por exemplo, o problema do caixeiro viajante [38]. As memorias associativas morfologicas discutidas nesta dissertacao foram aplicadas em problemas de localizacao de faces, auto-localizacao e analise de imagens hiperespectrais [68, 26]. Recentemente, Valle e Sussner apresentaram uma aplicacao das memorias associativas nebulosas em um modelo de previsao [103, 98, 102].
O termo memoria associativa veio da psicologia e nao da engenharia. Veio da psicologia porque o cerebro humano pode ser visto como uma memoria associativa. Ele associa o item a ser lembrado com um fragmento da recordacao. Por exemplo, ouvindo um trecho de uma musica podemos lembrar da cancao inteira, ou sentido um certo perfume podemos associar o cheiro a uma pessoa especial. Nao so o cerebro humano, mas moscas de fruta ou lesmas de jardim tambem possuem memorias associativas. Na verdade, qualquer sistema nervoso relativamente simples apresenta uma memoria associativa [12]. Isso sugere que a habilidade de criar associacoes e natural - praticamente espontanea - em qualquer sistema neural. Portanto, uma fonte de inspiracoes para os estudos das memorias associativas encontra-se nos estudos do funcionamento de um sistema nervoso, em particular, do cerebro humano.
O cerebro humano e composto por bilhoes de neurônios interligados formando uma rede. Dizemos que o cerebro e uma rede neural biologica. Em nossos estudos, apresentaremos modelos que descrevem um neurônio e chamaremos este modelo de neurônio artificial. Chamaremos de rede neural artificial, ou simplesmente rede neural, uma rede formada por neurônios artificiais [33]. A teoria das redes neurais e vasta e possui aplicacoes em varias areas, como por exemplo, no reconhecimento de padroes, controle, otimizacao e previsao de mercados financeiros [8, 45, 61]. Neste trabalho voltaremos nossa atencao para a intersecao das redes neurais com as memorias associativas. Especificamente, estudaremos as memorias associativas neurais.
Neste trabalho tambem discutiremos as memorias associativas morfologicas que sao modelos de memoria associativa onde usamos a morfologia matematica e a algebra de imagens como ferramenta [70, 73, 76]. Este tipo particular de memoria associativa neural sera o enfoque principal do nosso trabalho.
Antes de introduzirmos os conceitos sobre redes neurais e memorias associativas, apresentaremos brevemente a historia das redes neurais, memorias associativas, morfologia matematica e da algebra de imagens.
1.1	Contexto Histórico
1.1.1	Redes Neurais Artificias
Podemos dizer que os estudos das redes neurais artificiais iniciaram em 1943 quando o biólogo Warren McCulloch e o matematico Walter Pitts apresentaram um modelo matematico de um neurônio biológico [55]. Eles assumiram que um neurônio seguia uma lei “tudo ou nada” e acreditavam que
com um número suficiente de neurônios com conexões sinápticas apropriadas operando de forma síncrona (paralelamente), seria possível, a princípio, a computacao de qualquer funcao booleana com-putavel. Este foi um resultado muito significativo e com ele e aceito o surgimento das disciplinas de redes neurais e inteligencia artificial [33].
Em 1949, o neurofisiologista Donald Hebb publicou o livro “The Organization of Behavior”[34]. Neste livro foi apresentada pela primeira vez a formulacao de uma regra de aprendizagem. Hebb notou que as conexões sinapticas do cerebro sao continuamente modificadas conforme um organismo aprende novas tarefas, criando assim agrupamentos neurais. Ele propôs no seu livro o seguinte postulado de aprendizagem: “A eficiencia de uma sinapse e aumentada pela interacao entre dois neurônios atraves da sinapse”. Este postulado forma a base do que chamamos hoje de aprendizagem (ou regra) de Hebb. Outras regras de aprendizado foram apresentadas posteriormente e muitas sao generalizares da regra de Hebb [2, 32].
Cerca de 15 anos após a publicacao do classico artigo de McCulloch e Pitts, uma nova abordagem para o problema de reconhecimento de padroes foi introduzida por Rosenblatt [82] em seu trabalho sobre o Perceptron, um metodo inovador de aprendizagem supervisionada. O coroamento deste trabalho encontra-se no teorema da convergencia do perceptron, cuja primeira demonstracao foi delineada por Rosenblatt em 1960. Este teorema garante que o perceptron sempre converge para os pesos corretos se os pesos sinapticos que resolvem o problema existirem. Na mesma epoca, Wi-drow e Hoff introduziram o algoritmo do mínimo quadrado médio (LMS, Least Mean-Square) e usaram-no para formular o Adaline (Adaptive Linear Element, Elemento Linear Adaptativo) [107]. A diferenca fundamental entre o perceptron e o Adaline esta no procedimento de aprendizagem. Infelizmente existem limites fundamentais para aquilo que o perceptron de camada unica e o Adaline podem calcular [58]. Rosenblatt e Widrow estavam cientes destas limitacoes e apresentaram redes neurais de miultiplas camadas que poderiam superar tais limitacoes, mas nao conseguiram estender seus algoritmos de aprendizado para estas redes mais complexas. Uma destas redes e o Madaline (Multiple-Adaline), proposta por Widrow e seus estudantes, e uma das primeiras redes neurais em camadas de treinamento com m u ltiplos elementos adaptativos. Tais dificuldades e a falta de recursos tecnologicos nos anos 60 proporcionou uma adormecimento nas redes neurais e poucos pesquisadores como James Anderson, Shunichi Amari, Leon Cooper, Kunihiko Fukushima, Stephen Grossberg e Teuvo Kohonen permaneceram no ramo.
Nos anos 80, a ausencia de recursos tecnologicos foi superada e a pesquisa em redes neurais aumentou drasticamente. Computadores pessoais e estacoes de trabalho, que cresciam em capacidade, tornaram-se vitais para o desenvolvimento da pesquisa em redes neurais artificiais. Aleím disso, novos conceitos foram introduzidos. Dois deles tiveram grande influencia no meio científico. O primeiro foi o uso da mecanica estatística para explicar as operacoes e convergencia de algumas redes neurais recorrentes. Este conceito foi introduzido pelo físico John Hopfield em 1982 [40]. A segunda chave para o desenvolvimento na decada de 80 foi o algoritmo de retropropagacao (back-propagation), usado para treinar o perceptrons de miultiplas camadas. Este algoritmo foi descoberto por pesquisadores diferentes em diferentes pontos do mundo. Bryson talvez tenha sido o primeiro a estudar o algoritmo de retropropagacao [13, 83]. Entretanto, a publicacao mais influente foi o livro em dois volumes “Parallel Distributed Processing: Explorations in the Microstrutures of Cognition”, editado por Rumelhart e McClelland. Este livro exerceu uma grande influencia na utilizacao da aprendizagem por retropropagacao, que emergiu como o algoritmo de aprendizagem mais popular para o treinamento de perceptrons de miultiplas camadas devido a sua simplicidade computacional e eficiencia.
A historia das redes neurais segue com muitos capítulos desafiantes e interessantes, mas ficaremos por aqui, pois acreditamos que voce, leitor, ja esta bem situado no contexto histórico das redes neurais artificiais1. Agora voltaremos aos anos 50 e falaremos sobre as memórias associativas neurais.
1.1.2	Memórias Associativas Neurais
Os estudos sobre memória associativa iniciaram nos anos 50 por Taylor [100]. Em 1961, Steinbruch introduziu o conceito de matriz de aprendizagem [88]. Nos anos seguintes surgiram as memórias associativas holograficas [17, 22, 105, 106] que nao serâo discutidas neste trabalho. Em 1972, Anderson [4], Kohonen [47] e Nakano [60] introduziram, de maneira independente, a ideia de uma memória por matriz de correlacao, baseada na regra de aprendizagem por produto externo que pode ser interpretada como uma generalizacao do postulado de aprendizagem de Hebb. Nestes artigos, os autores apresentaram um modelo linear para os neurônios formando a memória associativa linear que estudaremos no capítulo 4.
Em 1977 foi publicado o livro “Associative Memory - A System Theoric Approach” de Kohonen [48]. Este foi um dos primeiros livros sobre memórias associativas e apresenta uma analise detalhada das memórias associativas lineares. No mesmo ano, Anderson et al. introduziram o modelo do estado cerebral numa caixa (BSB) [6, 5], uma das primeiras redes neurais que pode ser vista como uma memória auto-associativa dinamica. Entretanto, o comportamento deste modelo como uma memória associativa só foi amplamente estudado após a publicacao dos artigos de Hopfield. Alem de ter aplicacoes como uma memória associativa, a BSB tambem pode ser vista como um modelo cognitivo [5].
Em 1982, Hopfield publicou o classico artigo “Neural Networks and Physical Systems with Emergent Collective Computational Abilities” [40] onde introduz a famosa rede (ou memoria associativa) de Hopfield discreta. Neste artigo, Hopfield sugere que um sistema dinamico pode representar uma memória associativa dinamica onde cada estado estavel do sistema seria um padrâo memorizado. Para compreender a computacao executada e mostrar a convergencia da rede, Hopfield introduziu o conceito de funcao energia (funcao de Lyapunov) para uma rede neural, um conceito que foi posteriormente usado para a analise de varias outras memórias associativas dinamicas [23, 24, 41, 42]. Hopfield tambem apresentou alguns resultados sobre a capacidade de armazenamento da memória auto-associativa de Hopfield. Dois anos depois, Hopfield publicou um novo artigo apresentando a rede (ou memoria auto-associativa) de Hopfield contínua, uma extensao do modelo apresentado anteriormente que utiliza uma funcao sigmóide como funcao de ativacao [37]. Nos anos seguintes, Hopfield e Tank apresentaram aplicacoes da rede de Hopfield em problemas de otimizacao, como o classico problema do caixeiro viajante [38, 39, 99].
Na decada de 80 e início da decada de 90, foram apresentadas variacoes do modelo de Hopfield. Entre elas, podemos citar a memoria associativa de Hopfield com armazenamento por projecao. Este modelo foi inicialmente discutido por Personnaz et. al em 1985 [67], e posteriormente por Kanter e Sompolinsky em 1987 [44]. Em 1991, Chiueh e Goodman apresentaram a memoria associativa de capacidade exponencial (ECAM), um modelo que abrange o modelo de Hopfield com armazenamento por correlacao [15]. Em 1987, surgiram modelos de memorias associativas dinamicas para heteroassociacao [51, 62]. Entre estes modelos encontramos a memoria associativa bidirecional
1Para o leitor interessado em maiores detalhes da historia das redes neurais, recomendamos os livros: [7, 28, 33]
(BAM), introduzida por Kosko, que pode ser vista como uma uma generalização da memória associativa de Hopfield [51, 52].
Na metade dos anos 90 iniciaram-se os estudos sobre as memórias associativas morfológicas (MAM), que usam a morfologia matematica e a algebra de imagens como ferramenta matematica para descrever o neurônio. Antes de entrar nos detalhes do contexto histórico das memórias associativas morfológicas, vamos voltar novamente no tempo e apresentar brevemente a história da morfologia matematica e da algebra de imagens, que sao as ferramentas matematicas usadas nestas memórias associativas.
1.1.3	Morfologia Matemática e Algebra de Imagens
A morfologia matematica surgiu em 1964 enquanto Matheron e Serra estudavam a geometria de meios porosos e analise de textura. Esta nova ferramenta matematica esta baseada nos trabalhos de Minkowski e Hadwiger sobre teoria de medida geometrica e geometria integral [57,27]. Os primeiros anos, de 1964 a 1968, foram dedicados ao desenvolvimento de um corpo de notacoes teóricas e de um protótipo para a analise de textura. Deste período podemos citar o artigo “Elements pour une theorie des milieux poreux”, de Matheron, onde aparece a primeira transformacao morfológica para investigar a geometria dos objetos de uma imagem. Podemos citar tambem o trabalho em hardware especializado: “Texture Analyser” de J. Serra e J.-C. Kein. Neste período tambem foi criado o “Centre de Morphologie Mathematique” no campus da Escola de Minas de Paris, em Fontainebleu, Franca. Varios pesquisadores juntaram-se a este grupo e formaram o que chamamos hoje de “Escola de Fontainebleu”. Entre eles, podemos citar: Klein, Lantuejoul, Meyer e Beucher [85, 87].
No ano 1975, Matheron publicou o livro “Random Sets and Integral Geometry”, que contem as primeiras fundamentacóes teóricas da morfologia matematica. Este livro foi bem aceito pelos interessados em geometria estocastica, mas infelizmente levou alguns anos para ser aceito pela comunidade de processamento de imagens. Podemos dizer que a morfologia matematica só passou a fazer parte das ferramentas para processamento de imagens após a publicacao do classico livro “Image Analysis and Mathematical Morphology” de Jean Serra, publicado em 1982. Segundo Heijmans, este livro pode ser visto como o primeiro tratamento sistematico da morfologia matematica como uma ferramenta para a analise de imagens [35]. Uma breve revisao sobre a morfologia matematica em tons de cinza, incluindo abordagens usando a teoria dos conjuntos nebulosos, pode ser encontrada em [96].
Na decada seguinte houve uma explosao no mímero de artigos e pesquisadores trabalhando com a morfologia matematica e seria impossível listar todos eles. No Brasil encontramos os rnlcleos de pesquisa liderados por Barrera, na Universidade de Sao Paulo, e Banon, no Instituto Nacional de Pesquisas Espaciais em Sao Jose do Campos. Uma exposicao detalhada contendo os trabalhos mais influentes da morfologia matematica encontra-se em [84]. Esta explosao nao atingiu apenas a morfologia matemaítica, mas todas as atividades envolvendo processamento de imagens e resultou num excesso de tecnicas, notacoes e operacóes para o processamento de imagens. Surgiu entao a necessidade de uma estrutura algebrica padrâo, eficiente e com um certo rigor matematico, designado especificamente para o processamento de imagens. Em resposta a esta situacao, pesquisadores da Universidade da Flórida desenvolveram uma estrutura matematica para analise e processamento de imagens conhecida como Álgebra de Imagens (Image Algebra). Muitas sao as vantagens da algebra de imagens, mas nao vamos lista-las aqui. Vamos dizer apenas que ela abrange todas as areas de processamento de imagens e visao computacional usando uma linguagem comum. Para o leitor inte-
ressado, recomendamos [69, 79, 81].
A algebra de imagens nasceu na metade da decada de 80 e os primeiros artigos apareceram em 1987 [72, 78, 80]. Em 1990, Ritter,Wilson e Davidson publicaram o artigo: “Image Algebra: An Overview”[81]. Este artigo descreve as estruturas algebricas da algebra de imagens e teve um papel importante na divulgacao desta teoria matematica. Nos anos seguintes, a algebra de imagens comecou a ganhar aplicacóes. As redes neurais e as operacoes da morfologia matematica foram descritas e estudadas usando elementos e operacoes da algebra de imagens [20, 71, 69]. Sendo descritas pela mesma estrutura matematica, foi facil combina-las, criando as redes neurais morfológicas, antes conhecidas como redes da algebra de imagens (IA networks).
Uma rede neural morfológica e uma rede neural onde os neurônios sao descritos pelas operacoes da morfologia matematica. Estas operacoes podem ser formuladas usando a algebra de imagens, mas esta nao e a ilnica formulacao matematica de uma rede neural morfológica. Uma formulacao diferente, que nao sera discutida aqui, pode ser encontrada em [108]. Entre as redes neurais morfológicas, encontramos as memórias associativas morfológicas (Morphological Associative Memory, MAM). O estudo das memórias associativas morfológicas e recente, a primeira publicacao apareceu em 1996 [73]. Outras publicacóes vieram depois [74, 76, 91, 92, 93, 94, 97], mas ainda sao poucas e com certeza existem muitos modelos e teorias para serem descobertas e estudadas. Uma nova classe de memórias associativas neurais baseadas na teoria dos conjuntos nebulosos conhecida como “Memorias Associativas Nebulosas Implicativas” (Implicative Fuzzy Associative Memories, IFAM) [103, 98, 102]. As memórias associativas nebulosas implicativas generalizam as memórias associativas morfológicas quando a ultima e aplicada a padrões nebulosos. As memórias associativas nebulosas implicativas, por sua vez, podem ser vistas como um caso particular das memórias associativas morfológicas nebulosas que serao discutidas em trabalhos futuros. Neste trabalho nao discutiremos as memórias associativas nebulosas implicativas nem as memórias associativas morfológicas nebulosas.
1.2	Objetivos e Organização da Dissertação
Muitos modelos de memória associativa neural foram apresentados nos ultimos anos, entretanto, nao encontramos na literatura um trabalho reunindo e comparando os modelos mais influentes. Esta dissertacao de mestrado tem como objetivo principal discutir os principais modelos de memória associativa neural, incluindo as memórias associativas morfológicas. Os modelos de memória associativa neural mais influentes sao essencialmente modelos binarios. Por esta razao, esta dissertacao e dedicada principalmente as memórias associativas neurais binarias.
Tambem nao existe na literatura um criterio comum de comparacao para o desempenho de memórias associativas neurais. Esta dissertacao de mestrado tambem tem como objetivo formalizar conceitos usados empiricamente na literatura para a comparacao de modelos de memórias associativas. Formalizados os conceitos, usamos estes para comparar os modelos mais influentes de memória associativa binaria.
Resumindo, os objetivos desta dissertacao de mestrado sao:
1.	Reunir os modelos mais influentes de memória associativa neural (binarias) incluindo as memórias associativas morfológicas.
2.	Formalizar criterios para comparacao do desempenho das memórias associativas neurais binarias.
3.	Comparar os modelos mais influentes com base nos criterios discutidos no item anterior.
Esta dissertacao de mestrado esta dividida em 8 capítulos. Os capítulos 2 e 3 tratam dos conceitos basicos de redes neurais e memórias associativas neurais. Especificamente, no capítulo 2, discutimos o conceito de rede neural artificial e como classificaí-la. Apresentamos alguns modelos neurais e falamos brevemente sobre regras de aprendizado. No capítulo 3, apresentamos uma formulacao matematica para o problema das memórias associativas, como classifica-las e apresentamos uma lista contendo as características desejaveis para o bom desempenho de uma memória associativa neural.
Os capítulos 4, 5, e 6 tem como objetivo apresentar os modelos mais influentes de memória associativa neural incluindo as memórias associativas neurais morfológicas, isto e, estes tres capítulos cobrem o primeiro item dos objetivos desta dissertacao. Precisamente, no capítulo 4, discutimos as memórias associativas lineares. Estes sao os modelos mais simples de memória associativa neural e possuem um papel fundamental: definem as principais regras para armazenamento de padrões numa memória associativa neural. No capítulo 5 examinamos varios modelos de memórias associativas dinamicas para auto e heteroassocicao. Introduzimos neste capítulo a Memória Associativa Bidirecional com Capacidade Exponencial que e uma extensao da Memória Associativa com Capacidade Exponencial para o caso hetero-associativo. No capítulo 6 discutimos as memórias associativas morfológicas.
O capítulo 7 cobre os objetivos listados nos itens 2 e 3 acima. Neste capítulo formalizamos alguns conceitos para comparacao das memórias associativas neurais binarias. Neste capítulo tambem apresentamos resultados teóricos e empíricos para a comparacao dos modelos mais influentes de memória associativa neural. Terminamos a dissertacao no capítulo 8 com a conclusao.
Foram realizados varios experimentos computacionais nesta dissertacao de mestrado. Todos eles foram conduzidos no software MATLAB. As implementacóes dos modelos de memória associativa apresentadas nos capítulos 5 e 6, bem como as rotinas usadas nos experimentos computacionais do capítulo 7, podem ser obtidas com o autor em sua pagina pessoal [104].
As imagens usadas nesta dissertacao de mestrado podem ser encontradas na pagina eletrônica do grupo CVG (Computer Vision Group) [1]. As imagens originais foram convertidas em imagens menores de dimensao 64 x 64 pixels usando o comando imresize do MATLAB com o metodo de interpolacao padrâo. Depois transformamos cada imagem num vetor coluna com 4096 componentes usando o comando reshape do MATLAB. As imagens binarias apresentadas na figuras 1.1 e 1.2 serao usadas com frequencia nos exemplos computacionais com imagens binarias. Estas imagens foram obtidas aplicando um threshold nas imagens com tamanho reduzido, antes de serem transformadas num vetor coluna. Os níveis dos thresholds foram calculados usando o metodo de Otsu que minimiza a variancia entre os valores preto e branco dos pixels [63] e pode ser obtido usando o comando graythresh no MATLAB. Nesta dissertacao, o valor 1 representa o preto (objeto) e o valor 0 representa o branco numa imagem binaria. As imagens apresentadas na figura 1.3 serao usadas nos exemplos computacionais com imagens em tons de cinza.
1
x2
x3
x4
5
x
x
Fig. 1.1: Padrões x1, x2,..., x5 usados nos exemplos computacionais com imagens binárias.
y1
y2
y4
r
y
5
Fig. 1.2: Padrões y1, y2,..., y5 usados nos exemplos computacionais com imagens binárias.
x1
2
3
4
x
x
x
Fig. 1.3: Padroes x1, x2, x3, x4 usados nos exemplos computacionais com imagens em tons de cinza.
Capítulo 2
Conceitos Basicos de Redes Neurais
Neste capítulo discutimos os conceitos básicos de redes neurais artificiais; a notação e a nomenclatura que sera usada durante toda a dissertacao.
2.1	Introdução
Podemos dizer que os computadores modernos sao retardatarios no mundo da computacao, pois os computadores biológicos - o cerebro e o sistema nervoso animal e humano - existem por milhões de anos e sao extremamente eficientes no processamento de informacoes sensoriais e no controle da interacao entre o animal e o meio em que vive. Tarefas como procurar um sanduíche, reconhecer uma face ou relembrar coisas sao operacóes simples, como somas e multiplicacóes, para o nosso cerebro.
O fato dos computadores biológicos serem tao efetivos sugere que possamos extrair características similares a partir de um modelo do sistema neural. O modelo matematico que descreve o sistema nervoso biológico e conhecido como rede neural artificial ou simplesmente rede neural1 [33]. Apesar de nossos modelos serem apenas metaforas do sistema nervoso biológico, eles fornecem um modo elegante e diferente de compreendermos o funcionamento de maquinas computacionais, alem de oferecer informacoes uteis sobre o funcionamento do nosso cerebro.
Uma rede neural e caracterizada por três fatores:
1.	Modelos (ou características) neurais,
2.	Arquitetura (ou topologia) da rede,
3.	Regra de aprendizado.
Nas próximas secóes discutiremos estes fatores.
2.2	Modelos Neurais
Os neurônios, ou celulas nervosas, sao os elementos computacionais usados pelo sistema nervoso. Podemos identificar três elementos basicos no modelo neural:
1 As redes neurais também sao referidas na literatura como neurocomputadores, redes conexionistas ou processadores paralelamente distribuídos.
1.	Um conjunto de sinapses (ou elos de conexeos), cada uma caracterizada por um peso. Específicamente, um sinal Xj, na entrada da sinapse j do neurônio i, interage com o peso sinaptico Wj. Note que o primeiro índice de w refere-se ao neurônio em questao e o segundo refere-se ao terminal de entrada da sinapse.
2.	Uma regra de propagacao, que define as operacoes usadas para processar os sinais de entrada, ponderados pelas respectivas sinapses. Normalmente estas operacoes constituem um combinador linear. As operacoes usadas no modelo ciassico e a multiplicação seguida da soma; entretanto, no modelo morfológico do neurônio, usaremos a soma e uma operacao de maximo ou mínimo.
3.	Uma funcao de ativacao, usada para introduzir nao-linearidade no modelo e/ou restringir a amplitude da saída de um neurônio. Neste trabalho usaremos basicamente quatro tipos de funcoes de ativacao: identidade, funcao linear por partes, funcao sinal ou limiar, e a funcao exponencial. Deixaremos a funcao de ativacao implícita quando usarmos a identidade.
E comum encontramos um bias nos modelos neurais artificiais. O bias pode ser visto como uma sinapse (wi0) conectada a uma entrada constante (x0). Por efeito de simplicidade, nao acrescentaremos o bias nos nossos modelos neurais.
Encontramos varios modelos neurais na literatura [55, 73], como por exemplo os modelos neurais nebulosos [65, 103, 98, 102]. Neste trabalho, discutiremos somente o modelo neural classico e o modelo neural morfológico. Apresentaremos a seguir cada um destes modelos.
2.2.1	Modelo Neural Clássico
O modelo neural clássico foi proposto por McCulloch e Pitts [55]. Neste modelo, um neurônio i é descrito pelo par de equacoes:
n
Vi	XI wijxj, e yi = ^(vi),
j=i
(2.1)
ou pela unica equacao
yi y y wij xj^ ,
(2.2)
onde x1, x2,... ,xn são os sinais de entrada, wn, wi2,..., win são os pesos sinápticos do neurônio i, ^(•) e a funcão de ativacão, vi e a ativacão do neurônio e yi e o sinal de saída. Na figura 2.1 temos uma representacão simbólica de um neurônio clãssico com n entradas.
Um conjunto com m neurônios em paralelo poder ser escrito na forma matricial atraves da equacão
y = 4(W x,	(2.3)
onde x e o vetor coluna contendo os sinais de entrada, W G Rmxn representa a matriz de pesos sinãpticos, e uma funcão vetorial com componentes ^i : R —&gt; R e y e o vetor coluna contendo a saída da rede. Note que cada linha da equacão (2.3) representa um neurônio descrito por (2.2).
^(•)
yi
Fig. 2.1: Representação do modelo matemático clássico de um neurônio artificial.
2.2.2	Modelo Neural Morfológico
O modelo neural morfológico foi proposto por Davidson e Ritter no início dos anos 90 [20,73,74,76]. Neste modelo, a soma e substituída pelo maximo ou pelo mínimo e a multiplicacao e substituída pela soma. Uma motivacao biológica para esta substituicao e fornecida em [77]. Matematicamente, um neurônio morfológico i e descrito pela equacao
n
Vi = (Wil + X1) V (Wi2 + X2) V ... V (Win + Xn) = \/ (™ij + Xj) ,	(2.4)
j=1
ou
n
Vi = (Wi1 + X1) A (Wi2 + X2) A ... A (Win + Xn) = /\ (Wj + Xj) ,	(2.5)
j=1
em conjunto com a equacao
yi = ^(Vi).	(2.6)
Note que a equacao (2.6) e identica a segunda equacao de (2.1). Logo, a diferenca entre o modelo classico e o modelo morfológico esta no calculo da ativacao do neurônio (vi). Neste trabalho usaremos somente a funcao identidade e a funcao limiar como funcao de ativacao no modelo neural morfoloí gico.
O modelo neural descrito pelas equacoes (2.4) e (2.6), e o modelo descrito pelas equacoes (2.5) e
(2.6)	sao conhecidos na literatura como modelo neural morfológico porque (2.4) e (2.5) representam as operacoes basicas da morfologia matematica: dilatacao e erosão, respectivamente [79, 85, 87].
Um conjunto com m neurônios morfológicos em paralelo tambem pode ser escrito na forma matricial de um modo analogo a equacao (2.3) do modelo classico. Para tanto, precisamos definir um produto matricial em termos das operacoes de maximo ou mínimo, e da soma. Para uma matriz A E Rmxp, e uma matriz B E Rpxn, o produto matricial C = A IV B, tambem conhecido como produto maximo de A por B, e definido por
p
cij	\J (aik + bkj) .	(2.7)
k=1
O produto mínimo de A por B e definido de modo semelhante. Especificamente, os elementos de C = A El B sao dados por
p
cij	(aik + bkj) •	(2.8)
k=1
Um conjunto com m neurônios morfologicos, expresso na forma matricial, e dado por:
y = (W E x),	(2.9)
ou
y = (W 0 x) •	(2.10)
As equacoes (2.9) e (2.10) generalizam o par de equacoes (2.4) e (2.6), e o par (2.5) e (2.6), respectivamente.
Os neuronios morfologicos descritos pela equacao (2.9) estao baseado em operacoes da estrutura algebria (R, V, +), conhecida como belt [18,19]. Analogamente, os neuronios morfologicos descritos pela equacao (2.10) estao baseados em operacoes no belt (R, l, +). Os belts (R, V, +) e (R, l, +) podem ser combinados formando a estrutura algebrica de grupo ordenado-reticulado (lattice-ordered group) (R, V, l, +). Existe uma elegante dualidade em (R, V, l, +) obtida a partir da equacao r lu = —((—r) V (—u)), valida para os niímero reais. Dado r G R, definimos o conjugado aditivo r * como r* = —r. Desta forma, (r*)* = r e r l u = (r* V u*)* para todo r, u G R. Se A G Rmxn, entao a matriz conjugada A* de A e a dada por A* = — AT. Segue entao que
A l B = (A* V B*)*,	(2.11)
e
A 0 B = (B* 0 A*)*,	(2.12)
para matrizes de tamanho apropriado. Consequentemente, uma rede neural morfologica formulada usando a operacao 0 pode ser reformulada em termos da operacao l, e vice-versa, usando a relacao expressa na equacao (2.12). Alem disso, toda proposicao em (R, V, l, +) induz uma proposicao dual obtida substituindo o símbolo l por V e vice-versa, e revertendo as desigualdades.
Note que o modelo neural morfologico nao envolve operacoes de multiplicacao, mas apenas operacoes como maximo ou mínimo e soma. Temos entao um modelo neural com computacoes rápidas e de rácil implementacao em hardware. Problemas de convergencia e longos algoritmos de treinamento praticamente nao existem [75]. Alem disso, as redes neurais morfologicas sao capazes de resolver os problemas computacionais convencionais [73].
2.3	Arquiteturas de Redes Neurais
A arquitetura (ou topologia) de uma rede neural consiste na estrutura de interconexao dos seus neuronios que sao geralmente organizados em camadas com um ou varios neuronios. Na literatura encontramos a seguinte classificacao para as camadas de neuronios:
• Camada de Entrada: Camada de neuronios que introduz as entradas externas na rede. Os neuronios desta camada sao chamados de neuronios de entrada.
Camada de
Entrada
Camadas Ocultas
Camada de
Saída
O
O
O
Fig. 2.2: Nomenclatura das camadas de uma rede neural. Rede neural progressiva totalmente conexa de multiplas camadas.
•	Camada de Saída: Camada de neurônios que produz a saída da rede. Os neurônios desta camada sao chamados neurônios de saída.
•	Camada Oculta: Camada de neurônios que interage com outras camadas da rede. Os neurônios desta camada sao aqueles que nao pertencem nem a camada de entrada nem a camada de saída da rede.
Na figura 2.2 apresentamos uma rede de miíltiplas camadas com duas camadas ocultas, sendo uma delas composta por um único neurônio.
E comum caracterizarmos uma rede pelo numero de camadas. A contagem das camadas e feita considerando apenas as camadas com pesos ajustaveis. Nao contamos a camada de entrada, pois ela nao tem pesos ajustaveis. Uma rede neural que nao possui camadas ocultas e chamada rede de camada única, pois encontramos pesos ajustaveis somente na camada de saída. Na figura 2.3 apresentamos uma rede de camada unica. Numa rede de múltiplas camadas encontramos uma ou mais camadas ocultas. A figura 2.4 ilustra como e feita a contagem das camadas.
Uma rede de miáltiplas camadas e dita progressiva2 quando as conexões sinapticas avancam para a saída da rede, ou seja, quando nao houver laco de realimentacao3 na rede. Apresentamos exemplos de redes progressivas nas figuras 2.2, 2.3 e 2.4. Chamaremos rede recorrente quando houver conexoes entre neuronios de uma mesma camada e/ou conexoes retropropagadas. Apresentamos na figura 2.5 uma rede recorrente com uma camada de neuronios ocultos.
Uma rede de milltiplas camadas e dita totalmente conexa quando cada neurônio de uma camada esta conectado a todos os neurônios da camada seguinte. As figuras 2.2 e 2.3 apresentam redes totalmente conexas. Se alguns elos de comunicacao (conexões sinapticas) estiverem faltando, diremos que a rede eí parcialmente conexa. Na figura 2.4 encontramos uma rede parcialmente conexa. A mesma nomenclatura vale para uma rede recorrente de camada unica. Na figura 2.6 apresentamos uma rede recorrente de camada unica totalmente conexa. Neste caso, todos os neurônios possuem laco de alimentacao de modo que a saída da iteracao t e usada como entrada na iteracao t +1.
2Traducao para o termo ingles “feedforward”.
3 Existe realimentacao quando a saída de um elemento influencia em parte a entrada aplicada aquele elemento particu
lar.
Camada de
Entrada
Camada de Saída (Primeira Camada)
Fig. 2.3: Rede neural progressiva totalmente conexa de camada ilnica.
Camada de	Primeira
Entrada	Camada
Segunda Terceira Camada Camada
Camada de Saída (Quarta Camada)
Fig. 2.4: Numeracao das camadas de uma rede neural de mUltiplas camadas. Rede neural progressiva parcialmente conexa com multiplas camadas.
Camada de	Camada	Camada de
Entrada	Oculta	Saída
Fig. 2.5: Rede recorrente com uma camada oculta.
Fig. 2.6: Rede recorrente de camada única totalmente conexa.
2.4	Aprendizagem
A capacidade de aprender e uma das principais características da inteligencia. O aprendizado numa rede neural e realizado ajustando-se os pesos das conexões sinapticas. Em outras palavras, aprendizagem e o processo onde os parâmetros livres de uma rede sao modificados. O tipo de aprendizagem e determinado pela maneira pela qual ocorre a modificacao dos parâmetros.
Existem dois tipos basicos de aprendizagem numa rede neural: aprendizado supervisionado (ou aprendizado com professor) e aprendizado nao-supervisionado (ou aprendizado sem professor). Em ambos os casos precisamos de um conjunto de dados, conhecidos como dados de treinamento.
O aprendizado supervisionado consiste na apresentacao de exemplos de entrada-saída. Durante o processo de aprendizado e feito um ajuste nos pesos de forma a minimizar a diferenca (erro) entre a resposta da rede e a resposta desejada4. Temos assim a acao de um “professor” que apresenta a resposta correta indicando a acao ótima a ser realizada pela rede neural.
No aprendizado não supervisionado, apenas os dados de entrada sao fornecidos. Neste caso, o aprendizado esta baseado em agrupamentos de padrões. Os pesos sao ajustados de modo que padrões semelhantes produzam a mesma saída.
Nos estudos das memórias associativas usaremos somente o aprendizado supervisionado, pois sempre teremos os dados de entrada e as respectivas saídas desejadas.
2.4.1	Aprendizado Supervisionado
Uma rede neural articial com n neurônios na camada de entrada e m neurônios na camada de saída pode representar uma funcao G :	[21]. Por exemplo, uma rede neural progressiva classica
de camada unica pode ser escrita como
y = G(x) = $(W x).
4 A aprendizagem supervisionada pode ser vista como aprendizagem por correção de erro.
Lembre-se que a recíproca tambem e verdadeira visto que as redes neurais classicas sao aproxima-dores universais [33]. Temos que observar, porem, que falta conduzir pesquisas sobre a utilizacao de redes neurais morfológicas como aproximadores de funcoes.
No aprendizado supervisionado conhecemos o conjunto de vetores de entrada e suas respectivas saídas , para £ = 1, 2,..., k. Podemos entao interpretar a rede neural como uma funcao dos pesos sinapticos e resolver um problema de otimizacao onde minimizamos o erro cometido pela rede neural. Por exemplo, para encontrar a matriz dos pesos sinapticos de uma rede neural classica progressiva de camada ilnica, resolvemos o problema:
min ||y^ — (Wx^) ||, para todo £ = 1,..., k.
Existem varios algoritmos de aprendizado supervisionado que resolvem um problema de otimizacao. O mais conhecido e o algoritmo de retropropagação (backpropagation) [21, 33]. Entretanto, devido ao elevado custo computacional, nao utilizaremos nenhum algoritmo de otimizacao complexo para encontrar a matriz dos pesos sinapticos. Neste trabalho utilizaremos somente regras simples, como o armazenamento por correlacao ou o armazenamento por projecao, para treinar nossas redes neurais (veja Capítulo 4).
Capítulo 3
Conceitos Basicos de Memorias Associativas Neurais
Neste capítulo apresentamos os fundamentos matematicos das memorias associativas neurais e especificamos a notacao e a nomenclatura que sera usada durante toda a dissertacao.
3.1	Formulação Matemática, Armazenamento e Associação
Uma memória associativa (Associative Memory, AM) representa um sistema de entrada-saída (inputoutput) que armazena varios pares de padroes (x, y), onde x G e y G Rm. Numa memoria associativa criamos um mapeamento entre a entrada e a saída dado por y = G(x), onde G :	&gt;
e o mapeamento associativo da memoria. Cada par entrada-saída (x, y) armazenado na memoria e dito uma associacao. A entrada do sistema (vetor x) e conhecido como padrão-chave (ou memoria-chave) e a saída (vetor y) e chamado padrao recordado.
A formulacao matematica para um problema de memoria associativa pode ser escrita como: Dado um conjunto finito de pares de entrada-saída {(x^, y^) : £ = 1, 2,..., k} a ser armazenado, nossa tarefa e encontrar um mapeamento que recupere cada um destes pares, isto e, determinar uma funcao G tal que G(x^) = y^, para todo £ = 1,..., k [31]. Alem disso, desejamos que G tenha tolerância a ruído (capacidade de correcao de erros). Assim, se x^ e uma versao ruidosa de x^, isto e, se x^ = x^ e d(x^, x) &amp;lt;5 com 5 &gt; 0 pequeno, desejamos que x^ e x^ produzam a mesma saída, ou seja, G(x^) = y^ (a funcao G nao e injetora). Nas memorias associativas neurais utilizamos uma rede neural para representar o mapeamento associativo G e a fase de armazenamento reduz-se a determinar a(s) matriz(es) dos pesos sinaípticos.
Note que, se um par (x^, y^) foi corretamente armazenado numa memoria associativa e se G e um mapeamento ímpar, entao G(—x) = — G(x) = —y, ou seja, o par (—x^, y;) tambem foi armazenado na memoria. Podemos mostar que uma rede neural classica com funcoes de ativacao ímpares produz um mapeamento G ímpar. Este fato nao vale para as redes neurais morfológicas devido as operacoes de maximo e mínimo usadas no modelo neural morfologico. Varias memorias associativas neurais classicas utilizam funcoes de ativacao ímpar e, consequentemente, representam mapeamentos ímpares. Como exemplo temos a rede de Hopfield, a BAM e a BSB.
O conjunto das associacoes {(x^, y^), £ = 1,..., k} e chamado conjunto das memorias funda-
mentais. Cada associação (x^, ) neste conjunto é uma memória fundamental, cada padrão é uma chave fundamental e cada padrâo y^ neste conjunto é uma recordação fundamental. Quando o conjunto das memórias fundamentais e da forma {(x^, x^), £ = 1, 2,... , k}, dizemos que esta e uma memória auto-associativa. Neste caso particular, os termos memória fundamental, chave fundamental e recordacao fundamental sao sinônimos. No caso geral, quando y^ e diferente de x^, temos uma memória heteroassociativa.
O processo usado para determinar (ou sintetizar) uma memória associativa e conhecido como fase de armazenamento. Um dos principais objetivos numa memória associativa e criar um mapeamento com uma grande capacidade de armazenamento, isto e, uma vasta quantidade de memórias fundamentais podem ser armazenadas [33]. Um dos maiores problemas em uma memória associativa e a criacao de associates que nao fazem parte do conjunto das memórias fundamentais. Estas associates, armazenadas indevidamente, sao as chamadas memorias espúrias.
Quando a fase de armazenamento esta completa, inicia-se a fase de recordacao. Aqui, uma memória pode ser testada para verificar se as memórias fundamentais foram corretamente armazenadas e a capacidade de correcao de erro pode ser medida apresentando as chaves fundamentais corrompidas com varios tipos de ruídos e observando a saídas resultantes, i.e., comparamos a saída de uma entrada ruidosa com a saída desejada. O conjunto dos pontos x G tais que G(x) = y e chamado regiao de recordacao do padrâo y.
3.2	Classificacao das Memorias Associativas Neurais
As memórias associativas neurais podem ser divididas em duas grandes classes: as memórias associativas estáticas e as memorias associativas dinamicas (Dynamic Associative Memory, DAM). As memórias associativas neurais estaticas podem ser descritas por uma rede neural progressiva. Uma rede neural recorrente usada como mapeamento associativo produz uma memória associativa dinamica. Portanto, a arquitetura da rede neural utilizada (progressiva ou recorrente) define a arquitetura da memória associativa neural (estatica ou dinamica, respectivamente).
Os padrões armazenados numa memória associativa neural podem ser bipolares ({-1,1}), binarios ({0,1}), discretos (Z) ou contínuos (R). Apresentaremos nesta dissertacao modelos de memórias associativas neurais para padrões contínuos mas daremos enfase as memórias associativas bipolares e binarias.
A fase de recordacao de uma memória associativa dinamica pode ser interpretada como um processo temporal que assume valores discretos ou contínuos. Deste modo, classificamos as memórias associativas dinamicas como sendo discretas ou contínuas no tempo. Nesta dissertacao nao discutiremos as memórias associativas dinamicas contínuas no tempo [33, 37, 36]. Na figura 3.1 apresentamos um diagrama com a classificacao das memórias associativas neurais. Os modelos com caixas pontilhadas nao serao discutidos neste trabalho.
As memórias associativas dinamicas discretas no tempo podem ser descritas pelas equates
y(t)	=	F (x(t)),	(3.1)
x(t + 1)	=	H(y(t)),	(3.2)
V t	=	0,1,...	(3.3)
Fig. 3.1: Diagrama para classificação de uma memória associativa neural. Os modelos com caixas pontilhadas nao serao discutidos neste trabalho.
onde F :	e H :	sao funcóes nao lineares. Dizemos que o par (x, y) e um ponto
estacionário de uma memória associativa dinamica se F(x) = y e H (y) = x. Um ponto estacionario e tambem referido como ponto fixo no caso auto-associativo. O conjunto dos pontos x = x(0) para o qual a seqüencia dos pares {(x(0), y(0)), (x(1), y(1)),...)} gerada atraves das equacóes (3.1)-(3.3) nao converge e conhecido como regiao de indecisão. Note que uma memória associativa dinamica sempre converge para um ponto estacionario independente do padrâo-chave se e somente se a regiao de indecisao for vazia. Devemos impor um numero maximo de iteracóes na ausencia de informacóes sobre a regiao de indecisao ou quando soubermos que esta e nao vazia.
A interpretacao das equacóes (3.1) e (3.2) depende do tipo de atualizacao das componentes dos vetores y(t) e x(t +1). As duas formas de atualizacao mais comum sao: sincronizada (ouparalela) e assíncrona (ou sequencial). Numa memória associativa com atualizacao sincronizada, todas as componentes dos vetores y (t) e x(t + 1) sao atualizadas simultaneamente a cada iteracao. Na atualizacao assíncrona, a cada iteracao t, definimos It como sendo uma permutacao do conjunto {1, 2,..., m}, Jt como sendo uma permutacao de {1, 2,... ,n} e computamos
yi(t) = Fi(x(t)) , i G It
xj(t +1) = Hj(y(t)), j G Jt
(3.4)
(3.5)
seguindo a ordem proposta em It e Jt. Em outras palavras, na iteracao t, atualizamos uma ilnica componente de cada vez nos vetores y (t) e x(t + 1) seguindo uma seqüencia aleatória de índices ate atualizarmos todas as componentes. Na iteracao seguinte, repetimos o processo escolhendo seqüencias diferentes de índices It e Jt. Note que as atualizacao assíncrona adicionam incerteza na seqüencia entre o padrao-chave e o padrâo recordado. Por esta razao, podemos dizer que uma memória associativa dinamica com atualizacao assíncrona representa um modelo estocastico.
O modo de atualizacao das componentes pode afetar drasticamente a fase de recordacao de uma memoria associativa dinamica. Por exemplo, a memoria associativa de Hopfield com atualizacao assíncrona sempre converge para um ponto fixo se certas condicoes forem satisfeitas. Por outro lado, a mesma memoria associativa com atualizacao sincronizada pode ter uma regiao de indecisao nao vazia. Nesta dissertacao de mestrado, embora usamos com frequencia uma notacao vetorial semelhante as equacoes (3.1) e (3.2) para descrever uma memoria associativa dinamica, consideramos apenas atualizacao assíncrona.
O mapeamento associativo G de uma memoria associativa dinamica e definido como segue. Dado um padrao-chave x, tome x(0) = x e compute a seqüencia finita {(x(0), y(0)),..., (x(tf), y(tf))}, onde tf e, ouo menor t tal que (x(t), y (t)) e um ponto estacionario, ou o niímero maximo de iteracoes permitidas. O padrao recordado pela memoria associativa apos a fase de recordacao e dado pela seguinte equacao
y = G(x) := y(tf).	(3.6)
Note que o mapeamento associativo G inclui a dinamica da memoria associativa e considera, indiretamente, o modo de atualizacao das componentes.
3.3	Características para um Bom Desempenho
E de nosso interesse caracterizar o desempenho de uma memoria associativa. Um conjunto de características desejaveis para uma classe de memorias associativas encontra-se em [31, 32, 64].
Uma memoria associativa de baixa performance e aquela incapaz de armazenar todas as memorias fundamentais ou com baixa tolerancia a ruído. Ela possui um grande niímero de memorias espurias, e estas possuem grandes regioes de recordacao. No caso das memorias associativas dinamicas, uma baixa performance tambem pode ser caracterizada pela presenca de oscilacoes, onde um estado inicial proximo a uma memoria armazenada tem grande probabilidade de convergir para uma memoria espuria ou para um ciclo limite.
Uma memoria associativa dinamica ideal e aquela que possui grande tolerancia a ruído, um numero relativamente pequeno de memorias espurias, e cada memoria espuria possui uma pequena regiao de recordacao. No caso das memorias associativas dinamicas, ela deve ser uma memoria associativa estavel no sentido de nao possuir oscilacoes e possuir uma convergencia rápida para quaisquer padroes-chaves apresentados a rede. Resumindo, diremos que uma memoria associativa possui um bom desempenho quando possuir as seguintes caracteríísticas:
1.	Grande capacidade de armazenamento,
2.	Tolerancia a ruído ou entradas incompletas,
3.	Existencia de poucas memorias espurias,
4.	O armazenamento da informacao deve ser distribuído e robusto.
5.	Recordacao rápida e baixo custo computacional.
Utilizaremos estas características no capítulo 7 para comparar os varios modelos de memorias associativas neurais apresentados nesta dissertacao.
Capítulo 4
Memorias Associativas Lineares
Neste capítulo apresentamos as memorias associativas lineares que foram introduzidas em 1972 independentemente por Anderson, Kohonen e Nakano [4, 47, 60]. Numa memória associativa linear (Linear Associative Memory, LAM) criamos um mapeamento linear G : Rn —&gt; Rm entre a entrada e a saída. Neste caso, o mapemento G pode ser representado por uma matriz, digamos W E Rmxn, e dado um padrâo-chave x E Rn, encontramos o padrao recordado y E Rm atraves da equacao
y = Wx.	(4.1)
Uma memória associativa linear e descrita por uma rede neural classica progressiva de camada unica (veja Figura 2.3 no Capítulo 2) com a funcao identidade como funcao de ativacao. Discutimos dois procedimentos diferentes utilizados para obter a matriz W da equacao (4.1). Precisamente, o armazenamento por correlacao (Correlation Recipe) e o armazenamento por projecao (Projection Recipe) [31, 32]. Estes dois procedimentos definem a base para o aprendizado das memórias associativas discutidas nos próximos capítulos.
4.1	Armazenamento por Correlação
O armazenamento por correlação (Correlation Recording), tambem conhecido como aprendizado de Hebb, e um dos procedimentos mais usados para obter a matriz dos pesos sinãpticos W e estã baseado no postulado de aprendizagem de Hebb [34]. O postulado de Hebb afirma que se um neurônio A e ativado por um neurônio B repetidas vezes, então o neurônio A se tornarã mais sensível aos estímulos do neurônio B e a conexão sinãptica entre A e B serã aumentada [21, 33]. Em resumo, o peso sinãptico wij sofrerã uma variacão dada pela correlacão entre a entrada Xj e a saída yi. Se temos um conjunto finito de pares {(x^, y^) : £ = 1,..., k} a ser armazenado numa memôria associativa linear, o armazenamento por correlacão fornecerã uma matriz W G Rmxn onde
k
wij = 52 yi xj ■	(4.2)
í=i
Usando uma notacão matricial temos
k
W = YXt = 52 y« (x« )T,	(4.3)
í=i
Fig. 4.1: Padrões recordados pela memória auto-associativa linear com armazenamento por correlação quando usamos como entrada as chaves fundamentais x1,..., x5.
onde X = [x1, x2,..., xk] G Rnxk e a matriz obtida tomando as chaves fundamentais como coluna e Y = [y1, y2,..., yk] G Rmxk e obtida concatenando as recordacoes fundamentais.
A memória associativa linear com armazenamento por correlacao pode ser facilmente determinada, mas possui serias limitacoes. Por exemplo, substituindo (4.3) em (4.1) e assumindo que xh e uma chave fundamental, encontramos a seguinte expressao para o padrâo recordado yh:
r k
yh
£y4 (x4 )T
L 4=1
xh
k
x'‘||2 yh +	y4 (x4 )T xh.
4=h
(4.4)
O segundo termo do lado direito de (4.4) é um vetor ruído e surge devido à interferência cruzada (cross-talk) entre o padrào xh e as demais chaves fundamentais. Note que este termo serà zero se os vetores x1, x2,..., xk forem ortogonais. O primeiro termo do lado direito de (4.4) e proporcional a recordacao fundamental yh, com constante de proporcionalidade ||xh||2. Para evitar esta constante, podemos impor ||x^||2 = 1 para £ = 1, 2,... , k. Assim, uma condicao suficiente para recordar uma memória perfeitamente e ter um conjunto de vetores {x1, x2,..., xk} ortonormal. Dificilmente teremos uma recordacao perfeita das memórias fundamentais se os padrões x1, x2,..., xk nao forem ortonormais. Note que nao impormos condicoes sobre as recordacoes fundamentais, portanto, os vetores coluna y1, y2,..., yk podem ser quaisquer.
Exemplo 4.1.1. Considere as imagens com 256 tons de cinza apresentadas na figura 1.3. Estas imagens foram convertidas em padrões (vetores coluna) x1,..., x4 G [0,1]4096 e armazenadas na memória auto-associativa linear com armazenamento por correlacao. Usando as chaves fundamentais como entrada, encontramos como resposta os padrões apresentados na figura 4.1, respectivamente. Neste exemplo percebemos claramente o efeito da interferencia curzada discutido anteriormente. O erro quadrático medio normalizado (EQMN) calculado atraves da equacao
1 k
E ' =
4=1
||Wx4 - y4
x4
(4.5)
foi aproximadamente 3100.
Exemplo 4.1.2. A memoria associativa linear com armazenamento por correlacao tambem pode ser usada para armazenar padrões bipolares. Considere as chaves fundamentais x1, x2,..., x5 G
Fig. 4.2: Padrões recordados pela memória associativa linear com armazenamento por correlação quando usamos como entrada as chaves fundamentais x1,..., x5.
{—1,1}4096 apresentados na figura 1.1 e as recordacoes fundamentais y\ y2,..., y5 E {-1,1}4096 apresentados na figura 1.2. Armazenamos estes cinco pares de padrões na memória associativa linear usando o armazenamento por correlacao. Apresentando as chaves fundamentais x1,..., x5 como entrada, encontramos os padrões recordados apresentados na figura 4.2, respectivamente. Percebemos novamente o efeito da interferencia curzada neste exemplo. Note que, embora as recordacoes fundamentais sejam padrões bipolares, os padrões recordados nao sao padrões bipolares. De fato, Wx1, Wx2,..., Wx5 E [—14354,14354]4096. O erro quadrático medio normalizado (EQMN) foi aproximadamente 10471, um valor grande pois alem da interferencia cruzada temos o valor ||x||2 = 4096 multiplicando o vetor coluna y^ na equacao (4.4).
O armazenamento por correlacao parece nao ser muito eficiente, visto que dificilmente armazenara o conjunto das memórias fundamentais se existir uma chave fundamental que nao e ortogonal as demais. Todavia, ele possui grandes vantagens. A primeira delas esta na motivacao biológica do postulado de Hebb. A segunda vantagem esta no custo computacional realizado para encontrar a matriz W, pois neste armazenamento realizamos (2k — 1)mn operacões1.
4.1.1	Armazenamento por Correlação Auto-associativo Bipolar
No caso auto-associativo bipolar, os elementos da diagonal de W são wü = |=1 (xf) . Estes elementos são sempre positivos e seus valores aumentam indefinidamente quando adicionamos novos padrões. Deste modo, quando armazenamos muitos padrões, encontramos uma matriz cujos elementos da diagonal sao muito maiores que os demais elementos e se fizessemos uma normalizacao dos elementos da matriz, encontraríamos uma matriz parecida com uma matriz diagonal. Entretanto, uma matriz diagonal nao possui capacidade de correcao de erro e nao teremos uma memória associativa eficiente. Este e um problema comum no aprendizado de Hebb e pode ser resolvido impondo wü = 0. Quando impomos wif = 0, encontramos:
|P=1 4 xj,
wij =
se i = j, se i = j,
para 1 &amp;lt;i, j &amp;lt;n.
(4.6)
Usando uma notacao matricial temos
W = XXT — kI,
(4.7)
xNeste trabalho +, -, x e representam uma operaçao (1 flop).
onde I e a matriz identidade n x n e k e o número de padrôes armazenados na memoria associativa linear. A regra de aprendizado descrita pelas equacoes 4.6 e 4.7 e conhecido como armazenamento por correlacao com diagonal nula ou aprendizado de Hebb sem auto-conexão (ou auto-realimentacao). Este aprendizado sera usado nas memorias auto-associativas dinamicas apresentadas nos capítulo 5.
4.2	Armazenamento por Projeção
O armazenamento por projeção (Projection Recording) foi proposto por Kohonen e Ruohonen [50] e tem como objetivo resolver o problema
k
min ||y - WXHF = minJ2 lly€ - Wx€Il2&gt;	(4.8)
e=i
onde H • Hf representa a norma de Frobenius2 e || • ||2 representa a norma Euclidiana (para vetores). Uma memória associativa linear treinada usando o armazenamento por projecao e chamada memoria associativa linear ótima (Optimal Linear Associative Memory, OLAM) pois a matriz dos pesos sinapticos W G Rmxn e a matriz que minimiza o erro entre as recordacoes fundamentais y e os padrões recordados W. A solucao de (4.8) e
W = YX1,	(4.9)
onde X1 e a pseudo-inversa (ou inversa generalizada de Moore-Penrose) de X [25, 101]. A matriz W e a matriz de projecao no espaco gerado pelas recordacoes fundamentais y, por isso chamamos este aprendizado de armazenamento por projeçao.
Se o conjunto {x2, £ = 1, 2,..., k} e linearmente independente, i.e., se X e uma matriz de posto completo, entao a pseudo-inversa de X e dada por
	Xt = (XTX 1 XT,	(4.10)
e	W = Y (X TX )-1 XT.	(4.11)
Se os padrões ¡ x2, £ = 1,..	., k} forem ortonormais, entao XTX	= I e W = YXT e a matriz
encontrada usando o armazenamento por correlacao. Logo, o armazenamento por correlacao e um caso particular do armazenamento por projecao se os padrões armazenados forem ortonormais.
Existe uma versao iterativa do procedimento de armazenamento por projecao baseada no teorema de Greville que nao sera discutida aqui mas pode ser encontrada em [49]. Esta versao iterativa pode ser usada para adicionar uma nova associacao na memória associativa linear ótima.
No caso auto-associativo, Y = X, temos W2 = W (matriz de projecao ) e W = WT (matriz simeítrica). Estes fatos podem ser obtidos diretamente das propriedades da matriz pseudo-inversa [53, 48, 101]. Note que W = I e solucao do problema minW ||X — WX \\F independente da matriz X. Logo podemos armazenar um mímero ilimitado de padrões na OLAM no caso auto-associativo. No caso heteró-associativó, minW ||Y — WX||F pode ser maior que zero e nao garantimos sucesso
2Tambem referida como norma Euclidiana para matrizes.
na fase de armazenamento. Se X possui posto completo, pela equação (4.11) concluímos que a matriz W do armazenamento por projeçao e tal que WX = Y. Por outro lado, se k &gt; n, entao certamente teremos um conjunto linearmente dependente, X provavelmente nao tera posto completo e dificilmente conseguiremos armazenar o conjunto das memórias fundamentais na OLAM.
Exemplo 4.2.1. Considere o caso auto-associativo onde armazenamos os padrões em tons de cinza apresentados na figura 1.3. Vimos no exemplo 4.1 que a memória associativa linear com armazenamento por correlacao nao e capaz de armazenar o conjunto das memórias fundamentais. Armazenamos estas memórias fundamentais na OLAM e encontramos como saída as recordacóes fundamentais após apresentar as respectivas chaves fundamentais como entrada. De fato, todas as memórias fundamentais foram armazenadas com sucesso posto que o conjunto {x1,..., x4} e linearmente independente. O EQMN deste experimento foi 2, 7 x 10-15 por causa de erros numericos.
Exemplo 4.2.2. Vamos considerar o caso hetero-associative bipolar. Considere os padrões de entrada apresentados na figura 1.1 e os padrões de saída apresentados na figura 1.2. Primeiramente, notamos que o posto da matriz X e 5. Portanto, a OLAM deve ser capaz de armazenar o conjuto de memórias fundamentais {(xf, yf) : £ = 1,..., 5}. De fato, armazenando este conjunto e apresentando os padrões x1,..., x5 como entrada, encontramos um EQMN de 5, 6 x 10-14. Novamente, o valor encontrado para EQMN e diferente de zero devido a erros de arredondamento.
Teorema 4.2.1. Sejam X G Rnxk e Y G Rmxk as matrizes concatenando as chaves e recordações fundamentais, respectivamente. Considere a decomposicao em valores singulares (SVD)
r
X = U= ^2 a Uj	.	(4.12)
j=1
Seja N = [n1, n2,..., nk] G Rnxk uma matriz gerada aleatoriamente com distribucao gaussiana com média zero e variancia3 a2, isto é, E(nf) = 0 e
4"2 se 1 = j’	(4.13)
I 0	caso contrário,
onde E (X) representa a esperanca da variável aleatoria X .Se W = YX t é a matriz das conexoes sinapticas da OLAM, entao o erro quadrático medio (Mean Square Error, MSE) total das recordacoes da memoria associativa sera
MSE = E (||Y - W(X + N)||F)	(4.14)
= E IIY vy |I2 + ka2 ¿ "?'2 ' ’	(4.|5)
j=r+1	j=1	j
onde vi,..., vk sao os vetores singulares da direita e a1,..., ar sao os valores singulares de X.
Com base neste teorema concluímos que:
3Note que usamos aj, com índice, para representar os valores singulares e a2, sem índice, para representar a variância
de nf.
E (nf nf)
1.	O primeiro termo de (4.15) surge devido a dependência linear das chaves fundamentais. Se os vetores x1, x2,..., xk forem linearmente independentes, entao o posto r da matriz X sera k e o primeiro termo de (4.15) sera zero. Se os padrões-chave forem linearmente dependente, o erro do primeiro termo sera inevitavel e podemos chamar este termo de erro da dependência linear.
2.	O segundo termo de (4.15) e obtido devido ao ruído nos padrões de entrada e podemos chamar este termo de erro do ruído. Quando apresentamos como entrada um padrâo-chave sem ruído, entao a2 = 0 e este termo tambem sera zero. Por outro lado, se fornecermos uma entrada ruidosa, necessariamente teremos a interferência do erro do ruído na recordacao. Note que, no somatório do erro do ruído, se um dos valores singulares for muito pequeno, entao ka\\Yvj ||2/aj &gt;&gt; 0 e o erro de recordacao sera grande. Logo, quando uma entrada ruidosa e apresentada a memória associativa, o erro de recordacao pode ser descrito pelos valores singulares a de X. Observe tambem que, se X tem posto completo, entao r = k e quanto mais elementos sao armazenados, maior sera o erro devido ao termo do ruído.
Corolário. O erro quadratico medio total das recordacoes da OLAM no caso auto-associativo e
MSE = ka2 r.
(4.16)
Note que o MSE da memória associativa linear ótima no caso auto-associativo não possui o termo devido ao erro da dependencia linear.
Sabemos que a media da soma total do ruído e
(L"'")
E
= ka2 n.
(4.17)
Com base nas equacoes (4.16) e (4.17), concluímos que a medida de correcao de erro desta memória
associativa e:
ka2r
ka2n
r
n
(4.18)
Logo, se r &amp;lt;n, entao a memória auto-associativa linear com armazenamento por projecao reduz o ruído da entrada. O pior caso ocorre quando r = n onde o ruído nao diminui. Note tambem que, quanto menor for r (ou k, pois r &amp;lt;k), melhor sera a correcao de erro esta memória. Entretanto, nao temos uma restricao quanto ao mímero de padrões armazenados. Podemos armazenar um numero de padrões maior que a dimensao dos padrões de entrada, isto e, k &gt; n, mas perdemos com isso a capacidade de correcao de erro, pois a matriz W tende para a matriz identidade.
O teorema 4.2.1 e o corolario deste teorema podem ser encontrados no artigo de Murakami e Aibara [59]. Outros resultados sobre a capacidade de correcao de erro da memória associativa linear ótima (usando armazenamento por projecao) foram apresentados por Kohonen [49], Stiles e Denq [89] e Casasent e Telfer [14], entre outros.
Exemplo 4.2.3. Considere os padrões em tons de cinza apresentados na figura 1.3. Pelo teorema 4.2.1 e pelo exemplo 4.2.1, sabemos que a memória auto-associative linear ótima e capaz de armazenar os padrões x1,..., x4. Vamos verificar agora a tolerancia a ruído deste modelo. Na figura 4.3 apresentamos versões ruidosas x1,..., x4 das memórias fundamentais geradas com distribuyo gaussiana
Fig. 4.3: Memorias-chave apresentada a OLAM no exemplo 4.2.3.
Fig. 4.4: Padroes recordados pela OLAM no exemplo 4.2.3 quando apresentamos as memorias-chave apresentadas na figura 4.3.
com media zero e variancia 0,1. Na figura 4.4 apresentamos os padrões recordados apios apresentar os padroes da figura 4.3 como entrada. O erro quadrático medio normalizado calculado apõs 1000 simulacoes foi aproximadamente 0, 08.
Exemplo 4.2.4. Considere os padroes bipolares x1,..., x5 E {—1, +1}4096 e y1,..., y5 E {—1, +1}4096 apresentados nas figuras 1.1 e 1.2, respectivamente. Vimos no exemplo 4.2.2 que a OLAM e capaz de armazenar este conjunto de memorias fundamentais. Armazenamos estes padrões na OLAM e usamos como entrada os padrões x1,..., x5 apresentados na figura 4.5, respectivamente. Estes padrões ruidosos foram gerados a partir das chaves fundamentais revertendo o valor de um pixel seguindo uma distribuicao uniforme com probabilidade 0, 3. Na figura 4.6 apresentamos os respectivos padrões recordados. O erro quadrático medio normalizado (EQMN) foi aproximadamente 0, 6. Note que os padrões recordados pela OLAM foram imagens em tons de cinza [—1, +1]4096 com ruído vindo de outras recordacões fundamentais mas dominado pelo saída desejada (erro do ruído). Encontramos as memorias fundamentais y1,..., y5 e um EQMN igual a zero aplicando um threshold com corte no nível de cinza fornecido pelo meítodo de Otsu [63].
Fig. 4.5: Versóes corrompidas dos padróes x1,..., x5 da figura 1.1. Estes padróes foram gerados introduzindo ruído uniforme com probabilidade 0, 3 de reverter o valor de um pixel.
Fig. 4.6: Padróes recordados pela OLAM no exemplo 4.2.4 quando apresentamos as memórias-chave apresentadas na figura 4.5.
Capítulo 5
Memorias Associativas Dinamicas
Neste capítulo apresentamos os principais modelos de memórias associativas dinâmicas para padrões bipolares e introduzimos a memória associativa bidirecional com capacidade exponencial. Para cada modelo fornecemos uma breve introducao, especificamos a arquitetura da rede neural e a regra de aprendizado, apresentamos uma breve analise sobre a convergencia e exemplos computacionais.
5.1	Memória Associativa de Hopfield Discreta
A memória associativa de Hopfield foi introduzida em 1982 pelo físico J.J. Hopfield e é o modelo de memória associativa neural mais conhecido e estudado na literatura [40, 31, 33]. Este modelo forma a base para os demais modelos de memórias associativas discutidas neste capítulo.
5.1.1	Arquitetura da Rede
A memória associativa de Hopfield e descrita por uma rede neural classica recorrente de camada ilnica totalmente conexa com funcao sinal como funcao de ativacao [40]. A arquitetura da rede de Hopfield esta apresentada na figura 2.6. O modelo de Hopfield e descrito pela equacao
xi (t + 1) = sinal
wij xj (t)j
= sinal (wTx(t) ,
para i E In e t = 0,1,...
(5.1)
onde wT é a i-ésima linha da matriz W, x(t) é a entrada da rede no tempo t e In é uma permutação do conjunto de índices {1, 2,..., n}. A funçao sinal usada nas memórias associativas dinamiças e definida como segue:
I +1 sinal (wTx(t) = &amp;lt;xi(t)
1
se wTx(t) &gt; 0,
se wT x(t) = 0,	(5.2)
se wfx(t) &amp;lt;0.
Podemos descrever a rede de Hopfield simplificadamente atraves da equacao
x(t + 1) = sinal(Wx(t)), t = 0,1,...	(5.3)
Neste caso, devemos lembrar que a atualizacao das componentes e feita no modo assíncrono.
Pode-se usar atualizacao sincronizada na rede de Hopfield. Entretanto, a rede com atualizacao sincronizada pode apresentar ciclo limite, isto e, uma regiao de indecisao nao nula [3]. Lembre-se que o tipo de atualizacao altera a trajetória dos pontos x(t), mas nao altera os pontos fixos da memoria associativa dinamica.
5.1.2	Aprendizado
Na memoria associativa de Hopfield usamos o armazenamento por correlacao com diagonal nula. Neste caso, a matriz dos pesos sinapticos e computada através da equacao 4.6, ou pela equacao 4.7. Note que a matriz dos pesos sinapticos W e simetrica (W = WT) com diagonal nula (wi¿ = 0, Vi = l,...,n).
Repare na semelhanca entre a memoria associativa de Hopfield e a memoria associativa linear com armazenamento por correlacao discutida no capítulo 4. A diferenca entre estas duas memorias associativas esta na existencia da funcao sinal, que forca a saída a ser +1 ou — 1, e na recursividade presentes na rede de Hopfield. A seguir apresentaremos as propriedades da memoria associativa de Hopfield e veremos como esta simples mudanca (funcao sinal + recursividade) produz melhoras significativas no desempenho da memoria associativa.
5.1.3	Convergencia
O seguinte teorema, introduzido por Hopfield em [40], garante a convergencia da memoria associativa dinamica descrita pelas equacao 5.1 (ou 5.3). Em outras palavras, o seguinte teorema garante que a regiao de indecisao da memória associativa de Hopfield com atualizacao assíncrona e vazia. Outros resultados sobre a convergencia desta memoria associativa dinamica podem ser encontrados em [9].
Teorema 5.1.1 (Teorema da Convergencia de Hopfield). Seja W uma matriz simétrica (W = WT) com diagonal nao negativa. (wn &gt; 0, i = 1,..., n). A memória associativa dinamica descrita pela equacao 5.3 com atualizado assíncrona converge para um ponto fixo e minimiza a funcao energia
r&gt; r&gt;
(5.4)
O seguinte teorema fornece uma estimativa para o numero maximo de memorias fundamentais que podem ser armazenadas na memoria associativa de Hopfield. Este teorema foi introduzido por McEliece et. al. em [56]. Uma demonstracao simplificada pode ser encontrada em [3].
Teorema 5.1.2 (Teorema de McEliece et. al.). Sejam x1,..., xk e {—1,1}n, com n suficientemente grande, padroes nao-correlacionados gerados aleatoriamente com distribuicao uniforme.
1.	Uma memoria fundamental x; terá grande probabilidade de ser um ponto fixo da memoria associativa de Hopfield se
k &amp;lt;n/(2 logn).
(5.5)
2.	Todos os padrões x1,..., xk serão pontos fixos da memória associativa de Hopfield com grande probabilidade se
k &amp;lt;n/(4logn).	(5.6)
O teorema 5.1.2 está baseado na convergencia forte em probabilidade. Neste caso, dada a sequencia de variaveis aleatórias X1, X2,... e a variavel aleatória X, dizemos que Xn ■ X quando n ■ x com grande probabilidade se Pr(Xn ■ X quando n ■ 1) = 1 [43].
Exemplo 5.1.1. Considere uma rede de Hopfield com 100 neurônios. Geramos k padrões bipolares aleatoriamente com distribuicao uniforme e armazenamos todos eles na memoria associativa de Hopfield. Depois verificamos se o primeiro padrao e um ponto fixo e se todos os padrões sao pontos fixos. Repetimos o experimento 1000 vezes para diferentes valores de k (nilmero de memórias fundamentais) e calculamos a probabilidade empírica do primeiro e de todas as recordacoes fundamentais serem pontos fixos da memoria associativa de Hopfield. Na figura 5.1 apresentamos com linha marcada com o a probabilidade empírica de uma certa memoria fundamental (no nosso caso x1) ser um ponto fixo e com linha marcada com □ a probabilidade empírica de todas as memorias fundamentas serem pontos fixos. A linha tracejada representa uma estimativa teorica para a capacidade de armazenamento obtida usando a equacao 7.12 que sera introduzida no capítulo 7. No eixo horizontal colocamos k, o nilmero de memorias fundamentais. As linhas pontilhadas verticais indicam os valores n/(2 log n) e n/(4 log n). Note que a probabilidade empírica de todos os padrões serem pontos fixos deixa de ser 1 quando k &gt; n/(n log n). A probabilidade empírica de todas as memórias fundamentais serem pontos fixos para k = n/(2 log(n)) foi menor que 0, 8. Entretanto, a probabilidade empírica de uma certa memoria fundamental ser ponto fixo foi muito proxima de 1. Estes resultados numericos conferem com o teorema 5.1.
Note que a matriz dos pesos sinpaticos W = XXT fornecida pelo armazenamento por correlacao com auto-alimentacao tambem satisfaz as condiciOes do teorema 5.1.1. Logo, este procedimento tambem pode ser usado para treinar a memoria associativa descrita pela equacao 5.3. O comportamento desta nova memoria associativa dinamica sera muito parecido com o modelo com diagonal nula porque nao podemos armazenar muitos padroes e, com poucos padroes armazenados, nao temos uma matriz W com elementos na diagonal muito maior, em modulo, que os demais (veja secao 4.1.1 sobre armazenamento por correlacao auto-associativo).
Exemplo 5.1.2. Considere os padroes bipolares apresentados na figura 1.1. Armazenamos estes padroes na memoria associativa de Hopfield usando o armazenamento por correlacao (com diagonal nula) e depois apresentamos as chaves fundamentais como entrada. A memoria associativa de Hopfield encontrou os pontos fixos com no maximo 2 iteracoes. Na figura 5.2 apresentamos passo a passo os padroes recordados pela memoria associativa de Hopfield ate o final da segunda iteracao. Note que k = 5 e muito menor que o valor fornecido pelo teorema 5.1.2, (4096/(4 log(4096)) = 283, 5). Entretanto, nenhuma das memorias fundamentais e um ponto fixo. Isso acontece porque os padroes x1,..., x5 possuem 2173 componentes em comum (no background) e uma correlacao media (1/20)	&amp;lt;x€, xn &gt;= 2344. Um resultado similar sera obtido pela memoria associ-
ativa dinamica descrita pela equacao 5.3 treinana com o armazenamento por correlacao com auto-alimentacao.
Fig. 5.1: Probabilidade das memórias fundamentais serem pontos fixos na memoria associativa de Hopfield por k. A linha marcada com o representa a probabilidade de uma dada memoria fundamental ser ponto fixo e a linha marcada com □ representa a probabilidade de todas as memorias fundamentais serem pontos fixos. A linha tracejada representa a capacidade de armazenamento que seraí discutida no capítulo 7. As linhas pontilhadas verticais representam os valores n/(2 log n) e n/(4 log n).
Padroes (1) = sinal(W(0)), £ = 1,..., 5 obtidos no final da primeira iteracao.
* i i- ■.	* i i- ■.	* i i- ■.
Pontos fixos x^(2) = sinal(Wx^(1)), £ = 1,..., 5 obtidos no final da segunda iteracao.
Fig. 5.2: Padroes recordados pela memoria associativa de Hopfield no exemplo 5.1.2 quando apresentamos as chaves fundamentais como entrada.
W (Wi)	sinal(-)
WT (W2)	sinal(-)
xi (t)
x2 (t)
x3 (t)
x„(t)
0~yi(t)^ 0
0~ y2(t^ 0
0 —y«(t)^ 0
0
0
0
0
xi (t + 1)
x2 (t + 1)
x3 (t + 1)
x„(t + 1)
Fig. 5.3: Arquitetura da Memória Associativa Bidirecional (Assimetrica).
5.2	Memoriã Associativa Bidirecional
A Memória Associativa Bidirecional (Bidirectional Associative Memory, BAM), proposta por Kosko em 1987, e uma generalizacao da memória auto-associativa de Hopfield discreta com armazenamento por correlacao com auto-alimentacao [51, 52].
5.2.1	Arquitetura
A BAM eí descrita por uma rede neural claíssica recorrente totalmente conexa com duas camadas e funcao sinal como funcao de ativacao, como apresentado na figura 5.3. A matriz dos pesos sinapticos da segunda camada da BAM e a transposta da matriz dos pesos sinapticos da primeira cada. A BAM e descrita pelo par de equacóes
y(t) = sinal (Wx(t)),	(5.7)
x(t +1) = sinal (WTy(t)) , para t = 0,1, 2,...	(5.8)
com atualizacao assíncrona.
5.2.2	Aprendizado
As matrizes dos pesos sinapticos da BAM sao obtidas usando o armazenamento por correlacao (aprendizado de Hebb). A matriz dos pesos sinapticos da primeira camada e W = YXT ea matriz da segunda camada e WT = XYT. Note que WT e a matriz das conexóes sinapticas da memória associativa linear com armazenamento por correlacao com as memórias fundamentais {(y^, x^), £ = 1, 2,..., k}, onde y^ G {-1,1}m e a entrada e x^ G {-1,1}n e a saída, para £ = 1,..., k. No caso auto-associativo, isto e, se y^ = x^, para £ = 1,..., k, teremos a memória associativa de Hopfield.


Padrões (0) = sinal(W(0)), £ = 1,..., 5.
Padrões x^(1) = sinal(Wy^(0)), £ = 1,..., 5.
-:±,-s	-■

Padrões y^(1) = sinal(Wx^(1)), £ = 1,..., k.
Fig. 5.4: Padroes recordados pela BAM quando apresentamos as chaves fundamentais x1,..., x5 como entrada.
5.2.3	Convergência
A BAM pode ser convertida numa memória auto-associativa discreta de Hopfield com vetores de estados xT = [xT, yT]T e matriz das conexões sinapticas
W T
_0 WT
W0
(5.9)
A matriz WT e simétrica com diagonal nula. Lõgõ, pelõ teorema da convergencia de Hopfield, esta memõria associativa sempre converge para um ponto estacionario com atualizacao assíncrona. A convergencia da BAM, para ambos os casos, tambem pode ser verificada mostrando que a funcao energia
Energia(x, y) = —1 (xTWy + yTWTx) = —xTWy,	(5.10)
eí minimizada.
Exemplo 5.2.1. Considere os padrões x1,..., x5 G {—1,1}4096 e y1,..., y5 G {—1,1}4096 apresentados nas figuras 1.1 e 1.2. Armazenamos o conjunto das memórias fundamentais {(x^, y^), £ = 1,..., 5} e verificamos se as memórias fundamentais sao pontos estacionarios da BAM. Comecamos apresentando as chaves fundamentais x1,..., x5 como entrada e encontramos como resposta os padrões apresentados na figura 5.4. Os padrões x1 (2),..., x5(2) obtidos no final da segunda iteracao
sao iguais aos padróes x1 (1),..., x5(1) entrados no final da primeira iteracao. Neste exemplo a BAM convergiu com 2 iteracóes, entretanto nenhuma das memórias fundamentais e um ponto estacionario.
A seguinte conjectura, introduzida por nos nesta dissertacao de mestrado, e uma extensao do teorema de McEliece et. al. para a BAM. Lembre-se que Xn ■ X quando n ■ x com grande probabilidade se P(Xn ■ X quando n ■ 1) = 1 [43].
Conjectura 5.2.1. Sejam x1,..., xk G {-1,1}n e y1,..., yk G {-1,1}m, n e m suficientemente grandes, padrões nao-correlacionados gerados aleatoriamente com distribuyo uniforme.
1. Uma memória fundamental (x^,	) tera uma grande probabilidade de ser um ponto esta-
cionario da BAM se
1 . ( n&amp;lt;-mm -----
2	ylogm
m log n
(5.11)
k
2. Todas as memórias fundamentais com grande probabilidade se
(x1, y1),..., (xk, yk) serao pontos estacionarios da BAM
k &amp;lt;- min
_ 4
nm log m log n
(5.12)
Acreditamos que uma demostracao formal do resultado acima pode ser obtida fazendo as devidas modificacoes na demonstracao do teorema principal (The Big Theorem) apresentada em [56]. Nosso
objetivo nesta dissertacao e fazer um estudo comparativo em memórias associativas. Por esta razao apresentamos apenas um esboco da demonstracao desta conjectura. Uma demostracao formal requer detalhes que serao omitidos.
Seja X = [x1,..., xk ] G { —1,1}nxk e Y = [y1,..., yk ] G { —1,1}mxk onde xf e y? sao variaveis aleatórias independentes com probabilidade 1/2 de ser +1 ou —1 para todo j = 1,..., n, i = 1,..., m e £, n = 1,..., k. Sabemos que y? = sinal(wTxn) para n G {1,..., k} se e somente se y?wTxn &gt; 0 para todo i = 1,..., m e n G {1,..., k}. A matriz dos pesos sinapticos da BAM e W = YXT e
n k
wTx	yi■■4.
j=1 ?=1
(5.13)
Logo,
k n	k n
yn (wTxn)	J^y/yfxfx? = n '	y yfxfxn = n + r’	(544)
f=i j=i	f=n j=i
onde r representa o ruído.
O termo do ruído pode ser positivo ou negativo e seraí nosso objetivo estimar o seu valor. Para tanto, vamos tomar
k
zj	lí yi7 yf xf xj-	(545)
f=n
Deste modo, o termo do ruído sera
n
r = £ zj •	(5.16)
j=i
Sabemos que y?, yj, xj e xj sao variaveis aleatórias independentes e identicamente distribuídas (i.i.d.), portanto y?yjxfxj = ±1 com a mesma probabilidade. Sendo assim, podemos seguir a demonstracao rigorosa de McEliece et. al. [56].
Apos aplicar o teorema central do limite, encontramos
Pr [yn = sinal (wTxn)] = 1
1 + erf
5
(5.17)
para n, k suficientemente grandes com k/n pequeño. A l'uiicao erf(x) é afuncao erro dada por
2 í'x 2 erf(x) =	e ú dt.	(5.18)
Vn Jo
Para valores grandes no argumento, a funcao erro pode ser aproximada por
1 2 erf(x) « 1-----=ex .	(5.19)
Finalmente,
Pr [yn = sinal (Wxn)] = Pr [y? = sinal (wTxn) , V i = 1,..., m]
K1 -	exp (-2k)-
(5.20)
(5.21)
pois as componentes de X e Y sao variaveis aleatórias i.i.d. Dizemos que f g se f (x)/g(x)	1
quando x x. Seguindo o resultado apresentado em [3], teremos yn = sinal (Wxn) com probabilidade proxima de 1 se k &amp;lt;n/(2 log m). Analogamente, deveremos ter k &amp;lt;m/(2 log n) para termos xn = sinal (WTyn) com probabilidade proxima de 1. Logo, a memoria fundamental (xn, yn) sera um ponto estacionario se
í n m \ ylog m’ log n J
1
&amp;lt;- min
“ 2
k
(5.22)
Se queremos y^ = sinal (Wx^), para todo £ = 1,..., k, entao devermos ter
Pr (Y = sinal (WX)) = [Pr (y^ = sinal (wTxn}}]mk .	(5.23)
Esta probabilidade sera proxima de 1 se k &amp;lt;n/(4log m). Analogamente, Pr (X = sinal (WTY)) sera proxima de 1 se k &amp;lt;m/(4log n). Finalmente, todos as memorias fundamentais serao pontos fixos da BAM com probabilidade proxima de 1 se
k &amp;lt;- min
_ 4
í n m \ \log m’ logn J
(5.24)

para n, m, k suficientemente grandes.
Note que a conjectura 5.2.1 coincide com o teorema 5.1 se m = n.
Fig. 5.5: Probabilidade das memórias fundamentais serem pontos fixos na BAM por k. A linha marcada com o representa a probabilidade empírica de uma dada memória fundamental ser ponto fixo e a linha marcada com □ representa a probabilidade empírica de todas as memórias fundamentais serem pontos fixos. A linha tracejada representa a capacidade de armazenamento obtida usando a equacao 7.15 (probabilidade teórica). As linhas pontilhadas verticais representam os valores min(m/ log(n), n/ log(m))/2 e min(m/ log(n), n/ log(m))/4.
Exemplo 5.2.2. Considere o conjunto {(x^, ), G {-1,1}100, G {-1,1}80, £ = 1,..., k} gerado aleatoriamente com distribuição uniforme. Armazenamos este conjunto de memórias fundamentais na BAM. Depois verificamos se a associaçao (x1, y1) e um ponto estacionario e se todas as memórias fundamentais (x^, y^), £ = 1,..., k, sao pontos estacionarios. Repetimos o experimento 1000 vezes para diferentes valores de k (mímero de memórias fundamentais) e calculamos as probabilidades empíricas de termos pontos estacionarios. Na figura 5.5 apresentamos com linha marcada com o a probabilidade empírica de uma certa memória fundamental ser um ponto estacionario e com linha marcada com □ a probabilidade empírica de todas as memórias fundamentas serem pontos estacionarios. A linha tracejada representa uma estimativa teórica para esta probabilidade, que sera chamada capacidade de armazenamento no capítulo 7. No eixo horizontal colocamos k, o numero de memórias fundamentais. As linhas pontilhadas verticais indicam os valores min(m/ log(n),n/ log(m))/2 e min(m/ log(n),n/ log(m))/4, respectivamente. Note que a probabilidade empírica de todos os padrões serem pontos fixos deixa de ser 1 quando k &gt; min(m/ log(n),n/ log(m))/4. A probabilidade empírica de todas as memórias fundamentais serem pontos fixos para k = 8, sendo min(m/ log(n),n/ log(m))/2 = 8, 7 foi 0, 83, entretanto, a probabilidade empírica de uma certa memória fundamental ser ponto fixo foi 0, 97. Estes resultados numericos conferem com a conjectura 5.2.1. Lembre-se que n = 100 e m = 80 neste exemplo.
5.3	Memória Associativa de Personnaz
Personnaz et. al. apresentaram uma variacao da memória associativa de Hopfield que utiliza o armazenamento por projecao para obter a matriz dos pesos sinapticos [67]. Este modelo e referido na literatura simplesmente como “rede de Hopfield com armazenamento por projecao” [3, 31], mas sera referido neste trabalho como Memória Associativa de Personnaz (Personnaz Associative Memory, Personnaz AM) porque apresenta caracteríticas diferentes do modelo classico de Hopfield introduzido na secao anterior e tambem porque caracterizamos uma rede neural pelo modelo dos neurônios, arquitetura e aprendizado (veja Capítulo 2). Logo, regras de aprendizado distintas produzirao redes neurais (ou memórias associativas) distintas.
5.3.1	Arquitetura da Rede
A memória associativa de Personnaz e descrita por uma rede neural classica recorrente de camada unica totalmente conexa com a funcao sinal como funcao de ativacao. A arquitetura da memória associativa de Personnaz esta apresentada na figura 2.6. Este modelo e descrito pela equacao 5.1 (ou equacao 5.3). A diferenca entre a memória associativa de Personnaz e a memória associativa de Hopfield estaí na regra de armazenamento.
5.3.2	Aprendizado
A matriz dos pesos sinapticos da memória associativa de Personnaz e
W = XXf,	(5.25)
onde X = [x1, x2,..., xk] G {-1,1}nxk e a matriz com as memórias fundamentais.
Proposição 1. Todas as memórias fundamentais x1,..xk ser ao pontos fixos da memória associativa de Personnaz.
Demonstração. O problema minW ||X — WX || = 0 sempre tem solucao (em particular W = I) e sinal(WX) = sinal(X) = X.	□
Proposição 2. Seja X G { — 1,1}nxk. Se W = XXt entao W = WT e 0 &amp;lt;wii &amp;lt;1, para todo i = 1,... ,n.
Demonstração. Considere a decomposicao em valores singulares (decomposicao SVD)
r
X = UXVT	ViUxT,	(5.26)
i=1
onde u1un e v1vn sao os vetores singulares da esquerda e da direita, respectivamente, ai sao os valores singulares e r e o posto da matriz X [101, 25]. Sabemos que
r
X1 = VSfUT =	a-1 VjuT,	(5.27)
j=1
e portanto,
W = XX1 =

-1	TI
vu I
r r
V V '' u v V u •
i=1 j = 1
(5.28)
Os vetores vb • • •, vn sao ortogonais, logo vf Vj = ôj e
rr
W = xx 1 = ^2 aí a-1ujuf = 52 uj•
(5.29)
j=i
j=1
Logo, wh	Vj=1 u2j &gt; 0, onde uj e a i-esima componente do vetor uj. Por outro lado, sabemos
que a matriz U e ortogonal, logo I = UUT e
n	r	n	n
i = 52	= 52 x +52	= Wii +52	•	(5-30)
j=1	j=1	j=r+1	j=r+1
Logo,
n
0 &amp;lt;Wii = 1 - Y, uij &amp;lt;1	(5.31)
j=r+1
Pela Equacao (5.29) concluímos tambem que
	r WT = (è uuT j=1	Tr 1 = èu uf=w, j=1	(5.32)
ou seja, W = WT.			□
5.3.3 Convergência
A Proposicao 2 e o Teorema 5.1.1 mostram que a memória associativa de Personnaz com atualizacao assíncrona sempre converge para um ponto fixo.
Exemplo 5.3.1. Considere os padrões bipolares apresentados na figura 1.1. Armazenamos estes padrões na memória associativa de Personnaz e verificamos que todas as memórias fundamentais sao pontos fixos conforme a proposicao 1.
Considere agora os padrões ruidosos apresentados na figura 4.5. Estes padrões foram gerados a partir dos padrões x1, • • •, x5 G {-1,1}4096 da figura 1.1 intoduzindo ruido uniforme com probabilidade 0, 3 de reverter o valor do pixel. A memória associativa de Personnaz encontrou as memórias fundamentais no final da primeira iteracao.
5.4 Memória Associativa de Kanter-Sompolinsky
Kanter e Sompolinsky discutiram outra variacao da memória associativa de Hopfield que utiliza o armazenamento por projecao, neste caso com diagonal nula, para obter a matriz dos pesos sinapticos [44]. Este modelo, tambem referido na literatura como “memória associativa de hopfield com armazenamento por projecao” [3, 31], sera referido neste trabalho como Memória Associativa de Kanter-Sompolinsky (Kanter-Sompolinsky Associative Memory, Kanter-Sompolinsky AM).
5.4.1	Arquitetura da Rede
A memoria associativa de Kanter-Sompolinsky e descrita por uma rede neural classica recorrente de camada ilnica totalmente conexa com a funcao sinal como funcao de ativacao. A arquitetura da memoria associativa de Kanter-Sompolinsky esta apresentada na figura 2.6. Este modelo e descrito pela equacao 5.1 (ou pela equacao 5.3).
5.4.2	Aprendizado
A matriz dos pesos sinapticos da memoria associativa de Kanter-Sompolinsky e
M = XXt, com mu = 0 Vi = 1,..., n,	(5.33)
onde X = [x1, x2,..., xk] G {-1,1}nxk e a matriz com as memórias fundamentais.
Proposição 3. Todas as memórias fundamentais x1,..., xk serão pontos fixos da memoria associativa de Kanter-Sompolinsky.
Demonstracao. A matriz dos pesos sinapticos da memoria associativa de Kanter-Sompolinsky e M = W — D, onde D e uma matriz diagonal n x n com 0 &amp;lt;dn &amp;lt;1 e W e obtida resolvendo o problema da equacao 4.8. Note que Mxf = Wxf — Dxf = (I — D)xf para toda memoria fundamental xf. Logo, 1 — dn &gt; 0, para todo i = 1,..., n e pela definicao da funcao sinal temos que
sinal (mTxf) = sinal ((1 — dií)x0 = xf,	(5.34)
para todo i = 1,..., n e para todo £ = 1,..., k.	□
Note que a diferenca entre a memoria associativa de Kanter-Sompolinsky e a memoria associativa de Personnaz esta na diagonal nula da matriz dos pesos sinapticos do primeiro modelo. Por causa da diagonal nula, a memoria associativa de Kanter-Sompolinsky possui uma tolerância a ruído melhor que a memoria associativa de Personnaz [44]. Verificamos este fato atraves de experimentos computacionais no capítulo 7.
5.4.3	Convergencia
A Proposicao 2 e o Teorema 5.1.1 mostram que a memoria associativa de Kanter-Sompolinsky com atualizacao assíncrona sempre converge para um ponto fixo.
Exemplo 5.4.1. Repetimos o mesmo experimento realizado no exemplo 5.3.1. Primeiro, verificamos que todas as memorias fundamentais sao pontos fixos da memoria associativa de Kanter-Sompolinsky. Depois apresentamos os padroes da figura 4.5 como entrada e verificamos que as recordacoes fundamentais x1,..., xk foram encontradas no final da primeira iteracao.
5.5	Memoria Associativa Bidirecional Assimétrica
A Memoria Associativa Bidirecional Assimétrica (Asymmetric Bidirectional Associative Memories, ABAM), tambem conhecida como Projection HDAM e uma extensao da memoria associativa de Personnaz para o caso heteroassociativo [109, 32].
5.5.1	Arquitetura
A memória associativa bidirecional assimetrica e uma rede neural classica recorrente totalmente conexa descrita pelas equacóes
y(t) = sinal (Wix(t)),
x(t + 1) = sinal (W2y(t)), para t = 0,1, 2,...
(5.35)
(5.36)
com atualizacao assíncrona. A arquitetura desta rede esta apresentada na figura 5.3.
5.5.2	Aprendizado
Considere os pares de padrões (x^, y^), onde G {-1,1}n e y^ G {-1,1}m, para todo £ = 1,..., k. Tome X = [x1, x2,..., xk] e Y = [y1, y2,..., yk]. As matrizes dos pesos sinapticos, W e W2, sao encontradas usando o armazenamento por projecao, isto e,
Wi = YX1 e W2 = XY1.	(5.37)
Teorema 5.5.1. Sejam X = [x1,..., xk] G { —1,1}nxk e Y = [y1,..., yk] G { —1,1}mxk as matrizes das memórias fundamentais {(x^, y^), £ = 1,..., k}. Se X e Y possem ambas posto completo, então todas as memorias fundamentais serão pontos estacionários.
Demonstração. Se X tem posto completo, entao Xt = (XTX)-1XT e
sinal (W1X) = sinal (Y(XTX)-1 XTX) = sinal(Y) = Y.	(5.38)
Analogamente, substituindo X por Y e vice-versa, encontramos sinal(W2Y) = X se Y tem posto completo.	□
Nao garantimos a convergencia da ABAM porque
0 W2

Wi	0
(5.39)
nao e uma matriz simetrica (W2 = W^). Resultados empíricos mostraram que este modelo apresenta um comportamento oscilatorio principalmente quando o numero de padroes armazenados esta proximo ou e maior que min(m, n) [30].
Exemplo 5.5.1. Considere os padroes x1,..., x5 G {—1,1}4096 e y1,..., y5 G {—1,1}4096 apresentados nas figuras 1.1 e 1.2. Neste caso, as matrizes X e Y possuem ambas posto completo. Armazenamos o conjunto das memorias fundamentais {(x^, y^), £ = 1,..., 5} na ABAM e verificamos que todas as memorias fundamentais sao pontos estacionarios segundo o teorema 5.5.1. Depois apresentamos como entrada os padroes corrompidos x1,..., x5 apresentados na figura 4.5 e verificamos que os padroes recordados pela ABAM no final da primeira iteracao sao exatamente as recordacoes fundamentais.
5.6	Memória Associativa com Capacidade Exponencial
A Memoria Associativa com Capacidade Exponencial (Exponential Correlation Associative Memory, ECAM) pode ser vista como um caso particular das Memorias Associativas Recorrentes por Correlacao (Recurrent Correlation Associative Memories, RCAM) que serao discutidas nesta secao [15, 16, 32]. As memórias associativas recorrentes por correlacao podem ser vistas como generaliza-coes da memória associativa de Hopfield discreta com auto-alimentacao.
5.6.1	Arquitetura
Na memória associativa de Hopfield discreta com auto-alimentacao, a i-esima componente do vetor de estados, x(t + 1), e calculado atraves da equacao
x»(t + 1) = sinal (wTx(t)) ,	(5.40)
onde wT e a i-esima linha da matriz W. No armazenamento por correlacao temos
k
wT = £ xf (x? )T.	(5.41)
?=i
Portanto, a i-esima componente de x(t +1) e
x»(t + 1) = sinal	x?(x?)Tx(t)^ = sinal &amp;lt;x?, x(t) &gt; x?^ ,	(5.42)
onde &amp;lt;•, • &gt; representa o produto interno Euclidiano. O termo &amp;lt;, x(t) &gt; representa a correlação entre o estado atual da memória x(t) e o £-esimo padrâo armazenado x^. Podemos dizer, de um modo intuitivo, que quanto mais “parecido” o vetor x(t) for de x^, maior sera o valor de &amp;lt;x^, x(t) &gt;. Para enfatizar esta correlaçao, podemos aplicar uma funçao f no resultado de &amp;lt;x^, x(t) &gt;. A funçao f deve ser monótona nao-decrescente, pois desejamos que vetores pouco correlacionados apresentem um valor menor do que aqueles mais correlacionados.
Ao introduzir a funcao f, encontramos a seguinte expressao para a i-esima componente do vetor x(t + 1):
x»(t + 1) = sinal f (&amp;lt;xe, x(t) &gt;)xf^ .	(5.43)
Numa notacao matricial temos
x(t + 1) = sinal (XF (Xtx(t))) ,
(5.44)
onde F(•) = [f (•),..., f 0]T. Observando a equacao (5.44), podemos dizer que as RCAM’s sao redes neurais recorrentes de duas camadas totalmente conexa com neurônios classicos com funcao de ativacao f na primeira camada e funcao sinal na camada de saída. Na figura 5.6 apresentamos a arquitetura das RCAM’s.
XT	f (•)	X	sinal(-)
x1(t)
x2(t)
xa(t)
x„(t)
xi (t + 1)
x2 (t + 1)
x3 (t + 1)
x„(t + 1)
Fig. 5.6: Arquitetura das Memorias Associativas Recorrentes por Correlacao.
A função f introduzida representa um papel importante na capacidade e dinâmica da RCAM. Existem infinitas funções que podem ser usadas, mas precisamos que esta funçao seja facil de ser implementada e forneca uma RCAM assintoticamente estavel com uma capacidade de armazenamento grande. Apresentamos a seguir algumas funcoes usadas na RCAM. Depois apresentaremos condicoes suficientes para obtermos uma memória assintoticamente estavel.
1.	Para f (v) = v, encontramos a memória associativa de Hopfield discreta com armazenamento por correlacao.
2.	Para f (v) = (n+v)q, onde q e um inteiro maior que 1 e n e a dimensao dos vetores , encontramos a memoria associativa de ordem alta por correlacao (High-Order Correlation Associative Memory). A capacidade de armazenamento desta memória e assintoticamente proporcional a nq [16].
3.	Para f (v) = (n — v)-p, onde p &gt; 1, encontramos a memoria associativa comfuncao potencial por correlacao (Potential-Function Correlation Associative Memory). A capacidade de armazenamento deste modelo aumenta exponencialmente com a dimensao n dos padrões armazenados. Este modelo foi proposto independentemente por Dembo e Zeitouni, e por Sayeh e Han [16]. Apresentamos aqui uma versao com tempo discreto e estados bipolares, embora, originalmente tenha sido proposto com tempo contínuo e padrões com valores reais.
4.	Quando f (v) = av, com a &gt; 1, encontramos a memoria associativa com capacidade exponencial. Esta e a unica RCAM discutida neste trabalho.
5.6.2	Aprendizado
O aprendizado da RCAM e direto pois a matriz dos pesos sinapticos da camada de entrada e XT e a matriz dos pesos sinapticos da camada de saída e X .O armazenamento de um novo padrao xh e feito
concatenando as matrizes XT e X com (xh)T e xh, respectivamente.
5.6.3	Convergência
As quatro RCAMs apresentadas anteriormente (itens (1) - (4)) com atualizacao assíncrona possuem regioes de indecisao vazias. O teorema sobre a convergencia das RCAM’s foi enunciado e demonstrado por Chiueh e Goodman em [15].
Teorema 5.6.1 (Teorema da Convergencia de Chiueh e Goodman). Seja f (v) uma funcao contínua monotona nao-decrescente definida sobre um intervalo fechado. A RCAM descrita pela equacao (5.43) com atualizacao assíncrona sempre converge para um ponto fixo para qualquer padrao-chave apresentado a RCAM.
O seguinte teorema, introduzido por Chiueh e Goodman em [15], nos diz que o nilmero de memorias fundamentais que podem ser armazenadas como pontos fixos da ECAM aumenta exponencialmente com n, o numero de componentes dos padrões.
Teorema 5.6.2. Sejam x1,..., xk E {-1,1}”, com n suficientemente grande, padroes nao correlacionados gerados aleatoriamente com distribuicao uniforme. Todos as memorias fundamentais serao pontos fixos com grande probabilidade se k &amp;lt;ac”, onde c E [1, 2] é uma constante fixa que depende de a.
E importante observar que o valor da constante c do teorema acima decresce quando o valor de a diminui [16].
Exemplo 5.6.1. Repetimos o experimento realizado no exemplo 5.3.1. Verificamos que todas as memorias fundamentais sao pontos fixos da ECAM. Depois apresentamos os padrões ruidosos da figura 4.5 como entrada e verificamos que os padrões recordados sao as respectivas recordacoes fundamentais. Neste exemplo utilizamos a = 1, 007 (ou f (x) = 2(x/100)) para evitar overflow. A ECAM encontrou as recordacoes fundamentais com uma única iteracao.
5.7	Memoria Associativa Bidirecional com Capacidade Exponencial
A Memoria Associativa Bidirecional com Capacidade Exponencial (Bidirectional Exponential Capacity Associative Memory, BECAM) e uma generalizacao da memória ECAM para o caso heteroasso-ciativo. Este modelo de memoria associativa foi introduzido por nós nesta dissertacao de mestrado.
5.7.1	Arquitetura
A BECAM e descrita por uma rede neural classica recorrente com quatro camadas totalmente conexa com funcao sinal e funcao exponencial como funcoes de ativacao.
Seja {(x*, y*), £ = 1, 2,..., k} com x* E {-1,1}” e y* E {-1,1}m, para todo £ = 1,..., k. Na BAM, a i-esima componente do vetor de estados y (t) e dada por
y(t)i = sinal (wTx(t)) ,
(5.45)
5.7 Memoria Associativa Bidirecional com Capacidade Exponencial
45
onde wT e a i-esima linha da matriz W. Quando usamos o armazenamento por correlacao, temos wT = V?' |	(xf)T. Assim, a i-esima componente de y(t) e
y(t)i = sinal	yf(x)Tx(t)^ = sinal	x, x(t) &gt; yf^ ,
(5.46)
onde &amp;lt;•, • &gt; representa o produto interno Euclidiano. Podemos aplicar uma funcao f no valor &amp;lt;xf, x(t) &gt; a fim de enfatizar esta correlacao. As quatro funcões apresentadas na secao 5.6 podem ser usadas neste modelo e cada uma produz uma memória heteroassociativa diferente. Voltamos nossa atencao para a funcao f (v) = av, para a &gt; 1, que chamaremos de BECAM (Bidirectional Exponential Capacity Associativa Memory).
Encontramos a seguinte expressao para a i-esima componente do vetor y (t) quando introduzimos a funcao f:
yi(t) = sinal	f (&amp;lt;x, x(t) &gt;)y^
(5.47)
Numa notacao matricial temos
y(t) = sinal (YF (XTx(t))),
(5.48)
onde F(•) = [f (•),..., f (-)]T. Analogamente, podemos encontrar a seguinte expressao para x(t +1):
x(t + 1) = sinal (XF (Yty(t))) .
(5.49)
Observando as equacoes (5.48) e (5.49) percebemos que a BECAM e uma rede recorrente com quatro camadas. Na figura 5.7.1 apresentamos a arquitetura da BECAM.
	X T	f (•)	Y	sinal(-)	Y T	f (•)	X	sinal(-)
X1 (t) 	&gt; O —				O —yi(t)^- O				H O —&gt; xi(t + 1)
		71 O =3						
X2(t)  &gt; O				o —y2'í — O __				33 O —&gt; x2(t +1)
		3b O 33				3b 1		
X3(t) 	&gt; O ■"				O 	V3(t)^ O "				O —&gt; X3(t + 1)
X„(t)
O  Vm(t)^~ O
Fig. 5.7: Arquitetura da Memoria Associativa Bidirecional com Capacidade Exponencial.
Xn(t + 1)
5.7.2	Aprendizado
As matrizes dos pesos sinapticos da BECAM sao: XT, Y, YT e X. Note que o armazenamento de um novo par (xh, yh) e feito concatenando as matrizes XT, X, YT e Y com (xh)T, xh, (yh)T e yh, respectivamente.
5.7.3	Convergencia
A convergencia da BECAM pode ser verificada convertendo ela numa ECAM. Tome
X,
0,
x
y
0
Y
e X' =
x
/
(5.50)
como sendo os padroes e a matriz dos pesos sinápticos, respectivamente. Pela equacao (5.44), teremos
x(t + 1)
y(t +1)
sinal
sinal
0
Y
0
Y
X
0
X
0
YT
0
= sinal
Y T y(t)l\\ X T x(t)JJJ
Xf (YTy(t)) Yf (XTx(t))
Exemplo 5.7.1. Repetimos o experimento realizado no exemplo 5.5.1. Verificamos que todas as memórias fundamentais sao pontos fixos da BECAM. Depois apresentamos os padrões ruidosos x1,..., x5 da figura 4.5 como entrada e verificamos que os padrões y1(1),..., y5(1) recordados no final da primeira iteracao sao as respectivas recordacóes fundamentais. Neste exemplo utilizamos a = 1, 007 (ou f (x) = 2(x/100)) para evitar overflow. A BECAM encontrou as recordacóes fundamentais com uma ilnica iteracao.
5.8	Modelo do Estado Cerebral numa Caixa (BSB)
O Modelo do Estado Cerebral numa Caixa (Brain-State-in-a-Box, BSB) foi proposto por Anderson et al. em 1977 [6] como uma rede recorrente de tempo discreto, nao-linear e totalmente conexa. Como nos modelos anteriores, o modelo do estado cerebral numa caixa pode ser visto como uma memória auto-associativa que minimiza a energia de um sistema dinamico nao-linear.
5.8.1	Arquitetura
O modelo BSB e uma rede neural classica recorrente com tempo discreto de camada unica. Seja x(t +1) o vetor de estado no tempo t + 1. A recorrencia e dada por aWx(t), onde W e a matriz dos pesos sinapticos e a e uma constante positiva que controla o peso deste termo. Na BSB, adicionamos um termo de decaimento ao termo de recorrencia dado pelo vetor x(t), o estado anterior. Assim, o próximo estado, x(t + 1), sera dado por
x(t + 1) = L (x(t) + a (Wx(t) + 0)).
(5.51)
Neste trabalho consideraremos 0 = 0. A funcao de ativacao usada no modelo BSB e a funcao linear por partes definida como
L(x) = 1 A [(-1) V x].	(5.52)
Note que esta funcao limita os vetores de estados numa regiao restrita do espaco, especificamente, no hipercubo Hn = [-1,1]n. Daí o nome: Modelo do Estado Cerebral numa Caixa [46].
5.8.2	Aprendizado
O modelo original da BSB proposto por Anderson et al. utiliza o aprendizado de Hebb (armazenamento por correlacao) onde tomamos W = XXT [6]. Neste caso, a matriz dos pesos sinapticos e simeítrica, semi-definida positiva e com diagonal positivia.
Outras regras de aprendizado podem ser utilizadas na BSB, por exemplo, o armazenamento por projecao ou o armazenamento iterativo proposto em [66]. Neste trabalho discutiremos somente o armazenamento por correlacao.
5.8.3	Convergencia
A BSB sempre converge para um ponto fixo independente da memória-chave apresentada e minimiza
a funcao energia
Energia(x) = —1 xT W x.
(5.53)
se impormos pequenas condicoes ao modelo [41, 23, 24, 66].
Teorema 5.8.1 (Teorema de Golden da Minimização da Função Energia do Modelo BSB). Considere o modelo neural descrito pela equação (5.51). Se W = WT e semi-definida positiva ou a &amp;lt;(2/| Amin|), onde Xmin é o menor autovalor da matriz simétrica W, então:
1.	Energia(x(t + 1)) &amp;lt;Energia(x(t)) se x(t + 1) = x(t),
2.	Energia(x(t + 1)) = Energia(x(t)) se e somente se x(t + 1) = x(t),
onde Energia(x) e afunçao definida na equaçao (5.53).
A demonstracao do teorema 5.8.1 pode ser encontrada em [24].
Teorema 5.8.2. Seja W E Rnxn com wn &gt; 0 para i = 1,... ,n. Se x E [—1,1]n e um ponto fixo da BSB então x E { — 1,1}n, isto e, x e um vértice do hipercubo [—1,1]n.
A demonstracao do teorema 5.8.2 encontra-se em [66]. Note que o teorema 5.8.2 nao impõe nenhuma hipótese sobre a simetria da matriz dos pesos sinapticos. O seguinte teorema foi introduzido por nós e relaciona o modelo BSB com a memória associativa de Hopfield.
Teoremã 5.8.3. Um padrão x G { — 1,1}n sera um ponto fixo da BSB (com 0 = 0) se e somente se x for um ponto fixo da memória associativa de Hopfield com a mesma matriz de pesos sinapticos.
Demonstração. Seja x G {-1,1}n. Pela definicao da funcao linear por partes, da funcao sinal e lembrando que a &gt; 0, temos:
x = L(aW x + x) o	(a(Wx)i + xi) xi &gt; 1 Vi = 1,..., n	(5.54)
O	axi(Wx)i + 1 &gt; 1 Vi = 1,..., n	(5.55)
o	xi (Wx)i &gt; 0 Vi = 1,..., n	(5.56)
o	x = sinal(W x).	(5.57)
□
Note que o teorema 5.8.3 nao impoe nenhuma hipotese sobre a matriz dos pesos sinapticos W. Portanto, este resultado permanece valido para outras regras de aprendizado, por exemplo, para o armazenamento por projecao. Note tambem que nao impomos nenhuma condicao sobre a constante a, exceto a &gt; 0. Finalmente, observe que este resultado so e valido para padroes nos vertices de [—1,1]n. Poderemos ter pontos fixos na BSB que estao no iterior de [-1,1]n se as hipoteses do teorema 5.8.2 nao forem satisfeitas. Estes pontos fixos da BSB obviamente nao serâo pontos fixos da memoria associativa de Hopfield e suas variacoes.
Capítulo 6
Memorias Associativas Morfológicas
Neste capítulo discutimos as memórias associativas morfológicas (Morphological Associative Memories, MAM). Discutimos primeiro o caso heteroassociativo em tons de cinza apresentando exemplos e resultados sobre a capacidade de armazenamento e tolerância a ruído. Depois voltamos nossa atencao para o caso auto-associativo. Terminamos o capítulo discutindo o caso auto-associativo binario e as memorias associativas de duas camadas.
6.1	Memorias Associativas Morfologicas Heteroassociativas
As memorias associativas morfologicas sao redes neurais descritas pelo modelo neural morfologico e foram introduzidas em [73, 74, 76]. O caso mais simples ocorre quando temos uma rede alimentada adiante, totalmente conexa e de camada línica, cuja funcao de ativacao e a funcao identidade. Neste caso, o mapeamento associativo da memoria G : Rn ■ Rm e descrito por uma matriz W G Rmxn ou M G Rmxn e o produto-mínimo ou o produto-maximo. Dado um padrâo-chave x G Rn, encontramos o padrao recordado y G Rm através da equacao
y = W 0 x,	(6.1)
ou
y = M H x.	(6.2)
Repare na semelhanca entre o mapeamento associativo das memorias associativas morfologicas he-teroassociativas e o mapeamento associativo das memorias associativas lineares. A diferenca esta no produto-maximo ou produto-mínimo usado nos modelos morfologicos. Lembre-se que os modelos morfologicos apresentam um comportamento nao-linear devido a estas duas operacoes.
Podemos fazer uma interpretacao das equacoes (6.1) e (6.2) usando a morfologia matematica [85, 86, 87]. Podemos verificar que os operadores 5(x) = W0 x e s(x) = MH x sao operacoes de dilatacao e erosao na morfologia matematica, respectivamente. Desta forma, a tarefa na fase de armazenamento das memorias associativas morfologicas seria encontrar uma dilatacao (erosao) tal que 5(x^) = y^ (s(x^) = y^) para todo f = 1,..., k. Nao usaremos esta interpretacao na fase de armazenamento, mas podemos extrair resultados interessantes (e intuitivos) para a fase de recordacao. Por exemplo, a dilatacao (erosao) e um operador extensivo (anti-extensivo), isto e, a dilatacao (erosao) expande (reduz) um objeto preto de uma imagem com fundo branco. A memoria
associativa morfológica descrita pela equacao (6.1) (Equacao (6.2)) tambem possui esta característica. Alem disso, uma dilatacao (erosao) e usada para remover ruído erosivo (dilativo) de uma imagem. Dizemos que uma imagem x e uma versao de x corrompida com ruído erosivo (dilativo) se x &amp;lt;x (x &gt; x).
6.1.1	Aprendizado
Suponha que armazenamos um único par (x, y) na memória. Pelas equações (6.1) e (6.2), encontramos, respectivamente,
n
yi = (W 0 x)i = \/ (wij + xj),	(6.3)
j=i
e
n
yi = (M 0 x)i = /\ (Wij + xj).	(6.4)
j=i
Pela equaçao (6.3) temos que yi &amp;lt;wij + Xj, ou seja, yi — Xj &amp;lt;wij. Podemos definir W atraves da equaçao wij = yi — Xj. Usando a notaçao matricial da algebra de imagens, temos
yi - xi
y1 xn
= y s x*,
(6.5)
ym x1	• • •	ym	xn
onde x* = — xT é o conjugado aditivo do vetor x.
Podemos verificar que WXY recupera o padrao y quando apresentamos x como entrada. De fato,
Vn=i (yi - xi + xi)
Wxy E x =
= y.
(6.6)
Vi=1 (ym	xi + xi)
Repare na semelhanca entre as equacoes (6.5) e (4.3) do armazenamento por correlacao quando armazenamos um unico padrao. Por analogia a (4.3), quando armazenamos o conjunto de memórias fundamentais {(x^, y^), £ = 1,..., k}, temos
Wxy = Y 0 X *,
(6.7)
onde X = [x1, x2,..., xk] e Y = [y1, y2,..., yk]. Note que usamos 0 para gerar a matriz WXY (equacao (6.7)) e usamos 0 na fase de recordacao (equacao (6.6)).
Partindo de (6.4) e seguindo um raciocínio anaiogo, encontramos
Mxy = Y I X*,
(6.8)
como sendo a matriz dos pesos sinápticos da memória associativa morfológica descrita pela equação
(6.2).
Exemplo 6.1.1. Seja
	0	0	0		0	-1	0
X =	0	-2	-3	e Y =	1	-1	-2
	0	-4	0		0	0	0
De acordo com a equacao (6.7) temos
Wxy = Y H X *
000		-1 1 3		030
111	H	-1 1 3	H	-2 1 -2
000		0	2 4		030
-10 0
—2 1 —2
000
(6.10)
e pela equacao (6.8) encontramos
e
Mxy = Y H X *
000		-1 1 3		030		033
111	V	-1 1 3	V	-2 1 -2	=	113
000		0	2 4		030		034
(6.11)
Podemos verificar que WXY V x^ = y^
MXY H xe, para todo £ = 1, 2, 3. Por exemplo,
Wxy V x1
-1	0	0		0
-2	1	-2	V	0
0	0	0		0
Mxy H x2
033		0	
113	H	-2	=
034		-4	
-1
-1
0
= y1,
(6.12)
(6.13)
0
1
0
= y2.
Chamamos a atenção do leitor para o fato de que 0 (x^) * = y^ 0 (x^) *. Podemos reduzir a notação denotando estes produtos externos morfológicos por y+ (x^) *. Desta forma, as equações
(6.7)	e (6.8) possuem um anaiogo a equaçao (4.3), isto e
k	k
Wxy = f\ (y€ + (x€), e Mxy = \/ (y€ + (x€).	(6.14)
€=1	€=1
Podemos adicionar facilmente um novo par de padrões nas memórias WXY e MXY usando as equações de (6.14). Se armazenamos os pares (x^, y^), para £ = 1,..., k,e desejamos adiçionar um novo par (xk+1, yk+1), entao definimos
ou
WXy“ = WXY°z h y
k+1 + (-xk+1)
yk+1 + (-xk+1) T
(6.15)
(6.16)
k+1 c,

= M'#"
V
Entretanto, nao podemos excluir um padrâo armazenado.
A seguinte definicao, introduzida em [76], diz respeito ao armazenamento das memórias fundamentais em uma memoria associativa morfológica.
Definição 6.1.1. Uma matriz A e uma memoria 0 -perfeita para (xf, yf), com £ = 1,..., k, se e somente se, A 0 xf = yf, para todo £ = 1,..., k. Analogamente, uma matriz B e uma memoria El -perfeita para (xf, yf), com £ = 1,..., k, se e somente se, BE xf = yf, para todo £ = 1,..., k.
Pela definicao, se X = [x1,..., xk], Y = [y1,..., yk] e A e uma memoria 0 -perfeita, entao A 0 X = Y .Se B e El -perfeita, entao B l X = Y. Temos tambem que, se A e 0 -perfeita, entao (A 0 xf). = yf para todo £ = 1,..., k e todo i = 1,..., m. Equivalentemente,
n
V (%■ + j = yf, V£ = 1,...,kei = i,...,m.	(6.17)
j=i
Da equacao (6.17) obtemos a seguinte desigualdade para um índice j G {1,..., n} arbitrario:
	&amp;lt;yf,	V£ = 1,..		.,k	(6.18)
O	Gjj	&amp;lt;yf - ,	V£	= 1,..	.,k	(6.19)
O X	k (yi - f=1	xj )	= •		(6.20)
Temos com isso que A &amp;lt;WXY e consequentemente
Y = A E X &amp;lt;Wxy E X.	(6.21)
Pela equacao (6.14), temos WXY &amp;lt;yf x (xf) * &amp;lt;MXY para todo £ = 1,..., k e usando a equacao (6.6) concluímos que WXY E xf &amp;lt;(yf x (xf) *) E xf = yf = (y; x (xf) *) E xf &amp;lt;MXY E xf, para £ = 1,..., k, ou equivalentemente,
Wxy E X &amp;lt;Y &amp;lt;Mxy E X.	(6.22)
Finalmente, pelas equacoes (6.21) e (6.22) concluímos que
Y = A E X &amp;lt;Wxy E X &amp;lt;Y,	(6.23)
e portanto WXY 0 X = Y. Um argumento similar mostra que se B e E -perfeita para (xf, yf), para todo £ = 1,..., k, entao MXY &amp;lt;B e MXY E X = Y.
Teorema 6.1.1. Se A é E -perfeita e B é E -perfeita para (xf, yf), com £ = 1,..., k, entao
A &amp;lt;WXY &amp;lt;Mxy &amp;lt;B e WXY E X = Y = MXY E X.	(6.24)
Este teorema mostra que WXY e o maior elemento (maximo) do conjunto das memorias E -perfeitas e MXY e o menor elemento (mínimo) do conjunto das memorias E -perfeitas. Alem disso, se existe uma memoria E -perfeita, entao WXY e tambem E -perfeita. O mesmo vale para MXY, substituindo E por E .
O seguinte teorema fornece uma condicao necessarias e suficientes para o perfeito armazenamento das memorias fundamentais em uma memoria associativa morfologica no caso hetero-associativo [73, 76].
Fig. 6.1: Padrões recordados pela memória associativa morfológica MXY após apresentarmos as chaves fundamentais x1,..., x5 como entrada.
r
Teorema 6.1.2. A matriz WXY e 0 -perfeita para (x^, y^), com £ = 1,..., k, se e somente se, para cada £ = 1,..., k, cada linha da matriz [y^ x (x^) *] — WXY contém um elemento nulo. Por dualiade, a matriz MXY e El -perfeita para (x^, y^}, com £ = 1,..., k, se e somente se, para cada £ = 1,..., k, cada linha da matriz MXY — [y^ x (x^) *] contem um elemento nulo.
Note que o teorema 6.1.2 nao contem nenhuma hipotese sobre o nUmero maximo de memorias fundamentais. Logo, podemos armazenar quantos padroes desejarmos, desde que, cada linha de y^ x (x^ — WXY e Mxy — y^ x (x^ contenha um elemento nulo para cada £ = 1,..., k. Exemplo 6.1.2. No exemplo 6.1.1, verificamos que WXY 0 x^ = y^ = MXY 0 x^, para £ = 1, 2, 3, ou seja, WXY e MXY sao memorias 0 e E -perfeitas, respectivamente. Logo, elas satisfazem as condicoes do teorema 6.1.2. Por exemplo, para £ = 2, temos que
W11	=	= vl-	x2 = 1	y2	X	(x2 )*]	11 ,		^13	=	= V12-	x2 = x3	=	y2	X	(x2)*	13 ,	
W22	=	= vl-	x2 = x2 =	y2	X	(x2 )*	22 ,	e	m22 =	=	x2 = x2 =	y2	X	(x2)*	22 ,	(6.25)
W31 =	=	x2 = 1	y2	X	(x2)*	31 ,		m33	=	= V32-	x2 = x3	=	y2	X	(x2)*	33 .	
Exemplo 6.1.3. Considere os padrões binários x1,..., x5 e y1,..., y5 apresentados nas figuras 1.1 e 1.2. Armazenamos estes padrões nas memórias associativas morfológicas MXY e Wxy. Verificamos que Wxy 0 x- = y-, para todo £ = 1,..., k. Entretanto somente as equacóes Wxy 0 x1 = y1 e Wxy 0 x5 = y5 valem para a memória associativa morfológica MXY. Na figura 6.1 apresentamos os padrões recordados pela memória associativa morfológica MXY após apresentarmos as chaves fundamentais x1,..., x5 como entrada. O erro quadratico normalizado
lly- - mxy 0 x- ||	(626)
- (6.26)
lly- h
calculado sobre os padrões x2, x3 e x4 foi 0, 068, 0, 034 e 0, 016, respectivamente.
Teorema 6.1.3. Sejam X = [x1,..., xk] G {0,1}nxk e Y = [y1,..., yk] G {0,1}mxk matrizes geradas aleatoriamente com probabilidade 1/2 de um elemento ser 0 ou 1. Se Wxy = Y 0 X* e Mxy = Y 0 X * entao
Pr (Wxy 0 X = Y) = Pr (Mxy E X = Y)
1 - 1L - 30LG1.1
4	4k-1
(2
k—1
n—1
k—1
! -i mk k—1\ n—1
1
4
D
Demonstração. Vamos demonstrar o teorema 6.1.3 somente para a memória associativa morfológica WXY. O resultado para MXY pode ser obtido de modo anaiogo. Durante a demonstracao usaremos o fato de xf e serem variaveis aleatórias independentes e identicamente distribuídas para todo z = l,...,rn, j = l,...,n e £ = l,...,k
Sabemos que WXY 0 X &amp;lt;Y para todo X e Y. Assim, WXY 0 X = Y se e somente se WXY 0 X &gt; Y. A probabilidade de WXY 0 X &gt; Y sera obtida atraves da probabilidade de Vj=i (w j + xj) &gt; y 1- O calculo desta ultima sera dividida em quatro casos distintos, disjuntos e equiprovaveis. No final teremos Pr(V”= 1 (w 1j + xj) = y 1) = (Pr(Caso 1) + Pr(Caso 2) + Pr(Caso 3) + Pr(Caso 4))/4.
Caso (1). Se x1 = 1 e y1 = 1, teremos
Pr (w 1 1 &gt; 0) = Pr í	(yf — xf) &gt; 0 J = Pr í 0 A	(yf — xf) &gt; 0
Y= 1	/	\	f=2
Pr (w 1 j + xf &amp;lt;0) = Pr	(yf - xf + xj )	&amp;lt;0^ = Pr ^1 A	(yf - xf	+	xj ) &amp;lt;0^
(k	\	k 1	/1 \	k — 1
/\(yf - xf + xj)	&gt;	0l =1 - [Pr((yf - xf + xj) &gt;	0)] =1	,
Pr (wij	+	xj)	&gt; 1^ = 1-Pr	(wij	+ xj) &amp;lt;1^ =	1-[Pr (wj + xj &amp;lt;0)]" 1 = 1-
Assim,
f=2
k-
k-
-i n— 1
i -( 2)
k-1\ n-1
k-1
Pr ( V (wij + xj) &gt; y1	= Pr ((wii + 1 ) V (y (wb- + xj)^ &gt; 1
= Pr ((w11 + 1 ) &gt; 1 ) + Pr ( V (w1j + xj) &gt; 1	— Pr ((w11 + 1 ) &gt; 1 ) Pr (y (w1j + xj) &gt; 1^
- (- (í m-f r)	"
Caso (2). Se x1 = 1 e yj = 0, teremos
Pr (y (w 1 j + xj) &gt; yj^ = Pr ((w 11 + 1 ) V (y (w 1 j + xj)^ &gt; 0^ = Pr ((w11 + 1) &gt; 0) + Pr (y (w1j + xj) &gt; 0^ — Pr ((w11 + 1) &gt; 0) Pr (y (w1j+ xj) &gt; = 1 + Pr (y (w1j + xj) &gt; 0^ - Pr (y (w1j + xj) &gt; 0^ =1.
Caso (3). Se x1 = 0 e y1 = 1, teremos
(k	\	/	k	\	i f 1 \ k— 1
A(yj - xj) &gt; 1| = Pr í1 A f\ (yj - xj) &gt; n = [Pr (yj - xj) &gt;	= (j)	’
Assim,
Pr V (w1j + xj) — y1	= Pr ^W11 V V (wj + xj)
Pr (w11 — 1 ) + Pr V (w1j + xj) — 1	— Pr (w11 — 1 ) Pr V (w1j + xj) — 1
■—( ■—(4 Oí ■ - (2 )T	J
Caso (4). Se x} = 0 e yj = 0, teremos
(k	\	í k	i	k 1 f 3 \ k— 1
A (yj - xj) &gt; 0 I = Pr í 0 A A (yj - xj) &gt; 0 I = [Pr (yj - xj) &gt; 0	= (j)	’
f=2
k — 1
Pr (wij + xj &amp;lt;-1) = Pr A (yj - xj + xj) &amp;lt;-1^ = Pr ^0 A A (yj - xj + xj) &amp;lt;-1^ (k	\	kl	f 7 \ k—1
A (yj - xj + xj) &gt; -1! = 1 - /'A[,p - xj + xj) &gt; oj]	=1 -Ç,
Pr	(wj + xj) &gt; 0
Assim,
= 1-Pr	(wtj + xj) &amp;lt;0^ = 1-[Pr (w ij + xj &amp;lt;-1)]" 1 = 1-
1A T
q n— 1
Pr V (w 1 j+xj) — yi1^ = Pr ^w 11 v V (w 1 j+xj)
Pr (w 1 1 — 0) + Pr V (w 1 j + xj) — 0^ — Pr (w 1 1 — 0) Pr V (w j + xj) — 0^ ■ — (■ — (4)“')(■ — (8 )T
Finalmente teremos
Pr (y (w1j+xJ) = yi
' ’( ' (4)k—')( ■-(8)
'-8 (&gt; - )('-(8)
k-1 n-1
+ 1
k-1
k-1
'-( -I8U)( -«)-■) k "n 1 ■-(■ - (3 )k 4' -(8)
4 (' - (4 )k ■)(■ - (i)
k—1\n—1
k-1
k-1
k-1
k-1	n-1
1
4
)
)
k -1
) )
n—1
Portanto,
Pr (Wxy 0 X = Y) = [Pr (Wxy 0 x* = y*)]k
Pr (y (wj + x*) = y*)
-i mk
■-4(' -
3k—1 + 1
4k—1
k-1	n-1
k-1
k-1	n-1
-i mk
□
Exemplo 6.1.4. Considere o conjunto de padrões binarios {(x*, y*), x* G {0,1}100, y* G {0,1}80, £ = 1,..., k} gerado aleatoriamente com distribuyo uniforme. Armazenamos este conjunto de memórias fundamentais nas memórias associativas morfológicas WXY e MXY. Depois verificamos se y* = W0 x* para todo £ = 1,..., k. Repetimos o experimento 1000 vezes para diferentes valores de k (mímero de memórias fundamentais armazenados). Na figura 6.2 apresentamos com y a probabilidade empírica de y* = W 0 x* e com A a probabilidade empírica de y* = M 0 x*, para todo £ = 1,..., k. A linha tracejada representa a probabilidade teórica dada pela equacao (6.27). No eixo horizontal colocamos k, o numero de memórias fundamentais. Note que que as probabilidades empíricas Pr(W 0 X = Y) e Pr(W 0 X = Y) coincidiram e ambas sao menores que 1 para k &gt; 4. Este resultado mostra que as memórias associativas morfológicas binarias heteroassociativas nao conseguem armazenar muitas memórias fundamentais.
O seguinte teorema, introduzido em [73, 76], caracteriza a recordacao de um padrâo y7 após apresentarmos como entrada uma versao corrompida da chave fundamental xY. Um resultado analogo pode ser obtido para a memória associativa MXY usando o conceito de dualidade.
Teorema 6.1.4. Seja xY uma versao ruidosa do padrão xY. WXY 0 xY = yY se e somente se
~Y xJ
m k
&lt;xj0 AV(
-i=1 *=Y
vi- y* + J , v7 = 1,--
(6.27)

e para cada índice de linha i G {1,..., m}, existe um índice de coluna ji G {1,..., n} tal que
~Y rp I •J-x A .
Jl
k
= x0 V (vY - v*+4)
-*=7
(6.28)
Fig. 6.2: A linha com y representa a probabilidade empírica = W 0 para todo £ = 1,..., k. A linha com A representa a probabilidade empírica de y^ = M 0 x^ para todo £ = 1,..., k. A linha tracejada representa a probabilidade dada pela equacao 6.27.
O seguinte teorema, introduzido por Sussner em [93], determina precisamente a ação da memória associativa morfológica Wxy no caso binario. Um resultado anaiogo pode ser obtido para Wxy usando o conceito de dualidade.
Teorema 6.1.5. Seja X G {0,1}”xk, Y g {0,1}mxk e x G {0,1}”. Se 0 = x&amp;lt;\j|=1 x^ então
q
Wxy 0 x = VV y ,	(6.29)
t=i te©t
onde {015 02, • • •,	} e o conjunto de todos Qt Ç {1,..., k} tais que
\J x^ &gt; x .	(6.30)
íeet
Exemplo 6.1.5. Considere os padrões binarios x1,..., x5 e y1, • • •, y5 apresentados nas figuras 1.1 e 1.2. Armazenamos estes padrões na memória associativa morfológica Wxy. No exemplo 6.1.3 verificamos que Wxy 0 x^ = y^ para todo £ = 1,..., k. Considere agora os padrões x1,..., x5 apresentados na figura 6.3. Estes padrões foram obtidos introduzindo ruído salt (ruído erosivo) com probabilidade 0, 3. Verificamos que as recordacões fundamentais y1,..., y5 foram encontradas como saída após apresentarmos os padrões corrompidos x1,..., x5 como entrada. Repetimos o experimento 1000 vezes e verificamos que o erro quadrático medio normalizado
EQMN = E í1 V	~ W*Y B x1
v	|y II
Fig. 6.3: Padróes corrompidos com ruído salt com probabilidade 0, 3 usado no exemplo 6.1.5.
Fig. 6.4: Padróes corrompidos com ruído pepper com probabilidade 0, 3 usados como entrada, abaixo os respectivos padróes recordados pela memória associativa morfológica MXY.
Tambem armazenamos o conjunto das memórias fundamentais {(xf, yf), £ = 1,..., 5} na memória associativa MXY. Utilizamos como entrada os padrões corrompidos com ruído pepper (ruído dila-tivo) com probabilidade 0, 3 apresentados no topo da figura 6.4 e encontramos como resposta da memória os padrões apresentados na segunda linha da figura 6.4. O erro quadrático normalizado ||yf — MXY H xf ||/Hyf || para £ = 1,..., 5 foi, respecitvamente, 0, 081, 0, 068, 0, 035, 0, 016 e 0. Repetimos o experimento 1000 vezes e encontramos um EQMN = 0, 04.
6.2	Memórias Auto-Associativas Morfológicas
Nesta secao consideramos o caso auto-associativo, isto e, Y = X. Um discucao detalhada das memórias auto-associativas morfológica para padrões em tons de cinza encontra-se em [97]. Neste caso, as matrizes WXX e MXX sao dadas por
wij
k
= A (4 - 4)
e=i
e	mij
k
=	4 - 4),
xi=1
(6.32)
Fig. 6.5: Padrões corrompidos com ruido pepper (ruido erosivo) utilizados no exemplo 6.2.1.
Fig. 6.6: Padrões recordados pela memória auto-associativa morfológica Wxx quando apresentamos os padroes da figura 6.5 como entrada.
e na fase de recordacao usamos as equacoes (6.1) e (6.2), respectivamente.
A fase de recordacao das memorias auto-associativas morfológicas sao descritas em termos dos seus pontos fixos e regioes de recordacao [90, 94]. Um resultado analogo pode ser obtido para a memoria associativa morfologica MXX usando o conceito de dualidade.
Teorema 6.2.1. Para todo X = [x1,..., xk ] G Rnxk, o conjunto dos pontos fixos de WXX inclui os padroes x1,..., xk. Além disso, para todo x G Rn, temos
Wxx El x = x,	(6.33)
onde x denota o supremo de x no conjunto dos pontos fixos de WXX.
O seguinte corolario afirma que a fase de recordacao das memorias auto-associativas morfologicas e efetuada em um unico passo.
Corolario. Seja X G Rn. O conjunto dos pontos fixos de WXX consiste de todos WXX E x tais que x G Rn. Alem disso, se x, para £ = 1,... ,k e o padrão recordado por WXX após apresentarmos o padrão-chave x, entao x &amp;lt;x.
Lembre-se que uma versao ruidosa x de um padrâo x e uma versao erodida de x quando x &amp;lt;x e e uma versao dilatada de x quando x &gt; x. Usando esta terminologia temos que se WXX E x7 = x7, entao pelo teorema 6.2.1, x7 deve ser uma versao erodida de x7. Analogamente, se MXX E x7 = x7, entao x7 deve ser uma versao dilatada de x7.
Exemplo 6.2.1. Considere os padroes x1,..., x4 apresentados na figura 1.3. Armazenamos estes padroes na memoria associativa morfologicas WXX usando a equacao (6.32). Depois geramos os
Fig. 6.7: Padrões corrompidos com ruido salt (ruido dilativo) utilizados no exemplo 6.2.1.
		
	Zr .	.^1
	1 j	
Fig. 6.8: Padrões recordados pela memória auto-associativa morfológica MXX quando apresentamos os padroes da figura 6.7 como entrada.
padrões x1,..., x4 introduzindo ruído pepper (ruido erosivo) com distribuição uniforme e probabilidade 0, 3. Na figura 6.5 apresentamos os padroes corrompidos x1,..., x4 e na figura 6.6 apresentamos os padroes recordados pela memoria associativa morfológica WXX apos apresentarmos os padroes corrompidos x1,..., x4. Repetimos o experimento acima 1000 vezes e encontramos um erro quadratico medio normalizado (EQMN) de 0, 0028.
Realizamos um experimento analogo usando a memoria associativa morfologica MXX e os padroes corrompidos com ruído salt (ruído dilativo) gerados com distribuicao uniforme com probabilidade 0, 3 apresentados na figura 6.7. Na figura 6.8 apresentamos os padroes recordados pela memoria associativa morfologica MXX apos apresentarmos os padroes corrompidos com ruído dilativo da figura 6.7. Repetimos o experimento 1000 vezes e encontramos um EQMN = 0, 0039.
Um padrao x contendo ruído dilativo nao pode ser recordado usando WXX, pois pelo corolario acima, se xY &gt; xj para algum i, entao WXX 0 xY &gt; xY. Analogamente, um padrao xY contendo ruído erosivo nao pode ser recordado usando MXX pois se xY &amp;lt;xY para algum i, entao Mxx 0 xY &amp;lt;xY.
Exemplo 6.2.2. Considere os padroes binarios x1,... x10 G {0,1}35 apresentados na figura 6.9. Armazenamos estes padroes na memoria associativa morfologica WXX e verificamos que todas as recordacoes fundamentais sao pontos fixos. Depois, introduzimos ruído dilativo nas memorias fundamentais revertendo um ilnico pixel do fundo da imagem obtendo os padroes apresentados na figura 6.10. Na figura 6.11 apresentamos os padroes recordados pela memoria associativa morfologica WXX quando apresentamos os padroes da figura 6.10 como entrada. Note que nenhuma das memorias fundamentais foi recordada. Por outro lado, a memoria associativa morfologica MXX e eficiente para recordar padroes corrompidos com ruído dilativo e todas as memorias fundamentais foram recordadas
por esta memoria apos apresentarmos os padroes da figura 6.10 como entrada.
123^567890
Fig. 6.9: Padrões binarios xi,..., xi0 usados nos exemplos 6.2.2 e 6.3.1.
123'4567890
Fig. 6.10: Padrões x1,..., x10 corrompidos com ruído dilativo.
Fig. 6.11: Padroes recordados pela memoria associativa MXX apos apresentar os padroes da figura 6.10 como entrada.
6.3	Memórias Auto-associativas Morfológicas Binárias
Nesta secao discutimos as memorias associativas morfologicas binarias. Lembramos que um padrao e binarario se xf G {0,1}n para todo £ = 1,..., k. Sabemos que as memorias auto-associativas morfologicas podem armazenar infinitos padroes. No caso binario poderemos armazenar 2n padroes, onde n e a dimensao do padroes.
Dado um conjunto de padroes binarios {x1,..., xk} e um índice l, denotaremos por Ll o maior subconjunto de {1,..., k} tal que xf = 1 para todo £ G Ll. Em outras palavras, o conjunto Ll diz
a)	b)	c)
Fig. 6.12: Nos itens a), b) e c) temos e31, WXX E e31 e x5 A x9, respectivamente.
quais sao as memorias fundamentais xf que possuem valor 1 na l-esima componente. Usando esta notacao podemos enunciar o seguinte teorema [94].
Teorema 6.3.1. Seja y um ponto fixo de WXX tal que y = 1 e y &amp;lt;f\feL xf, então y = AfeL¡ xf. Corolario. Se ei é a l-esima coluna da matriz identidade, entao
Wxx E ei =	xf.	(6.34)
Exemplo 6.3.1. Considere os padroes x1,..., x10 apresentados na figura 6.9. Na figura 6.12 a) apresentamos o padrao e31 G {0,1}35 que foi usado como entrada da memoria WXX. Na figura 6.12
b)	apresentamos o padrao recordado pela memoria auto-associativa morfologica, isto e WXX E e31. Note que os padroes x5 e x9 possuem valor 1 na componente 31. Logo, L31 = {5, 9}. Na figura 6.12
c)	apresentamos /\feL xf = x5 A x9. Note que as imagens dos itens b) e c) sao iguais, verificando a validade do corolario 6.3.
Na demonstracao do corolario, usamos o fato de /\feLi xf ser um ponto fixo de WXX. Na verdade, podemos mostrar um resultado muito mais forte. Para isso, vamos apresentar a seguinte definicao
[11]:
Definicao 6.3.1. Uma expressao envolvendo x1,..., xk e os síbolos V e A e chamado polinomio reticulado em x1,..., xk.
Pela definicao acima, temos que \/f=^\xf e a forma geral de um polinomio reticulado. Em particular, /\^eLl e um polinomio reticulado em x1,..., xk e e um ponto fixo de WXX. O seguinte teorema relaciona o conjunto dos pontos fixos das memorias associatias morfologicas com os polinomios reticulados em x1,..., xk [95].
Teorema 6.3.2 (Teorema dos Pontos Fixos das Memorias Associativas Morfogicas Binarias). Seja X = [x1,..., xk] G {0,1}nxk. Um padrao binario y, diferente dos padroes 0 e 1, e um ponto fixo de WXX se e somente se y e um polinomio reticulado em x1,..., xk. Analogamente, um padrao binario z = 0, 1 e um ponto fixo de MXX se e somente se z e um polinomio reticulado em x1,..., xk.
O teorema 6.3.2 mostra que uma memoria auto-associativa morfologica binaria tem grande probabilidade de ter um grande numero de estados espurios. Com base nos teoremas apresentados anteriormente podemos dizer:
1.	A capacidade absoluta das memorias auto-associativas binarias e 2n se n for a dimensao dos padroes armazenados.
2.	Todo padrao recordado e um ponto fixo da memoria (convergencia com uma cínica iteracao).
3.	Os pontos fixos de Wxx e MXX incluem os padroes originais bem como um grande mímero de estados espurios.
4.	A memoria Wxx exibe uma excelente tolerancia com respeito a padroes erodidos e MXX, com respeito a padroes dilatados.
5.	Wxx e MXX nao sao eficientes na recordacao de padroes corrompidos por ambos ruído dilati-vos e erosivos.
Na proxima secao apresentaremos um modelo com características melhores com respeito aos itens 3, 4 e 5, que manteím as características dos itens 1 e 2.
6.4	Memorias Associativas Morfologicas de Duas Camadas
As memorias Wxy e MXY possuem excelente tolerancia com respeito a padroes corrompidos com ruído somente erosivo ou somente dilativo, respectivamente, mas nao sao eficientes quando o padrao apresentado possui ambos ruídos. Para evitar este problema, podemos fazer combinacoes das memorias Wxy e Mxy. Estas combinacoes produzem as memórias associativas morfológicas de duas camadas. Como veremos, as memorias associativas de duas camadas possuem um numero menor de pontos fixos, aumentando assim a tolerancia a ruídos e diminuindo o numero de memorias espurias.
6.4.1	Arquitetura
Primeiramente vamos lembrar a definicao do niicleo de uma memoria associativa morfologica W.
Definição 6.4.1. Sejam X G {0,1}nxk e Y G {0,1}mxk. Uma matriz Z = [z1, z2,..., zk] de dimensões n x k e um núcleo para (X, Y) se e somente se existe uma memoria W tal que
W 0 (Mzz 0 X) = Y.	(6.35)
Se X = Y, dizemos que Z e um niicleo para X.
Suponha que existe Z (niicleo) tal que WZY 0 (Mzz H X) = Y. Como Mxx H X = X para todo X G {0,1}nxk, entao podemos dizer que:
Wzy 0 (Mzz H (Mxx H X)) = Y.	(6.36)
Assim, dado um padrao x, encontramos
y = Wzy 0 (Mzz El (Mxx 0 x)) = Wzy 0 ((Mzz El Mxx) 0 x) = Wzy 0 (M 0 x),
(6.37)
xi (t)	0
X2 (t)	0
X3 (t)	0
MXz	f (•)	Wzy
0 —*■ X1(t + 1)
0 —* X2(t + 1)
0	X3 (t + 1)
00 —*• Xn(t + 1)
Fig. 6.13: Arquitetura das Memória Associativa de Duas Camadas descrita pela equacao (6.40).
onde M e uma matriz que depende de Z e X. Na deducao acima, M = MZZ 0 MXX, mas faremos uma pequena mudanca na definicao da matriz M.
Nossa dificuldade esta em encontrar um nilcleo para (X, Y). Sabemos que o nilcleo depende
somente de X. Precisamente, devemos ter Z &amp;lt;X, com z7 A z^ = 0
para todo
Entretanto, vamos supor apenas que zY A = 0 e zY z^ para todo 7, £, £ = 7. Note que Z nao depende mais de X .Se X G {0, 1}nxk, Y G {0,1}mxk e Z G {0,1}pxk, entao WZY e MXX sao matrizes m x p e n x n respectivamente. Para manter uma coerência nas operacoes, M deve ser uma matriz p x n. Logo, tomaremos
M = MXz = Mxz A Mxx •
(6.38)
Xn(t) —&gt; 0
0
0
0
Assim, dado um padrâo-chave x, tomamos
y = Wzy
(6.39)
como sendo o padrao recordado.
A operacao M^Z 0 x pode produzir como resposta um vetor que nao pertence ao conjunto {0,1}” e isso pode ser ruim quando trabalhamos com memórias binarias. Introduzimos uma funcao limiar aplicada no resultado de M^_Z 0 x na equacao (6.39) para evitar este problema. Precisamente, definimos a memória associativa:
y = Wzy
y = Wzy E f (MXZ A x), onde MXz = Mxz E Mxx e f (x) = [fi(x), f2(x), ..., fp(x)]' com
1 se Xj &gt; 0
0 caso contrario.
(6.40)
(6.41)
Na figura 6.4.1 apresentamos a arquitetura da rede deste modelo neural. O modelo dual pode ser obtido de um modo analogo.
6.4.2	Aprendizado
O aprendizado da memória associativa morfológica de duas camadas consiste em escolher Z = [z1, z2,..., zk] G {0,1}pxk com z^ A zY = 0 e z^ zY, V£, 7, £ = 7 e construir as matrizes WZY = Y A Z* e M^Z = MXZ A MXX = (Z H X*) A (X H X*). A escolha de Z e arbitraria desde que satisfaca as condicões apresentadas acima. Note que I = [e1, e2,..., ek] G {0,1}pxk, onde ei e a i-esima base do espaco Rp, pode ser usada como a matriz Z neste modelo. Neste trabalho usaremos somente Z = Ipxk.
Teorema 6.4.1 (Teorema dos Pontos Fixos da Memória Associativa Morfológica de Duas Camadas). Seja X G {0,1}nxk tal que MXX G {0,1}nxk. Seja Y G {0,1}mxk, Z = [z1, z2,..., zk] G {0,1}pxk e x G {0,1}n arbitrário. Usaremos a notação 8 = {£ | x &gt; x*} e MXjZ = MXZ A MXX. Se 8 = 0, zY &gt; z^ e zY A z^ = 0 para todo £, 7 com 7 = £. Entao, para todo x = 1 = [1,1,..., 1]' G {0,1}n, temos
Wzy 0 f (M*Z A x) = \/ /.	(6.42)
iee
Além disso, para todo £ = 1, 2,..., k e todo x G {0,1}n,
Mxx H x = x^	Wzy H f (MXXZ H x) = y^.	(6.43)
A demonstração deste teorema encontra-se em [94]. A memória associativa morfológica de duas camadas apresentada acima possui menos estados espUrios que o modelo WXX e portanto, possui uma melhor tolerância a ruído. Um resultado anaiogo pode ser obtido para a versao dual da memória associativa morfológica de duas camadas usando o conceito de dualidade.
Exemplo 6.4.1. Considere os padrões binarios x1,..., x5 e y1,..., y5 apresentados nas figuras 1.1 e 1.2. Armazenamos estes padrões na versao dual da memória associativa morfológica de duas camadas usando Z = I4096x5 e verificamos que y^ = WZY 0 f (MXZ El x^) para todo £ = 1,..., k. Depois verificamos que as recordacóes fundamentais y1,..., y5 sao recordadas após apresentarmos como entrada os padrões X1,..., X5 apresentados na figura 6.3. Repetimos o experimento 1000 vezes e verificamos que o erro quadraítico meídio normalizado
EQMN = E
í 1A ||yg - Mxy a x- 11 \ v	lly II	J
y«
= 0.
(6.44)
Finalmente, utilizamos com entrada os padrões corrompidos com ruído pepper (ruído dilativo) com probabilidade 0, 3 apresentados no topo da figura 6.4. Encontramos como resposta da memória memória associativa de duas camdas os padrões apresentados na figura 6.14. O erro quadrâtico normalizado para £ = 1,..., 5 foi, respectivamente, 0, 081, 0, 068, 0, 035, 0, 016 e 0. Repetimos o experimento 1000 vezes e encontramos um EQMN = 0, 04.
Fig. 6.14: padroes reçordados pela memoria assoçiativa morfológiça de duas çamadas apos apresentarmos os padroes çorrompidos apresentados no topo da figura 6.4
Capítulo 7
Desempenho das Memorias Associativas Binarias
Neste capítulo apresentamos comparações do desempenho das memórias associativas binárias introduzidas nos capítulos 5 e 6. Inspirados nas 5 caracteríticas desejaveis em uma memória associativa apresentadas no capítulo 3, formalizamos os conceitos: capacidade de armazenamento, distribuição da informaçao, raio de alraçao e probabilidade de memoria espúria. E importante observar que o conceito de raio de atracao foi utilizado de um modo empírico em [44]. Nesta dissertacao apresentamos uma definicao rigorosa deste conceito. O conceito de capacidade de armazenamento apresentado nesta dissertacao de mestrado difere do conceito de capacidade absoluta introduzido em [56]. Discutimos a diferenca entre estes dois conceitos na secao 7.1. Nao encontramos na literatura um trabalho discutindo o esforço computacional das memórias associativas neurais. Tambem nao encontramos um trabalho mencionando os conceitos de distribuicao da informacao e a probabilidade de memória espuria de uma memória associativa.
Nao discutimos neste capítulo o modelo BSB devido a semelhanca deste com o modelo de Hopfield.
7.1	Capacidade de Armazenamento
A capacidade de armazenamento, denotada por Ca, e uma funcao que diz a probabilidade de armazenarmos um conjunto de memórias fundamentais numa memória associativa. Esta funcao depende do mapeamento associativo (memória associativa), da dimensao dos padrões de entrada (n), da di-mensao dos padrões de saída (m) e do numero de memórias fundamentais (k). Usaremos a notacao Ca(Memória, n, m; k) no caso heteroassociativo e Ca(Memória, n; k) no caso auto-associativo.
Seja
U = {(X, Y) : X = [x1, x2,...] G , Y = [y1, y2,...] G R},	(7.1)
o conjunto dos pares (X, Y) de matrizes X (gerada aleatóriamente) com n linhas e infinitas colunas e das matrizes Y (geradas aleatóriamente) com m linhas e infinitas colunas. Considere a variavel aleatória C : U —&gt; N dada por
C = max{p G N : Gp(xe) = ye,	= 1,..., p},	(7.2)
onde	e o mapeamento associativo da memória associativa neural treinada usando os
pares (x^,	) para £ = 1,..., p (primeiras p colunas de X e Y).
Definição 7.1.1 (Capacidade de Armazenamento). Considere o espaco amostral Q definido na equacao (7.1) e a variavel aleatória C definida pela equacao (7.2). A capacidade de armazenamento de uma memória associativa e a funcao dada pela equacao
Ca(Memória, m, n; k) := 1 — Fc(k) = Pr (C &gt; k),	(7.3)
onde Fc e a funcao de distribuição da variavel aleatória C.
Note que Pr (C &gt; k) e a probabilidade de Gk(x^) = y^ para todo £ &amp;lt;k. Logo,
Ca(Memória, m, n; k) = Pr (Gfc(x€) = y€, £ = 1,..., k) ,	(7.4)
onde e o mapeamento associativo obtido após armazenar os pares (x^, y^) para £ = 1,..., k.
Podemos estimar a capacidade de armazenamento de uma memória associativa usando o conceito de funçao de distribuição empírica em conjunto com o teorema de Glivenko-Cantelli [10]. A funcao de distribuicao empírica para variaveis aleatórias C1, C2,..., Cs e a funcao FE (x) dada por
s
1s
FE (x) = - 'S Ind [Ci &amp;lt;x],
s
1=1
onde Ind e a funcao indicadora dada por
Ind[Ci &amp;lt;x] = {
se Ci &amp;lt;x,
caso contrario,
(7.5)
(7.6)
O teorema de Glivenko-Cantelli afirma que se C1, C2,... sao variaveis aleatórias independentes çom uma funçao de distribuiçao çomum FC, entao supx |FE (x) — FC (x)| —&gt; 0 çom probabilidade 1. O teorema de Glivenko-Cantelli impliça que, çom probabilidade 1, lims FE(x) = FC(x), para çada x onde FC e çontínua. Assim, çom probabilidade 1, teremos
1s
Ca (Memória, n; k) = 1 - lim FE(k) = lim (1 - FE(k)) = lim - \ Ind [Ci &gt; k].	(7.7)
s	s	s s
i=1
O çonçeito da çapaçidade de armazenamento foi inspirado no çonçeito de çapaçidade absoluta introduzido por MçElieçe et. al. em [56]. A capadidade absoluta e definida çomo max{x G N : Ca(Memoria, m, n; x) &gt; 1 — e}, çom e &gt; 0 pequeno. Por exemplo, o teorema 5.1.2 apresenta a çapaçidade absoluta da rede de Hopfield e a çonjeçtura 5.2.1 apresenta a çapaçidade absoluta da BAM. Açreditamos que existe uma perda de informaçao na medida çapaçidade absoluta çomo apresentado no exemplo abaixo.
Exemplo 7.1.1. Considere duas memorias assoçiativas A e B. Na figura 7.1 apresentamos çom linha çontínua a çapaçidade de armazenamento da memoria assoçiativa A e çom linha traçejada a çapaçidade de armazenamento da memoria assoçiativa B. A linha vertiçal pontilhada representa a çapaçidade absoluta da memoria assoçiativa A e a linha vertiçal çom ponto-traço representa a çapaçidade
Fig. 7.1: Capacidade de armazenamento das memórias associativas A e B do exemplo 7.1.1. A linha contínua representa capacidade de armazenamento da memória associativa A e a linha tracejada representa a capacidade de armazenamento da memória associativa B. A linha vertical com pontilhada representa a capacidade absoluta da memória associativa A e a linha vertical com ponto-traco representa a capacidade de armazenamento da memória associativa B.
de armazenamento da memória associativa B. Note que a memória associativa A tem probabilidade 1 de armazenar um conjunto com menos de 33 memórias fundamentais, mas tem probabilidade próxima de zero para armazenar um conjunto com mais de 70 memórias fundamentais. Por outro lado, a memória associativa B tem probabilidades 0, 911 e 0, 9 de armazenar um conjunto com 33 e 90 memórias fundamentais, respectivamente. Neste exemplo, a capacidade absoluta da memória A e maior que a capacidade absoluta da memora B. Entretanto, este resultado nao e suficiente para afirmar que a memória A pode armazenar mais padrões que B. De fato, temos probabilidade 0, 9 de armazenar um conjunto com 90 memórias fundamentais em B. Por outro lado, certamente nao poderemos armazenar o mesmo conjunto em A.
7.1.1	Memoria Associativa de Hopfield
Seja X = [x1,..., xk] G {-1,1}nxk a matriz das memórias fundamentais gerada aleatoriamente com distribuicao uniforme onde Pr(xf = 1) = 1/2 para todo i = 1,..., n e Ç = 1,... ,k. Daniel Amit mostrou em [3] que
Pr(x1 = sinal(W x1)^) = 1
1 + erf
5
(7.8)
onde erf e a funcao erro definida pela equacao
2 [x +2 erf(x) =	1 - dt.
o
Logo, a capacidade de armazenamento da memória associativa de Hopfield e
Ca (MA Hofield, n; k)
Pr (sinal(Wx^) = x^, £ = 1,..., k)
Pr
(smal(W x* )i = xf)f
{2
1 + erf
(7.9)
(7.10)
(7.11)
(7.12)
Na figura 5.1 apresentamos o gráfico da capacidade de armazenamento pelo número de memórias fundamentais. A linha tracejada representa a capacidade de armazenamento obtida pela equacao (7.12) e a linha contínua com □ representa a capacidade de armazenamento estimada usando a equacao (7.7). Neste exemplo usamos n = 100 e s = 1000 para estimar a funcao de distribuicao empírica.
7.1.2	Memoria Associativa Bidirecional
Substituindo o resultado da equacao (5.17) na equacao (5.23) obtemos
Pr (y£ = sinal (Wx^), V£ G {1,..., k})
Analogamente,
Pr (X = sinal (WT, V£ G {1,..., k})
(7.13)
(7.14)
Logo,
1
1
Ca (BAM, n, m; k) =
1 + erf
1 + erf
(7.15)
Na figura 5.5 apresentamos com linha tracejada a capacidade de armazenamento da BAM dada pela equacao (7.15) e com linha contínua com □ a capacidade de armazenamento estimada usando a equacao (7.7). Neste exemplo tomamos n = 100, m = 80 e usamos s = 1000 para calcular a funcao de distribuicao empírica.
7.1.3	Memoria Associativa de Personnaz
A matriz dos pesos sinapticos da memória associativa de Personnaz e obtida atraves da equacao (4.8) que sempre tera solucao no caso auto-associativo (W = I e uma solucao ). Logo, a capacidade de armazenamento desta memória associativa e
Ca (MA Personnaz, n; k) = 1.
(7.16)
7.1.4	Memória Associativa de Kanter-Sompolinsky
A proposição 3 garante que todas as recordações fundamentais são pontos fixos da memória associativa de Kanter-Sompolinsky. Portanto
Ca (MA Kanter, n; k) = 1.
(7.17)
7.1.5	Memória Associativa Bidirecional Assimétrica
As matrizes dos pesos sinapticos da ABAM sao obtidos usando o armazenamento por projeçao. Pelo teorema 4.2.1, o erro da dependencia linear sera nulo somente se o vetores x1,..., xk forem linearmente independentes. Entretanto, a probabilidade de um conjunto {x1,..., xk} ser linearmente independente e 1 se k &amp;lt;n e 0 se k &gt; n. Analogamente para os vetores y1,..., yk. Podemos afirmar que
Ca (ABAM, n, m; k) = 1 — f (k — (n A m)),	(7.18)
onde f e a funcao limiar definida na equacao (6.41).
7.1.6	Memoria Associativa com Capacidade Exponencial
Chiueh e Goodman mostraram em [15] que
xn &gt;] 4^ )
Pr
&amp;lt;, ,
V 4nT
(7.19)
para n G {1,..., k}, i G {1,..., k} e T grande. Logo,
Ca (ECAM, n, m; k) = Pr ^x^ = sinal exp [&amp;lt;x^, xn &gt;] x^ , G {1,..., k}^(7.20)
z	T x nk
&gt; V =cta
(7.21)
onde c e uma constante próxima de 1.
7.1.7	Memoria Associativa Bidirecional com Capacidade Exponencial
Podemos interpretar a BECAM como uma ECAM usando a equacao (5.50). Concluímos que a capacidade de armazenamento da BECAM satisfaz
Ca (BECAM, n, m; k) &gt; ck(n+m),
(7.22)
onde c e uma constante próxima de 1.
7.1.8	Memorias Associativa Morfológicas
No teorema 6.2.1 mostramos que WXX 0 X = MXX El X = X. Logo, a capacidade de armazenamento das memórias auto-associativas morfológicas WXX e MXX e
Ca (WXX, n; k) = Ca (MXX, n: k) = 1-	(7.23)
Pelo teorema 6.1.3 concluímos que
Ca (Wxy, n, m; k) = Ca (Mxy , n, m; k)
V9 _ 3k-1 + 1 4 V	ík-1 )
(I
k-1
n- 1
4(' (3)“)
k-1
®“)
n-n mk
(7.24)
Na figura 6.27 apresentamos o gráfico da capacidade de armazenamento pelo número de memórias fundamentais armazenadas. Este grafico foi construído tomando n = 100, m = 80 e repetindo 1000 vezes cada experimento para obter as probabilidades. As linhas contínuas com y e A representam as estimativas da capacidade de armazenamento atraves da equacao (7.7) para as memórias associativas WXY e MXY, respectivamente. A linha tracejada reprensenta a estimativa dada pela equacao (7.24).
7.1.9	Memória Associativa Morfológica de Duas Camadas
O teorema 6.4.1 afirma que WZY 0 f Í-V'2 0 x) = sempre que MXX 0 x =	. Como
WXX El xf = xf para todo £ = 1,..., k, entao a capacidade de armazenamento das memórias associativas morfológicas de duas camadas e
Ca (MAM Duas Camadas, n, m; k) = 1.
(7.25)
7.2	Distribuição da Informação
A Função Distribuição da Informação (FDI) e uma medida da distribuyo da informacao armazenada numa memória associatva neural. Esta medida e uma funcao que depende da memória, da dimensao dos padrões de entrada e saída, do numero de memórias fundamentais armazenadas e da porcentagem do numero das conexões sinapticas excluídas da memória associativa neural.
Considere o espaco amostral Q = {(X, Y )|X = [x1,..., xk ] G Rnxk, Y = [y1,..., yk ] g Rmxk} e defina sobre Q a variavel aleatória D : Q —&gt; [0,100] dada por
D = max{x G [0, 100] : GjX(x^) = y^, V£ = 1,..., k},	(7.26)
onde G£ :	e o mapeamento associativo da memória associativa neural treinada com as
memórias fundamentais (x^, y^), £ = 1,..., k e com x % do numero total de conexões sinapticas excluídas (primeiro treinamos a memória associativa e depois excluímos aleatoriamente as conexões sinpaticas). Note que Gk = Gk. e o mapeamento associativo com todas as conexões sinapticas.
Definicão 7.2.1 (Funcão da Distribuição da Informacão). A função da distribuição da informação (FDI) de uma memoria associativa com k padroes armazenados e
D(Memoria, n, m, k; x) : = 1 — FD(x) = Pr(D &gt; x)	(7.27)
= Pr (GX (x' )= y' ,Ç =1,...,k) ,	(7.28)
onde FD e a funcao de distribuyo da variavel aleatoria D definida na equacao (7.26).
Usamos o conceito de funcao de distribuyo empírica e o teorema de Glivenko-Cantelli para estimar a distribuyo da informacao de uma memoria associativa neural. Nas figuras 7.2 e 7.3 apresentamos a FDI empírica das memorias associativas binarias apresentadas nesta dissertacao. As memorias associativas binarias foram treinadas com 6 padroes gerados aleatoriamente. Nos experimentos computacionais usamos n = 100, m = 80 e estimamos a FDI realizando 100 simulates.
Na figura 7.2 apresentamos a FDI empírica para o caso auto-associativo. A linha com o representa a ECAM e a linha com x representa a memoria associativa morfologica de duas camadas. Note que estes dois modelos forneceram a mesma FDI empírica. A linha com □ representa a memoria auto-associativa morfologica WXX. A memoria auto-associativa MXX produziu um resultado semelhante e nao foi apresentada no grafico. A linha com * representa a memoria associativa de Hopfield, a linha com V representa a memoria associativa de Personnaz e a linha com A representa a memoria associativa de Kanter-Sompolinsky. Note que a memoria associativa de Personnaz apresentou a maior FDI empírica ponto a ponto. A informacao armazenada na ECAM e na memoria associativa morfológica de duas camadas nao e bem distribuida nas conexoes sinapticas pois estas duas memorias apresentaram a menor FDI empírica ponto a ponto.
Na figura 7.3 apresentamos a FDI empírica para o caso heteroassociativo binaírio. A linha com o representa a BECAM e a linha com x representa a memoria associativa morfologica de duas camadas. A FDI empírica destes dois modelos coincidem. A linha com □ representa a memoria associativa morfologica WXY. Note que a memoria associativa morfologica WXY teve uma probabilidade 0, 33 de armazenar todas as memorias fundamentais como pontos fixos. Este resultado confere com o grafico da capacidade de armazenamento discutido na secao anterior. A memoria associativa MXY produziu um resultado semelhante. A linha com * representa a BAM e a linha com A representa a ABAM. Note que a ABAM apresentou a maior FDI empírica ponto a ponto.
7.3	Raio de Atraçao
O raio da bacia de atracao, ou simplesmente, raio de atração e uma medida para a tolerancia a ruído de uma memoria associativa. Este conceito foi usado empiricamente por Kanter-Sompolinsky em [44]. Nesta secao fornecemos uma definyo rigorosa deste conceito.
Definicao 7.3.1 (Raio de Atracao). Sejam Q = {(X, Y) : X = [x1,..., xk] E Rnxk, Y = [y1,..., yk] E Rmxk} e Rk : Q —&gt; R a variavel aleatoria
Rk = sup{r E R : d(x, x1) &amp;lt;r, Gk(x) = y1},	(7.29)
onde Gk : Rn —&gt; Rm e o mapeamento associativo da memoria treinada com as memorias fundamentais (x', y'), para Ç = 1,..., k, e d : Rn x Rn —&gt; [0, +to) e uma metrica. O raio de atracao de
Fig. 7.2: FDI empírica das memorias auto-associativa neurais pela porcentagem de conexoes sinapticas excluídas. As linhas representam: ECAM (o), MAM Duas Camadas (x), MAM (□), MA Hopfield (*), MA Personnaz (A) e MA Kanter (V).
Fig. 7.3: FDI empírica das memorias heteroassociativa neurais pela porcentagem de conexoes sinapticas excluídas. As linhas representam: BECAM (o), MAM Duas Camadas (x), MAM (□), BAM (*) e ABAM (A).
uma memoria associativa sera
p(Memoria, m, n; k) := E(Rk),	(7.30)
onde E(Rk) representa a esperanca da variavel aleatoria Rk.
Na equacao (7.29), se x e uma versao ruidosa de x1 com d(x, x1) &amp;lt;Rk, entao o padrao recordado pela memoria e y1, que e o mesmo padrao recordado quando apresentamos a memoria fundamental x1. No caso binario, podemos substituir sup por max pois d(x, x1) assume somente valores discretos. Nesta dissertacao usamos como metrica a distancia de Hamming definida como sendo o número de componentes distintas de dois padroes binarios ou bipolares.
Na pratica estimamos Rk da seguinte forma:
1.	Tome x = x1,
2.	Enquanto Gk (x) = y1 faca:
(a)	Escolha aleatoriamente um índice i G {1,..., n} que ainda nao foi escolhido,
(b)	Defina x» = x„- no caso bipolar (ou x» = 1 — x» no caso binario).
3.	Defina Rk = dH(x, x1) — 1.
O procedimento acima fornece um valor Rk maior que o valor teorico definido na equacao (7.29). Entretanto, esta diferenca e irrelevante pois usaremos Rk somente para comparar os modelos de memorias associativas binarias ou bipolares e usaremos sempre o procedimento descrito acima. Tendo Rk, podemos estimar o raio de atracao usando a lei dos grandes numeros.
Lembre-se que o padrao recordado por uma memoria associativa dinamica e obtido somente apos a convergencia para um ponto fixo e a recursividade da fase de recordacao esta implicita no mapeamento associativo da memoria.
Sabemos que a memoria associativa morfologica Mxy e a memoria associativa morfologica de duas camadas apresentam tolerância a ruído somente se o padrâo-chave x &gt; x1. Por esta razao impomos x1 = 0 na hora de gerar as memorias fundamentais (xf, yf), para £ = 1,..., k. Analogamente, impomos x1 = 1 para a memoria associativa morfologica WXY e a versao dual da memoria associativa de duas camadas. Portanto, o raio de atracao obtido para as memorias associativas morfologicas so fara sentido para padroes corrompidos somente com ruído dilativo ou erosivo.
Na figura 7.4 apresentamos o grafico do raio de atracao pelo niímero de memorias fundamentais armazenadas nas memorias auto-associativas bipolares e binarias discutidas nesta dissertacao. Os graficos foram obtidos usando xf G {—1,1}100 (ou xf G {0,1}100) e calculando a media apos 100 simulacoes. A linha com x representa a memoria associativa morfologica de duas camadas e a linha com o representa a ECAM. Estes dois modelos possuem os maiores raios de atracao. A linha com 0 representa a memoria associativa morfologica WXX. A memoria associativa Mxx produziu um resultado semelhante e nao foi apresentada na figura 7.4. Note que o raio de atracao da memoria associativa morfologica WXX (e Mxx) apresentou um grande decaímento. Este resultado e uma con-sequencia do teorema 6.3.2. A linha com * representa a memoria associativa de Hopfield, a linha com V representa a memoria associativa de Personnaz e a linha com A representa a memoria associativa de Kanter-Sompolinsky. Note que a memoria associativa de Kanter-Sompolinsky apresentou um raio
Fig. 7.4: Raio de atração das memorias auto-associativas binárias pelo número de memórias fundamentais. As linhas representam: ECAM (o), MAM Duas Camadas (x), MAM Wxx (□), MA Hopfield (*), MA Personnaz (A) e MA Kanter (V). As linhas tracejadas com x e □ representam as memórias associativas morfológicas de duas camadas e de camada única, respectivamente, sem impor x1 = 0 ou x1 = 1.
de atração maior que o raio de atração da memória associativa de Personnaz. Esta é a vantagem de impor wu = 0, para i = 1,..., n [44]. Note que a memória associativa morfológica de duas camadas apresentou o maior raio de atracao, entretanto, sua correcao de erro e limidada para memórias-chave x &amp;lt;x1.0 segundo maior raio de atracao foi da ECAM que e valido para ambos os tipos de ruído, di-lativo e erosivo. As linhas tracejadas com x e □ representam as memórias associativas morfológicas de duas camadas e de camada ilnica, respectivamente, sem impor x1 = 0 ou x1 = 1. Note que ambas memórias associativas morfológicas tiveram raio de atracao próximo de zero.
Na figura 7.5 apresentamos o grafico do raio de atracao pelo mímero de memórias fundamentais armazenadas. Os gráficos foram obtidos usando x^ G {-1,1}100 (ou x^ G {0,1}100), G {-1,1}80 (ou y^ G {0,1}80) e calculando a media após 100 simulates. A linha com x representa a memória associativa morfológica de duas camadas e a linha com o representa a BECAM. A linha com □ representa a memória associativa morfológica WXY. A memória associativa MXY produziu um resultado semelhante. A linha com * representa a BAM e a linha com A representa a ABAM. Em analogia ao caso auto-associativo, a memória associativa morfológica de duas camadas apresentou o maior raio de atracao, entretanto, sua correcao de erro e limidada para memórias-chave x &amp;lt;x1. O segundo maior raio de atracao foi da BECAM que e a generalizacao da ECAM para o caso heteroassociativo.
Fig. 7.5: Raio de atração das memorias heteroassociativa pelo número de memórias fundamentais. As linhas representam: BECAM (o), MAM Duas Camadas (x), MAM	(□), BAM (*) e ABAM
(A). As linhas tracejadas com x e □ representam as memórias associativas morfológicas de duas camadas e de camada única, respectivamente, sem impor x1 = 0 ou x1 = 1.
As linhas tracejadas com x e □ representam as memórias associativas morfológicas de duas camadas e de camada Unica, respectivamente, sem impor x1 = 0 ou x1 = 1. Note que ambas memórias associativas morfológicas tiveram raio de atracao próximo de zero.
7.4	Memórias Espúrias
Nesta secao medimos a probabilidade de um padrâo-chave convergir para um padrao que nao faz parte do conjunto das memórias fundamentais de uma memória associativa treinada com k padrões.
Definição 7.4.1 (Probabilidade de Memória Espúria, PME). Seja Q = {(X, Y, x) |X = [x1,..., xk] G
Rnxk, Y = [y1,..., yk] G Rm*k, x G }. A probabilidade de uma memória associativa treinada com k memórias fundamentais convergir para uma memória espiúria e
E(Memória, n, m; k)	:= Pr (Gk(x)	G	{y1,..yk})	= 1 - Pr (Gk(x)	G {y1,..., yk})	, (7.31)
onde Gk :	e o mapeamento associativo da memória associativa treinada com as memórias
fundamentais (x^, y^), £ = 1,..., k.
Usaremos a lei dos grandes numeros para estimar a probabilidade de memoria espiíria (PME) de uma memoria associativa neural. Devemos ter um cuidado especial com as memorias associativas morfologicas pois sabemos que a MAM e a MAM de duas camadas sempre fornecerao uma memoria espiíria como resposta se o padrao-chave x &amp;lt;/\|=1 xf, onde xf, £ = 1, • • • k sao as memorias fundamentais armazenadas. Para evitar este problemas, definimos x1 = 0. Assim, para todo padrao-chave x G {0,1}n, teremos x &gt; /\|=1 xf e podemos interpretar x como sendo uma versao corrompida com ruído dilativo. Analogamente, a MAM e a versao dual da MAM de duas camadas sempre fornecerao uma memoria espiíria se o padrao-chave x &gt; \/1=1 xf e definiremos x1 = 1 para evitar este problema.
Na figura 7.6 apresentamos a PME pelo niímero de memorias fundamentais armazenadas nas memorias auto-associativas binarias apresentadas nos capítulos 5 e 6. Neste experimento usamos n = 100 e realizamos 1000 simulacoes para calcular as probabilidades empíricas. A linha com x representa a memoria associativa morfologica de duas camadas e a linha com o representa a ECAM. Note que a PME destes dois modelos foi sempre nula. A linha com □ representa a memoria associativa morfológica . A memoria associativa produziu um resultado semelhante e nao foi apresentada na figura 7.6. Note que a PME da memoria associativa morfologica (e ) tende rapidamente para 1. Este resultado e uma consequencia do teorema 6.3.2 sobre os pontos fixos das memorias auto-associativas morfologicas binarias. A linha com * representa a memoria associativa de Hopfield, a linha com V representa a memoria associativa de Personnaz e a linha com A representa a memoria associativa de Kanter-Sompolinsky. Note que a PME e sempre maior que 1/2 pois o mapeamento associativo destes três ultimos modelo e um mapeamento ímpar e portanto, se xf e um ponto fixo, entao —xf tambem e um pontos fixos da memoria associativa. As memorias associativas morfologicas apresentaram ambas um probabilidade empírica proxima de 1 se nao impormos a condicao discutida no parágrafo anterior sobre o padrao x1.
Na figura 7.7 apresentamos a probabilidade de memoria espiíria pelo niímero de memorias fundamentais armazenadas nas memorias heteroassociativas binarias. Neste experimento tomamos n = 100, m = 80 e efetuamos 1000 simulacoes para calcular as probabilidades empíricas. A linha com x representa a memoria associativa morfologica de duas camadas e a linha com o representa a BECAM. Ambos modelos apresentaram uma probabilidade de memoria espiíria nula. A linha com □ representa a memoria associativa morfologica . A linha com * representa a BAM e a linha com A representa a ABAM. A probabilidade de memoria espiíria da BAM e da ABAM e sempre maior que 1/2 porque estes modelos possuem um mapeamento associativo ímpar. Novamente, as memorias associativas morfologicas apresentaram ambas um probabilidade empírica proxima de 1 se nao impormos a condicao discutida anteriormente sobre o padrao x1.
7.5	Esforço Computacional
O esfoco computacional pode ser medido pelo niímero de operacoes e efetuacoes de funcoes nao lineares realizadas pela memoria associativa neural na fase de armazenamento e na fase de recordacao. Nas memorias associativas dinamicas tambem devemos considerar o niímero de iteracoes necessarias para encontrar a saída da rede na fase de recordacao.
Nesta secao vamos considerar X = [x1, x2,..., xk] G Rnxk e Y = [y1, y2,..., yk] G Rmxk.
Fig. 7.6: Probabilidade de memória espúria pelo número de memórias fundamentais. As linhas representam: ECAM (o), MAM Duas Camadas (x), MAM WXX (□), MA Hopfield (*), MA Personnaz (A) e MA Kanter (V).
Fig. 7.7: Probabilidade de memória espúria pelo número de memórias fundamentais armazenadas. As linhas representam: BECAM (o), MAM Duas Camadas (x), MAM WXY (□), BAM (*) e ABAM (A).
7.5.1	Numero de Operações na Fase de Armazenamento
A matriz de pesos sinápicos da BAM é W = YXT. Logo, serão necessárias (2k — 1)mn operações de soma e de multiplicaçao para obtermos W. Em particular, a matriz dos pesos sinapticos da memoria associativa de Hopfield requer (2k — 1)n2 operacoes de soma e multiplicacao.
Na memoria associativa de Personnaz, a matriz dos pesos sinapticos e W = XX1 = UUT, onde
X	= LÊVT e a decomposicao SVD reduzida de X (Thin SVD Decomposition). O calculo da matriz U usando o metodo R-SVD requer 6nk2 + 11k3 operacoes e o produto UUT requer (2k — 1)n2 operacoes de soma e multiplicacao [25]. O mímero total de operacoes de soma e multiplicacao necessarias para obter a matriz dos pesos sinapticos da memoria associativa de Personnaz e 6nk2 + 11 k3 + (2k — 1)n2. Na memoria associativa de Kanter-Sompolisky tomamos W = XX1 e impomos wh = 0 para todo i = 1,..., n. Logo, o numero de operacoes necessarias para encontrar a matriz dos pesos sinapticos da memoria associativa de Kanter-Sompolisky e tambem 6nk2 + 11 k3 + (2k — 1)n2 operacoes de soma e multiplicacao.
Na ABAM definimos W1 = YX1 e W2 = XYÊ O numero de operacoes necessaria para calcular
XI	usando a decomposicao SVD reduzida e 6nk2 + 20k3 operacoes. Calculado a decomposicao SVD reduzida X = UÊVT, computamos Wi atraves do produto Wi = (y(VÊ1)) UT que requer (2m + 1)k2 + 2mnk operacoes de soma e multiplicacao. Analogamente, o calculo da matriz W2 requer 6mk2 + 20k3 operacoes para calcular a decomposicao SVD reduzida de Y e (2n + 1)k2 + 2mnk operacoes para calcular o produto matricial em W2. O numero total de operacoes de soma e multiplicacao necessarias para computar as matrizes de pesos sinapticos da ABAM e 40k3 + 2(4n + 4m + 1)k2 + 4mnk.
Na BECAM usamos as matrizes das memórias fundamentais como matriz dos pesos sinapticos. Portanto, nenhuma operacao e efetuada na fase de armazenamento desta memoria associativa. Em particular, a ECAM tambem nao efetua nenhuma operacao na fase de armazenamento.
Nas memorias associativas morfológicas tomamos WXY = Y El X* e MXY = Y 0 X*. Considerando as operacoes de maximo ou mínimo como operacoes binarias, serao necessarias kmn operacoes de soma e (k—1)mn operacoes de mínimo para calcular WXY ou kmn operacoes de soma e (k—1)mn operacoes de maximo para calcular MXY. O numero total de operacoes necessarias para calcular a matriz dos pesos sinapticos de uma memoria associativa morfológica e (2k — 1)mn operacoes de maximo ou mínimo e soma.
Nas memorias associativas morfogicas de duas camadas precisamos da matriz WZY que requer (2k — 1)pm operacoes de mínimo e soma. A matriz Mj^Y = MXZ 0 MXX que requer (2k — 1)pn operacoes para o calculo de MXZ, (2k — 1)n2 operacoes para o calculo de MXX e (2n — 1)pn operacoes para o calculo do produto MXZ 0 MXX. Como as operacoes de maximo e mínimo requerem o mesmo esforço computacional, o numero total de operacoes necessarias para obter as matrizes dos pesos sinapticos da memoria associativa morfológica de duas camadas sera 2pn(k + n — 1) + (2k — 1)(pm + n2). Nos nossos experimentos computacionas tomamos Z = Ikxk, ou seja, p = k.
Na tabela 7.1 apresentamos um resumo do que foi dito anteriormente.
Note que o numero de operacoes necessarias para calcular a matriz dos pesos sinapticos da BAM e de uma MAM sao os mesmos. Entretanto, teremos uma esforço computacional menor na fase de armazenamento das MAM pois as operacoes de maximo ou mínimo e soma requerem um esforço computacional menor que as operacoes de soma e multiplicacao.
Memória Associativa	Numero Aproximado de Operacoes	Tipo das Operacoes
MA Hopfield	(2k — 1)n2	soma e multiplicacao
BAM	(2k — 1)nm	soma e multiplicacao
MA Personnaz	6nk2 + 11k3 + (2k — 1)n2	soma e multiplicacao
MA Kanter-Sompolinsky	6nk2 + 11k3 + (2k — 1)n2	soma e multiplicacao
ABAM	40k3 + 2(4n + 4m + 1)k2 + 4mnk	soma e multiplicacao
ECAM	—	—
BECAM	—	—
MAM	(2k — 1)nm	maximo ou mínimo e soma
MAM Duas Camadas	2pn(k + n — 1) + (2k — 1)(pm + n2)	maximo ou mínimo e soma
Tab. 7.1: Esforço computacional na fase de armazenamento das memorias associativas neurais.
7.5.2	Numero de Operações por Iteração na Fase de Recordação
Na fase de recordacao devemos considerar o esforço computacional realizado pela memória por iteracao e o numero de iteracoes necessario para recordar um padrao. Discutiremos primeiro o numero de operacoes por iteracao, depois apresentaremos uma estimativa do numero de iteracoes das memórias associativas dinamicas. O esforço computacional sera descrito pelo numero de operacoes efetuadas e o numero de chamadas de funcóes nao-lineares por iteracao na memória associativa.
Nas memórias associativas de Hopfield, Personnaz e Kanter-Sompolinsky realizamos um produto matriz-vetor e depois aplicamos a funcao sinal em cada componente do resultado do produto matriz-vetor. Estas operacoes requerem (2n — 1)n operacoes de soma e multiplicacao e n efetuacóes da funcao sinal por iteracao.
Na ABAM computamos sinal(Wix) e sinal(W2y) por iteracao. Na BAM realizamos os mesmos calculos com W1 = W e W2 = WT na BAM. O total de operacoes efetuadas por iteracao na BAM ou na ABAM e 4mn — (m + n) operacoes de soma e multiplicacao e m + n efetuacóes da funcao sinal.
Na ECAM computamos sinal(X exp(XTx)). Por iteracao realizamos 4kn — (k + n) operacoes, efetuamos a funcao exponencial k vezes e a efetuamos a funcao sinal n vezes. Na BECAM, computamos sinal(Y exp(XTx)) e sinal(X exp(YTy)) realizando 4k(n + m) — (2k + m + n) operacoes de soma e multiplicacao, 2k efetuacóes da funcao exponencial e m + n efetuacóes da funcao sinal por iteracao.
Nas memórias associativas morfológicas realizamos o produto WXY 0 x ou MXY El y. Ambos requerem (2n — 1)m operacoes de maximo ou mínimo e soma. O padrao recordado pela memória associativa morfológica de duas camada e dado por WZY 0 f (M%Z El x), onde Mxz é uma matriz p x n e WZY e uma matriz m x p. Logo, na fase de armazenamento das memórias associativas morfológicas de duas camadas efetuamos 2p(m + n) — (p + m) operacoes de maximo ou mínimo e soma, e computamos a funcao f definida na equacao (6.41) p vezes. Nos nossos experimentos tomamos Z = Ikxk (p = k).
Na tabela 7.2 apresentamos o que foi dito anteriormente. A segunda coluna da tabela 7.2 representa o mímero total de operacoes binarias incluindo soma, produto, maximo e mínimo. A terceira coluna contem o numero de efetuacóes da funcao sinal ou da funcao f definida na equacao (6.41). A
Memória Associativa	N. Operacoes	f (x) ou sinal(x)	exp(x)
MA Hopfield	2n2 — n	n	—
BAM	4nm — (n + m)	m + n	—
MA Personnaz	2n2 — n	n	—
MA Kanter-Sompolinsky	2n2 — n	n	—
ABAM	4nm — (m + n)	m + n	—
ECAM	4kn — (k + n)	n	k
BECAM	4k(m + n) — (2k + m + n)	m + n	2k
MAM	2mn — m	—	—
MAM Duas Camadas	2p(m + n) — (p + m)	k	—
Tab. 7.2: Esforço computacional por iteracao na fase de recordacao das memorias associativas neu-rais.
quarta coluna representa o numero de eleluacoes da funcao exponencial.
7.5.3	Número de Iterações na Fase de Recordação
O número de iterações na fase de recordação de uma memória associativa dinâmica treinada com k memórias fundamentais e uma funcao da dimensao dos padrões de entrada e saída e do número de padrões armazenados. Precisamente, o número de iteracoes na fase de recordacao e definido como sendo a media do número de iteracoes necessarias para a convergencia do sistema dinamico para um ponto estacionario quando iniciamos a memoria com um padrao-chave qualquer. As memórias associativas estaticas serâo consideraras como sistemas dinamicos que convergem para um ponto estacionario com 1 iteracao.
Na figura 7.8 apresentamos o grafico do mímero de iteracoes pelo mímero de padrões armazenados de varias memórias auto-associativas dinamicas. Esta figura foi gerada usando padrões com 100 componentes e o grafico foi construido calculando a media após 1000 simulacoes. A linha marcada com * representa a MA de Hopfield, a linha com A representa a MA de Personnaz, a linha com V representa a MA de Kanter-Sompolinsky e a linha com o representa a ECAM. Note que o mímero de iteracoes da MA Personnaz e da MA de Kanter-Sompolinsky tendem para 1 quando o numero de memórias fundamentais armazenadas tende para a dimensao do padrões armazenados. De fato, W I na MA de Personnaz e W 0 na MA de Kanter-Sompolinky quando k n . Em ambos os casos sinal(Wx) = x para todo padrao-chave x quando k n. Logo, todos os pontos do espaco sao pontos fixos da MA de Personnaz e da MA de Kanter-Sompolinsky quando k n.
Na figura 7.9 apresentamos o grafico do numero de iteracoes pelo numero de padrões armazanados em memórias heteroassociativas dinamicas. Neste exemplo tomamos padrões de entrada com 100 componentes, padrões de saída com 80 componentes e calculamos a media após 1000 simulacoes. O numero maximo de iteracoes permido foi 1000. A linha com * representa a BAM, a linha com A representa a ABAM e a linha com o representa a BECAM. Note que a ABAM atingiu o numero maximo de iteracoes permitido. Isso mostra que a ABAM pode convergir para um ciclo limite. Lembre-se que nao temos um resultado que garante a convergencia da ABAM. Pelo grafico, a ABAM atingiu o numero maximo de iteracoes para k &gt; 40. Neste caso, podem haver ciclos-limite na ABAM
Fig. 7.8: Numero de iteracóes na fase de recordacao pelo niímero de padróes armazenados. As linhas marcadas representam: ECAM (o), MA Hopfield (*), MA Personnaz (A) e MA Kanter (V).
que impedem a convergencia do padrao-chave para um ponto de equilíbrio.
Iteracoes
Fig. 7.9: Numero de iteraçoes na fase de reçordaçao pelo numero de padroes armazenados. As linhas marçadas representam: BECAM (o), BAM (*) e ABAM (A).
Capítulo 8
Conclusão
Nesta dissertação apresentamos um estudo comparativo em memórias associativas neurais com ênfase nas memórias associativas morfológicas. Nos concentramos nos modelos de memórias associativas binarias que sao citados frequentemente na literatura de redes neurais.
No capítulo 1 apresentamos uma revisao história e bibliográfica sobre redes neurais, morfologia matematica, algebra de imagens e principalmente sobre memórias associativas neurais. Nos capítulos 2 e 3 discutimos conceitos basicos de redes neurais e memórias associativas. Existe uma variedade grande de notacóes para modelos de memórias associativas neurais e estes capítulos basicos esclarecem a notacao usada durante a dissertacao. Adotamos uma notacao matricial comum em ambos livros de algebra linear e redes neurais artificiais, como por exemplo, nas referencias [101] e [33]. As cinco características para um bom desempenho apresentadas por nós no capítulo 3 foram inspiradas nos trabalhos de Hassoun e Pao [31, 64].
No capítulo 4 discutimos as memórias associativas lineares. Este capítulo tem um objetivo didatico visto que as memórias associativas lineares definem as principais regras de aprendizado utilizadas nas memórias associativas neurais discutidas nesta dissertacao. Verificamos que o armazenamento por correlacao possui limitacóes devido a interferência cruzada. No armazenamento por projecao temos erro devido a dependencia linear das memórias fundamentais e devido ao ruído introduzido no padráo-chave. Um mímero ilimitado de padrões podem ser armazenados na OLAM no caso auto-associativo.
No capítulo 5 discutimos as memórias associativas dinamicas. Este e o maior capítulo da dissertacao devido ao grande numero de modelos apresentados na literatura. Para cada modelo apresentamos a arquitetura, regra de aprendizado, exemplos computacionais e uma breve anaílise sobre a convergencia. A ABAM foi o ilnico modelo que nao possui nenhum resultado garantindo sua convergencia para um ponto fixo. A conjectura 5.2.1 e uma proposta nossa baseada no resultado do artigo de McEliece et. al. [56] e pode ser vista como uma generalizacao do teorema 5.1.2 apresentado para a rede de Hopfield. As memórias associativas de Personnaz e Kanter-Somplinsky sao ambas referidas como “rede de Hopfield com armazenamento por projecao”. Verificamos que existem diferencas entre estes dois modelos, principalmente com respeito ao raio de atracao (tolerância a ruído). As proposicoes que garantem a convergencia da memória associativa de Personnaz foram introduzidas por nós. A BECAM foi introduzida por nós como uma generalizacao da ECAM inspirada na BAM. Todos os resultados relativos a BECAM sao novos. O teorema 5.8.3 e inedito e relaciona o modelo BSB com o modelo de Hopfield.
No çapítulo 6 apresentamos um estudo detalhado das memorias assoçiativas morfologiças. Começamos disçutindo o çaso hetero-assoçiativo onde apresentamos a arquitetura, regra de aprendizado, exemplos e teoremas que garantem o armazenamento e a reçordaçao de padroes. O teorema 6.1.3 foi introduzido e demonstrado nesta dissertaçao. Conçluímos que as memorias assoçiativas mor-fologiças hetero-assoçiativas de çamada uniça nao sao çapazes de armazenar muitos padroes binarios e possuem uma tolerânçia a ruído restrita a padroes çorrompidos çom ruído dilatio ou ruído erosivo. O çaso auto-assoçiativo tambem foi disçutido detalhadamente. Mostramos que as memorias auto-assoçiativas morfologiças de çamada uniça podem armazenar um numero ilimitado de padroes, çonvergem para um ponto fixo çom uma uniça iteraçao, e tambem apresentam restriçoes no tipo de ruído introduzido nos padroes-çhave. Apresentamos um teorema que çaraçteriza todos os pontos fixos das memorias auto-assoçiativas binarias e verifiçamos que estas apresentam um grande numero de padroes espilrios. Disçutimos brevemente o metodo do nuçleo e depois introduzimos as memorias assoçiativas morfologiças de duas çamadas. Terminamos o çapítulo çom um teorema que çaraçteriza os pontos fixos deste ultimo modelo.
No çapítulo 7 formalizamos os çonçeitos para a medida do desempenho de uma memoria asso-çiativa e usamos estes çonçeitos para çomparar os varios modelos de memoria assoçiativa binaria apresentados nos çapítulos 5 e 6. Com base nos resultados obtidos çonçluímos que:
•	As memorias auto-assoçiativas morfologiças, de Personnaz e Kanter-Sompolinsly apresentaram uma çapaçidade de armazenamento çonstante igual a 1, isto e, podemos armazenar infinitos padroes. A ECAM e a BECAM apresentam uma çapaçidade de armazenamento igual a c
e ck(«+m) çom c proximo de 1, respeçtivamente.
•	A funçao de distribuiçao informa quantos pesos sinaptiços podemos exçluir sem perder a informaçao armazenada numa memoria assoçiativa neural. As memorias assoçiativas de Kanter-Sompolinsky, Personnaz e Hopfield apresentaram os melhores resultados no çaso auto-assoçiativo. A ABAM e a BAM apresentaram os melhores resultados no çaso heteroassoçiativo. Os piores resultados para a distribuiçao da informaçao foram obtidas pela ECAM, BECAM e as memorias assoçiativas de duas çamadas.
•	As memorias assoçiativas morfologiças de duas çamadas apresentaram a maior tolerânçia a ruído (maior raio de atraçao), entretanto, este modelo e restrito a padroes-çhave çorrompidos çom ruído dilativo ou erosivo. A ECAM e a BECAM sao os modelos mais reçomendados para padroes-çhave çorrompidos çom ambos ruído dilativo e erosivo.
•	As memorias assoçiativas morfologiças de duas çamadas, a ECAM e a BECAM apresentaram as menores probabilidades de çonvergir para uma memoria espúria. Lembramos que este resultado e valido para as memorias assoçiativas morfologiças supondo que o padrao-çhave representa uma versao erodida ou dilatada de uma memoria fundamental.
•	O esforço çomputaçional depende da dimensao dos padroes de entrada e saída (n e m) e do numero de memorias fundamentais armazenadas (k). Se k&amp;lt;&amp;lt;n, m, a ECAM e a BECAM serao os modelos que requerem o menor esforço çomputaçional, supondo que as memorias çonvergem rapidamente para um ponto-fixo. Se k e proximo de n ou m, entao as memorias assoçiativas morfologiças serao os modelos que efetuam o menor numero de operaçoes.
Finalmente, com base nos resultados apresentados no capítulo 7, nao podemos afirmar qual e o melhor modelo de memoria associativa neural, pois cada modelo apresenta pontos positivos e negativos. Esta dissertacao de mestrado serve como um guia para a escolha do modelo de memoria associativa que melhor se enquadra a um dado problema. Por exemplo, as memorias associativas morfológicas de duas camadas serao os modelos recomendados para um problema onde queremos armazenar um grande número de memorias fundamentais buscando um baixo custo computacional e sabendo-se que os padroes-chave serao versoes corrompidas somente com ruído dilativo ou somente com ruído erosivo. Lembre-se que no capítulo 1 citamos varias referencias contendo aplicacoes de memorias associativas neurais.
Referencias Bibliográficas
[1]	Computer Vision Group Image Database,	Available at
http://decsai.ugr.es/cvg/index2.php.
[2]	Amari, S.-I. Neural theory of association and concept-formulation. Biological Cybernetics 26 (1977), 175-185.
[3]	Amit, D. J. Modeling Brain Function-The World of Attractor Neural Network. Cambridge University Press, 1989.
[4]	ANDERSON, J. A simple neural network generating interactive memory. Mathematical Biosciences 14 (1972), 197-220.
[5]	ANDERSON, J. An Introduction to Neural Networks. MIT Press, MA, 1995.
[6]	Anderson, J., Silverstein, J., Ritz, S., and Jones, R. Distinctive features, categorical perception, and probability learning: Some applications of a neural model. Psychology Review 84 (July 1977), 413-415.
[7]	ANDERSON, J. A., AND Rosenfeld , E. Neurocomputing: Foundations of Research, vol. 1. MIT Press, Cambridge, MA, 1989.
[8]	Beale, R., AND Fiesler , E., Eds. Handbook of Neural Computation. Institue of Physics Publishing and Oxford University Press, 1997.
[9]	Bhaya, A., Kaszkurewicz, E., and Kozyakin, V. Existence and stability of a unique equilibrium in continuous-valued discrete-time asynchronous hopfield neural networks. IEEE Trans. on Neural Networks 7, 3 (May 1996), 620 - 628.
[10]	Billingsley, P. Probability and Measure, 2 ed. John Wiley and Sons, 1986.
[11]	Birkhoff, G. Lattice Theory, 3 ed. American Mathematical Society, Providence, 1993.
[12]	Brunak, S., AND Lautrup, B. Neural Networks: Computers with Intuition. World Scientific, 1990.
[13]	BRYSON, A., AND Ho, Y. Applied Optimal Control, rev ed edition ed. John Wiley and Sons, October 1979.
[14]	Casasent, D., and Telfer, B. Associative memory synthesis, performance, storage capacity, and updating: new heteroassociative memory results. SPIE, Int. Robots Comput. Vision 848 (1987), 313-333.
[15]	Chiueh, T., and Goodman, R. Recurrent correlation associative memories. IEEE Trans. on Neural Networks 2 (Feb. 1991), 275-284.
[16]	Chiueh, T., AND Goodman, R. Recurrent Correlation Associative Memories and their VLSI Implementation. In Hassoun [31], 1993, ch. 16, pp. 276-287.
[17]	Collier, R. J. Some current views on holography. IEEE Spectrum 3 (July 1966), 67-74.
[18]	CUNINGHAME-G REEN, R. Minimax Algebra: Lecture Notes in Economics and Mathematical Systems 166. Springer-Verlag, New York, 1979.
[19]	CUNINGHAME-GREEN, R. Minimax algebra and applications. In Advances in Imaging and Electron Physics, P. Hawkes, Ed., vol. 90. Academic Press, New York, NY, 1995, pp. 1-121.
[20]	Davidson, J., and Ritter, G. A theory of morphological neural networks. In Digital Optical Computing II (July 1990), vol. 1215 of Proceedings of SPIE, pp. 378-388.
[21]	Fu, L. Neural Networks in Computer Intelligence. McGraw-Hill, New York, NY, 1994.
[22]	Gabor, D. Associative holographic memories. IBM J. Res. Develop 13 (1969), 156-159.
[23]	Golden, R. M. The brain-state-in-a-box neural model is a gradient descent algorithm. Journal of Mathematical Psychology 30 (1986), 73-80.
[24]	Golden, R. M. Stability and optimization analysis of the generalized brain-state-in-a-box neural network model. Journal of Mathematical Psychology 30 (1993), 73-80.
[25]	Golub, G. H., AND loan, C. F. V. Matrix Computations, 3rd ed. Johns Hopkins University Press, 1996.
[26]	Grain a, M., Gallego, J., Torrealdea, F. J., and D’Anjou, A. On the application of associative morphological memories to hyperspectral image analysis. Lecture Notes in Computer Science 2687 (2003), 567-574.
[27]	HADWIGER, H. Vorlesungen Uber Inhalt, Oberflache und Isoperimetrie. Springer-Verlag, Berlin, 1957.
[28]	Hagan, M., Demuth, H., and Beale, M. Neural Network Desing. PWS Publishing Company, Boston, 1996.
[29]	Hanson, S., and Kegl, J. Parnip: a connectionist network that learns natural language grammar from exposure to natural language sentences. In Proc. 9th Annu. Conf. Cognitive Science (1987), pp. 106-119.
[30]	HASSOUN, M. H. Dynamic heteroassociative neural memories. Neural Networks 2, 4 (1989), 275-287.
[31]	HASSOUN, M. H. Dynamic associative neural memories. In Associative Neural Memories: Theory and Implementation, M. H. Hassoun, Ed. Oxford University Press, Oxford, U.K., 1993.
[32]	HASSOUN, M. H. Fundamentals of Artificial Neural Networks. MIT Press, Cambridge, MA, 1995.
[33]	HAYKIN, S. Neural Networks: A Comprehensive Foundation. Prentice Hall, Upper Saddle River, NJ, 1999.
[34]	Hebb, D. The Organization of Behavior. John Wiley &amp;amp; Sons, New York, 1949.
[35]	HEIJMANS, H. Morphological Image Operators. Academic Press, New York, NY, 1994.
[36]	HIRSCH, M. W. Convergent activation dynamics in continuous time networks. Neural Networks 2, 5 (1989), 331-349.
[37]	HOPFIELD, J. Neurons with graded response have collective computational properties like those of two-state neurons. Proceedings of the National Academy of Sciences 81 (May 1984), 3088-3092.
[38]	Hopfield, J., and Tank, D. Neural computation of decisions in optimization problems. Biological Cybernetics (1985).
[39]	Hopfield, J., and Tank, D. Computing with neural circuits: A model. Proceedings of the National Academy of Sciences 233 (August 1986), 625-633.
[40]	Hopfield, J. J. Neural networks and physical systems with emergent collective computational abilities. Proceedings of the National Academy of Sciences 79 (Apr. 1982), 2554-2558.
[41]	HUI, S., Lillo, W. E., AND Zak, S. H. Dynamics and Stability Analysis of the Brain-State-in-a-Box (BSB) Neural Models. In Hassoun [31], 1993, ch. 11, pp. 212-224.
[42]	HUI, S., AND ZAK, S. H. Dynamical analysis of the brain-state-in-a-box (bsb) neural models. IEEE Transactions on Neural Networks 3, 1 (January 1992), 86-94.
[43]	JAMES, B. Probabilidade: Um Curso em Nivel Intermedidrio. Publicacao IMPA, 1996.
[44]	KANTER, I., AND Sompolinsky, H. Associative recall of memory without errors. Physical Review 35 (1987), 380-392.
[45]	KAPPEN, B., AND Gielen, S. Neural Networks: Best Practice in Europe. Progress in Neural Processing, 8. World Scientific, Amsterdam, 1997.
[46]	Kawamoto, A. H., and Anderson, J. A. A neural network model of multistable perception. Acta Psychologica 59 (1985), 35-65.
[47]	KOHONEN, T. Correlation matrix memory. IEEE Transactions on Computers C-21 (1972), 353-359.
[48]	KOHONEN, T. Associative Memory - A System Theoric Approach. Berlin: Springer-Verlag, 1977.
[49]	KOHONEN, T. Self-Organization and Associative Memory. Springer Verlag, 1984.
[50]	Kohonen, T., AND Ruohonen, M. Representation of associated data by computers. IEEE Transactions on Computers C-22 (1973), 701-702.
[51]	Kosko, B. Adaptive bidirectional associative memories. Applied Optics 26, 23 (Dec. 1987), 4947-4960.
[52]	KOSKO, B. Bidirectional associative memories. IEEE Transactions on Systems, Man, and Cybernetics 18 (1988), 49-60.
[53]	Lima, E. L. Algebra Linear, 3 ed. Instituto de Matematica Pura e Aplicada, Rio de Janeiro, RJ, 1998.
[54]	Marcantonio, A., Darken, C., Kuhn, G. M., Santoso, I., Hanson, S. J., and Petsche, T. A neural network autoassociator for induction motor failure prediction. In Adv. Neural Inf. Process. Syst. (1996), vol. 8, pp. 924-930.
[55]	McCulloch, W., and Pitts, W. A logical calculus of the ideas immanent in nervous activity. Bulletin of Mathematical Biophysics 5 (1943), 115-133.
[56]	McEliece, R. J., Posner, E. C., Rodemich, E. R., and Venkatesh, S. The capacity of the Hopfield associative memory. IEEE Transactions on Information Theory 1 (1987), 33-45.
[57]	MINKOWSKI, H. Gesammelte Abhandlungen. Teubner Verlag, Leipzig-Berlin, 1911.
[58]	Minsky, M., and Papert, S. Perceptrons. MIT Press, Cambridge, MA, 1969.
[59]	Murakami, K., and Aibara, T. An improvement on the moore-penrose generalized inverse associative memory. IEEE Transactions on Systems, Man, and Cybernetics SMC-17, 4 (July/August 1987), 699-707.
[60]	NAKANO, K. Associatron: A model of associative memory. IEEE Trans. on Systems, Man, Cybernetics SMC-2 (1972), 380-388.
[61]	Nelson, M. M., and Illingworth, W. A Practical Guide to Neural Nets. Addison-Wesley Publishing Company, 1994.
[62]	Okajima, K., Tanaka, S., and Fujiwara, S. A heteroassociative memory with feedback connection. In Proceedings of the IEEE First International Conference on Neural Networks (San Diego, 1987), vol. II, pp. 711-718.
[63]	OTSU, N. A threshold selection method from gray-level histograms. IEEE Transactions on Systems, Man, and Cybernetics 9, 1 (1979), 62-66.
[64]	Pao, Y. H. Adaptive Pattern Recognition and Neural Networks. Addison-Wesley, Reading, MA, 1989.
[65]	Pedrycz, W. Heterogeneous fuzzy logic networks: Fundamentals and development studies. IEEE TRANSACTIONS ON NEURAL NETWORKS 15, 6 (NOV 2004), 1466-1481.
[66]	PERFITTI, R. A synthesis procedure for brain-state-in-a-box neural networks. IEEE Transactions on Neural Networks 6, 5 (Sept 1995), 1071-1080.
[67]	Personnaz, L., Guyon, I., and Dreyfus, G. Information storage and retrieval in spin glass like neural networks. Journal of Physics Letter 46 (1985), L359-L365.
[68]	Raducanu, B., Grain a, M., and Albizuri, X. F. Morphological scale spaces and associative morphological memories: Results on robustness and practical applications. Journal of Mathematical Imaging and Vision 19, 2 (2003), 113-131.
[69]	Ritter, G. X. Image algebra with applications. Unpublished manuscript, available via anonymous ftp from ftp://ftp.cis.ufl.edu/pub/src/ia/documents, 1997.
[70]	Ritter, G. X., de Leon, J. L. D., and Sussner, P. Morphological bidirectional associative memories. Neural Networks 6, 12 (1999), 851-867.
[71]	Ritter, G. X., Li, D., and Wilson, J. N. Image algebra and its relationship to neural networks. In Technical Symposium Southeast on Optics, Electro-Optics, and Sensors (Orlando, FL, Mar. 1989), Proceedings of SPIE.
[72]	Ritter, G. X., Shrader-Frechette, M. A., and Wilson, J. N. Image algebra: A rigorous and translucent way of expressing all image processing operations. In Technical Symposium Southeast on Optics, Electro-Optics, and Sensors (Orlando, FL, May 1987), Proceedings of SPIE.
[73]	Ritter, G. X., and Sussner, P. An introduction to morphological neural networks. In Proceedings of the 13th International Conference on Pattern Recognition (Vienna, Austria, 1996), pp. 709-717.
[74]	Ritter, G. X., and Sussner, P. Morphological neural networks. In Intelligent Systems: A Semiotic Perspective; Proceedings of the 1996 International Multidisciplinary Conference (Gaithersburg, Maryland, 1996), pp. 221-226.
[75]	Ritter, G. X., and Sussner, P. Morphological perceptrons. In ISAS'97, Intelligent Systems and Semiotics (Gaithersburg, Maryland, 1997).
[76]	Ritter, G. X., Sussner, P., and de Leon, J. L. D. Morphological associative memories. IEEE Transactions on Neural Networks 9, 2 (1998), 281-293.
[77]	Ritter, G. X., and Urcid, G. Lattice algebra approach to single-neuron computation. IEEE Transactions on Neural Networks 14, 2 (March 2003), 282-295.
[78]	Ritter, G. X., and Wilson, J. N. Image algebra: A unified approach to image processing. In Medical Imaging (Newport Beach, CA, Feb. 1987), vol. 767 of Proceedings ofSPIE.
[79]	Ritter, G. X., AND WILSON, J. N. Handbook of Computer Vision Algorithms in Image Algebra, 2 ed. CRC Press, Boca Raton, 2001.
[80]	Ritter, G. X., Wilson, J. N., and Davidson, J. L. AFATL standard image algebra. Tech. Rep. TR 87-04, University of Florida CIS Department, Oct. 1987.
[81]	Ritter, G. X., Wilson, J. N., and Davidson, J. L. Image algebra: An overview. Computer Vision, Graphics, and Image Processing 49, 3 (Mar. 1990), 297-331.
[82]	Rosenblatt, F. The perceptron: A probabilistic model for information storage and organization in the brain. Psychological Review 65 (1958), 386-408.
[83]	RUSSELL, S. J., AND Norvig, P. Artificial Intelligence: A Modern Approach, 2nd edition ed. Prentice Hall, December 2002.
[84]	Serra, J. Mathematical morphology and cmm : a historical overview. Avaliable at: http://cmm.ensmp.fr/Recherche/pages/nav0b.htm.
[85]	SERRA, J. Image Analysis and Mathematical Morphology. Academic Press, London, 1982.
[86]	SERRA, J. Image Analysis and Mathematical Morphology, Volume 2: Theoretical Advances. Academic Press, New York, 1988.
[87]	SOILLE, P. Morphological Image Analysis. Springer Verlag, Berlin, 1999.
[88]	STEINBRUCH, K. Die lernmatrix. Kybernetick 1 (1961), 36-45.
[89]	Stiles, G., and Denq, D. On the effect of noise on the moore-penrose generalized inverse associative memory. IEEE Transaction on Pattern Analysis and Machine Intelligence PAMI-7, 3 (May 1985), 358-360.
[90]	SUSSNER, P. Fixed points of autoassociative morphological memories. In Proceedings of the International Joint Conference on Neural Networks (Como, Italy, July 2000), pp. 611-616.
[91]	Sussner, P. Observations on morphological associative memories and the kernel method. Neurocomputing 31 (Mar. 2000), 167-183.
[92]	Sussner, P. A relationship between binary morphological autoassociative memories and fuzzy set theory. In Proceedings of the IEEE/INNS International Joint Conference on Neural Networks 2001 (Washington, July 2001).
[93]	Sussner, P. Associative morphological memories based on variations of the kernel and dual kernel methods. Neural Networks 16,5 (July 2003), 625-632.
[94]	SUSSNER, P. Generalizing operations of binary morphological autoassociative memories using fuzzy set theory. Journal of Mathematical Imaging and Vision 9, 2 (Sept. 2003), 81-93. Special Issue on Morphological Neural Networks.
[95]	Sussner, P. New results on binary auto- and heteroassociative morphological memories. In Proceedings of the International Joint Conference on Neural Networks 2005 (Montreal, Canada, 2005), pp. 1199-1204.
[96]	Sussner, P., and Valle, M. A brief account of the relations between gray-scale mathematical morphologies. In Proceedings of the Brazilian Symposium on Computer Graphics and Image Processing (SIBGRAPI) (Natal, Brazil, October 2005), pp. 79 - 86.
[97]	Sussner, P., and Valle, M. Gray-scale morphological associative memories. Accepted for publication in IEEE Transactions on Neural Networks, June 2005.
[98]	Sussner, P., and Valle, M. Implicative fuzzy associative memories. Accepted for publication in IEEE Transactions on Fuzzy Systems, May 2005.
[99]	Tank, D., and Hopfield, J. J. Collective computation in neuronlike circuits. Scientific American 257, 6 (December 1987), 104-114.
[100]	TAYLOR, W. Eletrical simulation of some nervous system functional activities. Information Theory 3 (1956), 314-328.
[101]	TREFETHEN, L. N., AND Bau III, D. Numerical Linear Algebra. SIAM Publications, Philadelphia, PA, 1997.
[102]	Valle, M., and Sussner, P. IFAMs - memorias associativas baseadas no aprendizado nebuloso implicativo. In Anais do VII Congresso Brasileiro de Redes Neurais (Natal, October 2005).
[103]	Valle, M., Sussner, P., and Gomide, F. Introduction to implicative fuzzy associative memories. In Proceedings of the IEEE International Joint Conference on Neural Networks (Hungary, July 2004), pp. 925 - 931.
[104]	Valle, M. E. MATLAB Source Code for Associative Memory Models, Available at http://www.ime.unicamp.br/ ~mevalle/.
[105]	VAN Heerden, P. J. A new optical method of storing and retrieving information. Appl. Opt. 2 (1963), 387-392.
[106]	VAN Heerden, P. J. Theory of optical information storage in solids. Appl. Opt. 2 (1963), 393-400.
[107]	WIDROW, W., AND Hoff, M. Adaptive switching circuits. WESCON Convention Record (1960), 96-104.
[108]	WILSON, S. Morphological networks. In Visual Communication and Image Processing IV (Philadelphia, PA, Nov. 1989), Proceedings of SPIE.
[109]	Xu ZB, Leung Y, H. X. Assymmetric bidirectional associative memories. IEEE Trans. on Systems, Man, and Cybernetics 24, 10 (OCT 1994), 1558-1564.
[110]	YEUNG, D., AND Chow, C. Parzen window network intrusion detectors. In Int. Conf. Pattern Recognit. (2002), pp. 385-388.
[111]	Zhang, B.-L., Zhang, H., and Ge, S. S. Face recognition by applying wavelet subband representation and kernel associative memory. IEEE Transactions on Neural Networks 15,1 (Jan. 2004), 166-177.
[112]	Zhang, H., Huang, W., Huang, Z., and Zhang, B. A kernel autoassociator approach to pattern classification. IEEE Transactions on Systems, Man and Cybernetics, Part B 35, 3 (June 2005), 593- 606.</field>
	</doc>
</add>