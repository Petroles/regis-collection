<?xml version="1.0" encoding="utf-8"?>
<add>
	<doc>
		<field name="docid">BR-TU.13506</field>
		<field name="filename">19510_ulfc120705_tm_Joana_Estevens.pdf</field>
		<field name="filetype">PDF</field>
		<field name="text">
 

2016 

UNIVERSIDADE DE LISBOA 

FACULDADE DE CIÊNCIAS 

DEPARTAMENTO DE MATEMÁTICA 

 

INSTITUTO SUPERIOR DE CIÊNCIAS DO TRABALHO E DA EMPRESA 

DEPARTAMENTO DE FINANÇAS 

 

 

 

 

 

 

Stochastic Modelling of Non-Stationary Financial Assets 

 

 

 

 

Joana Ribeiro Estevens 

 

 

 

Mestrado em Matemática Financeira   

  

 

 

Dissertação orientada por: 

Professor Doutor João Pedro Silva Brito Boto 

Doutor Pedro Gonçalves Lind 

 





Acknowledgements

I would like to thank for the amazing opportunity of finishing my master’s dissertation in

Germany under the programme Erasmus+ sponsored by the European Union. This research

was partially supported by the grant they gave me.

I would also like to express my sincere gratitude to my colleagues from the department of

Statistical-Physics from the University of Osnabru?ck who gave me a warm welcome in Germany

and provided me with useful comments and remarks about my work. A special thanks to

Professor Philipp Maass not only for his insights but also for the hard questions that allowed be

to expand my work.

This dissertation could not have been completed without the guidance, support and encour-

agement that I received from my advisors, Professor Joa?o Boto and Doctor Pedro Lind. A very

special and heartfelt thank you for both of them who always kept the office door open and were

always available whenever I needed it.

I thank my fellow colleague Paulo Rocha for the useful discussions, the data sharing and for

his infinite patient in helping me overcome my difficulties during this period.

Finally, I would like to thank my family and friends who were always there when I needed.

To my parents that supported me in each and every imaginable way throughout my entire life.

To Cristiana who has always given me the moral support that I needed. To my partner in life,

my everyday support, Gonc?alo, I thank you for giving me more than I could have asked for.

i



ii



Abstract

In this dissertation, we propose a framework to model the evolution of the non-stationary

time series of the volume-price from 2000 companies from the New York Stock Exchange (NYSE).

It was shown that for each 10 minutes window, the distribution that best fits our data is the

log-normal distribution. Since the volume-price series is non-stationary, we treat the parameters

? and ? of the log-normal distribution as stochastic variables. We assume that all the time

dependency of the volume-price series is included in the parameters, ?(t) and ?(t). Therefore,

by describing the evolution of ? and ? we are able to model the volume-price series.

Our analysis decomposes the parameters evolution into average daily patterns, ? and ?, and

fluctuations around these patterns, ?? and ??.

The daily patterns ? and ? are easily modelled using cubic curves. The fluctuations ?? and ??

are modelled using Langevin equations. We show that both parameters fluctuations, ?? and ??,

are weakly correlated, which leads us to consider two different models. One, where we assume

that the variables are not correlated and therefore yields two independent Langevin equations for

our data. And another, two-dimensioned, where we take into account the correlation between the

two variables ?? and ?? which leads to the derivation of a system of coupled Langevin equations.

We used the first approach because most of the times, the simplest models are the ones used in

the real world applications. However, we know that the variables are indeed correlated. Thus,

we also followed the other approach where we took into account this correlation since we expect

it to yield better results.

Finally, we use the system of Langevin equations and the cubic curves describing the daily

patterns to reconstruct two synthetic time series statistically reproducing the evolution of the

parameters ? and ?, and show that they unequivocally determine the expected value of each

statistical moment of the log-normal distribution of the volume-price. By comparing the prob-

ability density function of these expected values using the ? and ? time series obtained from

our model versus the empirical ones, we arrive to the conclusion that our model describes well

the first moments of the volume-price time series. This framework proposed by us is general

enough to be applied to other fields of study where non-stationary stochastic variables need to

be modelled, like biology, medicine, geology, among others.

The data from the NYSE was collected directly from Yahoo Finance between January 2011

and April 2014, with a sampling frequency of 10 minutes, which give us a total of 976 days.

Keywords: Stochastic Differential Equations, Non-Stationarity, Time Series, Brownian Motion.

iii



iv



Resumo

Nesta dissertac?a?o vamos propor um modelo que nos permite estudar a se?rie temporal na?o

estaciona?ria do volume-prec?o de 2000 empresas cotadas no New York Stock Exchange. Estes

dados foram recolhidos diretamente do Yahoo Finance entre 27/01/2011 e 06/04/2014, com uma

freque?ncia de amostragem de 10 minutos. O volume-prec?o num dado instante corresponde ao

produto entre o prec?o de determinada ac?a?o pelo volume transacionado da mesma. Esta e? uma

varia?vel muito importante na matema?tica financeira pois incorpora a interac?a?o existente entre

o volume e o prec?o de uma ac?a?o. Por exemplo, volumes altos te?m tende?ncia a originar prec?os

altos, enquanto volumes baixos esta?o habitualmente associados a prec?os mais baixos. Para ale?m

disso, quando uma pessoa decide comprar/vender uma ac?a?o, o prec?o da mesma na?o e? a u?nica

varia?vel importante. Tambe?m e? necessa?rio ter em conta o volume, visto que este esta? associado

a? liquidez do t??tulo, isto e?, ao qua?o fa?cil ou dif??cil e? comprar/vender a ac?a?o. Para ale?m disso,

a distribuic?a?o do volume-prec?o da?-nos informac?a?o sobre a quantidade de capital que esta? a ser

transacionada no mercado.

Apesar da se?rie temporal do volume-prec?o ser na?o estaciona?ria, esta segue uma forma fun-

cional constante ao longo do tempo. Ja? foi mostrado em trabalhos anteriores [1] que, para cada

janela temporal de 10 minutos, o modelo que melhor explica o volume-prec?o das 2000 empre-

sas da bolsa de Nova Iorque e? uma log-normal com para?metros ? e ?. A me?dia do logaritmo

do volume-prec?o e? representada pelo para?metro ? enquanto o desvio-padra?o do logaritmo do

volume-prec?o e? representado pelo para?metro ?.

E? sabido da estat??stica cla?ssica que e? poss??vel chegar a uma fo?rmula fechada para o n-

e?simo momento de uma log-normal e que, sabendo a expressa?o de todos os momentos de uma

distribuic?a?o, podemos chegar a? sua func?a?o densidade de probabilidade usando transformadas

de Fourier. Portanto, o nosso objetivo de estudar a se?rie na?o estaciona?ria do volume-prec?o

resume-se a estudar as se?ries estaciona?rias dos para?metros ? e ?.

A nossa ana?lise decompo?e a evoluc?a?o dos para?metros ? e ? na soma dos seus padro?es me?dios

dia?rios, ? e ?, com as flutuac?o?es em torno destes para?metros, ?? e ??. Vamos modelar estas duas

partes separadamente e utilizando abordagens diferentes.

Modelar os padro?es dia?rios ? e ? e? um procedimento simples, visto que ambos se podem

descrever por uma func?a?o cu?bica. A modelac?a?o das flutuac?o?es ja? tem que ser mais cuidadosa

pois estamos perante se?ries temporais com um comportamento fortemente estoca?stico. Como

tal, vamos modelar estas se?ries atrave?s de equac?o?es diferenciais estoca?sticas, tambe?m conhecidas

como equac?o?es de Langevin. E? importante referir que no?s dividimos as se?ries temporais ? e ? em

duas partes porque na?o e? poss??vel modelar dados que contenham algum tipo de periodicidade

atrave?s de equac?o?es de Langevin. Portanto, tivemos que retirar a parte perio?dica dos nossos

dados. Utiliza?mos a transformada ra?pida de Fourier para nos certificarmos de que na?o persistia

nenhum tipo de periodicidade nas nossas se?ries temporais das flutuac?o?es.

v



Comec?a?mos por modelar as flutuac?o?es assumindo que as se?ries ?? e ?? sa?o independentes uma

da outra. Apesar de isto na?o se verificar na realidade, o coeficiente de correlac?a?o entre as se?ries

e? muito baixo pelo que podemos supor, para um modelo mais simples, que e? aproximadamente

zero. Para ale?m disso, na maior parte das situac?o?es da vida real, os modelos mais simples sa?o

prefer??veis face aos mais complexos, visto que e? mais fa?cil implementa?-los e interpretar os seus

resultados. Como tal, no?s extra??mos dos nossos dados os coeficientes que regem as duas equac?o?es

de Langevin que modelam as flutuac?o?es dos nossos dados. Em ambos os casos, o coeficiente da

parte determin??stica da equac?a?o, D(1), corresponde a uma func?a?o linear da varia?vel em estudo,

enquanto o coeficiente da parte estoca?stica, D(2), e? uma func?a?o quadra?tica.

Apo?s esta primeira abordagem com um modelo mais simples, passa?mos para um modelo que

incorpora a interac?a?o entre ?? e ??. Vamos descrever estas flutuac?o?es atrave?s de um sistema

de duas equac?o?es Langevin acopladas. Agora o coeficiente da parte determin??stica, D(1), e? na

verdade um vetor de duas func?o?es enquanto o coeficiente da parte estoca?stica, D(2), e? uma

matriz sime?trica com tre?s func?o?es distintas.

Visto que temos um sistema de equac?o?es que nos permite descrever a evoluc?a?o de ?? e ??,

podemos construir as nossas pro?prias se?ries temporais das flutuac?o?es. Somando estas se?ries

teo?ricas ao padra?o me?dio obtido pelas func?o?es cu?bicas, ficamos com se?ries temporais teo?ricas

para o ? e para o ? . Por outras palavras, estes sa?o os valores de ? e ? que o nosso modelo

preve?. Compara?mos estes valores teo?ricos das se?ries do ? e do ? com os valores emp??ricos que

ja? t??nhamos. Chega?mos a? conclusa?o de que o nosso modelo e? capaz de explicar muito bem a

evoluc?a?o da se?rie temporal do ?, visto que a densidade da se?rie teo?rica e da se?rie emp??rica sa?o

quase coincidentes. Contudo, a se?rie do ? ja? na?o e? ta?o bem explicada pelo nosso modelo. Isto

ja? era esperado visto que, como o ? e? um momento de primeira ordem, enta?o e? mais facilmente

modelado do que um momento de segunda ordem. Notar que o ? e? a raiz quadrada do segundo

momento centrado.

Como ja? vimos anteriormente, e? poss??vel chegar a? fo?rmula fechada para o o n-e?simo momento

da distribuic?a?o log-normal que depende apenas de ? e ?. Apo?s termos esta fo?rmula, podemos

substituir os valores de ? e ? pelos que obtivemos atrave?s do nosso modelo e obtemos a se?rie

temporal do n-e?simo momento teo?rico. No?s calcula?mos as se?ries dos primeiros quatro momentos

teo?ricos. Para percebermos se os nossos resultados estavam de acordo com a realidade, fizemos

um processo semelhante para as se?ries do ? e do ? emp??ricas. Finalmente, compara?mos as

func?o?es densidade de probabilidade de ambas as se?ries e percebemos que o nosso modelo descreve

muito bem os primeiros momentos da se?rie do volume-prec?o. Mais uma vez, isto tambe?m ja? era

esperado visto que os momentos de ordens superiores esta?o mais dependentes do ? do que aqueles

de ordens inferiores.

Ha? inu?meros modelos na literatura que nos permitem estudar se?ries temporais. Uma per-

gunta pertinente seria o porque? de termos escolhido esta ana?lise de Langevin em detrimento dos

outros modelos. Uma particularidade muito interessante sobre o nosso modelo e? que ele na?o

nos permite apenas descrever a evoluc?a?o temporal da se?rie do volume-prec?o. Para ale?m disso,

tambe?m podemos chegar a uma equac?a?o a?s derivadas parciais, de tipo Fokker-Plank, que nos da?

a evoluc?a?o da func?a?o densidade de probabilidade do volume-prec?o. A deduc?a?o desta equac?a?o na?o

foi efetuada no a?mbito desta tese, mas e? um excelente ponto para ser desenvolvido em trabalhos

futuros.

Para terminar, o modelo que no?s propusemos nesta tese pode ser aplicado a? bolsa de valores

de Nova Iorque para calcular medidas de risco como o Value at Risk. Uma das questo?es que

vi



se levantaram a partir da realizac?a?o deste trabalho e? se este modelo e? adequado para fazer

previso?es sobre a evoluc?a?o do volume-prec?o. Este seria um bom ponto de partida para trabalhos

futuros. Para ale?m disso, este modelo e? suficientemente geral para poder ser aplicado a se?ries

temporais de outras a?reas cient??ficas, como por exemplo no estudo da variabilidade card??aca na

fisiologia ou no estudo de se?ries s??smicas na geologia. Este trabalho proporcionou-nos uma visa?o

bastante aprofundada do estudo das se?ries tempora?rias na?o estaciona?rias e permitiu-nos propor

uma metodologia que sera? bastante u?til em va?rias a?reas.

Palavras-Chave: Equac?o?es Diferenciais Estoca?sticas, Na?o Estacionariedade, Se?ries Temporais,

Movimento Browniano.

vii



viii



Contents

1 Introduction 1

2 State of the art 5

2.1 Stationary Stochastic Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5

2.2 Brownian Motion and White Noise . . . . . . . . . . . . . . . . . . . . . . . . . . 6

2.3 Markov Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9

2.4 Stochastic Differential Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . 12

2.5 The Fokker-Planck Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16

2.6 The Black-Scholes Model and its limitations . . . . . . . . . . . . . . . . . . . . . 18

2.7 From Stochastic Volatility to Superstatistics . . . . . . . . . . . . . . . . . . . . . 19

2.8 The Langevin Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19

2.9 Statistical Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21

3 Getting to know the data 23

3.1 Outliers and Daily Patterns . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23

3.2 Log-Normal Parameter Fluctuations . . . . . . . . . . . . . . . . . . . . . . . . . 26

3.3 Markov Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30

4 The Langevin Analysis 31

4.1 A Simple Model without Correlation . . . . . . . . . . . . . . . . . . . . . . . . . 31

4.2 Modelling the Coupling between ?? and ?? . . . . . . . . . . . . . . . . . . . . . . 33

5 Approaching Non-Stationarity 39

6 Discussion and Conclusions 43

ix



x



List of Figures

1.1 Structure of the thesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2

3.1 Time series without outliers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23

3.2 Daily Patterns from ? and ? time series . . . . . . . . . . . . . . . . . . . . . . . 24

3.3 Flucuations time series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26

3.4 Power spectrum and autocorrelation function . . . . . . . . . . . . . . . . . . . . 27

3.5 Marginal PDF of the fluctuations . . . . . . . . . . . . . . . . . . . . . . . . . . . 28

3.6 Joint PDF and contour plot of the fluctuations . . . . . . . . . . . . . . . . . . . 29

3.7 Wilcoxon test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30

4.1 Computation of the D(1) and D(2) functions . . . . . . . . . . . . . . . . . . . . . 31

4.2 D(1) and D(2) functions assuming independency . . . . . . . . . . . . . . . . . . . 32

4.3 D(4) function assuming independency . . . . . . . . . . . . . . . . . . . . . . . . 33

4.4 D(1) and D(2) functions with correlation . . . . . . . . . . . . . . . . . . . . . . . 34

4.5 g functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35

5.1 Empirical and modelled series of the moments ?sn? . . . . . . . . . . . . . . . . 40
5.2 PDF for the modelled and empirical fluctuations and moments ?sn? . . . . . . . 41

xi



xii



List of Tables

4.1 Coefficients for the D(1) and D(2) functions with correlation . . . . . . . . . . . . 36

4.2 Coefficients for the g functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37

xiii



xiv



Chapter 1

Introduction

In this dissertation we are going to explore a way of understanding the evolution of the

volume-price in the stock market. We use data from 2000 companies in the New York Stock

Exchange (NYSE). We are going to do this using stochastic differential equations.

Volume-price is an important variable in mathematical finance since it incorporates the

interaction between volume and price. For instance, the volume has as an important role in

the assets’ price. High volumes trigger high prices and low volumes are associated with low

prices. Moreover, while we need to know the price of an asset if we want to sell or buy it at

the right time, we should also know the volume that is being transitioned in the market, since

high volumes reflect a good market liquidity, i.e. it is easy to sell or buy the asset. Finally, the

distribution of the volume-price provides information about the capital that is being transitioned

in the market.

We want to study the temporal evolution of the volume-price which is a non-stationary

random variable. If it would be a stationary variable, it would be very easy to solve this problem.

In the literature we have numerous ways to model stationary variables [2]. However, since we

have a non-stationary random variable, it seems almost impossible to fit a good model to it.

Recently, it was shown [1] that although the distribution of the volume-price is non-stationary,

the series has a constant functional shape (log-normal) throughout time. The parameters of

this log-normal distribution are time dependent and they are stationary. We will study these

stationary parameters in order to describe the evolution of the non-stationary time series of the

volume-price.

Figure 1.1 shows a simple scheme of the idea that will be explored in this dissertation. The

starting points of our study are the volume and price time series of 2000 companies in the New

York Stock Exchange (NYSE). This data was collected directly from Yahoo Finance, with a

sampling frequency of 10 minutes, starting in January 27th 2011 and ending in April 6th 2014,

which yields a total of 976 days (? 105 data points). The data preprocessing was done in a
previous work and can be found in Refs. [3, 4]. We can see this time series for just one company

in Figure 1.1a. Multiplying the two series yields the volume-price time series. For the same

time interval, we have a sample of approximately 2000 companies. Thus, for each 10-minutes

snapshot, we have 2000 observations of volume-price, one for each company.

It was shown [1] that the log-normal distribution had the best fit to this data. We can see a

10-minutes snapshot in Figure 1.1b: the dots represent the empirical probability density function

(PDF) of the logarithm of the volume-price time series for the 2000 companies in that particular

10 minutes. The solid line is the PDF of a normal distribution with mean and standard deviation

1



42

44

46

48
P

ri
c
e

800 850 900 950 1000 1050 1100

Time

10
3

10
4

10
5

10
6

V
o
lu

m
e

10
4

10
5

10
6

10
7

Volume-Price

1

1.1

1.2

1.3

13

14

?

800 850 900 950 1000 1050 1100

Time

1.4

1.6

1.8

?

13

14

?

-1

-0.5

0

0.5

1

?’

800 850 900 950 1000 1050 1100

Time

1.4

1.6

1.8

?

800 850 900 950 1000 1050 1100

Time

-0.2

-0.1

0

0.1

0.2

?’

(a) Single stock

Stock market log-normal model

(c)

_

_

(d) (e)

Parameter evolution

Average pattern (daily) Fluctuations

+

(b)

e

e

?

?

Figure 1.1: We start with the price and volume series of 2000 different companies. In (a) we can see the volume
and price series for just one of the companies. Multiplying the two series yields the volume-price series which
follows a log-normal distribution with parameters ? and ?. In (b) we can see the empirical density represented
by the dots and the adjusted log-normal to the data, solid line, for a particular window of 10 minutes. Each
10-minutes window yields a log-normal with different parameters. Thus, we will have (c) a time series for the
parameter ? and another one for ?. Each time series can be decomposed in (d) a daily pattern and (e) fluctuations
around this pattern. We will describe the evolution observed in (c) by analysing this components separately.

equal to the ones of the volume-price logarithm. However, the series is non-stationary: another

10-minutes window yields a log-normal with different parameters. This means that our volume-

price series follows a constant functional form throughout time (log-normal) but the parameters

of this distribution are themselves stochastic variables.

We are going to study the evolution of these parameters illustrated in Figure 1.1c. For each

10-minute window, we will have a value to the mean ? and to the standard deviation ? of the

volume-price logarithm. We can also see in this figure that both time series appear to have a

daily pattern. Because of this, we are going to split our analysis in two parts: one considering the

average daily patterns, Figure 1.1d, and another one with the fluctuations around this pattern,

Figure 1.1e. The original series is the sum of these two parts.

We will propose a framework to describe the evolution observed in Figure 1.1c, modelling

average behaviour and stochastic contributions separately. We are going to extract stochastic

differential equations for the parameters ? and ? following a recent framework [5]. By knowing

how the fluctuations from the ? and ? time series behave, we will be able to describe the evolution

of the non-stationary series of volume-price. The main goal of this dissertation is to model the

2



original non-stationary time series of volume-price, s.

In this introductory chapter we stated the problem that is going to be addressed by this

dissertation, the importance of this subject and what we did, in general lines, to solve this

problem as we can see in Figure 1.1. In Chapter 2, we will do a quick review of the main

concepts which are necessary to understand the subsequent chapters. In Chapter 3, we will

explain the preprocessing of the ? and ? time series. After that, we explore various aspects

of these series and we check if it is possible the analysis of these time series. In Chapter 4

we will analyse both time series independently and coupled. In Chapter 5, we will use the

results presented in the previous chapters to derive the equations that govern the volume-price

non-stationary time series. Finally, the discussions and conclusions are presented in Chapter 6.

3



4



Chapter 2

State of the art

2.1 Stationary Stochastic Processes

A stochastic process (Xt) is a family of random variables indexed by t belonging to some

index set T. The index set T can be any abstract set. However, in order to make it simple, we

take it as time, and hence T can be taken as R or some subset of R. In this case, we often call
the stochastic process (Xt) a time series. When we are trying to explain real world phenomena

with stochastic processes, we can notice that we only have a incomplete sample path of the

process. However, we want to figure out the probability structure from this data, i.e. the finite

dimensional distributions. If we do not make more assumptions, this is basically an impossible

task. In this section, we are going to represent by B(Rn) the ?-algebra generated by the family
of open intervals from Rn.

Definition 1 (Finite Dimensional Distributions). Let (Xt)t?0 be a stochastic process de-

fined on some probability space (?,F,P). The probabilities

PXt1,Xt2,...,Xtn (B) = P((Xt1, ...,Xtn) ? B) (2.1)

where n ? N, t1, ..., tn ? R, 0 ? t1 &amp;lt;t2 &amp;lt;... &amp;lt;tn &amp;lt;?,B ?B(Rn), i.e. B is a Borel measurable
set of Rn, are called finite dimensional distributions.

Finite dimensional distributions completely characterize the probability structure of a stochas-

tic process since they involve the knowledge of all marginal distributions. However, if you do not

make further simplifications or study special cases we will not be able to get tractable models

for stochastic processes. One special case that is easier to study is a stationary process. A

stationary stochastic process in its simplest form assumes that it is in equilibrium. In fact the

probability structure of the process does not change in time.

Definition 2 (Strict Stationarity). A stochastic process (Xt)t?0 is strictly stationary if for

any t1, t2, ..., tn and h, the joint distributions of (Xt1, ...,Xtn) and (Xt1+h, ...,Xtn+h) are identi-

cal, that is

PXt1,Xt2,...,Xtn (B) = PXt1+h,Xt2+h,...,Xtn+h(B) (2.2)

where n ? N, t1, ..., tn ? R, 0 ? t1 &amp;lt;t2 &amp;lt;... &amp;lt;tn &amp;lt;?,B ?B(Rn)

5



It is important to notice that a strictly stationary process has a probability structure which

is invariant under a time shift. This means the random variables Xt and Xt+h have the same

distribution. Therefore, if the moment exists then E(Xnt ) = E(X
n
t+h). In particular, a strictly

stationary process will have constant mean and variance. Besides that, the covariance function

of a strictly stationary time series depends only on the time interval between the time points,

not on the specific location of the points along the time axis.

Strict stationarity is a very restrictive property defined on all of the finite dimensional dis-

tributions which is very difficult to verify empirically. There are weaker forms of stationarity,

expressed in terms of the moments which are often sufficient to construct very useful processes.

Definition 3 (mth-order Stationarity). The process Xt is said to be stationary up to order

m, if for any t1, t2, ..., tn and for all m1,m2, ...,mn such that
?n

i=1 mi ? m, all product moments
exist E(Xm1t1 ...X

mn
tn

) and for any h ? 0,

E(Xm1t1 ...X
mn
tn

) = E(Xm1t1+h...X
mn
tn+h

) . (2.3)

Second-order stationarity, which is also often called weak stationarity or covariance station-

arity, is fundamental in studying large classes of processes. Strictly stationary processes do not

need to be mth-order stationary. However, when moments up to order m exist, then a strictly

stationary process will also be a mth-order stationary. Since the knowledge of full infinite se-

quence of moments under certain conditions define the finite dimensional distributions, loosely

speaking, strict stationarity corresponds to mth-order stationarity in the limit as m ??.

2.2 Brownian Motion and White Noise

Until the nineteenth century, it was commonly thought that if we could collect all the initial

data then we would predict the future with certainty. This is known as the Laplace’s dream: we

would be able to create a model of the universe which would be completely deterministic [6]. We

now know that this is not true. The limited predictability may arise in the form of fluctuations

due to interactions with the environment.

In 1828, the botanist Robert Brown observed and tried to describe the irregular movement

of pollen, suspended in water [7]. This movement is now known as the Brownian movement and

is attributed to the collisions of the pollen with the water molecules, resulting in a diffusion of

the pollen in the water. His work was largely ignored by the scientific community at his time.

In 1900, independently from Brown’s work, L. Bachelier [8] derived the law governing the

position wt at time t of a single grain of pollen performing a one-dimensional Brownian motion

starting at a ? R at time t = 0:

Pa{Wt ? dx} = p(t,a,x)dx , with p(t,a,x) =
1
?

2?t
e?

(x?a)2
2t (2.4)

where p(t,a,x) is the solution of the heat equation:

6



?u

?t
=

1

2

?2u

?a2
, (2.5)

In 1905 Albert Einstein addressed the Brown’s question without having knowledge of Bache-

lier work [9]. He predicted what the Brownian motion should be. These predictions are nowadays

a part of one of the formal definitions of Brownian motion [10].

However, it was only in 1923 that Nobert Wiener proved the existence of Brownian motion

and set down a firm mathematical foundation for its analysis. In his honour, the mathemat-

ical formulation of the Brownian motion is know as the Wiener process. Before we give the

mathematical definition of a Brownian motion, it is important to understand the concept of a

filtration.

Definition 4 (Filtration). Let X : [0, +?[×? ? R be a stochastic process on a probability
space ? with a ?-algebra F. For each 0 ? t &amp;lt;?, we define a ?-algebra Ft as the sigma algebra
generated by the variables Xs, 0 ? s ? t:

Ft = ?(Xs : 0 ? s ? t) (2.6)

If 0 ? s &amp;lt;t, then Fs ?Ft ?F. Such a family of ?-fields Ft : 0 ? t &amp;lt;? is called a filtration of
F.

This filtration is generated by the process Xt since it contains all of its past at each time t.

In an intuitive way, it is the smallest filtration available to study the process Xt: is contains all

the information regarding the past of the process, but only that information. Now we can see

what a Brownian motion is:

Definition 5 (Brownian Motion). A Brownian motion is a real valued, continuous stochastic

process (Wt)t?0, with the following properties:

• W0 = 0 almost surely.

• independent increments: If s ? t, Wt ?Ws is independent of Fs = ?(Wu : 0 ? u ? s).

• stationary increments: If s ? t, Wt ? Ws and Wt?s ? W0 have the same probability law
since they follow a normal distribution with mean zero and variance t?s.

Using the Kolmogorov theorem [10] it is possible to prove that the function t 7? W(t) is
almost surely continuous. However, the randomness allows Brownian motion to also be nowhere

differentiable as we are going to see. However, we clarify the concept of limit of a stochastic

variables that we are going to use here.

Definition 6 (Almost Sure Convergence). We say that Xn converges to X with probability

one (almost surely) if P

(
lim

n?+?
Xn = X

)
= 1. That is, for almost every ? ? ? except for a set

of ? ? ? with measure 0, the sequence Xn(?) converges to X(?).

Theorem 1 (Non-differentiability of the Brownian motion). Almost surely, Brownian

7



motion is nowhere differentiable. Furthermore, almost surely, for all t, either

lim
suph?0

sup
Wt+h ?Wt

h
= +? (2.7)

or

lim
infh?0

sup
Wt+h ?Wt

h
= ?? . (2.8)

Since the sample paths of a Brownian motion are nowhere differentiable with probability 1,

a stochastic process of the form (?t)t?0 with:

?t =
dWt
dt

= W ?t or dWt = ?tdt (2.9)

cannot be introduced by taking the almost sure limit in a difference quotient. Nevertheless, it

is possible to arrive to a definition via an integral [11]. In order to approach this definition, let

g(t) be any function with a continuous derivative g?(t) in the interval [a,b] and t0, t1, ..., tn a

sequence of numbers satisfying:

a = t0 &amp;lt;t1 &amp;lt;... &amp;lt;tn = b and ?ti = ti+1 ? ti; i = 0, 1, 2...,n? 1

Then, the stochastic integral
? b
a
g(t)dWt is defined as the almost sure limit:

? b
a
g(t)dWt = lim

n?+?, max?ti?0

n?1?
i=0

g(ti)(Wti+?ti ?Wti) = (2.10)

= lim
n?+?, max?ti?0

g(b)Wb ?g(a)Wa ?
n?1?
i=0

Wti+1
g(ti + ?ti) ?g(ti)

?ti
?ti (2.11)

The stochastic integral is an almost sure limit of a sum of normally distributed variables.

Therefore, it also has a normal distribution. It is important to notice that we are going to give a

more general definition of the stochastic integral later, after we have introduced more concepts.

Now we just want to motivate the following definition by taking the almost sure limit on both

sides of the previous equation.

Definition 7 (White Noise). Let (Wt)t?0 be a Brownian motion. A stochastic process

(?t)t?0 is called a white noise if it satisfies, for any function g(t), with a continuous deriva-

tive g?(t) in [a,b],a &amp;lt;b, the following relationship:

? b
a
g(t)?tdt = g(b)Wb ?g(a)Wa ?

? b
a
Wtg

?(t)dt . (2.12)

It is important to notice that in the left side of Equation (2.12) we are writing an abusive

notation (although common and inoffensive) since ?t does not exist as a function whereas it

is defined as a linear functional in the space of continuous differentiable functions through the

right side of Equation (2.12).

If Wt had a first derivative in order to time, then ?t =
dWt
dt

would satisfy the relationship

in the previous definition. Therefore, the white noise ?t can be interpreted as a generalized

8



derivative of the Brownian motion Wt.

If s ? t we know that Wt ? Ws ? N(0, t ? s). Therefore E(Wt) = E(Wt ? W0) = 0
and var(Wt) = var(Wt ? W0) = t ? 0 = t. If s &amp;lt;t, E(WsWt) = E(Ws(Ws + Wt ? Ws)) =
E(W 2s ) + E(Ws)E(Wt ? Ws) = var(Ws) + 0 = s. We can replicate these calculations if t &amp;lt;s
and we would get E(WsWt) = t. Therefore, E(WsWt) = min{s,t}.

Since E(Wt) = 0 and E(WsWt) = min{s,t}, we know that Cov(Ws,Wt) = min{s,t}.
Using this result for the Brownian motion and realizing that ?t =

dWt
dt

, it is easy to see that

Cov(?s,?t) = 0. Therefore, for s 6= t there is no correlation between ?s and ?t, even if we make
the difference |s? t| very small.

As a result, white noise can be seen as the most random stochastic process and this is why it is

so often used to model random noise in various fields like electronics, engineering, econometrics,

finance, among others. But also because of this property, the white noise cannot exist in the

real world.

It is not plausible for any physical phenomena to generate uncorrelated ?s and ?t even

when |s ? t| is very small. This mismatch from physical reality appears in its mathematical
representation as well: one cannot define a continuous white noise process with non-zero, finite

variance. The variance of a white noise process is undefined [12].

In practice, a weakly stationary stochastic process (?t)t?0 can approximately be considered

white noise if the covariance between ?t and ?t+? tends extremely fast to 0 with increasing ?.

The denomination white noise comes from the spectral theory of stationary random pro-

cesses that states the white noise has a power spectrum which is uniformly distributed over all

frequencies (like white light).

The range of application of Brownian motion goes far beyond a study of microscopic particles

in suspension and included modelling of stock prices, thermal noise in electrical circuits, limiting

behaviour in queuing and random perturbations in a variety of other physical, biological, eco-

nomic and management systems. Moreover, integration with respect to Brownian motion gives

us a unifying representation for a large class of diffusion processes.

Diffusion processes represented this way exhibit a rich connection with the theory of partial

differential equations. In particular, to each diffusion process there corresponds a second order

parabolic equation which governs the transition probabilities of the process.

Bachelier not only was the pioneer in modelling the Brownian motion using stochastic pro-

cesses, but he was also the first one to build a theory for the fluctuations of the stock market using

advanced mathematics. Besides that, he uncovered the Markovian property of the Brownian

motion. However, his work went largely unknown for nearly a century.

2.3 Markov Process

The Brownian motion is an example of a Markov process. This type of processes are very

important since we can derive results assuming the Markovian property which we would not be

able to get without it. The Langevin Analysis that we are testing in this thesis is one example

of this. We can only use this approach if our time series are Markovian [13] . Before we define

a Markov process, it is important to understand the concept of conditional expectation. It is

9



known from basic probability theory that the conditional probability of an event A ? F given
B ?F and P(B) &gt; 0 is the real number:

P(A|B) =
P(A?B)
P(B)

(2.13)

Using this simple concept, we can create a new probability measure:

Theorem 2 (Conditional probability of an Event). Let us consider a probability space

(?,F,P) and B ?F with P(B) &gt; 0. Thus, the application QB : F ? [0, 1] defined by QB(A) =
P(A|B) is a new probability measure in (?,F) and it is called the conditional probability of event
A given event B.

And now we are able to define the conditional expectation of a stochastic random variable,

generalizing Equation (2.13):

Definition 8 (Conditional Expectation of a Stochastic Variable given an event). Let

B ? F with P(B) &gt; 0 and let ? : ? ? R be a stochastic integrable variable, i.e., E(|?|) &amp;lt;?.
The conditional expectation of ? given an event B, E(?|B), is the real number:

E(?|B) =
E(1B?)
P(B)

(2.14)

where

1B(x) =

??
?1, if x ? B0, if x /? B (2.15)

Definition 9 (Conditional Expectation of a Stochastic Variable given a ?-algebra).

Let G be a sub- ?-algebra from F and let ? : ? ? R be a stochastic integrable variable. The
conditional expectation of ? given the ?-algebra G is a stochastic integrable variable, E(?|G) that
satisfies the following properties:

• E(?|G) is measurable with respect to G.

• If A ?G then E(1AE(?|G)) = E(1A?)

Since we are now familiar with the concept of conditional expectation, we can easily under-

stand that E(X|Ft) is the best estimate of X based on observations of the process up to time
t. The properties of conditional expectations with respect to filtrations define various types of

stochastic processes. One of the most important ones is the Markov process.

Definition 10 (Markov Process). Let us consider a stochastic process (Xt)t?0 in a probability

space (?,F,P). We say that (Xt)t?0 is a Markov process if it satisfies the following property
(Markov property): to all the functions f : R ? R, Borel measurable and bounded and 0 ? s ?
t &amp;lt;?:

E(f(Xt)|Fs) = E(f(Xt)|Xs) (2.16)

10



If X is any random variable, then E(X | Ft) is the best estimate of X based on observations
of the process up to time t. Intuitively, one interprets Equation (2.16) as follows: given the

present of the process, the future is independent of its past. A Markov process only cares about

its present state, and has no memory of how it got there.

Markov processes are very useful in applied mathematics for several reasons. Four of the

more important reason are:

• Many real world phenomena can be modeled by a Markov process.

• Usually the input needed for the application of a Markov process is more easily given than
for other non-Markovian processes.

• There are various computer algorithms for numerical simulations for Markov processes.

• All stochastic processes that have independent increments also have the Markov property.

Theorem 3 (Brownian Motion as a Markov Process). Let us consider a probability space

(?,F,P). A Brownian motion (Wt)t?0 is a Markov process.

It is also possible to characterize a Markov process using its finite dimensional distributions.

Let us consider the times:

0 ? t1 &amp;lt;t2 &amp;lt;... &amp;lt;tm &amp;lt;tm+1 &amp;lt;... &amp;lt;tn

The conditional probability that Xti = xi for m + 1 ? i ? n given that Xti = xi, for
1 ? i ? m, is given by:

p(xn, tn; ...; xm+1, tm+1 | xm, tm; ...; x1, t1) =
p(xn, tn; ...; x1, t1)

p(xm, tm; ...; x1, t1)
(2.17)

We have a Markov process if these conditional densities depend only on the most recent

time, which means:

p(xn+1, tn+1 | xn, tn; ...; x2, t2; x1, t1) = p(xn+1, tn+1 | xn, tn) (2.18)

Therefore, we also have for a Markov process:

p(xn, tn; ...; x2, t2 | x1, t1) = p(xn, tn | xn?1, tn?1)...p(x2, t2 | x1, t1) (2.19)

It is now possible to deduce all joint finite dimensional probability densities of a Markov

process Xt using only the transition density p(x,t | y,s) and the probability density of its initial
value, p0(y):

P(x,t) =

? +?
??

p(x,t | y, 0)p0(y)dy (2.20)

The transition probabilities of a Markov satisfy the Chapman-Kolmogorov equation:

p(x,t | y,s) =
? +?
??

p(x,t | z,r)p(z,r | y,s)dz , s &amp;lt;r &amp;lt;t (2.21)

11



Intuitively this means that the probability of a transition from y at time s to x at time t

is equal to the probability of the transition to z at an intermediate time r, multiplied by the

probability of the transition from z at the time r to x at the time t, summed over all possible

intermediate values z.

Definition 11 (Homogeneous Markov Process). A Markov process (Xt)t?0 is (time) ho-

mogeneous if

p(x,t | y,s) = p(x,t?s | y, 0) (2.22)

In other words, a Markov process is said to be time homogeneous if its transition probability

is stationary. A homogeneous Markov process is essentially a Markov process with invariable

stochastic properties under a time shift. The probability of a transition from y to x only depends

on the time difference t ? s. Therefore, we can write p(x,t|y,s) = p(x,t ? s|y). We can also
rewrite the Chapman-Kolmogorov equation:

p(x,t | y) =
? +?
??

p(x,t?s | z)p(z,s | y)dz , 0 &amp;lt;s &amp;lt;t (2.23)

Theorem 4 (Brownian Motion as a Homogeneous Markov Process). The Brownian

motion (Wt)t?0 is a homogeneous Markov process with state space R and transition probability
function given by

p(x,t | y) =
1
?

2?t
e?

(x?y)2
2t t &gt; 0 . (2.24)

2.4 Stochastic Differential Equations

In Equation (2.9) we established that the white noise can be seen as the time derivative of

the Brownian motion. In fact, the Brownian motion is the foundation for the constitution of

an extensive class of Markov processes with continuous sample paths, called diffusion processes.

The white noise was an example. But we can also have more sophisticated examples. For

instance, we can add a mean drift to the white noise. This diffusion process can be described

by the stochastic differential equation:

dXt
dt

= b(Xt) + ?t (2.25)

where b : R ? R is a given smooth function and ?t is a white noise. We can think about this
equation as a white noise perturbed by a drift term b or as a deterministic ordinary differential

equation perturbed by an additive white noise [13].

The white noise is the time-integral of the Brownian motion. Therefore, any differential

equation with a white noise can be rewritten as an integral equation with a Brownian motion.

We rewrite Equation (2.25) as:

12



Xt = X0 +

? t
0
b(X(s))ds + W(t) (2.26a)

or in the differential form

dXt = b(Xt)dt + dWt (2.26b)

These are stochastic differential equations (SDE) which contain a white noise with a constant

strength. In order to study SDE where the strength of the white noise depends on the solution,

we will need to give a meaning to the expression:

? T
0
ft(?)dWt(?) (2.27)

where (?,F,P) is a probability space and (Wt)t?0 is a Brownian motion.
One of the most important properties of the Brownian motion is that its paths are almost

surely not differentiable at any point. Therefore, we cannot define the integral in Equation

(2.27) as a Lebesgue-Stieltjes integral. Nevertheless, we are able to define this type of integral

with respect to a Brownian motion and we will call them stochastic integrals.

To start, we will construct the integral for a set of processes called simple processes. After

that, the definition of the integral will be extended to a more general class of processes by taking

a limit [10].

Definition 12 (Simple Process). (St(?))0?t?T is called a simple process if it can be written

as

St(?) =

p?
i=1

?i(?)1]ti?1,ti] . (2.28)

where 0 = t0 &amp;lt;t1 &amp;lt;... &amp;lt;tp = T and ?i is Fti?1 -measurable and bounded.

Definition 13 (Stochastic Integral of a Simple Process). The stochastic integral of a

simple process S defined as in Equation 2.28 is the continuous process:

? T
0
St dWt :=

n?
i=1

?i(Wti ?Wti?1 ) (2.29)

Now we want to extend the previous definition to a broader class of stochastic processes.

We will define the stochastic integral for all the stochastic processes that belong to H2([0,T]),
defined as:

Definition 14 (Stochastic Processes in H2([0, T])). Given a time interval [0,T], let H2([0,T])
be the class of stochastic processes H = (Ht)t?[0,T ] that satisfy the following conditions:

• (Ht)t?[0,T ] is a measurable process, i.e. the function [0,T] × ? ? R : (t,?) 7? Ht(?) is
measurable.

• (Ht)t?[0,T ] is adapted to the filtration .(Ft)t?0, i.e. for all t ? [0,T] the variable Ht is
Ft-measurable

• E(
?T

0
H2t ) &amp;lt;? .

13



For every process in H2([0,T]) there is a sequence of simple processes that converge to that
one process. This result is going to let us define the stochastic integral for all the processes in

H2([0,T]).

Theorem 5. To every stochastic process H ?H2([0,T]), there is a sequence (Hn)n?N of simple
stochastic processes that:

lim
n??

E

(? T
0

(Ht ?Hnt )
2

)
= 0 . (2.30)

Therefore, it is easy to see that if we have a process H ?H2([0,T]) and a sequence (Hn)n?(N)
in the conditions of the previous theorem then

?T
0

(Hnt )n?N dWt also converges and the limit

does not depend on the sequence (Hn)n?(N). Hence, we are now ready to give a definition of the

stochastic integral in H2([0,T]).

Definition 15 (Stochastic Integral). For a given process H ? H2([0,T]),we will define the
stochastic integral of Ito? of H in [0,T] as the almost sure limit of the sequence of stochastic

integrals
?T

0
(Hnt )n?N dWt, where (H

n)n?N is in the conditions of Theorem 5, i.e.? T
0
Ht dWt = lim

n??

? T
0
Hnt dWt (2.31)

Since we already know what a stochastic integral is, we can then generalize Equations (2.26)

and define a SDE in a more general way. In Equations (2.26) we examined SDE that contains

a white noise with a constant strength. Now we are going to define SDE where the strength of

the white noise depends on the solution and we are going to allow the coefficients do depend

explicitly on time.

Definition 16 (Stochastic Differential Equation). Let us consider a probability space (?,F,P)
with a filtration (Ft)t?0 and let (Wt)t?0 be a Brownian motion. A stochastic differential equation
is an equation with the following form:

Xt = ? +

? t
t0

f(s,Xs)ds +

? t
t0

g(s,Xs)dWs , t ? t0 ? 0 (2.32)

where f : R+×R ? R, (t,x) 7? f(t,x), g : R+×R ? R, (t,x) 7? g(t,x) are measurable functions,
? is a random variable and (Xt)t?0 is a stochastic process. The functions f and g are called the

coefficients of the equation and ? is the initial condition.

We can also write a SDE in the differential form:??
?dXt = f(t,Xt)dt + g(t,Xt)dWt , t ? t0Xt0 = ? (2.33)

A non rigorous explanation of Equation 2.33 (but a very useful one) is that for a little time

increase dt &gt; 0, the stochastic process (Xt)t?0 changes its value in a random quantity that is

normally distributed with mean f(t,Xt)dt and variance g(t,Xt)g
T (t,Xt)dt and it is independent

from the past of the process. This happens because the increments of the Brownian motion are

14



independent and normally distributed with mean 0 and variance equal to the time increase. In

this sense, the term g(t,Xt)dWt is used to model the perturbation of the random noise that

affects the deterministic system dXt = f(t,Xt)dt. In a physicist tradition we sometimes refer to

a SDE as a Langevin equation. Both designations mean the same.

If we want to work with SDE, we not only have to know what the stochastic integral means,

but we also have to know how to work with it. The traditional rules of calculus will not apply

to the stochastic integral.

One of the key results that allows us to work with the stochastic integral is a new version of

the chain rule called Ito?’s formula.

Theorem 6 (Ito?’s Formula). Let (Xt)t?0 be a solution of Equation 2.33 and h : R+ × R ?
R, (t,x) 7? h(t,x) a function with continuous derivative with respect to t and continuous deriva-
tives with respect to x until the second order. In another words, this means h is a function in

C2([0, +?[×R).

Then (Yt)t?0 where Yt = h(t,Xt) is a stochastic process that satisfies the equation

dYt =

(
?h

?t
(t,Xt) + f(t,Xt)

?h

?x
(t,Xt) +

1

2
g2(t,Xt)

?2h

?x2
(t,Xt)

)
dt + g(t,Xt)

?h

?x
(t,Xt)dWt .

(2.34)

Now that we have already understood how the Ito?’s formula work in one dimension, we will

generalize it to several dimensions [14].

Theorem 7 (Ito?’s Formula in d-dimension). Let Xt = (X1t , ...,X
d
t )
T be a vector process

satisfying:

dXt = Adt + HdWt (2.35)

where Wt is a multi-dimensional independent Brownian motion defined as Wt = (W 1t , ...,W
n
t )

T ,

A = (a1(t,Xt), ...,ad(t,Xt))T is the drift vector and H is the diffusion matrix:

H =

?
?????????

h11(t,Xt) ... h1n(t,Xt)

...
. . .

...

hd1(t,Xt) ... hdn(t,Xt)

?
?????????

(2.36)

Let h : R+ × Rd ? R be a given bounded function in C2([0, +?[×Rd). Let Yt = h(t,Xt).
Then,

dYt =

?
??h
?t

+
d?
i=1

ai
?h

?xi
+

1

2

d?
i,j=1

n?
p=1

hjphip
?2h

?xi?xj

?
?dt + d?

j=1

n?
p=1

hjp
?h

?xj
dW

p
t (2.37)

15



2.5 The Fokker-Planck Equation

We have already seen that the Brownian motion is a time-homogeneous Markov process with

a transition density that satisfies Equation (2.5). Fokker and Planck derived this differential

equation for the distribution function describing the Brownian motion and it is now known

as the Fokker-Planck equation for the Brownian motion. However, we can generalize this to

other stochastic processes. In fact, the Fokker-Planck equation is an equation of motion for the

distribution function of fluctuating macroscopic variables, i.e. it describes the evolution through

time of the probability density function of macroscopic variables. By solving the Fokker-Planck

equation one obtains distribution functions from which any averages of macroscopic variables

are obtained by integration.

In order for us to present the Fokker-Planck equation, it is necessary to be familiar with

the concept of a diffusion process which is a special case of a Markov process with continuous

sample functions which serve as probability-theoretic models of physical diffusion phenomena

[15]. The simplest and oldest example of a diffusion process is the Brownian motion. There are

different approaches to the class of diffusion processes. We are going to define them in terms of

the conditions on the transition probabilities p(Xt ? B|Xs = x).

Definition 17 (Diffusion Process). A diffusion process is a Markov process Xt, t0 ? t ? T ,
with values in Rd and almost sure continuous sample functions whose transition probability
p(Xt ? B|Xs = x) satisfies the following three conditions for every s ? [t0,T[, x ? Rd and
? &gt; 0:

•
lim
s?t

1

t?s

?
|y?x|&gt;s

p(Xt = y|Xs = x)dy = 0 . (2.38)

• There exists a Rd-valued function D(1)(s,x) such that:

lim
s?t

1

t?s

?
|y?x|?s

(y ?x)p(Xt = y|Xs = x)dy = D(1)(s,x) . (2.39)

• There exists a d×d matrix-valued D(2)(s,x) such that:

lim
s?t

1

t?s

?
|y?x|?s

(y ?x)(y ?x)?p(Xt = y|Xs = x)dy = D(2)(s,x) . (2.40)

The functions D(1) and D(2) are called the coefficients of the diffusion process. In particular,

D(1) is called the drift vector and D(2) is called the diffusion matrix. D(2) is non-negative-defined.

Now we can proceed to see what a Fokker-Planck Equation is.

Theorem 8 (Fokker-Planck Equation). Let Xt, t0 ? t ? T ,denote a d-dimensional diffusion
process that satisfies the conditions in Definition 17 and which possesses a transition density

p(Xt = y|Xs = x). If the derivatives:

?p

?t
,

?

?yi

(
D

(1)
i (t,y)p

)
,

?2

?yi?yj

(
D

(2)
ij (t,y)p

)
16



exist and are continuous functions, then for fixed s and x such that s ? t, the transition proba-
bility p(Xt = y|Xs = x) is a fundamental solution of the Fokker-Planck equation:

?p

?t
+

d?
i=1

?

?yi

(
D

(1)
i (t,y)p

)
?

1

2

d?
i,j=1

?2

?yi?yj

(
D

(2)
ij (t,y)p

)
= 0 (2.41)

The Fokker-Planck equation can be used to get the probability density function associated

with a SDE. In fact, the Ito?’s formula gives us a simple way of deriving the Fokker-Planck

equation [13]. If we do this, we will see that the coefficients of the SDE are related to the

coefficients of the Fokker-Planck equation.

Analytical solutions to the Fokker-Planck equations can only be reached in very special case.

Most of the times we cannot find an analytical solution. However, it is possible to use other

methods of solutions like simulation methods, transformation of the Fokker-Planck equation in

a Schro?dinger equation, numerical integration methods among others.

One of the main goals of this dissertation is to determine the coefficients D(1) and D(2) that

govern the Fokker-Planck equation of the probability density function of the fluctuations of the

parameters of the log-normal distribution of volume-price.

As an example of application of the Fokker-Planck equation, we will study the oldest example

of a stochastic differential equation which describes the Brownian motion of a particle under the

influence of friction but no other force field and it is known as the Ornstein-Uhlenbeck process:

Definition 18 (Ornstein-Uhlenbeck Process). The Ornstein-Uhlenbeck process is the uni-

variate continuous Markov process Xt that evolves with time t according to the following stochas-

tic differential equation:

dXt = ?(µ?Xt)dt + ?dWt (2.42)

where ?,µ,? &gt; 0 and (Wt)(t?0) is a Brownian motion.

We can solve Equation (2.42) and we get a solution that is the sum of a deterministic

behaviour plus a stochastic term :

Xt = X0e
??t + µ(1 ?e??t) + ?

? t
0
e??(t?s))dWs (2.43)

The probability density function, p(Xt = x|Xs = y), of the Ornstein-Uhlenbeck process
satisfies the Fokker-Planck equation:

?p

?t
?

?

?x
(?(x?µ)p) ?

1

2

?2

?x2
(
?2p
)

= 0 (2.44)

In order to simplify the results, we are going to assume µ = 0 and D = 1
2
?2. Then the

solution for this Fokker-Planck equation is:

p(Xt = x|Xs = y) =

?
?

2?D(1 ?e?2?(t?s))
exp

(
?
?(x?e??(t?s))y2

2D(1 ?e?2?(t?s))

)
(2.45)

17



2.6 The Black-Scholes Model and its limitations

Although Bachelier had already applied results from stochastic calculus to the finance world,

this methodology only started to be widely used when Fischer Black, Myron Scholes [16] and

Robert Merton [17] addressed the problem of pricing and hedging an European option on a

non-dividend paying stock in 1973. This is one of the most important problems in Financial

Mathematics. Black and Scholes worked independently from Merton but arrived to the same

conclusions. The Nobel Prize in Economics for 1997 was awarded to Merton and Scholes. If

Black had been alive in that year, he would have shared the prize.

Although their model have some problems which have been pointed out throughout the years,

it is widely employed as a useful approximation. However, a proper application requires a good

understanding of its limitations. Blindly following the model exposes the user to unexpected

risk.

The Black-Scholes model assumes that the asset price, St, follows a geometric Brownian

motion:

dSt = µStdt + ?StdWt (2.46)

where Wt is a standard Brownian motion, µ ? R and ? ? R+. This stochastic differential
equation tells us that the price variation of the asset in the time interval dt follows a normal

distribution with mean µdt and variance ?2dt. Equation (2.46) is the main assumption of

the Black-Scholes model. Besides that, we also assume short selling is allowed, there are no

transactions costs, the assets are perfectly divisible and pay no dividend, there are no arbitrage

opportunities, trading takes place continuously in time and the risk-free rate is constant and the

same for all maturities. From all these assumptions, we can derive the time-t fair value of an

European-style option (call or put) on an asset with spot price St [10].

Despite the importance of the Black-Scholes model, various authors have showed that its

underlying assumptions do not agree with the market’s reality. For example, the geometric

Brownian motion cannot explain the negative skewness and the high kurtosis that are usually

seen in the empirical asset return distributions [18]. Another example is the negative correlation

between stock returns and realized volatility, known as leverage effect [19]. However the most

noticed drawback in the Black-Scholes model is the inverse relationship between the implied

volatility and the strike price, known as volatility smile [20].

Other models were developed in order to overcome these problems. The Constant Elasticity

of Variance (CEV) model was developed by Cox in 1975 [21]. It is consistent with the leverage

effect and with the volatility smile. The CEV model assumes that the asset price, St, follows

the following diffusion process:

dSt = µStdt + ?S
?
2
t dWt (2.47)

where ? is the constant of elasticity that controls the relationship between volatility and price.

When ? &amp;lt;2 we have the leverage effect, i.e. the observed variance of stock returns will be

inversely related with the asset’s price: it will increase as the asset’s price decreases and decrease

when the price increases.

Another important and useful alternative to the Black-Scholes model is the Heston model

[22]. This is one of the most well known stochastic volatility models. It assumes that the asset

18



price, St, is governed by the following stochastic differential equation:

dSt = µStdt +
?
vtStdW

(1)
t (2.48)

where Wt is a standard Brownian motion and µ ? R. The instantaneous variance of the asset’s
returns is assumed to follow another stochastic differential equation:

dvt = a(b?vt)dt + ?
?
vtdW

(2)
t (2.49)

where b is the long term mean of vt, a ? 0 is the speed of mean reversion, i.e., the rate at which
vt reverts to b and ? is the volatility of the variance process. Moreover, the parameters have to

obey the Feller condition so the process vt is strictly positive:

2ab

?2
&gt; 1 (2.50)

The Brownian motions in Equations (2.48) and (2.49) are correlated, with correlation ?.

2.7 From Stochastic Volatility to Superstatistics

Superstatistics is a branch of statistics aimed to the study of non-linear and non-equilibrium

systems. Complex systems often show a behaviour which can be regarded as a superposition of

different dynamics[23]. Superstatistics uses the superposition of multiple statistical models to

explain the complex system hence the prefix “super”.

One example of a superstatistics is the Heston model. We saw that this model assumes that

the asset price follows a stochastic differential equation (SDE), in particular Equation (2.48).

However, the parameter vt of SDE follows another SDE, Equation (2.49). Thus, we have a

superposition of statistics that explain the evolution of the asset price St.

In our case, we have a non-stationary time series: the volume-price series. This is a complex

system that we cannot describe using ordinary statistics. We will also use an ensemble of

different statistics.

It was proved [1] that the volume-price series follows a log-normal distribution throughout

time as we can see Equation (3.1). If the volume-price time series would be stationary we would

not need superstatistics. However, the parameters ? and ? are not constants. They follow

another statistical model. Thus, we are going to study the evolution of these parameters with

the aim of uncovering the evolution of volume-price.

2.8 The Langevin Analysis

We are interested in modelling the parameters ? and ? of the log-normal distribution of

volume-price. We are going to do this using stochastic differential equations (SDE) since we

are going to find the functions D(1) and D(2) governing the Fokker-Planck equation for the

19



probability density function of the parameters. The approach that we are going to follow was

introduced in 1997 [24] and reviewed by Friedrich et al. in a paper from 2011 [5].

We want to describe the log-normal parameters with a stochastic differential equation of the

type:

dX

dt
= D(1)(X,t) + g(X,t)?t , (2.51)

where X is a vector which contains our parameters and ?t is a white noise. The Langevin analysis

allow us to extract directly from the data the vector of functions D(1)(X) and the matrix of

functions D(2)(X) = g(X)gT (X).

A stochastic process described by Equation (2.51) can be modeled by stochastic evolution

laws that relate the state vectors X(t) at times ti, ti+1 = ti+?, ti+2 = ti+2?, ..., for small but

finite values of ?. Here, we deal with the SDE that are defined by the following discrete time

evolutions:

X(ti+1) = X(ti) + D
(1)(X(ti), ti)? + g(X(ti), ti)

?
??(ti) . (2.52)

This discrete SDE must be considered in the limit ? ? 0. We are going to explain how the
discrete time processes are related to Equation (2.51).

In order to motivate the discrete time approximations, we integrate the Equation (2.51) over

a finite but small time increment ?.

X(t + ?) = X(t) +

? t+?
t

D(1)(X(s),s)ds +

? t+?
t

g(X(s),s)?(s)ds (2.53)

? X(t) + D(1)(X(t), t)? +
? t+?
t

g(X(s),s)?(s)ds (2.54)

These are the quantities for which a statistical characterization can be given. We are going

to interpret the integral for the wildly fluctuating stochastic quantities ?(t) in the Ito? sense:

? t+?
t

g(X(s),s)?(s)ds ? g(X(t), t)
? t+?
t

?(s)ds = g(X(t), t)
?
??(t) (2.55)

where ?(t) is a stochastic variable with a standard Gaussian distribution since Wt =
? t

0
?(s)ds

and Wt+? ?Wt ?N(0,
?
?).

We have just discussed processes described by stochastic equations. Now we are going to

summarize the corresponding statistical description needed to the Langevin analysis.

D(1) and D(2) are the drift vector and the diffusion matrix in Equation (2.41). By considering

the Ito?’s definitions of the stochastic integrals, the coefficients D(1) and D(2) of the Fokker–Planck

equation and the coefficients of the Langevin equation are related. They are defined according

to the following equations, where ?X? is the mean of the variable X:

D
(1)
i (x,t) = lim

??0

1

?
&amp;lt;Xi(t + ?) ?xi &gt;|X(t)=x (2.56a)

D
(2)
ij (x,t) = lim

??0

1

?
&amp;lt;(Xi(t + ?) ?xi)(Xj(t + ?) ?xj) &gt;|X(t)=x (2.56b)

These equations are the discrete versions of Equations (2.39) and (2.40). They show us that

20



the drift vector and the diffusion matrix are determined as the first and second moments of the

conditional probability distributions in the small time limit.

We will know describe the Langevin analysis which allows us to get the drift vector and the

diffusion matrix directly from the data:

• The time series are represented in a state space, i.e., the set of values that a process can
take.

• The state space is partitioned into a set of small bins.

• For each bin ?, located at point x? of the partition we consider the quantity:

x(tj + ?) = x(tj) + D
(1)(x(tj), tj)? + g(x(tj), tj)

?
??(tj) , (2.57)

where the points x(tj) are taken from the bin located at x?.

The drift vector assigned to the bin located at x? is determined as:

D(1)(x?, t) = lim
??0

1

?
M(1)(x?, t,?) (2.58)

on the conditional moment:

M(1)(x?, t,?) =
1

N?

?
x(tj)??

[x(tj + ?) ?x(tj)] (2.59)

where N? is the number of points contained in the bin ?.

The diffusion matrix is estimated by:

D(2)(x?, t) = lim
??0

1

?
M(2)(x?, t,?) (2.60)

on the conditional moment:

M(2)(x?, t,?) =
1

N?

?
x(tj)??

([(x(tj + ?) ?x(tj)) ? ?D(1)(xj, t)]2 (2.61)

2.9 Statistical Tests

In this dissertation we are going to use some statistical tests that we present here in this

section.

In order to apply the Langevin Analysis, it is necessary that the time series of the fluctuations,

?? and ??, are Markovian. Most of the physical phenomena can only be considered Markovian

if we take a sufficient large time step. Einstein recognized this for the Brownian motion. He

discussed the smallest time scale for which the Brownian motion can be seen as a Markovian

stochastic process [25]. We call this time step the Markov length, ?M .

21



We will use the Wilcoxon rank-sum test [26] to compare the conditional probabilities:

p(x1,?1|x2,?2) and p(x1,?1|x2,?2; x3,?3) . (2.62)

We will compute the value of t/t0, where t0 =
?

2
?

and t =
|Q?&lt;Q&gt;|
?(Q)

where Q denotes the

total number of inversions in the Wilcoxon test. Q is a Gaussian distributed variable with mean

value &amp;lt;Q &gt; and standard deviation ?(Q). Therefore t is the absolute value of a Gaussian-

distributed random variable with mean value zero and standard deviation one. The expected

value of t, where averaging is done with respect to x2 should be
?

2
?

. Therefore, values of t/t0

close to 1, indicates the data has the Markovian property.

Shapiro and Wilk’s [27] W-test is a wide used and powerful test of departure from normal-

ity. It tests the null hypothesis that the sample x1, ...,xn comes from a gaussian distributed

population. The test statistics is:

W =

(?n
i=1 aix(i)

)2?n
i=1(xi ?x

2)
(2.63)

where x(i) is the i-th order statistics, i.e., the i-th smallest number in the sample, x =
?n
i=1 xi
n

and:

(a1, ...,an) =
mTV ?1

?
mTV ?1V ?1m

(2.64)

where m = (m1, ...,mn)
T and mi, for i = 1, ...,n is the expected value of the order statistics

of independent and identically distributed random variables sampled from the standard normal

distribution, and V is the covariance matrix of those order statistics.

22



Chapter 3

Getting to know the data

In previous works, it was shown that, to the volume-price at this time scale and with this

data, the log-normal distribution had the best fit to the data among other four statistical models

[28]. The probability density function of the log-normal is:

p(s) =
1

s
?

2??
exp

[
?

(log s??)2

2?2

]
(3.1)

If volume-price is log-normal distributed with parameters ? and ? then the volume-price

logarithm has a normal distribution with mean ? and standard deviation ?. This relationship

is easily observed if we plot a log-normal variable in a logarithmic scale, like we did in Figure

1.1b. In this section, we are going to study the time series of the parameters ? and ? of this

distribution.

3.1 Outliers and Daily Patterns

We start by removing the outliers from our data. An outlier is an observation that is distant

from the other ones. The outliers may be the result of measuring errors. In order for us to have

a robust model we should discard the outliers. After we have studied our data series, we decided

to consider as outliers all the data points which do not lie within five standard deviations of the

mean. By doing this, we removed 33 data points from our original series.

Figure 3.1: The first 27 trading days of the (a) ? and (b) ? time series after we have removed the outliers.

23



Figure 3.2: (a) ?ma represents the 20-day moving average pattern of parameter ? over a period of 9 days and
(b)?ma represents the same for ?. (c) The average over all trading days of parameters ?, i.e. ?, and (d) ?, with
the respective fitting functions.

We can see the result from this process in Figure 3.1. We can also see that we clearly have a

daily pattern in both time series. In order to apply the Langevin analysis described in Chapter

2, we will have to remove this pattern in order to obtain the time series of the fluctuations since

the Langevin analysis does not support any kind of periodicity in the data. The original series

? and ? are the sum of the average daily pattern with the fluctuations around this pattern.

In Figure 3.2 we can see that both the average of ? and ? are described by the following

cubic curves:

?(td) = a?t
3
d + b?t

2
d + c?td + d? (3.2a)

?(td) = a?t
3
d + b?t

2
d + c?td + d? (3.2b)

where td = (t mod 144) in units of 10 min, a? = 8.216 × 10?5, b? = ?2.316 × 10?3, c? =
?2.016×10?2 and d? = 13.52 for the ? times series and a? = ?1.006×10?5, b? = 5.616×10?4,
c? = ?1.324 × 10?2 and d? = 1.792 for the ? times series.

If we observe the behaviour of ? in Figure 3.2a, we can notice that the trading is heavier,

i.e. we have a higher ?, at the beginning and end of the day than during the rest of the day.

The opening and the closing of the NYSE are very peculiar times: they occur after and before

the market is closed. This can explain the high volume-prices at the beginning and end of the

24



day.

In the beginning of the day, the volume-price series has high values. This happens because

our time series is not continuous. We have a time gap between the end of one day and the

beginning of the next day. Information arrives during this period and the traders will have

different opinions about what is going to happen in the morning. There is a lot of speculation.

Therefore, when the Stock Market opens, there will be a huge quantity of money transactions.

Notice that the volume-price is essentially the amount of money which is being traded. Everyone

wants to sell or to buy fast so they can make more money than the others. Time is money so

the faster you give your order, the higher profit you will have. As time passes by, the money

traded will decrease. Now we do not have a urgency to sell or to buy.

However, after lunch time, we see an alteration in this pattern. Now the volume-price series

starts to grow again. At the closing time, traders anticipate price changes overnight that can

alter their portfolios, so they exchange larger amounts of money. Besides that, there are index

mutual fund managers who need to make trades at the closing prices for administrative purposes

and there are short sellers and hedgers who frequently close and hedge their positions at the end

of the day. The volume-price increases at the end of the day at a higher rate than it decreases

in the beginning of the day and the closing value is typically grater than the opening value.

This may happen because there is a deadline (i.e. the closing time) that we do not have in the

beginning of the day.

The volume traded has a bigger impact in the shape of the volume-price series than the price.

Besides that, the series of volume-price inherit the oscillation-like structure from the series of

trading volumes that we can see in Figure 1.1a. Rocha [1] showed that the correlation between

the volume and the volume-price is approximately 0.8. Therefore we can admit that the pattern

followed by ? and ? in the volume-price series is approximately the same that we would see in

the volume series.

The U-Shaped pattern followed by ? is long known in the finances world [29, 30], it is very

typical and it has been documented in various studies. This pattern is a characteristic of the

volume series. However, we saw that the correlation between the volume and volume-price series

is almost one. Therefore, it is expected that the volume-time series follows the same pattern.

Various authors gave different reasons for the existence of this pattern. Admati and Pfleiderer

[29] argue that high volume in a particular time segment reveals the presence of asymmetric

information. Brock and Kleidon [31] defend that different trading strategies at the open and

close of the markets form these volume patterns.

We can also see in Figure 3.2d the pattern followed by ?. We have to remember that ?

accounts for the standard deviation of the volume-price series logarithm. In the beginning of

the day there is a great variance in our data. This may happen because the traders have different

perceptions about what is going to happen. As the time goes by, the standard deviation starts to

decrease, first slowly and then faster. Finally, the standard deviation is very low at the end of the

day. Apparently the traders exchange similar amounts of money. Maybe this happens because

of the information received during the day. At the beginning of the day, people have different

informations so they adopt different strategies. However, as the time passes, the information

and the observation of the NYSE behaviour during that day leads the different traders to similar

strategies.

These patterns, ? and ?, happen every day and they are easily modelled by Equations (3.2).

However, the fluctuations around these patterns, ?? and ??, need to be addressed more carefully

25



Figure 3.3: The fluctuations time series of the first 27 trading days: (a) ?? and (b) ??.

since they have a strong stochastic behaviour. We will study the time series of the fluctuations

in the next section.

3.2 Log-Normal Parameter Fluctuations

We got the time series of the fluctuations by subtracting the 20-day moving average pattern

(Figure 3.2) from the data without the outliers (Figure 3.1). In each day, we subtracted the

pattern from the 20 days before that day. We got Figure 3.3.

In order to make sure that we have removed all the periodicity from our time series, we

build a power spectrum for both time series, before and after the process of removing the daily

patterns, using the Fast Fourier Transform.

When the time series are viewed in the form of a frequency spectrum, certain aspects of the

underlying processes are revealed. If the frequency spectrum include distinct periodic peaks, we

may infer that the original processes have some kind of periodicity.

In Figures 3.4a and 3.4b we can see the power spectrum in a log-log scale. The solid lines

are the power spectrum before we have removed the daily patterns. We can see some peaks

that confirm our idea that the original time series are periodic. The power spectrum for the

fluctuations is represented by the dots in the same picture. It is clear, for both time series, that

the peaks observed in the solid lines disappeared. We have removed efficiently the periodicity

from our time series.

We also computed the autocorrelation function, ?, in a log-lin scale (see Figures 3.4c and

3.4d) for the ?? and ?? fluctuations time series to check the correlation between values of the

series at different times. The ACF ? has an exponential decay:

26



Figure 3.4: (a) The power spectrum of the ? and (b) ? time series is represented by the solid lines. The power
spectrum of the (a) ?? and (b) ?? fluctuations is represented by the dots. (c) Autocorrelation function (dots) and
linear function fitted to the data (dashed line) in a log-lin scale for the ?? time series and for the (d) ?? time series.
??,? represents the slope of the line fitted to the data.

??,? = ??,?e
? ?
??,? (3.3)

Therefore the logarithm of the ACF is a line. We fitted linear functions to both ACF. We

got a R2 bigger than 90% in both cases so we have a good fit. For ??, we have 1
??

= 0.0192 and

log(??) = ?0.8496. For ??, we have 1?? = 0.0132 and log(??) = ?0.7776. The
1
??,?

constants

0.0192 and 0.0132 have units that are the inverse of the time because the exponential exponent

cannot have units. Therefore, if we take the inverse of these constants, i.e. ?? = 52.08 and

?? = 75.76 we have the characteristic time for both series after which the process is uncorrelated,

or, in other words, the characteristic time after which the processes have no memory.

We can see the probability density functions (PDF) of the fluctuations ?? and ?? in Figure

3.5. At first sight, these probability density functions appear to be normally distributed. We

tested this hypothesis using the Shapiro-Wilk Normality test. According to Royston [32], an

approximate p-value below 0.1 is enough to reject the null hypothesis of normality. For both

time series, the Shapiro-Wilk test rejected the normality hypothesis.

Although the Shapiro-Wilk test rejectes the assumption of normality, we still plotted in

Figure 3.5 the probability density function of the fluctuations ?? and ?? in a log-lin scale (dots)

and the adjusted Gaussian PDF (dashed line), i.e., a Gaussian probability density function with

the same mean and standard deviation as our data. We have a good fit for both time series in the

27



Figure 3.5: (a) Marginal probability density function (solid line) and Gaussian adjusted PDF (dashed line) of the
fluctuations ?? and (b) ?? in a log-lin scale.

central region of the PDF. The marginal PDF of the ?? series has an almost perfect superposition

with the Gaussian PDF. However, the fit in the tails is not so good. For the ?? time series, we

also have a good fitting although it is not as good as for ??. The skewness is almost zero in the

empirical data. However, the kurtosis is approximately 6. This value is very high. Although the

kurtosis has been associated to the “peakedness” of the distribution, Westafall [33] proved that

the kurtosis has only an unambiguous interpretation in terms of tail extremity. A high kurtosis

(like the one found in our data) draw our attention to the existence of outliers since there are

outliers that contribute meaningfully to the computation of the kurtosis. In our data, we have

fatter tails than it was expected. This can be related to the process of removing the outliers.

It looks like some outliers have still remained in our data. Despite this, a Gaussian model is a

good approach to study the evolution of the fluctuations ?? and ??.

We have been studying each time series separately. However it is also important to see if

there is any kind of correlation between our variables. We computed the covariance matrix ?,

which has components ??? = 0.0619, ??? = 0.0039, ??? = ??? = ?0.0036. Besides this, we
computed a correlation coefficient of -0.2311 between the two variables which show us that our

variables are indeed correlated. Correlation and independence are different concepts. If we have

zero correlation that does not imply independence. However, if the correlation is different from

zero then we cannot have independence. In our case, the correlation coefficient is small. One can

argue that it is almost zero. But one can also argue that the variables are negatively correlated.

In Figure 3.6a we plotted the joint probability density function of the fluctuations time series,

?? and ??. In order to make a comparison, we also plotted in Figure 3.6b a multivariate normal

28



Figure 3.6: (a) Joint PDF of the empirical time series and (b) a multivariate normal distribution with mean
vector and covariance matrix equal to the ones of our empirical data. (c) Contour plot for the ?? and ?? time
series and (d) for a multivariate normal distribution with mean vector and covariance matrix equal to the ones of
our empirical data.

distribution with mean vector and covariance matrix equal to the ones from the fluctuations

time series. From these figures, we can see that the two plots are remarkably alike.

The PDF of the multivariate normal distribution represented in Figure 3.6b is:

p(x) =
1

2?
?
|?|

exp

(
?

1

2
(x?µ)T ??1(x?µ)

)
(3.4)

where x = (?,?), |?| = ????????2?? is the determinant of the matrix ? and µ is a 2-dimensional

29



vector of zeros since the mean of ?? and ?? is approximately zero.

We also draw in Figures 3.6c the contour plots of the fluctuations and (d) the contour plots

of the multivariate normal. We can notice that both contour plots area leaning towards the

left which indicates a negative correlation between the variables. It is important to notice that

the contour plot of the multivariate normal is not elliptical (as it was expected) because we

simulated data from a multivariate normal and, after that, we build the joint distribution and

the contour plot.

3.3 Markov Tests

To test the Markovianity of the series, we apply the Wilcoxon test to compare the distribu-

tions p(x1,?1|x2,?2) and p(x1,?1|x2,?2; x3,?3) for a fixed value of x3. We are also going to infer
the Markov length, ?M .

In Figures 3.7 we can see that the values for the quotient t/t0 are close to 1 for both time

series for all values of ?. Thus, this suggests a Markov length ?M = 600s = 10min which is our

time scale.

Figure 3.7: (a) Wilcoxon test to verify the Markovian property of the ?? time series and (b) the ?? time series,
showing the Markov length of ?M = 600s.

30



Chapter 4

The Langevin Analysis

4.1 A Simple Model without Correlation

In this section we will implement the one dimensional Langevin Approach [34] to the two

fluctuations time series represented in Figure 3.3. It is true that our time series are not indepen-

dent. However, the correlation coefficient is small, namely ? = ?0.2311, so we will try to test
a simple model in our data. Usually, the simplest models are the ones people use more often,

even though they are not the ones that describe reality the best (one flagrant example is the

Black-Scholes model).

We are going to assume this time series to be independent from each other. We use the

routine “Langevin1D” applied to the fluctuations as described in Chapter 2. This routine

retrieves M(1) and M(2) from Equations (2.59) and (2.61) which are essentially the means of the

first and second conditional moment, in each bin and for each time step ?. We want to derive

from our data the functions D(1) and D(2) that govern the following equations:

d??(t) = D(1)(??)dt +
?
D(2)(??)dWt (4.1a)

d??(t) = D(1)(??)dt +
?
D(2)(??)dWt (4.1b)

These are the derivatives with respect to time of M(1) and M(2). Thus, we computed the

quotient M
(n)

?
[35] for each bin and for n = 1, 2. In Figure 4.1 we can see this quotient for the

Figure 4.1: (a) Computation of D(1) and (b) D(2), in the bin that contains the mean, as the intersection of the
linear fit with ? = 0.

31



bin that contains the mean. We can see that M(n) is linear when we consider the time steps

between 10 and 20. Thus, to find the derivatives with respect to time D(n) we just have to make

a linear fit toM
(n)

?
(when the time steps are between 10 and 20) and see for which value that

line intersects ? = 0. That number is going to be our D(n) in that bin. If we do this for all the

bins, we will have the D(n) function.

Note that we had to use this proxy because, apparently:

lim
??0

M(n)

?
= +? . (4.2)

This may happen because the time series have measurement noise which makes the empirical

limit to diverge or simply because of round-off errors [36]. In order to surpass this, we use the

fit to a linear region before the limit starts diverging.

Applying this methodology to all bins from our time series we get Figure 4.2. Now it is

possible to write the Langevin equations for our time series using the values we obtained with the

Langevin analysis as we can see in Equations (4.1), where D
(1)
? = ?0.08?, D

(2)
? = 0.006 + 0.07?

2,

D
(1)
? = ?8.0 × 10

?2? and D
(2)
? = 3.9 × 10

?4 ? 7.9 × 10?4? + 5.6 × 10?2?2.

Figure 4.2: (a-d) D1 and D2 functions to the ?
? (left) and ?? (right) time series. (e-f ) Quotient D(1)/

?
D2 for ?

?

and ??, respectively.

32



Figure 4.3: (a) D4 function to the ?
? and (b) ?? time series.

The D(1) coefficient is linear for both the ?? and ?? time series. This means we have an

oscillator with a string constant that corresponds to the line’s slope. The string has a fixed

point in zero since the line contains the origin of the referential.

In Figure 4.2e and Figure 4.2f we plotted the quotient D(1)/
?
D(2) to check if the order of

magnitude of the functions that govern the deterministic and stochastic part of Equations (4.1)

is the same. That quotient varies between ?0.3 and 0.3 for both time series, which means that?
D(2), i.e., the coefficient of the stochastic part, has a heavier weight in the Equations (4.1)

than D(1), the coefficient of the deterministic part.

We also computed the D(4) function for both time series. The results obtained are in Figure

4.3. Here we can see that D(4) is almost zero for both time series. According to the Pawula

Theorem [37], if D(4) is zero or very small when compared to D(1) and D(2), then we can stop

the Kramers-Moyal expansion at n = 2 since the terms of bigger order are zero.

4.2 Modelling the Coupling between ?? and ??

Since our time series are not independent, we are going to perform the Langevin Analysis

2D described in Chapter 2, i.e., assuming there exists dependence between the two time series.

33



The routine gives us the values of M(1) and M(2), like we saw in the previous section. Thus,

we need to compute D(1) and D(2). However, we do not have a linear behaviour of M
(n)

?
like

we had when we assumed that ?? and ?? were independent from each other. This may happen

because our sampling frequency is too high. We are going to use as a proxy the value of M(n)(x)

in ? = 1.

Figure 4.4: Functions from the drift vector and diffusion matrix obtained by the routine Langevin2D: (a) D
(1)
? ,

(b) D
(1)
? , (c) D

(2)
?? , (d) D

(2)
?? , (e) D

(2)
?? .

An interval of 10 minutes in the NYSE is huge, considering there are transactions happening

in a microsecond scale. When we have a Markov process, the conditional moments change in a

linear way to small values of ?. Then, for larger values of ?, they start deviating from linearity.

34



Figure 4.5: The three components of the g matrix: (a) g??, (b) g??, (c) g??.

This may be happening in our data but we cannot notice because of the frequency of our time

series. Maybe we have this linear behaviour for a ? &amp;lt;10min. But our ?1 = 10min so we do

not notice this linear behaviour. One way of getting over this problem is to approximate the

D(n)(x) by the value of M(n)(x) in ? = 1.

We can be making an approximation error (or not). We do not know how the variation takes

place in the linear part. If the linear part is immediately below the 10 minutes, then we should

have a good approximation. However, if the linear variation is in the order of some seconds,

then the first 10 minutes are very far from the linear part and our approximation may not be

very good. Nevertheless, this is the best estimative that we can make with this data. It is better

than to use the other values of ?.

In Figure 4.4 we plotted the functions from the drift vector and diffusion matrix (notice that

this matrix is symmetric) obtained by the routine Langevin2D [34]. For simplicity we are going

to suppress the prime symbol noticing that we only address the fluctuations. Thus, in the two

dimensional case we have a system of coupled Langevin equations that govern the evolution of

? and ?:

?
????
d?(t)

d?(t)

?
???? =

?
????
D?

(1)(?,?)

D?
(1)(?,?)

?
????dt +

?
????
g??(?,?) g??(?,?)

g??(?,?) g??(?,?)

?
????
?
????
dW

(1)
t

dW
(2)
t

?
???? (4.3)

35



where ggT = D(2) with:

D(2) =

?
????
D

(2)
?? D

(2)
??

D
(2)
?? D

(2)
??

?
???? . (4.4)

We fitted polynomials of degree one to the functions from the drift vector and polynomials

of degree two to the functions from the diffusion matrix:

D(1) ? a + b? + c? (4.5a)

D(2) ? a + b? + c? + d?2 + e?? + f?2 (4.5b)

whose coefficients are presented in the next table:

Term 1 ? ? ?2 ?? ?2

D
(1)
? -0.0085 -0.7143 0.2812

D
(1)
? -0.0031 0.0293 -0.5023

D
(2)
?? -0.1233 0.1107 0.3563 0.9013 0.7350 5.8615

D
(2)
?? -0.0017 0.0037 -0.0104 0.0059 -0.0186 0.5253

D
(2)
?? -0.0081 -0.0046 -0.0267 -0.0222 0.4385 -0.1901

Table 4.1: Coefficients for the D(1) and D(2) functions, obtained by the Langevin analysis.

In order to have all of the coefficients of Equation (4.3), we have to compute the g matrix.

This matrix g is not unique. If we have an orthogonal matrix Q and ggT = D(2) then h = gQ

also satisfies hhT = D(2) since

hhT = (gQ)(gQ)T = gQQTgT = ggT = D(2) (4.6)

For now, we will just compute one possible matrix that satisfies this property. If the matrix

D(2) is diagonalizable then we know that there exists an invertible matrix P satisfying:

PD(2)P?1 = d (4.7)

where d is a diagonal matrix that contains the eigenvalues of D(2) in the main diagonal and P

is a matrix whose columns are the eigenvectors of D(2). Now it is easy to show that

g = P?1
?
dP (4.8)

where
?
d is the matrix obtained after taking the square root of each element of the diagonal

36



matrix d. Notice that:

ggT = (P?1
?
dP)(P?1

?
dP)T = P?1

?
dPP?1

?
dP = P?1dP = D(2) (4.9)

since P?1 = PT because symmetric matrices have orthogonal eigenvectors.

After implementing this procedure we obtained the three components of the g matrix (notice

that this matrix is symmetric) and we fitted quadratic forms to this components:

g ? a + b? + c? + d?2 + e?? + f?2 (4.10)

whose coefficients are presented in the next table:

Term 1 ? ? ?2 ?? ?2

g?? 0.2185 0.0918 0.2255 0.4850 0.2925 4.0541

g?? 0.0360 0.0174 -0.0128 0.0210 0.0245 1.5197

g?? -0.0111 -0.0051 -0.0158 -0.0134 0.2936 -0.1835

Table 4.2: Coefficients for the g functions.

We can see in Figure 4.5 the empirical results obtained and the surfaces fitted to this data.

37



38



Chapter 5

Approaching Non-Stationarity

After introducing our framework in Chapters 3 and 4 and applying it to the volume-price

series, we now deduce the formula of all the moments E [sn] ,n = {0, 1, 2, ...} of the log-normal
distribution. We are going to use the notation ?sn? with the same meaning as E [sn]. It is a well
known statistical result [38] that, if we have all the moments from a distribution, we can deduce

its probability density function using a Fourier transform. It is possible to have a closed-form for

the nth-moment of s, since s follows a log-normal distribution whose PDF is given by Equation

(3.1). The moments ?sn?,n = {0, 1, 2, ...} of our distribution are given by:

?sn? =
? +?
??

snp(s,?(t),?(t))ds = en?+
n2?2

2 . (5.1)

Assuming that all time dependency is incorporated in the distribution parameters ? and

? one is able to fully characterize the non-stationary time series of the volume-price: one just

needs to model the ? and ? evolution through time. Indeed, these parameters are the sum of a

daily average pattern ? and ? with the fluctuations ?? and ??:

? = ? + ?? (5.2a)

? = ? + ?? (5.2b)

The daily patterns ? and ? are the ones observed in Figure 3.2 and can be described by

the expressions in Equations (3.2). Since we already have the expressions for the daily patterns

? and ?, now we need to describe the fluctuations ?? and ??. In our model we are going to

assume that these fluctuations obey the system of Langevin equations given in Equation (4.3).

For simplicity we are going to suppress the prime symbol noticing that we only address the

fluctuations.

In order to integrate these equations for the parameters fluctuations, we will have to use

their discrete versions in the Ito?’s description, namely:

?(t + ?t) = ?(t) + D
(1)
? (?(t),?(t))?t + g11(?(t),?(t))

?
?t r1 + g12(?(t),?(t)

?
?t r2 (5.3a)

?(t + ?t) = ?(t) + D
(1)
? (?(t),?(t))?t + g21(?(t),?(t))

?
?t r1 + g22(?(t),?(t))

?
?t r2 (5.3b)

where r1 and r2 are random numbers from a Gaussian distribution with mean zero and standard

deviation 1.

Having generated a sample of fluctuations, we then add the daily patterns in Equations (3.2)

39



Figure 5.1: (a) Time series of empirical ?s? and (b) corresponding modelled series. Therefore, (c-d) ?s2?, (e-f )
?s3? and (g-h) ?s4?. Inside each plot, there is a sub-plot with the corresponding entire series.

and obtain the modelled time series for ?mod and ?mod. Inserting ?mod and ?mod in Equation

(5.1) yields the modelled volume-price moments.

We next compare the modelled time series of the first four moments ?sn?, n = 1, ..., 4, with
the empirical ones which are obtained by replacing in Equation (5.1) the original time series of

? and ? represented in Figure 3.1.

In Figure 5.1 we have the series obtained from our model versus the empirical ones, for the

first four moments. If we look to the entire time series, we can see that for n = 1, 2, our model

can explain the extreme events. However, for n = 3, 4, we have far more extreme events in the

empirical series than in the modeled ones. When we look to the zoom in Figure 5.1, we can see

the same pattern in both series. In our model, the extreme events do not happen at the same

time as in the empirical ones, but they will eventually happen.

In Figure 5.2 we have the empirical and modelled probability density functions for the fluc-

tuations of ? and ?. We can see that our model can explain better the ? fluctuations than the

? fluctuations. In Figure 5.2a we see that the densities are quite close, specially in the central

40



Figure 5.2: (a) PDFs for the empirical fluctuations of ? (dashed line) and for the modelled fluctuations (solid
line). (b) PDFs for the empirical fluctuations of ? (dashed line) and for the modelled fluctuations (solid line).
(c-f ) PDFs for the ?sn? time series. The dashed line represents the empirical PDF and the solid line is the PDF
obtained from our model.

region. However, in Figure 5.2b we do not have a fit as good since it is easier to model means

than standard deviations. This can be explained noticing that ? is a first-order moment while

? is the square root of a second-order central moment.

In Figure 5.2(c-f) we plotted the empirical and theoretical probability distributions of ?sn?,n =
1, ..., 4. Our model has a good fit in the first moments and can be used to model them since

the theoretical and empirical distributions are very close to each other. Since the ? parameter

is better modelled than the ? parameter, it is expected that for higher moments, when ? is

dominant over ?, we do not achieve such good results.

41



42



Chapter 6

Discussion and Conclusions

The main goal of this dissertation was to model the non-stationary time series of the volume-

price. By assuming that the log-normal had the best fit to the data in each 10-minutes window,

this goal resumes to the one of studying the parameters ? and ? of this distribution, which

are themselves stochastic variables. We were able to show that we can describe the time series

of these parameters by decomposing the variables as a sum of two terms: one accounting for

the daily pattern and another regarding the fluctuation around that average pattern. The

fluctuations are modelled using a system of Langevin equations whose coefficients we retrieved

from our empirical data. From here, we proposed a framework to reconstruct the evolution of

all the moments of the volume-price distribution.

Our model reproduces well the first moments, being therefore suitable to study the volume-

price of the stocks from the NYSE. It would be interesting if, in the future, we conduct tests to

see if this framework can be used to make previsions about the evolution of the volume-price.

In particular, it could be a suitable approach for the calculation of the Value at Risk (VaR) of

a stock’s portfolio.

This work leaves some open questions to be answered. It is true that we achieved a good

model to the ? fluctuations, but we could not match this result to the ? fluctuations. One

possible explanation is related with the outliers: we removed all the points which did not lie in a

5? interval from the mean. However, when we plotted the time series without the outliers, there

were still some extreme values that look more like measurement errors than fluctuations as we

can see in Figure 3.1. We chose to use the 5? criterion because we tried to minimize the number

of points taken from our sample in order to let our data as close as possible to the original one.

However, if one prefers to choose a stricter criteria, like using a 3? interval, then the time series

would have lesser outliers and maybe the results would be more easily modelled.

There are many models in the literature that enable us to study and to model stochastic

time series such as autoregressive models [39], moving-average models [40] and autoregressive

integrated moving average models [41]. One may ask, why did we choose the Langevin model

instead of all the others. One strong argument in favour of this model is that it not only allows us

to describe the evolution of our time series, but it may also give us an equation, Fokker-Planck

equation, to describe the evolution of the volume-price distribution. Further work should be

done in trying to extract such an equation from the equations we already have. If one is able to

do this, then we would have much more information about the volume-price evolution and we

could apply this information to the computation of the Value at Risk or other risk measures.

A comparison between our model and the classic models that have been used for studying

43



time series would be an interesting work to develop in the future. It is true that our model

has the advantage already stated of being able to produce an equation to the evolution of the

distribution of the volume price. However, the results achieved by our model may be indeed

better than the ones achieved by the classical models. In order to test this hypothesis, we should

do this comparative study.

Another question that raises from the main findings of this dissertation is related with

the sampling frequency of our data. In the NYSE, where transactions are happening at the

microsecond scale, 10 minutes is a very large interval. If we were able to redo this analysis

with a smaller sampling frequency, maybe we would be able to extract stochastic differential

equations which explain better the behaviour of the fluctuations in our data.

We proposed a model that is capable of modeling the central region of the volume-price

distribution. However, it does not have a good fit in the tails. It would be interesting to develop

a two dimensional model to describe the behaviour of the tails. Rocha [1] had already described

the tails of the volume-price as a one parametric inverse gamma distribution. But if one is able

to fit a two parametric model to the tails, then it may yield better results. After this, it would

be a good idea to combine these two models and see if we get a better fit to the real data.

Finally, this work gave us important insight in the study of non-stationary time series and

we have proposed here a methodology that is going to be very useful in numerous fields. This

framework is general enough to be applied to other markets besides the NYSE and also to other

fields of study like physiology, when we are trying to study the heart interbeat intervals or

geology, in order to study seismic time series.

44



Bibliography

[1] P. Rocha, “Stochastic evolution of parameters defining probability density functions: Ap-

plication to the new york stock market,” Master’s thesis, Faculty of Sciences of Lisbon

University, 2014.

[2] H. Furstenberg, Stationary Processes and Prediction Theory. Princeton University Press,

1960.

[3] P. Rocha, F. Raischel, J. Cruz, and P. Lind, in 3rd SMTDA Conference Proceedings, 2015,

pp. 619–627.

[4] P. Rocha, F. Raischel, J. Boto, and P. Lind, Journal of Physics: Conference Series, vol.

574, 012148, 2014.

[5] R. Friedrich, J. Peinke, M. Sahimi, and M. Tabar, “Approaching complexity by stochastic

methods: From biological systems to turbulence,” Physical Review, vol. 506, p. 87, 2011.

[6] P. Laplace, A philosophical essay on probabilities. New York: J. Wiley, 1902.

[7] R. Brown, “A brief account of microscopical observations made in the months of june, july

and august, 1827, on the particles contained in the pollen of plants; and on the general

existence of active molecules in organic and inorganic bodies.” Philosophical Magazine,

vol. 4, p. 161–173, 1828.

[8] L. Bachelier, “The?orie de la spe?culation,” Annales Scientifiques de l’ E?cole Normale

Supe?rieure, vol. 3, no. 17, pp. 21–86, 1900.

[9] A. Einstein, “U?ber die von der molekularkinetischen theorie der wa?rme geforderte bewegung

von in ruhenden flu?ssigkeiten suspendierten teilchen,” Annalen der Physik, vol. 17 (8), p.

549–560, 1905.

[10] D. Lamberton and B. Lapeyre, Introduction to Stochastic Calculus Applied to Finance.

Chapman and Hall/CRC, 2008.

[11] F. Beichelt, Stochastic Processes in Science, Engineering and Finance. New York: Chap-

man and Hall/CRC, 2006.

[12] M. Vetterli, J. Kovacevic, and V. Goyal, Foundations of Signal Processing. Cambridge

University Press, 2014.

[13] J. Hunter, Lecture Notes on Applied Mathematics: Methods and Models. University of

California, 2009. [Online]. Available: www.math.ucdavis.edu/?hunter/m280 09/ch.pdf

45

www.math.ucdavis.edu/~hunter/m280_09/ch.pdf


[14] F. Yilmaz, H. O?z, and G. Weber, “Ito?-Taylor expansions for systems of stochastic differential

equations with numerical applications,” 2013.

[15] L. Arnold, Stochastic Differential Equations: Theory and Applications. New York: J.

Wiley, 1974.

[16] F. Black and M. Scholes, “The pricing of options and corporate liabilities,” Journal of

Political Economy, vol. 81(3), pp. 637–654, 1973.

[17] R. Merton, “Theory of rational option pricing,” The Bell Journal of Economics and Man-

agement Science, vol. 4(1), pp. 141–183, 1973.

[18] J. Jackwerth and M. Rubinstein, “Recovering stochastic processes from option prices,”

Contemporary Studies in Economic and Financial Analysis, vol. 94, 2012.

[19] R. Schmalensee and R. Trippi, “Common stock volatility expectations implied by option

premia,” Journal of Finance, vol. 33, pp. 129–147, 1978.

[20] P. Dennis and S. Mayhew, “Risk-neutral skewness: Evidence from stock options,” Journal

of Financial and Quantitative Analysis, vol. 37, pp. 471–493, 2002.

[21] J. Cox, “Notes on option pricing I: Constant elasticity of variance diffusions,” Unpublished

Draft, 1975.

[22] S. Heston, “A closed-form solution for options with stochastic volatility with applications

to bond and currency options,” The Review of Financial Studies, vol. 6(2), pp. 327–343,

1993.

[23] C. Beck, “Superstatistics: Theoretical concepts and physical applications,” in Anomalous

Transport. Wiley-VCH Verlag GmbH and Co. KGaA, 2008.

[24] R. Friedrich and J. Peinke, “Description of a turbulent cascade by a fokker-planck equation,”

Physical Review Letters, vol. 78 (863), 1997.

[25] R. Stresing, D. Kleinhans, R. Friedrich, and J. Peinke1, “Different methods to estimate the

Einstein-Markov coherence length in turbulence,” Physical Review E, vol. 83, 046319, 2011.

[26] C. Renner, J. Peinke, and R. Friedrich, “Experimental indications for markov properties of

small-scale turbulence,” Journal of Fluid Mechanics, vol. 443, pp. 383–409, 2001.

[27] S. Shapiro and M. Wilk, “An analysis of variance test for normality (complete

samples),” Biometrika, vol. 52, no. 3-4, pp. 591–611, 1965. [Online]. Available:

http://biomet.oxfordjournals.org/content/52/3-4/591.short

[28] P. Rocha, F. Raischel, J. Boto, and P. Lind, “Uncovering the evolution of nonstationary

stochastic variables: The example of asset volume-price fluctuations,” Physical Review E,

vol. 93,052122, May. [Online]. Available: link.aps.org/doi/10.1103/PhysRevE.93.052122

[29] A. Admati and P. Pfleiderer, “A theory of intraday patterns: Volume and price variability,”

The Review of Financial Studies, vol. 1, pp. 3–40, 1988.

46

http://biomet.oxfordjournals.org/content/52/3-4/591.short
link.aps.org/doi/10.1103/PhysRevE.93.052122


[30] P. Jain and G. John, “The dependence between hourly prices and trading volume,” Journal

of Financial and Quantitative Analysis, vol. 23, pp. 269–284, 1986.

[31] W. Brocka and A. Kleidon, “Periodic market closure and trading volume: A model of

intraday bid and asks,” Journal of Economic Dynamics and Control, vol. 16, pp. 451–489,

1991.

[32] J. Royston, “Remark AS R94: A remark on algorithm AS 181: The W test for normality,”

Applied Statistics, vol. 44, pp. 547–551, 1995.

[33] P. Westfall, “Kurtosis as peakedness, 1905-2014 R.I.P.” The American Statistician, vol. 68,

pp. 191–195, 2014.

[34] P. Rinn, P. Lind, M. Wachter, and J. Peinke, “Langevin: An R package for stochastic data

analysis,” Journal of Open Research Software, 2016.

[35] P. Lind, M. Haase, F. Bo?ttcher, J. Peinke, D. Kleinhans, and R. Friedrich, “Extracting

strong measurement noise from stochastic series: applications to empirical data,” Physical

Review E, vol. 81, 041125, 2010.

[36] E. Darulova?, “Programming with numerical uncertaintie,” Ph.D. dissertation, E?cole Poly-

techinque Fe?de?rale de Lausanne, 2014.

[37] H. Risken, Fokker-Planck Equation. Berlin: Springer, 1984.

[38] S. Rabbani, “Probability density function in terms of moments,” 2008. [Online]. Available:

http://srabbani.com/moments2.pdf

[39] G. Yule, “On a method of investigating periodicities in disturbed series, with special ref-

erence to wolfer’s sunspot numbers,” Philosophical Transactions of the Royal Society of

London, vol. 226, p. 267–298, 1927.

[40] W. Enders, Stationary Time-Series Models. Applied Econometric Time Series. New York:

J. Wiley, 2004.

[41] J. Contreras, R. Espinola, F. Nogales, and A. Conejo, “Arima models to predict next-day

electricity prices,” Institute of Electrical and Electronics Engineers(IEEE) Transactions on

Power Systems, vol. 18, pp. 1014–1020, 2003.

47

http://srabbani.com/moments2.pdf

	Introduction
	State of the art
	Stationary Stochastic Processes
	Brownian Motion and White Noise
	Markov Process
	Stochastic Differential Equations
	The Fokker-Planck Equation
	The Black-Scholes Model and its limitations
	From Stochastic Volatility to Superstatistics
	The Langevin Analysis
	Statistical Tests

	Getting to know the data
	Outliers and Daily Patterns
	Log-Normal Parameter Fluctuations
	Markov Tests

	The Langevin Analysis
	A Simple Model without Correlation
	Modelling the Coupling between  and 

	Approaching Non-Stationarity
	Discussion and Conclusions

</field>
	</doc>
</add>