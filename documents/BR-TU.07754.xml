<?xml version="1.0" encoding="utf-8"?>
<add>
	<doc>
		<field name="docid">BR-TU.07754</field>
		<field name="filename">12488_TeseLeoCabral_vrsFinal.pdf</field>
		<field name="filetype">PDF</field>
		<field name="text">¿¿¿
Universidade Federal de Pernambuco
Centro de Tecnologia e Geociências
Departamento de Engenharia Civil
Leonardo Cabral Pereira
Quantificação de Incertezas Aplicada à
Geomecânica de Reservatórios
Leonardo Cabral Pereira
Quantificação de Incertezas Aplicada à Geomecânica de Reservatórios
Tese de Doutorado apresentada ao Programa de Pós-graduação do Departamento de Engenharia Civil da Universidade Federal de Pernambuco como requisito parcial para obtenção do grau de Doutor na área de Simulação e Gerenciamento de Reservatórios de Petróleo.
Orientador: Prof. Dr. Leonardo José do Nascimento Guimarães
Co-Orientador: Prof. Dr. Marcelo Javier Sánchez Castilla
Catalogação na fonte
Bibliotecária Margareth Malta, CRB-4 / 1198
P436q Pereira, Leonardo Cabral.
Quantificação de incertezas aplicada à geomecânica de reservatórios / Leonardo Cabral Pereira. - Recife: O Autor, 2015.
195 folhas, il., gráfs., tabs.
Orientador: Prof. Dr. Leonardo José do Nascimento Guimarães.
Coorientador: Prof. Dr. Marcelo Javier Sánchez Castilla.
Tese (Doutorado) - Universidade Federal de Pernambuco. CTG. Programa de Pós-Graduação em Engenharia Civil, 2015.
Inclui Referências e Apêndices.
1. Engenharia Civil. 2. Geomecânica de reservatórios. 3. Quantificação de incertezas. 4. Teoria da evidência. 5. Modelos de acoplamento. I. Guimarães, Leonardo José do Nascimento. (Orientador). II. Castilla, Marcelo Javier Sánchez. (Coorientador). III. Título.
UFPE
624 CDD (22. ed.)
BCTG/2015-255
A comissão examinadora da defesa da tese de doutorado
QUANTIFICAÇÃO DE INCERTEZAS APLICADA À GEOMECÂNICA DE RESERVATÓRIOS
defendida por Leonardo Cabral Pereira
considera o candidato APROVADO.
Recife, 08 de julho de 2015
Orientadores:
Prof. Dr. Leonardo José do Nascimento Guimarães
Prof. Dr. Marcelo Javier Sánchez Castilla
Banca Examinadora:
Dr. Alexandre Anoze Emerick (Examinador Externo)
Prof. Dr. Bernardo Horowitz (Examinador Interno)
Dr. Luis Glauber Rodrigues (Examinador Externo)
Prof. Dr. Nelson Inoue (Examinador Externo)
À minha maravilhosa família.
Manuela, meu anjo, minha filha;
Mariana, minha esposa, meu amor; Mãe Nilda, minha protetora, minha mãe; José Itamar, meu pai, minha inspiração; Leandro, meu amigo, meu irmão; Duque, meu parceiro, meu cão.
AGRADECIMENTOS
Agradeço do fundo do meu coração a todos que contribuíram para o desenvolvimento desta tese. Minha tese de doutorado foi uma das experiências mais transformadoras da minha vida, em todos os sentidos. Daqui surge uma pessoa mais compreensiva e amadurecida, um homem mais responsável e um pesquisador capaz de contribuir de forma mais rápida e eficiente para os desafios que irá enfrentar. Foram quase 4 anos (no meu caso, mais que 10% de toda a minha vida) dedicados a um único objetivo: transformar-me em um pesquisador, e mais do que isso, sentir-me um pesquisador. O caminho até aqui foi tortuoso e cheio de reviravoltas. Percebi que a tese é uma pequena parcela de toda a extensa experiência vivida, e que a educação é realmente a arma mais poderosa que possuímos para mudar o mundo.
Especificamente, agradeço a colaboração e amizade de meus orientadores Leonardo Guimarães e Marcelo Sánchez. Reparei que durante nosso convívio aprendi a admirá-los não somente como professores, mas também como amigos. Por falar em amigos, sem ser capaz de listar todos os que contribuíram nesta etapa da minha carreira, cito a maioria que esteve comigo durante o período desta tese. No Recife, os alunos que me muito bem me receberam: Manuel, Jonathas, Leila, Marcela, Inaldo, Rafael e Juliana. Em College Station, agradeço a toda comunidade de brasileiros que tornaram a vida no Texas mais aconchegante, em especial aos amigos: Marcelo, Priscila, Fabiano, Sandra, Gilson, Michele, Luigi, Willian, Marina, Bernardo, Victoria, Carol, Davi, Mozart, Vandré, Elesbão, Celso, Aline, José, Luana, Renato, Galvão, Josimar, Josicleda, Dani, Rodrigo, Samila, Glauco, Clesiane, Geraldo e Thavilla, São todos representantes da importância da parceria nos momentos em que somente uma conversa fiada é capaz de aliviar a tensão. Agradeço também a todos os pesquisadores e colegas da Petrobras que tiveram participação direta na minha tese: Bernardo Horowitz, Igor Gomes, Nelson Inoue, Alexandre Emerick, Régis Romeu, Mauro Becker, Luis Glauber, José Roberto, Antônio Luiz, Flávia Falcão e Verinha.
Agradeço por fim, à Petrobras, a empresa em que trabalho, que permitiu e proporcionou a realização deste sonho.
“Education is the most powerful weapon which you can use to change the world.” (Nelson Mandela)
RESUMO
A disciplina de geomecânica de reservatórios engloba aspectos relacionados não somente à mecânica de rochas, mas também à geologia estrutural e engenharia de petróleo e deve ser entendida no intuito de melhor explicar aspectos críticos presentes nas fases de exploração e produção de reservatórios de petróleo, tais como: predição de poro pressões, estimativa de potenciais selantes de falhas geológicas, determinação de trajetórias de poços, cálculo da pressão de fratura, reativação de falhas, compactação de reservatórios, injeção de CO2, entre outros. Uma representação adequada da quantificação de incertezas é parte essencial de qualquer projeto. Especificamente, uma análise que se destina a fornecer informações sobre o comportamento de um sistema deve prover uma avaliação da incerteza associada aos resultados. Sem tal estimativa, perspectivas traçadas a partir da análise e decisões tomadas com base nos resultados são questionáveis. O processo de quantificação de incertezas para modelos multifísicos de grande escala, como os modelos relacionados à geomecânica de reservatórios, requer uma atenção especial, principalmente, devido ao fato de comumente se deparar com cenários em que a disponibilidade de dados é nula ou escassa. Esta tese se propôs a avaliar e integrar estes dois temas: quantificação de incertezas e geomecânica de reservatórios. Para isso, foi realizada uma extensa revisão bibliográfica sobre os principais problemas relacionados à geomecânica de reservatórios, tais como: injeção acima da pressão de fratura, reativação de falhas geológicas, compactação de reservatórios e injeção de CO2. Esta revisão contou com a dedução e implementação de soluções analíticas disponíveis na literatura relatas aos fenômenos descritos acima. Desta forma, a primeira contribuição desta tese foi agrupar diferentes soluções analíticas relacionadas à geomecânica de reservatórios em um único documento. O processo de quantificação de incertezas foi amplamente discutido. Desde a definição de tipos de incertezas - aleatórias ou epistêmi-cas, até a apresentação de diferentes metodologias para quantificação de incertezas. A teoria da evidência, também conhecida como Dempster-Shafer theory, foi detalhada e apresentada como uma generalização da teoria da probabilidade. Apesar de vastamente utilizada em diversas áreas da engenharia, pela primeira vez a teoria da evidência foi utilizada na engenharia de reservatórios, o que torna tal fato uma contribuição fundamental desta tese. O conceito de decisões sob incerteza foi introduzido e catapultou a integração desses dois temas extremamente relevantes na engenharia de reservatórios. Diferentes cenários inerentes à tomada de decisão foram descritos e discutidos, entre eles: a ausência de dados de entrada disponíveis, a situação em que os parâmetros de entrada são conhecidos, a inferência de novos dados ao longo do projeto e, por fim, uma modelagem híbrida. Como resultado desta integração foram submetidos 3 artigos a revistas indexadas. Por fim, foi deduzida a equação de fluxo em meios porosos deformáveis e proposta uma metodologia explícita para incorporação dos efeitos geomecâni-cos na simulação de reservatórios tradicional. Esta metodologia apresentou resultados bastante efetivos quando comparada a métodos totalmente acoplados ou iterativos presentes na literatura.
Palavras-chave: Geomecânica de reservatórios. Quantificação de incertezas. Teoria da evidência. Modelos de acoplamento.
ABSTRACT
Reservoir geomechanics encompasses aspects related to rock mechanics, structural geology and petroleum engineering. The geomechanics of reservoirs must be understood in order to better explain critical aspects present in petroleum reservoirs exploration and production phases, such as: pore pressure prediction, geological fault seal potential, well design, fracture propagation, fault reactivation, reservoir compaction, CO2 injection, among others. An adequate representation of the uncertainties is an essential part of any project. Specifically, an analysis that is intended to provide information about the behavior of a system should provide an assessment of the uncertainty associated with the results. Without such estimate, perspectives drawn from the analysis and decisions made based on the results are questionable. The process of uncertainty quantification for large scale multiphysics models, such as reservoir geomechanics models, requires special attention, due to the fact that scenarios where data availability is nil or scarce commonly come across. This thesis aimed to evaluate and integrate these two themes: uncertainty quantification and reservoir geomechanics. For this, an extensive literature review on key issues related to reservoir geomechanics was carried out, such as: injection above the fracture pressure, fault reactivation, reservoir compaction and CO2 injection. This review included the deduction and implementation of analytical solutions available in the literature. Thus, the first contribution of this thesis was to group different analytical solutions related to reservoir geomechanics into a single document. The process of uncertainty quantification has been widely discussed. The definition of types of uncertainty - aleatory or epistemic and different methods for uncertainty quantification were presented. Evidence theory, also known as Dempster-Shafer theory, was detailed and presented as a probability theory generalization. Although widely used in different fields of engineering, for the first time the evidence theory was used in reservoir engineering, which makes this fact a fundamental contribution of this thesis. The concept of decisions under uncertainty was introduced and catapulted the integration of these two extremely important issues in reservoir engineering. Different scenarios inherent in the decision-making have been described and discussed, among them: the lack of available input data, the situation in which the input parameters are known, the inference of new data along the design time, and finally a hybrid modeling. As a result of this integration three articles were submitted to peer review journals. Finally, the flow equation in deformable porous media was presented and an explicit methodology was proposed to incorporate geomechanical effects in the reservoir simulation. This methodology presented quite effective results when compared to fully coupled or iterative methods in the literature.
Keywords: Reservoir geomechanics. Uncertainty quantification. Evidence theory. Coupling models.
Lista de Figuras
Figura 1: Comprimento da fratura ao longo do tempo calculada pela Equação 5...26
Figura 2: Comprimentos inicial (azul) e final (vermelho) da fratura e frente térmica inicial (azul) e final (vermelho) com formato elíptico conforme proposto por (Perkins and Gonzalez, 1985) após 5 anos de injeção de água................................................27
Figura 3: Critério de rutura de Mohr-Coulomb..................................30
Figura 4: Representação esquemática dos fenômenos de compactação (no nível do reservatório) e subsidência (na superfície).................................................32
Figura 5: Geometria para determinação do campo de deslocamentos no entorno de um reservatório em forma de disco (Ferreira, 2014)...............................34
Figura 6: Perfil de deslocamento vertical ao longo da profundidade analisada. O reservatório se encontra a 2900 metros de profundidade...............................................35
Figura 7: Fluxo de fluidos no processo WAG...........................................38
Figura 8: Aproximação por camadas utilizada para representar a frente de saturação de CO2 (Nordbotten et al., 2005)............................................................40
Figura 9: Frente de saturação de CO2 ao final de 3 anos de injeção...................42
Figura 10: Acréscimo de pressão nas proximidades do poço injetor em função do processo de injeção de CO2 a vazão constante.....................................................43
Figura 11: Deslocamento vertical na superfície: sem considerar efeito de dissolução e consequente enfraquecimento da rocha reservatório (azul) e considerando tais efeitos (verde). 43
Figura 12: Função distribuição acumulada (preto) e função de distribuição acumulada complementar (vermelho) para a variável K com uma distribuição normal com média zero e desvio padrão 1......................................................................51
Figura 13: Exemplo de amostragem por LHS de quatro amostras sob os parâmetros xj e x2. Os pontos vermelhos indicam a localização das amostras..................................55
Figura 14: Lançamento de um dado. Amostragens obtidas via método de Monte Carlo tradicional e LHS....................................................................55
Figura 15: Tipos de evidência: a) inclusiva; b) consistente; c) arbitrária e d) desconexa. As letras A, B, C, D e E representam diferentes fontes de evidência..................................66
Figura 16: Quantificação de incertezas em y. A curva verde representa uma CDF com distribuições uniformes para a e b; em rosa uma CDF para a e b com distribuição normal (média 0,5 e desvio padrão 0,05); em azul tem-se uma CDF considerando as BPAs como pesos de uma distribuição uniforme; em vermelho a CBF e em preto a CPF...........................69
Figura 17: Diferentes cenários de evidências para quantificação de incertezas em y. A curva verde representa uma CDF com distribuições uniformes para a e b; em rosa uma CDFpara a e b com distribuição normal (média 0,5 e desvio padrão 0,05); em azul tem-se uma CDF considerando as BPAs como pesos de uma distribuição uniforme; em vermelho a CBF e em preto a CPF................................................................................70
Figura 18: Definição da pressão de fratura. Uso da função densidade de probabilidade uniforme (em azul escuro), normal (em azul claro) e uma distribuição uniforme que considera as BPA ’s como pesos (em verde) nos respectivos intervalos. Via teoria da evidência calcula-se a CBF (em vermelho) e a CPF (em preto)...............................................................74
Figura 19: Cálculo da subsidência sobre um reservatório depletado. Incorporação de novos dados ao longo do tempo. Uso da função densidade de probabilidade uniforme (curvas em verde), e uma distribuição uniforme que considera as BPA’s como pesos (curvas em azul) nos respectivos intervalos. Via teoria da evidência calcula-se a CBF (em vermelho) e a CPF (em
preto).......................................................................................78
Figura 20: Cálculo da elevação da superfície sobre um reservatório pressurizado. Resultado da quantificação de incertezas via teoria da evidência, CBFs em vermelho e CPFs em preto........81
Figura 21: Diagrama representativo do gerenciamento da simulação acoplada...................93
Figura 22: Interpolação “Natural Neighbor”. A área dos círculos verdes definem os pesos da
interpolação. A região roxa é chamada nova célula de Voronoi após a inserção do ponto a ser interpolado (ponto preto). Os pesos representam a área de interseção da célula roxa com cada uma das 7 células circundantes.................................................................94
Figura 23: Geometría da malha geomecanica. a) vista superior, b) vista frontal, respectivamente. ..........................................................................................96
Figura 24: Malha de simulação geomecânica. a) vista superior, b) vista lateral e c) vista 3D. O
grid de simulação de reservatórios é definido pelas células 6-16, 6-16, 6-10 nas direções x, y e z, respectivamente.........................................................................96
Figura 25: Pressão média do reservatório ao longo do tempo comparando-se os resultados de
diferentes simuladores..................................................................98
Figura 26: Compactação ao longo do tempo de um reservatório depletado...................98
Figura 27: Subsidência ao longo do tempo de um reservatório depletado...................99
Figura 28: Mapa de pressões (psi) no topo do reservatório ao final de 2000 dias de simulação. a) simulação de reservatórios tradicional e b) metodologia acoplada..........................100
Figura 29: Mapa de porosidades no topo do reservatório ao final de 2000 dias de simulação. a) simulação de reservatórios tradicional e b) metodologia acoplada..........................100
Figura 30: Deslocamento vertical no topo do maciço (subsidência) entre passos de acoplamento. ..........................................................................................101
Figura 31: Deslocamento vertical no topo do reservatório (compactação) entre passos de
acoplamento......................................................................101
Figura 32: Deslocamento vertical na base do reservatório entre passos de acoplamento.102
Figura 33: Variação de pressão (Delta P) aplicada no último passo de acoplamento.102
Figura 34: Geometria da malha geomecânica do caso a) vista superior, b) vista frontal, respectivamente..................................................................104
Figura 35: Malhas de simulação a) malha de simulação de reservatórios; b) corte malha geomecânica representando a posição do reservatório e o underburden e c) malha geomecânica
completo......................................................................105
Figura 36: Processo de transição do número de camadas entre grids para redução do número de elementos no modelo geomecânico...............................................106
Figura 37: Comportamento da pressão média do reservatório ao longo do tempo...107
Figura 38: Comportamento das porosidades nas células dos poços injetor e produtor comparando-se a metodologia CBMEX e a simulação de reservatórios tradicional..107
Figura 39: Produção acumulada de óleo ao longo do tempo.......................108
Figura 40: Razão água-óleo ao longo do tempo..................................108
Figura 41: Delta de pressão no último passo de acoplamento....................109
Lista de Tabelas
Tabela 1: Dados de entrada necessários para o cálculo da pressão de fratura e comprimento da fratura ao longo do tempo...................................................................25
Tabela 2: Dados de entrada necessários para o cálculo da pressão máxima de injeção.........31
Tabela 3: Dados de entrada necessários para o cálculo do perfil de deslocamento vertical num reservatório depletado......................................................................34
Tabela 4: Dados de entrada necessários para o cálculo da elevação da superfície sobre um reservatório pressurizado...................................................................41
Tabela 5: Intervalos de possíveis valores caracterizando a incerteza epistêmica das variáveis a e b. Cada intervalo é definido por fontes independentes.......................................68
Tabela 6: Intervalos de possíveis valores caracterizando a incerteza das variáveis de entrada.
Cada intervalo é definido por opiniões de diferentes especialistas..........................73
Tabela 7: Cenário de disponibilização de informações (evidências) ao longo do tempo. O processo de quantificação de incertezas é modificado a cada nova evidência..................76
Tabela 8: Dados de entrada para o cálculo da elevação da superfície sobre um reservatório pressurizado via uma abordagem híbrida para a quantificação de incertezas...................80
Sumário
1.
1.1
1.2
1.3
2.
2.1
2.2
2.3
2.4
3.
3.1
3.2
3.3
3.4
3.5
3.6
4.
4.1
4.2
4.3
4.4
INTRODUÇÃO________________________________________________________16
Motivação e Objetivos	16
Organização da Tese	17
Lista de Publicações	18
geomecânica de reservatórios	20
Injeção Acima da Pressão de Fratura	20
Reativação de Falhas	27
Compactação de Reservatórios	31
INJEÇÃO DE CO2	36
ouantificação de incertezas	44
Tipos de Incerteza	45
Metodologias para Quantificação de Incertezas	47
Métodos para Quantificação de Incertezas Aleatórias	49
Métodos para Quantificação de Incertezas Epistêmicas	58
Tipos de Evidência	65
Exemplo de Aplicação	67
decisões sob incerteza	71
Ausência de Dados de Entrada	73
Dados de Entrada Disponíveis	75
Inferência de Novos Dados	76
Abordagem híbrida	79
5.1 Equação da Continuidade - Conservação de massa
82
5.2 Lei de Darcy
83
5.3 Fluxo Monofásico em Meios Porosos Deformáveis
84
5.4 Equilíbrio em Meios Porosos Deformáveis
86
5.5 FORMULAÇÃO EXPLÍCITA - CBMEX
88
5.6 Caso de Validação - Dean 2006
94
5.7 Modelo de Injeção Wag - Caso SPE 05
103
5.8 TEMPO DE CPU EM MODELOS ACOPLADOS
110
6. CONCLUSÕES E RECOMENDAÇÕES____________________________111
APÊNDICE A - ARTIGO PUBLICADO NA COMPUTERS &amp;amp; GEOTECHNICS	113
APÊNDICE B - MANUSCRITO SUBMETIDO AO JPSE	129
APÊNDICE C - MANUSCRITO SUBMETIDO AO SPE PAPER CONTEST 2015	176
REFERÊNCIAS	191
1.	INTRODUÇÃO
A disciplina de geomecânica de reservatórios engloba aspectos relacionados não somente à mecânica de rochas, mas também à geologia estrutural e engenharia de petróleo. A geomecânica de reservatórios deve ser entendida no intuito de melhor explicar aspectos críticos presentes nas fases de exploração e produção de reservatórios de petróleo, tais como: predição de poro pressões, estimativa de potenciais selantes de falhas geológicas, determinação de trajetórias de poços, cálculo da pressão de fratura, reativação de falhas (por efeitos da depleção e injeção), compactação de reservatórios, injeção de CO2, entre outros. Isto se deve ao fato de que na geomecânica de reservatórios a rocha é considerada deformável e o equilíbrio mecânico deve ser verificado.
Uma representação adequada da quantificação de incertezas é parte essencial de qualquer projeto. Especificamente, uma análise que se destina a fornecer informações sobre o comportamento de um sistema deve prover uma avaliação da incerteza associada aos resultados. Sem tal estimativa, perspectivas traçadas a partir da análise e decisões tomadas com base nos resultados são questionáveis. O processo de quantificação de incertezas para modelos multifísicos de grande escala, como os modelos relacionados à geomecânica de reservatórios, requer uma atenção especial, principalmente, devido ao fato de comumente se deparar com cenários em que a disponibilidade de dados é nula ou escassa.
1.1	Motivação e Objetivos
A importância da geomecânica na engenharia de reservatórios de petróleo tem sido cada vez mais reconhecida. Neste tema, dois aspectos relevantes têm sido vastamente discutidos na literatura: a quantificação de incertezas e a modelagem acoplada. Esta tese se propõe a contribuir para o entendimento da quantificação de incertezas em estudos que englobem a disciplina de geomecânica de reservatórios e para a inclusão de cálculos geomecânicos na simulação de reservatórios tradicional via modelagem acoplada, desenvolvendo métodos para: (1) estudos de quantificação de incertezas em projetos relacionados à geomecânica de reservatórios; (2) contemplar efeitos geomecâ-
nicos na simulação de reservatórios tradicional via uma formulação explicita que viabilize tal incorporação em tempos razoáveis de simulação.
1.2	Organização da Tese
Esta tese está dividida em seis capítulos. A estrutura e o conteúdo de cada capítulo podem ser resumidamente explicados da seguinte forma:
O Capítulo 1 introduz os temas de geomecânica de reservatórios e quantificação de incertezas que definem as duas linhas de pesquisa principais discutidas nesta tese. Além disso, apresenta as motivações que suportaram os estudos desses dois temas principais e os objetivos específicos delineados antes e durante a execução deste trabalho. Em seguida descreve a estrutura e a organização da tese, e, por fim, apresenta a lista de publicações produzidas no decorrer do período de doutorado.
O Capítulo 2 apresenta não somente uma revisão bibliográfica sobre problemas de engenharia relacionados à geomecânica de reservatórios como também reproduz diferentes soluções analíticas correlatas. São eles: injeção acima da pressão de fratura; reativação de falhas; compactação de reservatórios e impactos da injeção de CO2. Sendo assim, este capítulo foi dedicado à resolução de problemas analíticos de maneira deter-minística visando um entendimento do efeito dos diferentes parâmetros geomecânicos.
O Capítulo 3 dedica-se a revisar diferentes representações da quantificação de incertezas em projetos de engenharia. Inicialmente são descritos os diferentes tipos de incerteza. Em seguida, são apresentados métodos tradicionais para quantificação de incertezas chamadas aleatórias. Posteriormente, métodos de quantificação de incertezas epistêmicas são introduzidos. Finalmente, um exemplo de aplicação com o objetivo de se comparar os diferentes métodos discutidos é apresentado.
O Capítulo 4 trata da tomada de decisão sob incerteza em diferentes cenários. O primeiro cenário exemplifica uma típica situação relacionada à geomecânica de reservatórios: a ausência ou escassez de dados de entrada. Outro cenário é aquele em que informações sobre os parâmetros de entrada do modelo estão disponíveis ao inicio do projeto. O terceiro cenário de tomada de decisão sob incerteza analisado aborda a inferência de novos dados ao longo da vida útil do projeto, ou seja, informações sobre parâmetros são atualizadas com a aquisição de novos dados.
O Capitulo 5 descreve a formulação de acoplamento explícito desenvolvida e apresenta resultados comparativos entre a simulação de reservatórios convencional e a simulação com acoplamento geomecânico. Primeiramente delineia-se a dedução da equação da conservação de massas para fluidos e para sólidos, que juntamente com a lei de Darcy possibilitam a formulação do fluxo em meios porosos deformáveis. A equação de equilíbrio em meios porosos deformáveis se faz presente para apresentação das principais diferenças entre a simulação de reservatórios tradicional e a simulação acoplada. Em seguida dois casos são simulados: o primeiro reproduz a depleção de um reservatório ao longo do tempo e o segundo reporta a injeção WAG (water alternating gas). Conforme dito anteriormente, o objetivo desses exemplos é comparar a solução da simulação de reservatórios convencional com os resultados obtidos via simulação acoplada.
Ao final do trabalho, no Capítulo 6, algumas conclusões, a cerca da importância da quantificação de incertezas na geomecânica de reservatórios e a relevância da incorporação de efeitos geomecânicos na simulação de reservatórios, são discutidas. Sugestões para trabalhos futuros, considerando tanto novos estudos a serem realizados quanto melhorias nas metodologias apresentadas, são delineadas ao final deste mesmo capítulo.
1.3	Lista de Publicações
Durante o período de doutorado foram publicados/submetidos os seguintes trabalhos:
•	Leonardo C. Pereira, Leonardo J.N. Guimarães, Bernardo Horowitz, Marcelo Sánchez. Coupled hydro-mechanical fault reactivation analysis incorporating evidence theory for uncertainty quantification. Computers and Geotechnics, Volume 56, March 2014, Pages 202-215. - Reproduzido no APÊNDICE A
•	Leonardo C. Pereira, Marcelo Sánchez, Leonardo J. N. Guimarães, Erick S. R. Santos, Bernardo Horowitz. Defining Maximum Injection Pressures in an Offshore Petroleum Reservoir Using the Evidence Theory for Uncertainty Quantification. Journal of Petroleum Science and Engineering, 2015. (Under review) - Reproduzido no APÊNDICE B
• Leonardo C. Pereira, Leonardo J. N. Guimarães, Marcelo Sánchez. Uncertainty Quantification for Reservoir Geomechanics. Latin America / Caribbean SPE Student Paper Contest. To be submitted to the SPE Journal, 2015. (Under review) - Reproduzido no APÊNDICE C
2.	GEOMECÂNICA DE RESERVATÓRIOS
Neste capítulo serão discutidos tópicos da geomecânica de reservatórios de petróleo mais relacionados à engenharia de reservatórios. Aspectos geológicos e geofísicos dessa disciplina podem ser encontrados na literatura (e.g. (Zoback, 2010) e (Fjar et al., 2008)). O objetivo desse capítulo é, além de realizar uma revisão bibliográfica sobre assuntos relacionados ao tema de geomecânica de reservatórios, reproduzir cálculos analíticos de forma determinística para melhor entendimento dos conceitos envolvidos em cada tópico, e, posteriormente (Capítulo 4), expor diferentes exemplos e discussões sobre a importância da quantificação de incertezas em problemas relacionados à geome-cânica de reservatórios.
2.1	Injeção Acima da Pressão de Fratura
A injeção de água é um dos métodos mais utilizados para se manter a produção de reservatórios de petróleo. Tal método é bastante efetivo tanto após a produção do reservatório por mecanismos primários quanto como uma estratégia de produção ainda nos estágios iniciais da explotação do campo.
Durante o processo de injeção de água, pode ocorrer declínio de injetividade devido a características específicas das rochas e fluidos presentes, geometria dos poços injetores e produtores, precipitação de sais e pela presença de partículas sólidas na água de injeção. Uma das melhores maneiras de evitar a perda de injetividade é a injeção acima da pressão de fratura (de Souza et al., 2005).
Para a aplicação da técnica de injeção acima da pressão de fratura, é necessário um razoável conhecimento dos parâmetros que controlam o processo de iniciação e propagação da fratura. Desta forma, o conhecimento prévio das propriedades mecânicas e permo-porosas das rochas reservatório e capeadora, além do estado de tensões atuante na formação, se faz de suma importância.
Um convincente argumento físico é apresentado por Hubbert e Willis (1972) dizendo que fraturas hidráulicas em subsuperfície irão sempre se propagar perpendicularmente a direção da menor tensão principal. Isto ocorre porque o trabalho realizado para abertura da fratura é proporcional ao produto entre valor da tensão atuante perpendicular ao plano de fratura e o tamanho da abertura da fratura (i.e. trabalho é igual ao produto de força versus deslocamento). As fraturas hidráulicas irão sempre se propagar perpendicular à direção da tensão principal mínima, pois esta é a direção que define a menor configuração de energia.
A injeção acima da pressão de fratura tem sido amplamente discutida na literatura tanto através de modelos analíticos como por abordagens de simulação numérica. Os métodos evoluíram de simples modelos analíticos para a simulação numérica acoplada (fluxo-geomecânica). Esta última permite descrição mais realista dos problemas, mas métodos analíticos permitem a compreensão mais detalhada do processo físico. Neste capítulo a injeção acima da pressão de fratura será estudada via uma combinação de procedimentos analíticos.
Em 1980, um modelo semi-analítico para prever a propagação de fraturas durante o processo de injeção de água foi deduzido (Hagoort et al., 1980). No entanto, um dos fatores mais relevantes, a parcela de tensões induzidas termicamente, não foi considerada. Consequentemente, a transferência de calor entre o fluido injetado e a formação não foi discutida. Em seguida, em 1985, um modelo de três regiões acoplando o comportamento do fluxo de água com a fratura mecânica foi desenvolvido (Perkins and Gonzalez, 1985). Tal modelo é amplamente citado na literatura (e.g. (Saripalli et al., 1999), (Suri and Sharma, 2009) e (Rahman and Khaksar, 2012)), no entanto, nenhuma melhoria óbvia pode ser destacada.
Por outro lado, estudos de propagação de fraturas via simulação numérica tem se desenvolvido rapidamente desde abordagens desacopladas (Settari, 1988), acoplamentos one-way (Hustedt et al., 2008), simulações com acoplamento two-way (Ji et al., 2009), até abordagens totalmente acopladas (Chin et al., 2000). Tais abordagens numéricas podem ser computacionalmente complexas e demoradas. Em especial, Rodrigues (2009) acoplou externamente os softwares comerciais STARS (simulador de reservatórios desenvolvido pela CMG) e FLAC3D (simulador de análise de tensões desenvolvido pela ITASCA) para o estudo da propagação de fraturas durante o fluxo de fluidos no reserva
tório. O acoplamento entre diferentes softwares tende a ser mais efetivo do ponto de vista de redução de custo computacional para a modelagem de problemas reais da geo-mecânica de reservatórios.
Os cálculos para a determinação da pressão de fratura de um reservatório e da variação do comprimento com o tempo podem ser feitas de diferentes formas. A seguir, serão apresentadas as formulações descritas por (de Souza et al., 2005) e (Perkins and Gonzalez, 1985). A seção 4.1 discutirá a importância da quantificação de incertezas para a definição da pressão de fratura.
2.1.1	Pressão de Fratura
A pressão de fratura, em poços verticais e regime de falhamento normal onde crv &gt;gh &gt;a:, considerando fluido penetrante e uma parcela de alteração das tensões devido à temperatura pode ser descrita por (Yew and Weng, 2014):
Prat = maX ‘
(3Th — TH — eTaPe + TT ) — ^^fa'
(1)
(1 + 0 — eTa )
onde: crh é a tensão horizontal mínima no reservatório; define a tensão horizontal máxima no reservatório; eTa é um parâmetro elástico e pode ser calculado por a(1 — 2u¡(1 — u)) sendo a o coeficiente de Biot e v o coeficiente de Poisson; Pe é a pressão estática do reservatório; 0 é a porosidade do reservatório; tt define a resistência a tração da rocha reservatório e	a parcela de alteração das tensões em função da
diferença de temperatura entre o fluido injetado e o fluido da formação.
Para a resolução da Equação 1 devem ser definidos o estado de tensões e as propriedades mecânicas e térmicas da rocha reservatório. As Equações 2 a 4 propõem uma possível forma de definir tais características. A tensão vertical atuante no reservatório pode ser definida por:
T = YwLA+YrSo'
(2)
onde: yw define o gradiente de pressão da água do mar; yr define o gradiente de pressão litostático; LA equivale a profundidade da lâmina d'água e Sot o soterramento do
reservatório. A tensão horizontal mínima atuante no reservatório pode ser calculada através da seguinte relação:
-Pe) + Pe	(3)
onde K0 é o coeficiente de empuxo da rocha reservatório. A parcela de alteração das tensões em função da diferença de temperatura entre o fluido injetado e o fluido da formação pode ser definida por:
jç, - )E f	(4)
1—u
sendo: /&gt; o coeficiente de expansão térmica da rocha reservatório; ff o fator termo elástico de forma que varia entre 0,5 e 1 a depender da frente de avanço da temperatura (Perkins and Gonzalez, 1985). Este fator será definido na Equação 9; E define o módulo de elasticidade do reservatório; Tr é a temperatura do reservatório e T, a temperatura do fluido injetado.
2.1.2	Comprimento da Fratura
O comprimento linear da fratura é definido por (de Souza et al., 2005):
Lf (t) =
2nh,,C,^
(5)
onde: Qinj é a vazão de injeção, hR a espessura do reservatório e t o tempo. O coeficiente de filtração global CL é definido por (Howard and Fast, 1957):
Cl =
í CC
vw
(6)
C + C
v w y
Os coeficientes de filtração Cv e Cw podem ser obtidos em laboratório. No caso da não existência de ensaios laboratoriais, tais coeficientes podem ser estimados, respectivamente, pelas equações 7 e 8.
k$oh +AoJPfrat - Pe)
Cv =
(7)
Cw =
(8)
onde: k é permeabilidade média do reservatório; 0 é a porosidade; pw define a viscosidade da água injetada;&amp;lt;Jh é a tensão horizontal mínima atuante no reservatório; Pe define a pressão estática original do reservatório; kpref é a pressão de referência adotada (Howard and Fast, 1957) e k é o fator de qualidade da água (Yew and Weng, 2014). A redução da tensão horizontal pelo efeito do contraste de temperatura, \D"a, é definida pela Equação 4, sob influência da frente de calor que varia com o tempo (Perkins and Gonzalez, 1985):
ff =
(bo/ ao) +	1
1+(bo/ao) L1+(bo/ao).
/				o,9	r hR y
1/-	1+o, 5	1,45	R	+ o,35	
k			k 2bo J		k 2bo J
bo
ao
o,774
(9)
y
&gt;
onde a0 e b0 definem os eixos maior e menor da frente elíptica de calor:
ao=Lf (\lFi ^AMv2	(io)
bo = Lf gM-í/aMv2	(ii)
sendo F¡ definido por:
(12)
onde Vc é o volume da formação resfriada pela água de injeção, definido por:
____________P^W_______________
PC O-P.Cea\ Sor)+PoCeo0Sor
(13)
sendo W, o volume total de água injetada; pw a massa específica da água; Cew o calor específico da água; pgr a massa específica do grão; Cgr o calor específico dos grãos; po a massa específica do óleo; Ceo o calor específico do óleo e Sor a saturação residual de óleo.
2.1.3	Exemplo de Aplicação
Para ilustrar o uso e significância das equações expostas acima é apresentado um exemplo de aplicação utilizando as propriedades descritas na Tabela 1. A pressão de fratura calculada, através da Equação 1 é igual a 616,14 kgf/cm2.
Tabela 1: Dados de entrada necessários para o cálculo da pressão de fratura e comprimento da fratura ao longo do tempo.
Dados Geomecânicos
Módulo de elasticidade (E)	255 kgf/cm2
Coeficiente de Biot (a)	0,8
Resistência à tração (aT)	41 kgf/cm2
Coeficiente de Poisson (v)	0,25
Coeficiente de empuxo (K0)	0,8
Propriedades de fluidos e rocha	
Viscosidade da água (uw)	0,85 cP
Porosidade («)	7,3 %
Permeabilidade (k)	55 mD
Saturação de óleo residual (Sor)	0,25
Calor específico da água (Cw)	5 kJ/kg.K
Calor específico do óleo (Co)	2 kJ/kg.K
Massa específica x calor específico dos grãos (pgrCgr)	2000 kJ/m3.K
Massa específica da água (pw)	1000 kg/m3
Massa específica do óleo (po)	881 kg/m3
Dados de comprimento	
Lâmina d'água (LA)	2100 m
Soterramento (Sot)	2450 m
Espessura do reservatório (hR)	100 m
Dados de pressão	
Pressão estática do reservatório (Pe)	490 kgf/cm2
Gradiente da água do mar (yw)	0,101kgf/cm2/m
Gradiente litostático (yr)	0,209kgf/cm2/m
Dados de temperatura	
Temperatura do reservatório (Tr)	60 oC
Temperatura da água de injeção (T)	35 oC
Dados de injeção	
Vazão (Qn)	4000 m3/d
Tempo de injeção (t)	5 anos
O comprimento de fratura ao longo do tempo é apresentado na Figura 1. A frente elíptica de temperatura representada em vista superior está ilustrada na Figura 2.
Figura 1: Comprimento da fratura ao longo do tempo calculada pela Equação 5.
Maiores detalhes e discussões comparativas entre cálculos determinísticos e não determinísticos da pressão de fratura de reservatórios de petróleo serão discriminados na seção 4.1. A próxima seção apresenta os conceitos relacionados a outro aspecto importante da geomecânica de reservatórios: reativação de falhas geológicas.
Figura 2: Comprimentos (em metros) inicial (azul) e final (vermelho) da fratura e frente térmica inicial (azul) e final (vermelho) com formato elíptico conforme proposto por (Perkins and Gonzalez, 1985) após 5 anos de injeção de água.
2.2	Reativação de Falhas
Falhas geológicas podem ser selos naturais que impedem a migração ascendente de hidrocarbonetos e, assim, garantem o acúmulo destes hidrocarbonetos na rocha reservatório. A correta compreensão dos limites hidráulicos do reservatório é crucial para o desenvolvimento do seu projeto de explotação. Planos de produção robustos não devem permitir a reativação de falhas, pois a perda do selo tem que ser encarada como a perda de fluido descontrolada com impactos sobre os custos de produção e sobre o risco de acidente ambiental.
A reativação de falhas pode induzir a ruptura do selo com a possibilidade de comunicar diferentes blocos do reservatório, e, no pior cenário, comunicar o reservatório com a superfície. O processo de reativação de falhas pode ser desencadeado por diversos fatores, sendo um deles a injeção de fluidos a alta pressão, técnica amplamente utilizada na indústria para aumentar a produção de petróleo. A injeção de fluidos a alta pressão representa uma atividade central em torno de estratégias de produção de petróleo. Neste contexto, um grande desafio para os engenheiros e geólogos de reservatórios envolvidos em projetos de injeção de fluidos é a estimativa da pressão de injeção máxima admissível para o não comprometimento do selo da falha e, ao mesmo tempo, se elevar o fator de recuperação do campo.
Desenvolvimentos recentes na modelagem numérica permitiram um melhor entendimento do comportamento mecânico e hidráulico de reservatórios de petróleo quando água ou CO2 são injetados. Por exemplo, Rutqvist (2011) apresentou a modelagem de deformação da crosta terrestre causada por movimentos de fluidos. A partir da análise geomecânica, concluiu-se que a causa do terremoto estudado foi uma sobre pressão resultante da movimentação de fluidos leves em camadas superficiais.
A quantidade máxima de CO2 que pode ser armazenado em um reservatório compartimentado a 2.000 metros de profundidade foi calculada por Teatini et al. (2014). Um modelo de elementos finitos 3D foi usado para simular o deslocamento da superfície, a reativação de falhas, e um possível colapso de poros tanto da rocha reservatório quanto da rocha capeadora. A partir deste trabalho concluiu-se que a avaliação do critério de Mohr-Coulomb (ver seção 2.2.1) e as propriedades geomecânicas das falhas geológicas são fatores muito importantes para uma correta previsão do comportamento mecânico de um reservatório pressurizado.
O estudo do processo de reativação de falhas por meio de soluções analíticas e semi-analíticas tem sido amplamente discutido na literatura. Por exemplo, Soltanzadeh and Hawkes (2008) apresentaram soluções semi-analíticas para análises de reativação de falhas em reservatórios horizontais com seções transversais elípticas ou retangulares assumindo condições de estado plano de deformações. Foi mostrado que o potencial de reativação de falhas é dependente da geometria do reservatório e do ângulo de mergulho (dip) da falha geológica analisada. Neves et al. (2009) desenvolveram uma ferramenta gráfica para realizar a análise de reativação de falhas.
Uma característica fundamental do problema de reativação de falhas é o enorme intervalo de variação dos parâmetros associados à zona de falha, que pode ser altamente heterogênea. Além disso, a ausência de dados experimentais confiáveis para caracterização dos materiais da zona de falha é muito comum. Portanto, é necessária uma metodologia especial para lidar com as incertezas associadas a este tipo de análise.
A próxima seção apresenta um cálculo determinístico para se definir a máxima pressão de injeção sob o critério de reativação de falhas geológicas. A quantificação de incertezas considerando dados de entrada disponíveis será realizada na seção 4.2
2.2.1	Cálculo da Pressão Máxima de Injeção
Diferentes critérios de ruptura têm sido propostos para rochas (por exemplo, (Fjar et al., 2008)). Nos modelos analíticos utilizados para avaliar as alterações induzidas pelas tensões decorrentes da depleção ou pressurização do reservatório, é assumido que a compactação ou a expansão da rocha acontece somente na direção vertical (Fjar et al., 2008).
Uma boa estimativa do incremento de pressão de poros visando prevenir a reativação de falhas depende do estado de tensões atuante e do fluxo de fluidos no meio poroso. Uma vez que a tensão vertical está relacionada ao peso das camadas acima do reservatório, esta componente de tensão é um pouco afetada pelas mudanças de pressão de poros no reservatório. Por outro lado, as tensões horizontais podem ser altamente influenciadas por alterações de pressão de poros.
Supõe-se que o comportamento mecânico do reservatório é controlado pela tensão efetiva, definida por:
o = o - IPp
(14)
onde: o é o tensor de tensões totais; Pp é a pressão de poros, e I é o tensor identidade. O modelo constitutivo mecânico adotado neste trabalho se baseia no critério de Mohr-Coulomb em que a superfície de fluência pode ser expressa como:
(r-r3)-(r +r3)sin(0)-2c cos(^) = 0
(15)
onde (t1 e são as tensões principais, máxima e mínima, respectivamente; c' e definem a coesão efetiva e ângulo de atrito efetivo da rocha, respectivamente.
Seguindo as hipóteses discutidas em (Soltanzadeh and Hawkes, 2008), a mudança de pressão de poros resultante da produção ou injeção de líquido em um reservatório irá gerar alterações nas tensões efetivas. A mudança de tensão efetiva está relacionada com a variação de pressão de poros e a variação das tensões totais:
A^ =A0-. +aPp	(16)
onde APp é considerada positiva durante a injeção e se presume a (coeficiente de Biot) igual a 1 para a análise apresentada nesta seção.
O aumento da pressão do fluido induz um movimento progressivo do círculo de Mohr para a esquerda, podendo provocar a ruptura do material quando o círculo toca a superfície de fluência (Figura 3), caracterizando a reativação de falhas pré-existentes. Neste trabalho é adotada a hipótese de que o plano de falha tem os mesmos parâmetros de resistência do reservatório. Mesmo sabendo que isso pode não ser verdade, essa consideração acaba por sustentar a importância de um estudo de quantificação de incertezas, tal como apresentado na seção 4.2.
Figura 3: Critério de rutura de Mohr-Coulomb.
A próxima seção apresenta um exemplo de aplicação considerando valores únicos para as propriedades de resistência e deformabilidade da falha geológica. Conforme mencionado anteriormente, a quantificação de incertezas associada a este problema será discutida na seção 4.2.
2.2.2	Exemplo de Aplicação
Ilustrando os conceitos discutidos acima para estimar a pressão máxima de injeção, considerando a reativação de falhas geológicas, a Tabela 2 apresenta os parâmetros necessários para o cálculo.
Tabela 2: Dados de entrada necessários para o cálculo da pressão máxima de injeção.
Dados Geomecânicos	
Coesão (c')	2,0 MPa
Coeficiente de Biot (a)	1,0
Ângulo de Atrito («)	28o
Tensão Total Principal Máxima (S})	22,0 MPa
Tensão Total Principal Mínima (S3)	15,0 MPa
Com base nos resultados obtidos pelas Equações 15 e 16 utilizando os dados apresentados na Tabela 2, o valor da pressão máxima de injeção para este caso é de 14,5MPa.
2.3	Compactação de Reservatórios
Os exemplos provavelmente mais conhecidos dos efeitos mecânicos sobre o comportamento de reservatórios de petróleo estão relacionados à compactação de reservatórios e à subsidência associada (Fjar et al., 2008). A maioria dos problemas induzidos pela compactação de reservatórios está associada com colapso de revestimentos de poços perfurados. Em contrapartida, é conhecido que a compactação pode ser um importante mecanismo de produção em reservatórios de petróleo com baixa rigidez (Nagel, 2001).
Os termos subsidência e compactação possuem definições diferentes: compactação diz respeito à redução de espessura numa dada formação (diminuição de volume devido à compressão exercida sobre a rocha) e a subsidência se refere a uma diminuição da cota de superfície. O fenômeno de subsidência ocorre sobre áreas muito maiores que a área no entorno do reservatório que pode sofrer com a compactação. A diferença entre compactação e subsidência num dado ponto é determinada basicamente pela profundidade e propriedades mecânicas das rochas adjacentes ao reservatório (Figura 4).
Figura 4: Representação esquemática dos fenômenos de compactação (no nível do reservatório) e subsidência (na superfície).
A redução da pressão de poros leva a mudanças não somente nas tensões efetivas atuantes, mas também nas tensões totais presentes nas rochas reservatório e adjacentes. Tais alterações influem diretamente nos fenômenos de compactação e subsidência, além de poderem impactar significativamente no comportamento relacionado ao fluxo de fluidos no reservatório.
A maioria dos reservatórios de petróleo e gás, durante sua vida produtiva, deve sofrer apenas um pequeno grau de compactação, e a subsidência correspondente na superfície tenderá a ser insignificante. De modo a ocorrer um considerável grau de subsidência, uma ou várias das seguintes condições devem estar presentes (Fjar et al.,
2008):
•	A queda de pressão do reservatório deve ser significativa. A manutenção da pressão, através da injeção de água por exemplo, tende a reduzir a magnitude do fenômeno de compactação.
•	A rocha reservatório deve ser altamente compressível. Em rochas pouco rígidas o efeito da compactação passa a ter maior relevância.
•	O reservatório deve ter uma espessura considerável. No entanto, toda a zona de depleção (área com decréscimo de pressão) deve ser considerada.
•	Para que a subsidência ocorra, a compactação do reservatório deve ser significativa, e as características das rochas adjacentes complacentes com a disse
minação do efeito. O grau de proteção da rocha capeadora depende da profundidade e da geometria do reservatório e do contraste nas propriedades mecânicas entre o reservatório e rochas adjacentes.
Diversos autores relatam a importância do entendimento do fenômeno de compactação e subsidência de reservatórios durante a explotação de um campo de petróleo (e.g. (Zimmerman, 2000), (Bruno, 2002), (Holt et al., 2004) e (Schutjens et al., 2004)). Em especial, mais recentemente, (Ferreira, 2014) fez uma ampla revisão sobre casos de subsidência ao redor do mundo e comparou alguns modelos analíticos com soluções numéricas via o software comercial FLAC.
Na próxima seção serão apresentados cálculos determinísticos da compactação e subsidência de um reservatório de petróleo depletado. As equações apresentadas são baseadas na formulação descrita por (Geertsma, 1966). A importância da quantificação de incertezas para problemas associados à compactação e subsidência de reservatórios de petróleo será discutida na seção 4.3.
2.3.1	Perfil de Deslocamento Vertical
A dedução do campo de deslocamento vertical apresentada por (Geertsma, 1966) considera a geometria do reservatório em forma de disco, conforme ilustrado na Figura
5.
O perfil de deslocamento vertical é definido por:
uz (0, z) = -
CmhR
C(Z-1)
~	2-11/2
2	|_[1+C2(Z -1)2]:
(3 - 4u)C(Z+1) [1+C2(Z+1)2]1/2
2CZ
+ [1+C2(Z+1)2]3/2
+(3 - 4v+z)
Ap
(17)
onde: v é o coeficiente de Poisson; hR é a espessura do reservatório; Z = z/c; C = c/R e e = -1 para z &gt; c, e e = +1 para z &amp;lt;c (ver Figura 5). A constante elástica cm é definida por:
c
m
a(1- 2u)
2G(1-u)
(18)
sendo a o coeficiente de Biot e G o módulo de cisalhamento.
Figura 5: Geometria para determinação do campo de deslocamentos no entorno de um reservatório em forma de disco (Ferreira, 2014).
2.3.2	Exemplo de Aplicação
Um exemplo de aplicação utilizando as propriedades descritas na Tabela 3 é apresentado para ilustrar a utilização da Equação 17, a partir da qual o valor da compactação (deslocamento do topo do reservatório) máxima calculada foi igual a 1,40 metros.
Tabela 3: Dados de entrada necessários para o cálculo do perfil de deslocamento vertical num reservatório depletado.
Dados Geomecânicos	
Módulo de cisalhamento (G) Coeficiente de Biot (a) Coeficiente de Poisson (v)	1500 kgf/cm2 1,0 0,286
Propriedades do Reservatório	
Espessura (hR) Topo (c) Raio (R) Profundidade total para análise (z) Delta de pressão (depleção) (dp)	100 m 2900 m 1000 m 4000 m 133,5 kgf/cm2
	
A Figura 6 apresenta o perfil de deslocamento vertical ao longo da profundidade. No caso das propriedades utilizadas nota-se que o topo do reservatório desce e a base sobe. Esse efeito de arco é apresentado e discutido por (Fjar et al., 2008) e (Ferreira, 2014) e pode ser fisicamente explicado pelo contraste de rigidez entre o reservatório e as rochas adjacentes. A espessura do reservatório sofre uma redução de quase 3 metros enquanto que a subsidência fica na ordem de 20 centímetros.
Mesmo com as simplificações associadas ao modelo analítico, tem-se a resposta do modelo dependente dos parâmetros de entrada, neste caso, propriedades geomecâni-cas da rocha reservatório. A inferência de novos dados amostrados para o problema de compactação de reservatórios, ou seja, informações sobre parâmetros são atualizadas com a aquisição de novos dados, e o respectivo impacto na quantificação de incertezas será discutida na seção 4.3.
Figura 6: Perfil de deslocamento vertical ao longo da profundidade analisada. O reservatório se encontra a 2900 metros de profundidade
2.4	Injeção de CO2
O processo de injeção de CO2 em reservatórios de petróleo é um dos processos de fluxo de gases miscíveis mais conhecidos na engenharia de petróleo. Tal método é normalmente usado para a recuperação secundária de óleo em reservatórios depletados pela produção primária e, possivelmente, onde a injeção de água não tem sido efetiva (Holm, 1959). No processo WAG (water al'erna'ing gas), a água é injetada no reservatório até que se restaure o nível de pressão original e, em seguida, o gás (no caso CO2) é introduzido no reservatório através dos mesmos poços injetores. À medida em que o CO2 flui no reservatório uma zona de CO2 e hidrocarbonetos leves miscíveis formam uma frente de saturação solúvel ao óleo, tornando o óleo menos viscoso e, consequentemente, aumentando sua mobilidade em direção aos poços produtores. A produção é proveniente de um banco de petróleo que se forma à jusante da frente miscível (Minssieux, 1994).
Dependendo das condições dos fluidos no reservatório, o deslocamento do óleo pela injeção de CO2 pode ser miscível ou imiscível. As vantagens da injeção imiscível é que a pressão requerida não é grande, solventes não são caros e o óleo ainda assim pode ser recuperado. No processo miscível, a pressão tem que ser superior à pressão mínima de miscibilidade (PMM) que depende da temperatura, da pressão, da pureza do solvente e do peso molecular dos componentes do óleo bruto. Apesar de o CO2 supercrítico ter uma densidade equivalente a de líquido, sua viscosidade é como a de um gás em condições de reservatório e, por isso, é um solvente bastante eficiente (Holm, 1976). A Figura 7 apresenta uma representação do processo WAG.
O método WAG de recuperação de petróleo apresenta as seguintes características físico-químicas ((Holm, 1976), (Minssieux, 1994), (Pegoraro, 2012)):
•	Deslocamento miscível. O solvente extrai componentes do óleo até que sua frente torna-se miscível a este. Essa miscibilidade pode ocorrer inclusive com o óleo residual, que se torna móvel.
•	Inchamento da fase oleosa devido à solubilização do gás, que pode inclusive retardar a chegada (break'hrough) de gás. Este mecanismo é mais atuante em óleos subsaturados de gás.
•	Efeito de extração (stripping) de componentes leves do óleo pelo solvente injetado. A importância do mecanismo de stripping pode ser avaliada experimentalmente através de análises cromatográficas do gás eluído.
•	Redução da viscosidade do óleo, ajudando a aproximar as mobilidades do gás e do óleo.
•	Redução da permeabilidade relativa ao gás devido ao trapeamento desta fase pela água. Este mecanismo tem grande impacto no desempenho do processo. Mesmo quando não há miscibilidade do solvente na fase oleica, este mecanismo permite que se recupere óleo devido à redistribuição de fluidos no meio poroso e ao subsequente arraste deste óleo deslocado pela água. Para que se possa tirar vantagem dos fenômenos de histerese, os bancos de solvente e água devem ter tamanho suficiente para causar variações significativas de saturação durante os ciclos (Pegoraro, 2012).
•	Alteração da permeabilidade absoluta e da resistência da rocha por efeitos de dissolução. Especificamente, no caso do CO2 em contato com água injetado em rochas reativas (como por exemplo, rochas carbonáticas), existe um mecanismo adicional que é a alteração da permeabilidade da rocha. O CO2 se dissolve na água e gera ácido carbônico (H2CO3), capaz de atacar os componentes presentes na rocha carbonática (calcita e dolomita). A partir de dados experimentais em escala de laboratório, foram encontrados aumentos de permeabilidade de até três vezes o valor inicial após o escoamento de cerca de oito volumes porosos de água carbonatada (Holm, 1959). No campo, este aumento de permeabilidade seria mais notado nas proximidades dos poços in-jetores, pelo menos no início da injeção e caso nenhuma medida que impedisse o contato do CO2 com a água do ciclo anterior fosse tomada. Reduções da resistência de rochas carbonáticas por efeitos da dissolução (water weakning) foram discutidos por Zhou et al. (2009).
Poço Injetor
Ciclo de Injeção de CO2
CO2 e Óleo
Poço Injetor
2.4.1	Cálculo da Frente de Saturação de CO2
O objetivo desta seção é apresentar um cálculo simplificado da frente de saturação de CO2, reproduzir um possível impacto da dissolução (causada por essa frente) nas propriedades mecânicas da rocha reservatório e introduzir o conceito de modelagem acoplada hidromecânica e química, ou seja, impactos mecânicos e químicos na solução de fluxo em meios porosos. Obviamente, uma modelagem numérica possui um maior grau de refinamento para tratar tais impactos. Aqui, um cálculo simplificado, considerando diferentes soluções analíticas será apresentado.
Para encontrar a posição da frente de saturação, Nordbotten et al. (2005) impõe as seguintes hipóteses: o balanço do volume; a influência da gravidade (i.e. a pluma de CO2 viaja preferencialmente na parte superior do reservatório) e a minimização da energia no poço. A pressão de injeção é aplicada ao longo de toda a espessura do reservatório e as propriedades dos fluidos assumem valores médios. As propriedades médias são definidas como uma ponderação linear entre as propriedades das duas fases (CO2 e água, neste caso). Nordbotten et al. (2005) propõem uma solução em função da mobilidade definida como a razão entre a permeabilidade relativa e a viscosidade. Para o caso de uma interface abrupta, onde ambos os lados são completamente saturados com o fluido correspondente, o valor da permeabilidade relativa é 1 e a mobilidade torna-se o inverso da viscosidade da fase correspondente. Tais viscosidades são assumidas constantes. Com base nestas hipóteses, a posição da frente de saturação é definida por:
W.V (t)
a (t) =
WI-A+(B-b^,,+(B-b,)
(19)
onde: Ac define a mobilidade do gás; Aw a mobilidade da água; V(t) o volume total injetado; p define o valor da porosidade; B é a espessura do reservatório e b, e a definem a posição da frente de saturação de CO2 (indicado na Figura 8).
Integrando a equação de fluxo e assumindo propriedades dos fluidos constantes em toda a espessura do reservatório, Nordbotten et al. (2005) definem a seguinte expressão para acréscimo da pressão junto ao poço injetor:
' kt
__ Qp^mix ln
p	4nkBpcO2
+0.80907
(20)

onde, k define a permeabilidade média, Q0 é a vazão de injeção, pmix define a viscosidade média do fluido na frente de saturação (usado aqui como a média entre as viscosidades da água (uw) e do CO2 (uCO2)) e ct é a compressibilidade total do sistema.
Figura 8: Aproximação por camadas utilizada para representar a frente de saturação de CO2 (Nordbotten et al., 2005).
Conforme discutido anteriormente, a injeção de CO2 pode causar um processo de dissolução da rocha reservatório. Isto ocorrendo, propriedades mecânicas de resistência da rocha tendem a diminuir de valor (dano químico) e propriedades de fluxo, tais como porosidade e permeabilidade tendem a ter seus valores aumentados.
Neste trabalho, uma solução simplificada foi sugerida para reprodução do efeito de dano químico causado pelo processo de dissolução.
Uma lei linear de aumento da porosidade em função do tempo de injeção pode ser definida como:
=^t _1 +t / dq	(21)
Sendo a variação da porosidade função do tempo de injeção (t) e de uma variável de dano químico (dq).
O módulo de elasticidade da rocha (E) será reduzido em função do processo de dissolução seguindo a seguinte expressão:
Et = A$	(22)
onde A e B são parâmetros de ajuste da curva de dano químico.
A próxima seção descreve um exemplo de aplicação para o cálculo da elevação da superfície (Equação 17) em função do acréscimo de pressão exercido pela injeção de CO2 (Equação 20) em um reservatório considerando o processo de dissolução (Equações 21 e 22). Tal exemplo reproduz o efeito do acoplamento hidro-químico e mecânico de forma simplificada.
2.4.2	Exemplo de Aplicação
Um exemplo de aplicação utilizando as propriedades descritas na Tabela 4 é apresentado para ilustrar a utilização das Equações 17 a 22 sob efeito do processo de dissolução. O valor da elevação máxima da superfície calculada foi igual a 31 centímetros sem considerar o efeito químico e igual a 50 centímetros considerando aumento da porosidade e consequente redução no módulo de elasticidade. O tempo total de simulação foi igual a 3 anos.
A Figura 9 apresenta a frente de saturação de CO2 ao final de 3 anos de injeção segundo a Equação 19. O gráfico da Figura 10 ilustra o acréscimo de pressão ao longo do tempo em função da injeção de CO2 à vazão constante. Finalmente, a Figura 11 mostra o efeito do dano químico (função do processo de dissolução) no cálculo do deslocamento vertical da superfície.
É importante ressaltar que os cálculos desenvolvidos nesta seção possuem diversas limitações, mas o efeito qualitativo dos resultados é de enorme valia na identificação de casos em que uma modelagem numérica mais rigorosa se faz necessária. Em suma, esta solução simplificada mostra a importância do entendimento de fenômenos acoplados.
Tabela 4: Dados de entrada necessários para o cálculo da elevação da superfície sobre um reservatório pressurizado.
Dados Geomecânicos
Módulo de elasticidade inicial (E) Coeficiente de Biot (a) Coeficiente de Poisson (v)
Propriedades do Reservatório
Espessura (B)
Topo (c)
Raio (R) Porosidade (p)
1,0 GPa
1,0
0,30
100 m
2900 m
10000 m
0,10
Permeabilidade média (k) Compressibilidade total (ct)	1e-14 m2 130.10e-6 (kgf/cm2)'1
Parâmetros da Lei de Dano Químico	
dq	30000
A	0,1
B	-1,0
Propriedades dos fluidos	
Densidade do CO2 (pCO2)	714 kg/cm3
Viscosidade do CO2	0,0000577 Pa.s
Viscosidade da água (uw)	0,000795 Pa.s
Demais dados	
Tempo de análise (t)	3 anos
Vazão de injeção (Q0)	7,0 kg/s
Figura 9: Frente de saturação de CO2 ao final de 3 anos de injeção.
A consequência da consideração de uma abordagem híbrida (que utiliza variáveis aleatórias e epistêmicas - ver seção 3.1) para análise dos efeitos da injeção de CO2 e o respectivo impacto na quantificação de incertezas serão discutida na seção 4.4.
Cl. o
&lt;Z&gt;
CL&gt;
CU
3.2
4
3.8
3.6
3.4
3
x 106
4.2
2.8
0	200	400	600	800	1000	1200	1400	1600	1800 2000
Tempo (dias)
Figura 10: Acréscimo de pressão nas proximidades do poço injetor em função do processo de injeção de CO2 a vazão constante.
Figura 11: Deslocamento vertical na superfície: sem considerar efeito de dissolução e consequente enfraquecimento da rocha reservatório (azul) e considerando tais efeitos (verde).
3.	QUANTIFICAÇÃO DE INCERTEZAS
Uma representação adequada da quantificação de incertezas é parte essencial de qualquer projeto de engenharia. Especificamente, uma análise que se destina a fornecer informações sobre o comportamento de um sistema deve prover uma avaliação da incerteza associada aos resultados. Sem tal estimativa, perspectivas traçadas a partir da análise e decisões tomadas com base nos resultados são questionáveis.
Um estudo típico de quantificação de incertezas começa com a definição de um processo de quantificação de incertezas, que é a criação de um plano detalhado de ações para determinada aplicação. Um exemplo do processo de quantificação de incertezas para modelos multifísicos de grande escala, como os modelos relacionados à geomecâ-nica de reservatórios, pode ser constituído das seguintes etapas (Lin, 2012):
•	Definição do problema: principais objetivos da quantificação de incertezas; modelo a ser empregado; hipóteses iniciais; parâmetros de entrada do modelo, etc.
•	Modelo de verificação e teste: impactos associados a erros numéricos, existência de soluções analíticas ou semi-analíticas para validação.
•	Identificação dos parâmetros de incerteza: definição dos parâmetros e respectivos intervalos de incerteza.
•	Análise dos dados observados: definir distribuições iniciais para os parâmetros de incerteza.
•	Triagem sobre os parâmetros de incerteza: identificar os principais fatores de incerteza da resposta do modelo, ou seja, realizar uma análise de sensibilidade global sobre tais parâmetros.
•	Geração de um modelo substituto: no caso em que o modelo principal seja computanionalmente caro. Construir uma superfície de resposta ou qualquer
outro modelo substituto para acelerar a quantificação de incerteza e análise de sensibilidade.
• Quantificação de incertezas: análise de risco, calibração/validação do sistema, avaliação de previsibilidade.
3.1	Tipos de Incerteza
De uma maneira geral, incerteza é definida como a falta de conhecimento exato, independentemente de qual seja a causa deste desconhecimento (Uusitalo et al., 2015). Cada decisão ou conjunto de decisões está associado a diversos fatores e, portanto, é altamente incerto (Fenton e Neil, 2012). Normalmente, a incerteza está presente em todas as etapas de projetos relacionados à geomecânica de reservatórios, desde a definição dos parâmetros de entrada até a própria definição de hipóteses simplificadoras e modelos constitutivos a serem empregados.
Só muito recentemente, as comunidades científicas começaram a reconhecer a necessidade em se definir tipos de incerteza. Diferentes classificações de incerteza, algumas semelhantes e outras divergentes, podem ser encontradas na literatura (e.g., (Regan et al., 2002), (Skinner et al., 2013) e (Walker et al., 2003)). Os conceitos utilizados nesta tese estão definidos a seguir.
A dupla natureza da incerteza pode ser descrita a partir das seguintes definições: Incerteza Aleatória - é o tipo de incerteza que resulta do fato de que um sistema pode se comportar de forma randômica; também conhecida como: incerteza estocástica, Tipo A ou irredutível; e Incerteza Epistêmica - é tipo de incerteza que resulta da falta de conhecimento sobre um sistema e/ou suas propriedades; também conhecida como: incerteza subjetiva, Tipo B ou redutível.
Exemplo de incerteza aleatória: as condições atmosféricas no momento de um acidente em uma planta industrial poderia ter um efeito significativo sobre o número de sobreviventes, mas é essencialmente aleatória na medida em que a nossa capacidade para prever o futuro é limitada. Exemplo de incerteza epistêmica: o valor apropriado de permeabilidade média para se utilizar em uma célula de simulação de reservatórios, por definição, é um valor único, mas esse valor único "eficaz" nunca pode ser conhecido
quantificação de incertezas
com certeza. Isso mostra a falta de conhecimento sobre a variável apesar do valor da propriedade existir e ser único.
A incerteza pode ser procedente de diversas fontes, tais fontes foram classificadas em seis diferentes classes (Uusitalo et al., 2015):
•	Aleatoriedade inerente ao processo: Apesar de determinado processo ser bem conhecido, não se pode ter certeza do resultado final. Esta fonte de incerteza pode ser definida como sendo intrínseca à natureza do processo e é facilmente quantificada. Neste caso, modelos probabilísticos são eficientes e amplamente empregados.
•	Erro de medição: O erro de medição provoca incerteza sobre o valor da grandeza medida. Tal erro pode ser estimado através de métodos estatísticos, no caso de disponibilidade de dados. A abordagem via métodos probabilísticos é bem sucedida se a extensão do erro de medição puder ser estimada.
•	Erro sistemático: O erro sistemático nos resultados das medições, que pode ocorrer devido a uma tendência no procedimento de amostragem, é mais difi-cil de quantificar. Se o erro sistemático passa despercebido, ele pode ter efeitos cumulativos em modelos construídos sobre bases de dados com tais tipos de erro.
•	Variabilidade intrínseca aos parâmetros de entrada: Toda variável de entrada pode mudar com o tempo e com o espaço. Independente de medições realizadas, sempre existe uma incerteza relacionada a cada variável de entrada de um modelo. Esta fonte de incerteza pode ser quantificada, mas exige certas hipóteses. É necessário estimar intervalos e probabilidades associadas a estas variáveis
•	Incerteza do modelo: Modelos são sempre abstrações de um sistema natural. Algumas variáveis e interações não são consideradas e as formas das funções representam sempre simplificações de processos reais. A ciência pode ter conhecimento insuficiente sobre os processos pertinentes, as formas das funções e os valores de parâmetros. A incerteza do modelo pode ser explicada em modelos probabilísticos de forma semelhante à variabilidade intrínseca dos parâmetros de entrada, com uma análise cuidadosa do range de valores possí-
veis e suas probabilidades. Enquanto a incerteza sobre a estrutura do modelo, ou seja, a incerteza sobre o par causa e efeito, em diversas oportunidades é de difícil quantificação.
• Incerteza baseada em julgamento subjetivo: ocorre devido à interpretação dos dados, especialmente quando os dados são escassos ou propensos a erros. Modelos probabilísticos podem ser ineficientes quando utilizados para tratar essa fonte de incerteza.
As seções a seguir apresentam conceitos básicos de diferentes métodos utilizados na quantificação de incertezas. Todas as técnicas têm características positivas e negativas, e nenhuma metodologia é ideal para todas as situações. A escolha de um determinado processo é função de diferentes fatores relacionados ao objetivo do estudo.
3.2	Metodologias para Quantificação de Incertezas
Metodologias para quantificação de incertezas (também conhecidos como métodos de análise não determinísticos) envolvem o cálculo probabilístico do comportamento da função resposta de um modelo, simulado em diferentes cenários, cujos parâmetros de entrada possuem um intervalo de incerteza. Dito de outra forma, nesses métodos, a incerteza dos parâmetros de entrada é transformada em informações probabilísticas relacionadas às respostas do modelo.
Os métodos de quantificação de incertezas podem ser diferenciados em termos da sua capacidade de lidar com incertezas aleatórias ou epistêmicas. Conforme citado anteriormente, as incertezas associadas aos parâmetros de entrada podem ser caracterizadas como incertezas aleatórias, cujas variabilidades são irredutíveis, pois são inerentes à natureza, ou epistêmicas, que são incertezas redutíveis resultantes da falta de conhecimento sobre determinado parâmetro.
Tradicionalmente, a teoria da probabilidade é utilizada para se caracterizar os dois tipos de incerteza. É amplamente conhecido que a teoria da probabilidade tradicional reproduz corretamente a quantificação de incertezas de variáveis aleatórias. No entanto, há críticas recentes sobre a caracterização probabilística de incertezas epistêmi-cas que afirmam que a teoria da probabilidade tradicional não é capaz de capturar tal
incerteza. A aplicação dos métodos probabilísticos tradicionais em incerteza epistêmica ou subjetiva é muitas vezes conhecida como probabilidade Bayesiana.
A análise probabilística exige que um especialista tenha informações sobre a probabilidade de todos os eventos. Quando esta informação não estiver disponível, a função de distribuição uniforme é frequentemente utilizada, justificada por Laplace pelo Princípio da Razão Insuficiente (Principle of Insufficient Reason (Savage, 1955)). Este princípio pode ser interpretado de forma que para todos os eventos, onde a distribuição de probabilidades não é conhecida, as chances são equiprováveis, ou seja, quando não há uma razão conhecida para se estabelecer probabilidades diferentes, podemos simplesmente supor probabilidades iguais. Isto é, considere uma moeda, se não há razão para acreditar que ela seja assimétrica ou viciada, então é simples presumir que as chances de que caia cara ou coroa sejam iguais a 50%. Muito interessante para variáveis aleatórias, contudo, para variáveis epistêmicas, supor probabilidades iguais pode ser uma hipótese muito forte. Sabe-se também que é muito diferente dizer que uma variável se encontra num intervalo [a,b] e dizer que essa variável tem distribuição equiprovável em [a,b]. Esta estrutura rígida não se faz necessária na utilização de teorias não probabi-lísticas que serão apresentadas na seção 3.4.
Como já dito anteriormente, a teoria da probabilidade tem sido tradicionalmente empregada como estrutura matemática para representar tanto incerteza aleatória quanto incerteza epistêmica. Entretanto, a preocupação é que a definição de uma descrição probabilística completa de uma incerteza epistêmica pode implicar na obtenção de um conhecimento maior do que realmente existe.
Uma suposição adicional feita na teoria da probabilidade clássica é inerente ao axioma da aditividade que nos leva à conclusão de que o conhecimento de um determinado evento implica necessariamente no conhecimento do complementar desse evento, isto é, o conhecimento do valor da probabilidade de ocorrência de um evento pode ser traduzido no conhecimento da probabilidade em que o evento não ocorre. Se um especialista acredita que um sistema pode falhar devido a um determinado componente com uma probabilidade de 0,3, isso significa necessariamente que o especialista acredita que o sistema não falhará devido a esse componente com probabilidade de 0,7?
Embora os pressupostos da aditividade e do Princípio da Razão Insuficiente possam ser apropriados ao se modelar eventos randômicos associados com incertezas alea
tórias, estas restrições são questionáveis quando aplicadas a uma questão de conhecimento. Como consequência dessas preocupações, alguns matemáticos têm investigado uma representação mais geral de incertezas com o objetivo de lidar com situações envolvendo incerteza epistêmica. Exemplos destes tipos de situações: a) Quando há pouca informação sobre os parâmetros de entrada do modelo; b) Quando essa informação é ambígua ou contraditória.
Os métodos para quantificação de incertezas aleatórias se baseiam na representação da incerteza através de uma distribuição de probabilidades. Métodos como o de Monte Carlo (MC), o Hipercubo Latino (LHS), métodos de confiabilidade, além de metodologias de expansão estocástica são estratégias para aproximar ou estimar propriedades dessas distribuições. Como exemplo de métodos para quantificação de incertezas epistêmicas têm-se a teoria de probabilidades imprecisas, a teoria da possibilidade e a teoria da evidência.
3.3	Métodos para Quantificação de Incertezas Aleatórias
A teoria da probabilidade fornece a estrutura matemática tradicionalmente usada para representar a incerteza e baseia-se na atribuição de probabilidades para subconjuntos de um conjunto universal Ç As probabilidades atribuídas representam o montante de probabilidade associada a vários subconjuntos de Ç Formalmente, o espaço de probabilidade é definido por (Ç, Sp, p) onde: (i) Ç é um conjunto que contém tudo o que poderia ocorrer no universo particular em consideração, (ii) Sp é um conjunto de subconjuntos de Ç escolhidos apropriadamente (o-álgebra ); e (iii) p é a função que define a probabilidade dos elementos de Sp (Feller, 1971).
O conjunto Sp possui as seguintes propriedades: (i) sendo K um elemento de Sp (definido por exemplo por y' &amp;lt;y), e, consequentemente, K e Sp, então Kc e Sp, onde Kc é o complementar de K; (ii) se K¡, K2, K3, etc. é uma sequência de elementos de Sp, então U, Ki e Sp e P\i Ki e Sp. Além disso, p possui as seguintes propriedades: (i) se K e Sp, então 0 &amp;lt;p(K) &amp;lt;1, (ii) p(Ç)=1, e (iii) se K}, K2, K3, etc. é uma sequência de conjuntos independentes em Sp, entãop(UíKí) = ^íp(Ki) (Feller, 1971).
Uma das principais propriedades da teoria da probabilidade é conhecida como axioma da aditividade, e definida por (para K e Sp):
p(K) + p(Kc) _ 1	(23)
Ou seja, a probabilidade de um evento ocorrer (p(K)) e a probabilidade da não ocorrência do mesmo evento (p(Kc)) devem somar 1. Assim, quando se define a probabilidade de um evento, a teoria da probabilidade, também define, ou determina, um valor sobre a probabilidade da não ocorrência do mesmo evento. Conforme será apresentado na seção 3.4, condições menos restritivas sobre a probabilidade de eventos estão presentes em outras teorias.
Quando o espaço amostral Ç contém números reais, funções de distribuição acumulada, do inglês, cumulative distribution functions (CDFs), e funções de distribuição acumulada complementar, do inglês, complementary cumulative distribution functions (CCDFs) fornecem resumos visuais das informações contidas no espaço de probabilidades (Ç, Sp, p). Especificamente, CDFs e CCDFs são definidos pelos conjuntos de pontos:
CDF _ {[v, pZ)], veÇ}	(24)
CCDF _ {[v,p(Zv)],veZ}	(25)
Onde,
Z _ {x: x eZ, X &gt; v}	(26)
Previamente foram definidos p(Çvc) como sendo a probabilidade de que um valor menor ou igual a v ocorrerá, e p(Çv) é a probabilidade de que um valor maior do que v vai acontecer. Assim, um gráfico dos pontos presentes na CDF fornece uma representação visual da probabilidade em se ocorrer valores menores iguais a um elemento em Ç, e, analogamente, um gráfico dos pontos presentes na CCDF propicia um representação visual da probabilidade em se ter valores maiores do que determinado elemento em Ç (Figura 12).
O foco principal de muitos, se não a maioria, dos problemas envolvendo probabilidade está em funções definidas para elementos do vetor x de um espaço amostral X que é parte de um espaço de probabilidades (X, Xp, px).
y _ f (x)	(27)
Onde, x = [xy, x2, x3,..., xnx] é o vetor de variáveis de entrada e y = [yy, y2, y3,..., ynx] o vetor de resultados do modelo.
Na prática, a função f pode ser bastante complexa (por exemplo, a solução numérica de um sistema não linear de equações diferenciais parciais, ou o resultado de uma simulação de reservatórios), e a dimensão do vetor x, que define os parâmetros de incerteza do modelo, pode ser elevada. O vetor de resultados y também tende a possuir alta dimensionalidade.
Figura 12: Função distribuição acumulada (preto) e função de distribuição acumulada complementar (vermelho) para a variável K com uma distribuição normal com média zero e desvio padrão 1.
O espaço amostral X constitui o domínio da função f definida na Equação 27. Desta forma, o range de f é definido por:
Y = {y: y = f(x),x e X}	(28)
A incerteza nos valores de y contidos no espaço amostral Y é proveniente do espaço de probabilidade (X, Xp, px), que caracteriza a incerteza em x, e das propriedades
da função f. Desta forma, o espaço (X, Xp, px) e a função f definem o espaço de probabilidade da resposta do modelo (Y, Yp, py). A probabilidade py é definida para um subconjunto K pertencente a Y como sendo:
py (K) = pjf _1(K)1	(29)
Onde:
f_1(K) = {x: xG X,y = f(x)G K}	(30)
A incerteza em y caracterizada pelo espaço (Y, Yp, py) também pode ser apresentada por CDFs e CCDFs. Tais curvas são apresentadas de maneira análoga as Equações 24 e 25:
CDF = {[v,py(Yc)1,vG Y} = {[v,px(f-’[Y; 1)1,vG Y}	(31)
CCDF = {[v,py(Yv)1,vG Y} = {[v,px(f~\Yv])],vg Y}	(32)
Onde,
Yv = {y: y GY, y &gt; v}	(33)
A Equação 29 foi escrita de forma a se comparar a teoria da probabilidade com as teorias não probabilísticas que serão apresentadas na seção 3.4. Na práticapy(K) pode ser definida de diversas maneiras (ver seções 3.3.1, 3.3.2 e 3.3.3) como, por exemplo, uma técnica de amostragem (e.g. Monte Carlo), e pode ser definida pela seguinte relação:
nS
py(K) = L ôE[f (x)1dx(x)dV =	)] /nS	(34)
i=1
Onde: §E[f (x)| =1 se fx) e K, e §E[f (x)| = 0 caso contrário; nS é o número de amostras a serem sorteadas; dX(x) é a função densidade associada ao espaço de probabilidades (X, Xp, px); dV representa o volume diferencial de integração sobre X; e xi (onde i = 1,2,...,nS) um evento aleatório sorteado em X em consistência com a definição do espaço de probabilidades (X, Xp, px).
3.3.1	Técnicas de Amostragem
As técnicas de amostragem geram conjuntos de amostras em função das distribuições de probabilidades das variáveis de incerteza, além de definirem conjuntos correspondentes relacionados a funções de respostas do modelo. Nesta seção serão apresentadas duas técnicas de amostragem bastante utilizadas em problemas de engenharia: o método de Monte Carlo tradicional e o Hipercubo Latino, do inglês Latin Hypercube Sampling (LHS).
No método de Monte Carlo tradicional (Dunn and Shultis, 2011), se expressa a incógnita como o valor esperado de uma variável aleatória A (p = E(A)). Em seguida são gerados valores A}, A2, A3,..., AnS independentes e de forma randômica baseados na função densidade de probabilidade de A (por exemplo, normal, lognormal, uniforme, etc). Posteriormente, estima-se , momentos da distribuição de interesse, por exemplo, calcula-se a média:
1 nS finS = Z	Ai
nSÇ-f
(35)
A principal justificativa para a acurácia do método de Monte Carlo tradicional é baseada nas leis dos grandes números. Para uma variável aleatória A, a lei fraca dos grandes números diz que:
lim = p(\jUnS -p\&lt;e) =1
(36)
Para e &gt; 0. A lei fraca mostra que a chance em se cometer um erro maior do que s vai à zero. A lei forte dos grandes números diz que o erro absoluto | pnS -p\&lt;£ vai, eventualmente, ser menor do que s:
p(lim IZ^nS -E\= 0) =1
(37)
Apesar de ambas as leis informarem que o método de Monte Carlo tradicional produzirá erros pequenos, nenhuma se refere à quantidade de amostras que são necessárias para que isso ocorra. Mas, é possível afirmar que a acurácia da estimativa do método (Equação 35) é independente da dimensão do espaço amostral do problema (Kroese et al., 2013).
O LHS é uma técnica de amostragem estratificada onde os intervalos de cada variável de incerteza são divididos em “nS” segmentos de igual probabilidade (nS define o número de amostras). Os comprimentos relativos dos segmentos são determinados pela natureza da função densidade de probabilidade especificada. Por exemplo, na função de densidade uniforme, os intervalos têm segmentos de igual largura, no caso da função de densidade normal, os intervalos têm pequenos segmentos perto da média e segmentos maiores próximos aos extremos da função.
Para cada uma das variáveis de incerteza, uma amostra é selecionada aleatoriamente considerando tais segmentos de iguais probabilidades. Uma característica do conjunto de amostras resultante é que cada linha e coluna no hipercubo de partições tem exatamente uma amostra. Uma vez que o número total de amostras é exatamente igual ao número de partições utilizado para cada variável, a quantidade de amostras está implicitamente definida.
A Figura 13 apresenta a amostragem LHS em um espaço de duas variáveis (para nS igual a 4). O intervalo de ambos os parâmetros, x1 e x2, é definido por [0, 1]. Além disso, para este exemplo, x1 e x2 têm funções densidade de probabilidade uniformes. Conforme mencionado anteriormente, para a amostragem LHS, o intervalo de cada parâmetro é dividido em “nS caixas” de igual probabilidade. Para parâmetros com distribuição uniforme, os tamanhos das partições são idênticos entre si.
Nota-se que há mais do que um possível arranjo que atendem aos critérios do LHS. No exemplo da Figura 13, cada amostra é retirada aleatoriamente de um único intervalo. O algoritmo para gerar amostras pela técnica LHS é mais complexo do que o apresentado pela descrição acima. Maiores detalhes sobre a descrição e implementação do algoritmo, além de maiores informações sobre o método e sua relação com outras técnicas de amostragem podem ser encontrados na literatura (e.g. (Fang et al., 2005)).
O LHS, em geral, requer um menor número de amostras do que o Monte Carlo tradicional quando se compara a previsão em estatísticas, tais como média, desvio padrão, etc. A Figura 14 apresenta um conjunto de amostras comparando-se o método de Monte Carlo tradicional e o LHS para o simples problema de lançamento de um dado. A técnica LHS se mostrou bem mais eficiente mesmo quando o número de amostra é reduzido.
Vantagens de métodos baseados em amostragem incluem sua aplicação relativamente simples e a sua utilização em diferentes disciplinas científicas. A principal desvantagem destas técnicas é o grande número de avaliações da função necessárias para se gerar estatísticas convergentes, o que pode tornar a análise computacionalmente cara, e inviável para algumas aplicações em engenharia.
1
Figura 13: Exemplo de amostragem por LHS de quatro amostras sob os parâmetros x1 e x2. Os pontos vermelhos indicam a localização das amostras.
Monte Cario - 50 amostras
1	2	3	4	5	6
LHS - 50 amostras
1	2	3	4	5	6
Figura 14: Lançamento de um dado. Amostragens obtidas via método de Monte Carlo tradicional e LHS.
3.3.2	Métodos de Confiabilidade
Métodos de confiabilidade fornecem uma abordagem alternativa para a quantificação de incertezas, e podem ser bem mais baratos computacionalmente do que técnicas de amostragem. Tais métodos são baseados na teoria da probabilidade e calculam estatísticas aproximadas da função resposta, como por exemplo, a média, o desvio padrão e funções de distribuição acumulada (CDFs e CCDFs). Estes métodos são mais eficientes para o cálculo de propriedades estatísticas nos extremos das distribuições (eventos com baixa probabilidade) do que as técnicas de amostragem, pois o número de amostras necessárias, neste caso, pode ser proibitivo (Haldar and Mahadevan, 2000).
Todos os métodos de confiabilidade buscam responder a seguinte indagação: "Dado um conjunto de variáveis de incerteza, x, e uma função resposta, y (definida como escalar por simplificação), qual a probabilidade da função resposta ser menor, ou maior, do que um determinado valor Z?". Em termos matemáticos:
p (g(x) &amp;lt;Z) = Fg (Z)	(38)
Onde Fg(Z) (normalmente definida por uma integral multidimensional) é a CDF da incerteza relacionada a g(x).
Tais métodos transformam as variáveis de incerteza x com função densidade de probabilidade igual a d(xj,x2) (no caso de apenas duas variáveis de incerteza) para um espaço gaussiano com média igual a zero e desvio padrão igual a 1. Além disso, as integrais multidimensionais podem ser aproximadas por funções simples de um único parâmetro, chamado de índice de confiabilidade, definido pela distância euclidiana mínima da origem no espaço transformado até a superfície de resposta (Haldar and Mahadevan, 2000). Este ponto também é conhecido como o ponto mais provável, do inglês mostprobable point (MPP).
Como exemplos de métodos de confiabilidade (Haldar and Mahadevan, 2000) têm-se métodos analíticos Mean Value First Order Second Moment (MVFOSM) e o Mean Value Second Order Second Moment (MVSOSM), além de uma variedade de métodos de busca de MPPs, tais como: AdvancedMean Value (AMV); Two-point Adaptive Nonlinearity Approximation (TANA~); First Order Reliability Method (FORM) e Second Order Reliability Method (SORM).
3.3.3	Expansão Estocástica
O objetivo destas técnicas é caracterizar a resposta de sistemas governados por equações que envolvem coeficientes estocásticos. O desenvolvimento de metodologias associadas à expansao estocástica se espelha no método dos elementos finitos, pois utilizam noções de projeção, ortogonalidade e convergência (Ghanem and Red-Horse, 1999).
Métodos de expansão estocástica incluem a expansão em caos polinomial, do inglês Polynomial chaos expansión (PCE) (Maitre and Knio, 2o1o), que emprega uma família de polinómios definidos em função das distribuições de probabilidades das variáveis de entrada. Por exemplo, se uma solução pode ser definida como:
u (x, t, K ) =	(x, t	(K)	(39)
i=o
Onde a função ui é a parcela determinísca e a parcela estocástica da função. A depender da função densidade de probabilidades de K, (normal, uniforme, exponencial ou beta) polinónimos definem y¡ (Hermite, Legendre, Laguerre e Jacobi, respectivamente) (Maitre and Knio, 2o1o). Sendo possível o cálculo de u(x,t,K), se obtém propriedades tais como média, desvio padrão, etc. A idéia desse método se resume ao truncamento da série definindo:
P
u (x, t, K)« ¿ut (x, t )^t (K)	(4o)
i=o
Sendo P o grau do polinómio.
Em suma, PCE utiliza as simulações como “caixas pretas” e calcula os coeficientes do polinómio baseada nas respostas (determinísticas) de tais simulaçoes. Para o cálculo dos coeficientes diferentes abordagens estão disponíveis na literatura, como por exemplo, projeção espectral e regressão linear (Xiu, 2o1o).
Outro método de expansão estocástica é conhecido como colocação estocástica, do inglês stochastic collocation (SC), que é similar a PCE. A principal diferença está no fato de que enquanto a PCE calcula coeficientes para uma base polinomial conhecida o SC interpola funções de Lagrange para coeficientes conhecidos (Xiu, 2o1o).
Ambos os métodos proporcionam cálculos analíticos de propriedades tais como média e desvio padrão; no entanto, as funções CDF e CCDF são avaliadas por amostragem sobre a expansão.
3.4	Métodos para Quantificação de Incertezas Epistêmicas
Sempre que não seja possível caracterizar a incerteza com uma medida precisa, tal como uma probabilidade exata, é razoável considerar uma medida de probabilidade definida por intervalos ou conjuntos. Esta caracterização, de uma medida de probabilidade como um intervalo ou um conjunto, tem três implicações importantes: (a) não é necessário se ter uma medição precisa de determinada propriedade através de um experimento, se tal experimento não é realista ou inviável de se realizar; (b) o Princípio da Razão Insuficiente não é imposto, nem nenhuma estrutura de distribuição de variáveis de entrada. Probabilidades a priori podem ser estabelecidas para eventos em conjunto, sem ter que recorrer a suposições sobre as probabilidades dos eventos individuais, considerando total desconhecimento sob tais eventos; (c) o axioma da aditividade não é imposto, ou seja, as medidas de probabilidades de conjuntos complementares não tem que somar 1. Quando isto ocorre, trata-se da representação probabilística tradicional.
Existem três principais teorias (ver seções 3.4.1, 3.4.2 e 3.4.3) a partir da qual a representação de incertezas por intervalos tem sido abordada, são elas: teoria das probabilidades imprecisas (Aven et al., 2013); teoria da possibilidade (Aven et al., 2013) e a teoria da evidência, também conhecida por Dempster-Shafer theory ((Dempster, 1967), (Shafer, 1976); (Yager, 1986)).
Ter diferentes possíveis teorias para se caracterizar a incerteza introduz uma dúvida quanto à escolha do método a ser aplicado. Enquanto isto ainda está aberto a discussões, esta decisão é simplificada pelo nível de desenvolvimento das teorias e sua utilização em aplicações práticas. Esta tese apresenta conceitos básicos relativos à teoria das probabilidades imprecisas e teoria da possibilidade, mas detalha os conceitos associados à teoria da evidência para quantificação de incertezas. A motivação para a seleção da teoria da evidência pode ser caracterizada pelas seguintes razões: (a) o grau relativamente elevado de desenvolvimento teórico; (b) a fácil correlação entre a teoria da evidência e a teoria da probabilidade; (c) o grande número de aplicações da teoria em
problemas da engenharia nos últimos dez anos; (d) a versatilidade da teoria para representar e combinar diferentes tipos de evidências obtidas de várias fontes.
3.4.1	Teoria das Probabilidades Imprecisas
A teoria das probabilidades imprecisas (também conhecida como análise de intervalos) generaliza a teoria da probabilidade clássica, no sentido de que as incertezas sobre os eventos são quantificadas por intervalos (Jaulin, 2001). Para um evento K, onde a teoria da probabilidade clássica requer um valor p(K), a teoria das probabilidades imprecisas define um intervalo [pmin(K), pmáx(K)], com 0 &amp;lt;pmin(K) &amp;lt;pmáx(K) &amp;lt;1, onde: pmin(K) é chamada de probabilidade mínima para o evento K, pmáx(K) a probabilidade máxima para um evento K, e Ap = pmáx(K) - pmin(K) define a imprecisão para o evento K.
Há várias interpretações possíveis para a definição de probabilidades clássicas, e o mesmo vale para probabilidades imprecisas. Por exemplo, se o valor de uma probabilidade exata (p(K)) não é possível de ser definida, por não ser precisamente sabida, pode-se tentar determinar o intervalo em que tal valor está presente [pmin(K), pmáx(K)].
Desta forma, seguindo a mesma notação da seção anterior, a incerteza em cada elemento de xi pertencente a x seria representada por um intervalo, e o objetivo da análise seria construir o menor intervalo que contenha os possíveis valores da função f(x) (Kearfott and Kreinovich, 1996).
Ao contrário de aplicações baseadas nas teorias da probabilidade, possibilidade e evidência, a teoria de probabilidades imprecisas não busca inferir uma estrutura de incerteza sobre a função resposta (f(x)) baseada na incerteza assumida para x. Por essa razão, a análise de intervalos provê uma forma diferente da representação da incerteza quando comparada com as demais teorias. Conforme será apresentado nas seções seguintes, pode-se comparar a teoria das probabilidades imprecisas a uma degeneração da representação da incerteza pelas teorias da possiblidade e evidência (Helton et al., 2004). Pela primeira seria definir a função de distribuição de possibilidades igual a 1 e pela segunda seria a definição das basicprobability assignments (BPAs) iguais a 1 (ver seções 3.4.2 e 3.4.3).
3.4.2	Teoria da Possibilidade
A teoria da possibilidade também fornece uma alternativa à teoria da probabilidade para representação de incertezas. Como na teoria da evidência (3.4.3), a teoria da possibilidade define duas medidas de incerteza (neste caso essas funções são chamadas necessity e possibility) para cada subconjunto do espaço amostral. Estas funções estão associadas aos limites inferior e superior da caracterização da incerteza. Enquanto a teoria da evidência está bastante relacionada à teoria da probabilidade, a teoria da possibilidade segue uma abordagem mais relacionada à lógica fuzzy (Aven et al., 2013).
No geral, a teoria da possibilidade e a teoria da evidência fornecem, conceitual-mente, diferentes representações da incerteza associada a um modelo. Entretanto, pode se dizer que tais teorias convergem quando os conjuntos de dados relacionados aos parâmetros de entrada são inclusivos (ver seção 3.5) (Helton et al., 2004).
Maiores informações relacionadas à teoria da possibilidade podem ser encontradas na literatura (e.g. (Aven et al., 2013), (Jaulin, 2001), (Kearfott and Kreinovich, 1996), (Helton et al., 2004)).
3.4.3	Teoria da Evidência
A primeira referência sobre a teoria da evidência foi feita por Shafer (Shafer, 1976) como uma expansão da abordagem definida por Dempster (Dempster, 1967), por isso é também conhecida por Dempster-Shafer theory. Num espaço finito discreto, a teoria da evidência pode ser interpretada como a generalização da teoria da probabilidade onde probabilidades são quantificadas em conjuntos e não em valores singulares. Na teoria da probabilidade tradicional, uma evidência está associada a um único evento, enquanto que na teoria da evidência, tal informação pode estar associada a múltiplos eventos, ou seja, a um conjunto de eventos. Como resultado, a teoria da evidência é apresentada com um nível de abstração maior, isto é, sem a necessidade de se inferir uma função densidade de probabilidade a priori para variáveis de entrada. Quando as evidências são suficientes para se estimar probabilidades a eventos únicos, o modelo da teoria da evidência recai na teoria da probailidade tradicional (Helton et al., 2006).
Uma das mais importantes características da teoria da evidência é que o modelo é designado a tratar diferentes níveis de acurácia de informação (isto é, quantidade de dados disponíveis) e nenhuma outra hipótese se faz necessária para representá-la. A
teoria da evidência também permite uma representação direta da incerteza da resposta do modelo. Por exemplo, se a variável de entrada é imprecisa, ela pode ser caracterizada por um conjunto ou intervalo de valores. Consequentemente, a resposta também será fornecida em conjuntos.
Apresentando via formalismo matemático, a aplicação da teoria a evidência envolve a definição de um espaço de evidências (Ç, Se, m), onde: (i) Ç é um conjunto que contém tudo o que poderia ocorrer no universo particular em consideração, (ii) Se é um conjunto de subconjuntos de Ç, e (iii) m é uma função definida em Ç (Helton et al., 2004):
&gt; 0
0
m(K) =&amp;lt;
if K e Se if K cÇ Ke Se
(41)
L m(K) = 1
KeSe
(42)
Na terminologia da teoria da evidência tem-se: (i) Çé o espaço amostral, (ii) Se é o conjunto de elementos focais de Ç e m, e (iii) m(K) é chamada de basic probability assignment (BPA) associada ao subconjunto K de Ç. O BPA definido para um conjunto representa a probabilidade atribuída ao conjunto, mas que não pode ser decomposta em probabilidades de subconjuntos desse conjunto. Os elementos focais são aqueles conjuntos que possuem BPAs diferentes de zero.
O espaço amostral Ç desempenha o mesmo papel em ambas as teorias (probabilidade e evidência). No entanto, o conjunto S (definido como Sp na teoria da probabilidade e Se na teoria da evidência) possuem diferentes significados. Na teoria da probabilidade, S tem propriedades algébricas específicas fundamentais para o desenvolvimento da teoria além de conter todos os subconjuntos de Ç onde são definidas probabilidades. Na teoria da evidência, S não possui propriedades algébricas especiais, ou seja, S não precisa ser uma o-álgebra, e contém os subconjuntos de Çcom BPAs diferentes de zero.
Na teoria da probabilidade, a funçãop define as probabilidades dos elementos de S, sendo essas probabilidades utilizadas como medida fundamental do risco. Na teoria da evidência, a função m não é uma medida fundamental de risco, ao invés disso, duas funções, denomindadas belief eplausibility são obtidas a partir de m.
Bel(K) =	m(A)	(43)
AcK
Pl(K) = 1 m(A)	(44)
AnK/0
Num nível abstrato, a função belief é a medida de quanto uma informação é verdadeira e plausibility é a medida de quanto uma informação pode ser verdadeira. Na terminologia matemática, Bel(K) representa o menor valor de probabilidade associada a K e Pl(K) define o máximo valor de probabilidade que pode estar associado a K.
Tais funções satisfazem as seguintes relações:
Bel(K) + Pl(Kc) = 1	(45)
Bel(K)+Bel(Kc) &amp;lt;1	(46)
Pl(K)+Pl(Kc) &gt; 1	(47)
A função belief é capaz de incorporar a falta de certeza sobre determinado evento visto que o somatório da ocorrência (Bel(K)) e da não ocorrência (Bel(Kc)) de um evento K pode ser menor que 1. Analogamente, a função plausibility é capaz de incorporar o reconhecimento de alternativas dado que a soma da ocorrência (Pl(K)) e da não ocorrência (Pl(Kc)) de um evento K pode ser maior do que 1. Em contrapartida, a teoria a probabilidade impõe uma estrutura mais rígida pelo requerimento da soma da ocorrência e não ocorrencia do evento ser igual a 1 (Equação 23).
Para tentar esclarecer os conceitos apresentados, um exemplo do cálculo das funções belief e plausibility é apresentado a seguir: quatro fontes definem diferentes intervalos para uma propriedade A ([22, 26], [20, 22], [30, 35] e [21, 24]), por falta de maiores detalhes, um especialista definiu que as BPAs das fontes são equiprováveis (m(A) = 1/4). Qual a belief e aplausibility de A estar contida no intervalo K = [20, 25]?
Pela Equação 43 tem-se Bel(K) = 2/4 e pela Equação 44 define-se Pl(K) = 3/4. As relações apresentadas pelas Equações 45, 46 e 47 são respeitadas, pois Bel(Kc') = 1/4 e Pl(Kc) = 2/4.
Como descrito anteriormente, CDFs e CCDFs podem ser utilizadas para apresentar informações contidas num espaço de probabilidades (Ç, Sp, p). Similarmente, é
possível definir cumulative belief functions (CBFs), complementary cumulative belief functions (CCBFs), cumulative plausibility functions (CPFs) e complementary cumulative plausibility functions (CCPFs) que podem ser usadas para definir beliefs e plausibilities.
Analogamente ao desenvolvimento apresentado pelas Equações 24 a 32, só que agora sob os conceitos relacionados ao espaço de evidências (£ Se, m), define-se:
CBF = {[v,Bel(ZC)], ve Z}	(48)
CCBF = {[v,Bel(Zv)],ve Z}	(49)
CPF = {[v, Pl(ZC)], v eZ}	(50)
CCPF = {[v, Pl(Zv)], v eZ}	(51)
A incerteza nos valores de y contidos no espaço amostral Y é proveniente do espaço de evidência (X, Xe, mx), que caracteriza a incerteza em x, e das propriedades da função f Desta forma, o espaço (X, Xe, mx) e a função f definem o espaço de probabilidade da resposta do modelo (Y, Ye, my). As funções belief e plausibility são definidas para um subconjunto K pertencente a Y como sendo:
Bely(K) = Be/xLAK)] = £ mx(A)	(52)
A&lt;=f -1(K )
Ply (K) = PlxlfFK)] =	mx (A)	(53)
Anf-1( K )/0
A incerteza em y caracterizada pelo espaço (Y, Ye, my) também pode ser apresentada por CBFs, CCBFs, CPFs e CCPFs. Tais curvas são apresentadas de maneira aná
loga as Equações 24 e 25:
CBF = {[v,BelyF)],ve Y} = {[v,Belx])],ve Y}	(54)
CCBF = {[v,Bely(Yv)],ve Y} = {[v,Belx(f~'[Yv])],ve Y}	(55)
CPF = {[v, Ply (Yvc)], v eY} = {[v, Plx (f~\Y; ])], v e Y}	(56)
CCPF = {[v, Ply (Yv)], v eY} = {[v, Plx (f~'[Yv ])], v eY}	(57)
Uma técnica de amostragem (Monte Carlo, por exemplo) pode ser utilizada no cálculo das CDFs e CCDFs. Tal método também pode ser utilizado para definir as CBFs, CCBFs, CPFs e CCPFs. Nesta tese, foi desenvolvida uma metodologia para o cálculo das curvas associadas à teoria da evidência, apresentada a seguir.
Algoritmo de Otimização:
As funções de belief e plausibility são definidas para as variáveis de entrada x. Para cada variável de entrada são especificados intervalos de incerteza (com seus limites associados) e BPAs. Para se calcular as medidas de belief e plausibility, é necessário calcular o mínimo e o máximo da função resposta em cada combinação de parâmetros (as combinações são funções dos intervalos definidos). Isto implica que uma análise baseada na teoria da evidência exige a implementação de um algoritmo de otimização para se encontrar o máximo e mínimo da função resposta para todas as possíveis combinações consideradas. Maiores detalhes sobre a implementação de métodos típicos de otimização que podem ser utilizados são encontrados na literatura (Helton et al., 2004), (Waltz et al., 2006), (Byrd et al., 2000).
Os intervalos e suas BPAs associadas são, em seguida, propagados para se obter a CBF e CPF. Como mencionado acima, belief é o limite inferior de uma estimativa de probabilidade que é consistente com a evidência e plausibility corresponde ao limite superior em uma estimativa da probabilidade, sendo também consistente com a evidência. As etapas da metodologia são:
• Combinar os intervalos de cada variável de entrada (por exemplo, no caso de duas variáveis com três intervalos definidos para cada, o número total de combinações é igual a nove). O número total de células (combinações) é dado por:
niv
ncell = flnint(i)	(58)
i=1
Onde: ncell = número total de combinações, nint(i) = número de intervalos da variável i, niv = número de variáveis de entrada. A probabilidade associada a cada célula é o produto das BPAs dos intervalos envolvidos.
• Simular as realizações que envolvem todas as combinações possíveis definidas no item anterior. Para cada célula deve-se resolver dois problemas de otimização para se encontrar os limites inferior (Zb) e superior (ub) da função
resposta.
minimize
sujeita:
f (x)
(xj) &amp;lt;x &amp;lt;x)
(ub)	=
maxmize	f (x)
sujeita:	(xj) &amp;lt;x &amp;lt;(xub)
(59)
(60)
(ib) f «m=

Onde: (xj) e (x“b) são os limites que definem os intervalos das variáveis de entrada associados à combinação j.
Para se obter a CPF, o limite inferior, calculado para cada combinação, é plota-do contra o correspondente valor de probabilidade acumulada (ver exemplo - seção 3.6). A CBF é plotada de maneira similar, usando-se os limites superiores de cada combinação.
A próxima seção compara, através de exemplos, a teoria da probabilidade com a teoria da evidência. O objetivo não é definir uma melhor abordagem, mas proporcionar uma representação alternativa da quantificação de incertezas.
3.5	Tipos de Evidência
Para se estabelecer o tipo de análise a ser efetuada deve-se atentar não somente a disponibilidade de dados, mas também a proveniência e confiabilidade dessas informações. Existem dois aspectos críticos relacionados à combinação de evidências obtidas de fontes diferentes. O primeiro diz respeito ao tipo de evidência envolvida e o segundo está pautado em como se tratar evidências conflituosas. A Figura 15 apresenta 4 diferentes tipos de evidência que impactam a escolha de como tais evidências devem ser combinadas, são eles: evidências inclusivas, consistentes, arbitrárias e desconexas (Sentz, 2002).
As evidênicas inclusivas podem ser representadas por subconjuntos contidos em subconjuntos maiores e assim por diante. Pode corresponder a situação onde informações são obtidas com o tempo em intervalos refinados que reduz o espaço de evidências.
A evidência ser consistente significa que ao menos um elemento é comum a todos os subconjuntos do espaço amostral. A evidência arbitrária corresponde à situação onde não existe nenhum elemento comum a todos os subconjuntos, apesar de poder haver subconjuntos com elementos em comum. E, finalmente, tem-se a evidência desconexa que implica que para quaisquer dois subconjuntos do espaço amostral não há elementos em comum. Por exemplo, a acurácia de sensores elétricos com diferentes graus de resolução pode ser definida em diferentes tipos de evidência (Figura 15).
Cada uma dessas possíveis configurações de evidências provenientes de diferentes fontes tem distintas implicações sobre o nível de conflito associado a cada situação. É intuitivo afirmar que no caso de evidências desconexas, todas as fontes de evidências são conflitantes. Com a evidência arbitrária, há algum acordo entre algumas fontes, mas não há consenso sobre qualquer um dos elementos. A evidência consistente implica num acordo entre pelo menos um subconjunto de evidências. A evidência inclusiva representa a situação em que cada subconjunto está incluído num subconjunto maior, ou seja, é garantido que há uma concordância entre evidências, entretanto, há conflito inerente na evidência adicional suportada pelo subconjunto maior.
X
X
\ I
/ / ✓

Figura 15: Tipos de evidência: a) inclusiva; b) consistente; c) arbitrária e d) desconexa. As letras A, B, C, D e E representam diferentes fontes de evidência.
A teoria da probabilidade tradicional além de não conseguir lidar com evidências inclusivas, consistentes, ou arbitrárias, sem recorrer à definição de funções densidades de probabilidades para todos os elementos dos conjuntos, não é capaz de expressar o nível de conflito entre estes conjuntos de evidência. Os exemplos a seguir mostram como a teoria da evidência é mais flexível quando se depara com os cenários apresentados anteriormente. Além disso, na teoria da evidência, existem muitas maneiras de se incorporar conflitos iminentes do processo de combinação de múltiplas fontes de informações.
3.6	Exemplo de Aplicação
Conforme apresentado anteriormente, a teoria da evidência não é totalmente distinta da teoria da probabilidade. Em particular, a definição do espaço de evidência pode ser tratada como uma definição incompleta do espaço de probabilidade. Pode-se dizer então, que a teoria da probabilidade é um caso especial da teoria da evidência, ou seja, a teoria da evidência converge para a teoria da probabilidade quando se tem dados suficientes e confiáveis relacionados aos parâmetros de entrada do modelo.
Seja um modelo definido pela seguinte expressão:
f (a, b) = (a+b)a
(61)
Onde a e b são os parâmetros de incerteza. Tal modelo é bastante simples, mas poderíamos estar lidando com a saída de um simulador de reservatórios ou de uma modelagem geomecânica, ou seja, diversos parâmetros de incerteza e alto grau de não linearidade.
Serão expostos cinco exemplos, chamados de Exemplos 1, 2, 3, 4 e 5, todos envolvendo somente incertezas epistêmicas em a e b. Os ranges de possíveis valores de a e b são definidos por uma ou mais fontes de informação (Tabela 5). Em todos os exemplos, os possíveis valores de a e b estão contidos nos intervalos [0.1, 1.0] e [0.0, 1.0], respectivamente. Nenhuma informação sobre a distribuição de probabilidades dentro dos intervalos definidos para a e b está disponível.
A incerteza em y é derivada das incertezas presentes em a e b. Serão apresentadas quantificações da incerteza em y considerando a teoria da probabilidade e a teoria da evidência. Para a utilização da teoria da probabilidade, é necessária a especificação de
funções densidades de probabilidade para os intervalos das variáveis, enquanto que na teoria da evidência tal informação não é obrigatória.
De acordo com a Tabela 5, para o Exemplo 1 não há qualquer fonte adicional de evidência, ou seja, a única informação disponível diz que a é uma variável epistêmica e seu valor está contido no intervalo [0.1, 1.0]. De forma equivalente, o valor de b é desconhecido dentro do intervalo [0.0, 1.0].
Tabela 5: Intervalos de possíveis valores caracterizando a incerteza epistêmica das variáveis a e b. Cada intervalo é definido por fontes independentes.
Exemplo 1:	Sem evidências
A1 = [0.1, 1.0]
Bi = [0.0, 1.0]
Exemplo 2:	Evidências inclusivas
	A1= [0.5, 0.7] B1 = [0.6, 0.6]	A2 = [0.3, 0.8] B2 = [0.4, 0.85]	A3= [0.1, 1.0] B3= [0.2, 0.9]	B4= [0.0, 1.0]
Exemplo 3:	Evidências consistentes			
	A1= [0.7, 1.0]	A2 = [0.5, 0.8]	A3= [0.1, 1.0]	
	B1 = [0.6, 0.9]	B2 = [0.4, 0.8]	B3= [0.1, 0.7]	B4= [0.0, 1.0]
Exemplo 4:	Evidências arbitrárias			
	A1= [0.5, 1.0]	A2 = [0.2, 0.7]	A3= [0.1, 0.4]	
	B1 = [0.4, 0.6]	B2 = [0.2, 0.4]	B3= [0.6, 0.9]	B4= [0.0, 1.0]
Exemplo 5:	Evidências desconexas			
	A1 = [0.8, 1.0]	A2 = [0.5, 0.7]	A3= [0.1, 0.4]	
	B1 = [0.8, 1.0]	B2 = [0.5, 0.7]	B3= [0.3, 0.4]	B4= [0.0, 0.2]
Um dos principais dilemas associados ao uso da teoria da probabilidade para quantificação de incertezas de variáveis epistêmicas está associado à escolha da função densidade de probabilidade quando não se tem dados suficientes ou tais informações são controversas.
Como resultado da quantificação de incertezas de y sob as condições do Exemplo 1, a Figura 16 além de apresentar três diferentes CDFs ((i) considerando a função densidade de probabilidade uniforme para ambas as variáveis; (ii) considerando distribuições normais com média 0,5 e desvio padrão 0,05 e (iii) distribuição uniforme considerando as BPAs como pesos na amostragem, reproduz a CBF e CPF (Equações 54 e 56, respectivamente) considerando pesos idênticos para cada fonte de evidência (no caso do Exemplo 1 tem-se as seguintes condições: m(A1) = m(B1) = 1).
Figura 16: Quantificação de incertezas em y. A curva verde representa uma CDF com distribuições uniformes para a e b; em rosa uma CDF para a e b com distribuição normal (média 0,5 e desvio padrão 0,05); em azul tem-se uma CDF considerando as BPAs como pesos de uma distribuição uniforme; em vermelho a CBF e em preto a CPF.
Vale ressaltar que nenhuma informação sobre a distribuição dos valores a e b está disponível, desta forma, qualquer inferência extra de conhecimento sobre as variáveis de entrada do modelo pode agregar erros na resposta final. Pode parecer pouco informativa, mas a resposta proveniente da aplicação da teoria da evidência (CBF e CPF) sobre o Exemplo 1 fornece exatamente o que se é sabido sobre o modelo, ou seja, valores mínimos e máximos possíveis oriundos da combinação das variáveis de incerteza. Percebe-se também que diversas CDFs podem ser produzidas a depender da função de densidade de probabilidade utilizada (qualquer função, nesse caso de falta de informação sobre os parâmetros de entrada, tende a ser uma condição restritiva).
A Figura 17 mostra os resultados da quantificação de incertezas para os demais exemplos descritos na Tabela 5.
Exemplo 3
Exemplo 5
1
0	0.5	1	1.5	2	2.5
y=(a+b)a
Figura 17: Diferentes cenários de evidências para quantificação de incertezas em y. A curva verde representa uma CDF com distribuições uniformes para a e b; em rosa uma CDF para a e b com distribuição normal (média 0,5 e desvio padrão 0,05); em azul tem-se uma CDF considerando as BPAs como pesos de uma distribuição uniforme; em vermelho a CBF e em preto a CPF.
Analisando-se a Figura 17 percebe-se que a largura da banda da diferença entre a CBF e a CPF tende a diminuir quando os intervalos de evidência se apresentam reduzidos. Essa largura não é função do tipo de evidência disponível, mas sim da confiança representada pelos intervalos das variáveis de incerteza.
O próximo capítulo apresenta a aplicação da teoria da evidência para quantificação de incertezas associadas a problemas de geomecânica de reservatórios. Os modelos descritos e calculados de forma determinística no Capítulo 2 voltam a ser avaliados em diferentes situações, desde a falta de conhecimento total das variáveis de entrada até a disponibilidade extensiva de dados.
4.	DECISÕES SOB INCERTEZA
A tomada de decisão é, certamente, a tarefa mais importante de um gerente (tomador de decisão) e muitas vezes tende a ser bastante difícil. O domínio de modelos de análise de decisão se situa entre dois casos extremos, a depender do grau de conhecimento sobre o resultado de determinada ação. Um extremo desta escala define os problemas determinísticos enquanto que a extremidade oposta define a incerteza pura. Entre estes dois extremos estão definidos os problemas em que o risco pode ser avaliado (Ben-Haim, 2006).
Para qualquer problema, o grau de certeza varia entre os tomadores de decisão, dependendo do conhecimento que cada um tem sobre o mesmo tema. Conforme discutido anteriormente, a quantificação de incertezas é um instrumento usado para medir a probabilidade de ocorrência de um evento. Quando a probabilidade é usada para expressar o grau de incerteza, o extremo determinístico tem probabilidade igual a 1 (ou zero), enquanto que a outra extremidade (incerteza pura) tem um plano de probabilidades equiprováveis (Golub, 1997).
No que tange a modelos para tomada de decisão, duas funções distintas podem ser definidas. A primeira se refere ao tomador de decisão e a segunda a quem constrói o modelo, conhecido como o especialista. A função do especialista é ajudar o tomador de decisão em seu processo de tomada de decisão. Em algumas oportunidades, o tomador de decisão pode não entender determinado modelo e pode utilizá-lo cegamente ou ainda, rejeitá-lo inteiramente. O especialista pode sentir que o tomador de decisão é pouco sofisticado para apreciar o modelo, enquanto o tomador de decisão pode achar que o especialista vive em um mundo de hipóteses irrealistas e irrelevante linguagem matemática. Essa falta de comunicação pode ser evitada se o gerente trabalha com o especialista para desenvolver, primeiramente, um modelo simples que ofereça uma análise compreensível. Depois que se tenha adquirido confiança neste modelo, detalhes adicionais podem ser incluídos. Este processo exige um investimento de tempo por parte do gestor e sincero interesse por parte do especialista em resolver o problema real, em vez de criar e tentar explicar modelos sofisticados (Goodwin and Wright, 2009).
Na área de geomecânica de reservatórios existe uma tendência em se elaborar modelos cada vez mais sofisticados sem antes avaliar o impacto dos parâmetros por processos mais simples, como por exemplo, soluções semi-analíticas. Este é o objetivo deste capítulo. Apresentar decisões baseadas em soluções simples incorporando a quantificação de incertezas via teoria da evidência.
Em modelos determinísticos, a decisão é tomada através de um resultado especifico e independente. Em contrapartida, em modelos probabilísticos, o tomador de decisão está preocupado não somente com o valor do resultado, mas também com a quantidade de risco que cada decisão está suscetível. A maioria das decisões é tomada em face da incerteza. A quantificação de incertezas entra no processo no intuito de se aproximar do conhecimento pleno sobre determinado processo (van Gigch, 2003).
Os tomadores de decisão muitas vezes enfrentam uma significativa falta de informação sobre determinado processo. Conforme descrito no Capítulo 3, a quantificação de incertezas preenche a lacuna de informação entre o que é conhecido, e que precisa ser conhecido para uma decisão eficiente, além disso, uma dificuldade presente na avaliação da probabilidade surge em casos onde informações são escassas, inconsistentes ou incompletas. Desta forma, uma declaração como: "a probabilidade de uma queda de energia é entre 0,3 e 0,6" é mais natural e realista do que a definição de um valor exato, como "a probabilidade de uma queda de energia é 0,46342". Conforme descrito na seção 3.4.3, com a teoria da evidencia é possível criar diferentes cenários de avaliação da incerteza.
Existem tipos distintos de modelos de decisão que ajudam a analisar diferentes cenários. Dependendo da quantidade e grau de conhecimento disponível, diferentes metodologias podem ser aplicadas:
•	Tomada de decisão sob incerteza pura (ausência de dados de entrada);
•	Tomada de decisão através da disponibilidade de informações, aproximando o problema para um modo determinista (dados de entrada disponíveis);
•	Tomada de decisão com inferência de novos dados de entrada, reduzindo a incerteza ao longo do tempo devido à disponibilização de novos dados;
• Tomada de decisão com abordagem híbrida (variáveis aleatórias e epistêmi-cas nos parâmetros de entrada).
As próximas seções apresentam tais metodologias utilizando a teoria da evidência para quantificação de incertezas.
4.1	Ausência de Dados de Entrada
A ausência de dados de entrada é um problema recorrente associado à geomecâ-nica de reservatórios. Entre os principais motivos tem-se o custo de aquisição e compilação de dados geomecânicos, seja via sísmica, perfis ou testemunhos e a falta de credibilidade, que ainda persiste na indústria, quanto à importância de estudos aprofundados de geomecânica de reservatórios durante as fases de exploração e produção de um campo de petróleo.
O cálculo da pressão de fratura (descrito na Seção 2.1.1) será utilizado como exemplo da importância da quantificação de incertezas para tomada de decisão mesmo quando os dados de entrada são completamente escassos. Ainda, serão realizadas discussões comparativas entre resultados obtidos via teoria da probabilidade e via teoria da evidência. Neste exemplo em questão, foram coletadas opiniões de três especialistas para julgamento dos intervalos de parâmetros a serem utilizados nas análises. Estas opiniões definiram os pesos necessários para utilização da teoria da evidência (ver seção 3.4.3). Os intervalos dos parâmetros foram definidos por propriedades encontradas em reservatórios análogos. Os parâmetros de incerteza são o módulo de elasticidade (E), o coeficiente de Poisson (v), o coeficiente de Biot (a) e o coeficiente de empuxo (Ko). A Tabela 6 descreve as opiniões dos especialistas.
Tabela 6: Intervalos de possíveis valores caracterizando a incerteza das variáveis de entrada. Cada intervalo é definido por opiniões de diferentes especialistas.
Parâmetro	Especialista 1	Especialista 2	Especialista 3
E (GPa)	35-40	35-45	30-40
v	0,2-0,3	0,2-0,3	0,2-0,3
a	0,6-0,8	0,6-0,8	0,5-0,7
Ko	0,7-0,9	0,75-0,8	0,7-0,8
Conforme discutido no Capítulo 3, tais variáveis são consideradas epistêmicas e a teoria da evidência se apresenta como sendo uma abordagem eficiente para tratamento
da incerteza associada. A Figura 18 apresenta a CBF e CPF do cálculo da pressão de fratura utilizando os parâmetros apresentados na Tabela 6.
A estimativa natural de uma quantificação de incertezas associada à indisponibi-lidade de dados de entrada é o uso da teoria da probabilidade. Mas, um dos principais dilemas associados ao uso da teoria da probabilidade para quantificação de incertezas de variáveis epistêmicas está associado à escolha da função densidade de probabilidade. A Figura 18 mostra três CDF's associadas à escolha de uma função densidade de probabilidade uniforme (em azul escuro), normal (em azul claro) e uma distribuição uniforme que considera as BPA's como pesos (em verde) nos respectivos intervalos (ver seção 3.6).
Figura 18: Definição da pressão de fratura. Uso da função densidade de probabilidade uniforme (em azul escuro), normal (em azul claro) e uma distribuição uniforme que considera as BPA ’s como pesos (em verde) nos respectivos intervalos. Via teoria da evidência calcula-se a CBF (em vermelho) e a CPF (em preto).
Um importante tópico a ser avaliado é até que ponto termina a função do especialista e, consequentemente, se inicia a função do gerente para a tomada da decisão. As repostas relativas ao uso da teoria da probabilidade tradicional (CDFs), no caso de total
ausência de dados amplificam a função do especialista devido à necessidade de escolha da função densidade de probabilidade. Neste caso, a tomada de decisão sobre qual a pressão de fratura do reservatório é totalmente influenciada. Ao se utilizar dos conceitos da teoria da evidência (CBFs e CPFs), a informação está de acordo com o que se é sabido sobre os parâmetros de entrada do modelo. No limite, tais curvas convergem para uma única e “verdadeira” curva (CDF) quando se tem dados disponíveis que descrevem completamente as variáveis.
Sendo assim, remete-se ao tomador de decisões a definição do valor a ser escolhido para controle da produção e injeção no reservatório. Com as duas curvas provenientes da teoria da evidência (CBF e CPF), delega-se ao gerente a decisão baseada em seu perfil (otimista ou pessimista), o que é muitas vezes restringido pelo uso da teoria da probabilidade na quantificação de incertezas.
Os otimistas estão certos assim como os pessimistas. Os otimistas vêem uma oportunidade em cada problema enquanto os pessimistas vêem um problema em cada oportunidade. Ambos contribuem para a nossa sociedade. Os otimistas inventaram o avião enquanto os pessimistas inventaram o para quedas. Se o objetivo fosse fraturar o reservatório, um gerente otimista se basearia na CPF (curva em preto da Figura 18), enquanto que o pessimista se basearia na CBF (curva em vermelho da Figura 18). O cerne da questão é que a teoria da evidência é mais flexível ao processo de tomada de decisão.
Nesta mesma linha de raciocínio, o APÊNDICE A apresenta um artigo publicado na Computers &amp;amp; Geotechnics que modela numericamente o fenômeno de reativação de falhas utilizando-se da teoria da evidência para quantificação de incertezas e auxílio à tomada de decisão.
4.2	Dados de Entrada Disponíveis
Um estudo sobre a definição da máxima pressão de injeção, sob o critério de reativação de falhas, num reservatório de petróleo offshore usando a teoria da evidencia para quantificação de incertezas é apresentado no APÊNDICE B. Neste estudo foram utilizadas informações disponíveis (definidas por correlações) em poços injetores e produtores num reservatório de petróleo localizado na Bacia de Campos (Brasil) para definição de um quadro de risco que quantifica a pressão máxima de injeção sob um
critério analítico de reativação de falhas geológicas. Esse artigo foi submetido à JPSE (Journal of Petroleum Science andEngineering) e ainda está sendo avaliado para publicação.
4.3	Inferência de Novos Dados
Conforme descrito no Capítulo 3, um estudo típico de quantificação de incertezas começa com a definição de um processo de quantificação de incertezas, que é a criação de um plano detalhado de ações para determinada aplicação. Inicialmente, define-se o problema (cálculo do deslocamento vertical da superfície sobre um reservatório sujeito à depleção). Em seguida, se verifica impactos associados a erros numéricos (no presente exemplo, a solução analítica apresentada na seção 2.3 será utilizada como modelo). Posteriormente, os parâmetros de incerteza do modelo devem ser identificados e dados observados devem ser avaliados (ver Tabela 7). Depois, se faz uma triagem sobre os parâmetros de incerteza analisando suas respectivas influências sobre a resposta final do modelo (no caso de problemas com diversos parâmetros de incerteza, uma análise de sensibilidade deve ser feita para identificação de quais variáveis efetivamente influenciam a resposta do modelo, no exemplo apresentado nesta seção foram definidos somente três parâmetros de incerteza, sendo desnecessária uma análise de sensibilidade no intuito de reduzir o número de variáveis de incerteza). Finalmente, se quantifica a incerteza (Figura 19).
A Tabela 7 idealiza um possível cenário de disponibilização de informações ao longo do tempo. Inicialmente, não há informações sobre as variáveis mecânicas do reservatório, sendo possível somente uma representação das propriedades de entrada do modelo através de dados observados em reservatórios análogos. Desta forma, a incerteza associada tende a ser significativa.
Tabela 7: Cenário de disponibilização de informações (evidências) ao longo do tempo. O processo de quantificação de incertezas é modificado a cada nova evidência.
T = 0 - Reservatórios Análogos
Parâmetro	T = 0
a	0,6-1,0
v	0,2-0,4
G (kgf/cm2)	500-3000
BPA0	1,0
T = 1 - Poço Perfurado				
Parâmetro	T = o	T = 1		
a	o,6-1,o	o,7-o,8		
V	o,2-o,4	o,32-o,36		
G (kgf/cm2)	5oo-3ooo	25oo-32oo		
BPA1	o,2	o,8		
		T = 2 - Ensaios de Laborat	ório	
Parâmetro	T = o	T = 1	T = 2	
a	o,6-1,o	o,7-o,8	o,95-1,o	
V	o,2-o,4	o,32-o,36	o,35-o,4	
G (kgf/cm2)	5oo-3ooo	25oo-32oo	2ooo-22oo	
BPA2	o,1	o,7	o,2	
		T = 3 - Novo Poço Perfur	ado	
Parâmetro	T = o	T = 1	T = 2	T = 3
a	o,6-1,o	o,7-o,8	o,95-1,o	o,75-o,9
V	o,2-o,4	o,32-o,36	o,35-o,4	o,3-o,35
G (kgf/cm2)	5oo-3ooo	25oo-32oo	2ooo-22oo	26oo-3ooo
BPA3	o,o2	o,4	o,18	o,4
Após o primeiro poço perfurado, têm-se informações disponíveis através de correlações com perfis. Um ponto crucial nesse momento é a definição da forma de se introduzir a nova informação no processo de quantificação de incertezas. Conforme discutido anteriormente, a teoria da evidência é mais flexível, pois é capaz de definir probabilidades a intervalos, diferentemente da exigência da definição do prior pela teoria da probabilidade. Em seguida, ensaios de laboratórios apresentam novos resultados e a decisão fica por conta do especialista de como mensurar a significância de cada informação (definição das BPAs) Finalmente, um novo poço é perfurado e são disponibilizados novos ranges de parâmetros.
Os resultados apresentados na Figura 19 reúnem a variação do processo de quantificação de incertezas via teoria da evidência ao longo da aquisição de novas informações. Percebe-se que o intervalo de incerteza diminui ao longo das análises pela incorporação de novas informações.
No caso da inferência de novos dados, a teoria da evidência proporciona a redução da incerteza associada à resposta do modelo através do encurtamento entre o espaço entre a CPF e CBF, ou seja, a cada nova “evidência” disponível, as mínimas e máximas
probabilidades se aproximam (ver Figura 19). Isso pode não ocorrer somente se as evidências forem desconexas (ver seção 3.5).
Figura 19: Cálculo da subsidência sobre um reservatório depletado. Incorporação de novos dados ao longo do tempo. Uso da função densidade de probabilidade uniforme (curvas em verde), e uma distribuição uniforme que considera as BPA 's como pesos (curvas em azul) nos respectivos intervalos. Via teoria da evidência calcula-se a CBF (em vermelho) e a CPF (em preto).
A flexibilidade da teoria da evidência para inferência de novos dados fica nítida na avaliação dos resultados. Como era de se esperar, novas informações sobre os parâmetros reduzem a faixa de incerteza sobre o valor da subsidência no leito marinho. Inicialmente, com poucas informações (reservatórios análogos), se tem uma ampla faixa de possíveis valores de subsidência (0,05 a 0,9 metros), considerando todas as probabilidades de ocorrência. Após o primeiro poço perfurado, a incerteza sobre resultado diminui (0,05 a 0,7 metros). Com a informação de ensaios de laboratório a tendência se mantém e o valor mínimo e máximo possível de subsidência fica no intervalo de 0,05 a
0,52 metros. E, finalmente, após a perfuração do segundo poço, a incerteza se reduz aos possíveis valores contidos no intervalo entre 0,05 a 0,18 metros.
Vale ressaltar que o uso da teoria da evidência neste caso possibilitou a inferência de novos dados sem a obrigatoriedade de definição de priors. Percebe-se que a curva verde (resposta da teoria da probabilidade considerando distribuição uniforme) se manteve constante pelo fato de que o intervalo que define os valores mínimos e máximos dos parâmetros não se modificou, e, desta forma, a distribuição uniforme não deveria ser alterada. No que tange a discussão de que a informação fornecida pelo especialista deve ser a mais representativa possível para auxiliar a tomada de decisão, a teoria da evidência se mostra bastante flexível.
A próxima seção ilustra uma abordagem híbrida da quantificação de incertezas considerando parâmetros aleatórios e epistêmicos no mesmo processo de quantificação.
4.4	Abordagem Híbrida
A abordagem híbrida trata da análise simultânea de variáveis aleatórias e epis-têmicas, ou seja, no conjunto de variáveis de incerteza de um modelo se encontram parâmetros cuja representação se comporta de forma randômica e parâmetros que resultam da falta de conhecimento sobre um sistema e/ou suas propriedades (ver Capítulo 3).
Para exemplificar tal abordagem, usou-se a formulação descrita na seção 2.4 que calcula o impacto do fenômeno de dissolução da rocha no deslocamento vertical da superfície acima do reservatório pressurizado pela injeção de CO2. A Tabela 8 define os dados de entrada utilizados no modelo.
Inicialmente, sorteia-se uma combinação das variáveis aleatórias e em seguida realiza-se o processo de otimização para definição das probabilidades máximas e mínimas. Este processo foi realizado 50 vezes, ou seja, foram realizadas 50 combinações das variáveis aleatórias e calculados valores máximos e mínimos das probabilidades via teoria da evidência. A Figura 20 ilustra os resultados obtidos.
A abordagem híbrida é uma alternativa ainda pouco explorada na literatura. Sua interpretação permite uma adequação de resultados em função do perfil do tomador de decisões, pois define um range de incerteza associado também aos valores máximos e mínimos de probabilidade. Um gerente otimista, por exemplo, trabalharia com o inter
valo definido pelas curvas de CPFs (em preto), enquanto um gerente pessimista tenderia a tomar suas decisões baseado no range calculado nas CBFs (em vermelho).
Nos estudos relacionados à geomecânica de reservatórios, se faz necessário também o entendimento de métodos numéricos. O próximo capítulo apresenta uma das soluções mais utilizadas na simulação de problemas associados a este tema - a modelagem acoplada.
Tabela 8: Dados de entrada para o cálculo da elevação da superfície sobre um reservatório pressurizado via uma abordagem híbrida para a quantificação de incertezas.
Dados Geomecânicos - considerados epistêmicos
Módulo de elasticidade inicial (E) Coeficiente de Biot (a) Coeficiente de Poisson (v)	1,0 - 2,0 GPa 0,8 -1,0 0,20 - 0,30
Propriedades do Reservatório	
Espessura (B)	100 m
Topo (c)	2900 m
Raio (R)	10000 m
Porosidade (p)	0,10
Permeabilidade média (k)	2 1e-14 m
Compressibilidade total (ct)	130.10e-6 (kgf/cm2)-
Parâmetros da Lei de Dano Químico - considerados aleatórios	
dq	25000 - 30000
A	0,1 - 0,2
B	-0,8 - -1,0
Propriedades dos fluidos	
Densidade do CO2 (/&gt;,)	714 kg/'cm3
Viscosidade do CO2 (wCO2)	0,0000577 Pa.s
Viscosidade da água (uw)	0,000795 Pa.s
Demais dados	
Tempo de análise (t)	3 anos
Vazão de injeção (Q0)	7,0 kg/s
1 i----------------------1---------------------1--------------------1---------------|||||||i|||||||i ||||||||||||||i||| llllllllllll-r
0.9 -
0.8
Elevação Vertical da Superfície (metros)
Figura 20: Cálculo da elevação da superfície sobre um reservatório pressurizado. Resultado da quantificação de incertezas via teoria da evidência, CBFs em vermelho e CPFs em preto.
5.	MODELAGEM ACOPLADA
A simulação do comportamento de um reservatório é feita com base na conservação de massa dos fluidos e espécies contidas nele. No caso de considerar o acoplamento geomecânico, a rocha reservatório passa a ser deformável e a variação de sua porosidade é calculada a partir da equação de conservação de massa da fase sólida.
5.1	Equação da Continuidade - Conservação de Massa
Nesta seção deduz-se a equação de conservação de uma quantidade física genérica, que será aplicada nas seções seguintes.
Sendo y uma quantidade física por unidade de massa, podendo ser escalar, vetorial ou tensorial, num ponto qualquer pertencente ao volume V no tempo t; Q é a fonte de y por unidade de massa; V.n é o fluxo de y por unidade de área em V. Desta forma a equação da continuidade para fluxo monofásico no volume V é escrita como (Eringen, 1980):
d
S
(62)
A Equação 62 mostra que a taxa de variação de y no volume V é igual a taxa de variação de y devido a produção no volume V menos a taxa de variação de y devido ao fluxo pela superfície de contorno S.
Aplicando o teorema do divergente ao último termo do lado direito da Equação 62, utilizando-se do teorema de Reynolds (Lin and Segel, 1988) no lado esquerdo da Equação 62 e considerando o resultado válido para qualquer sub-região do volume V, tem-se:
d(py) + v.V(py) + (py)V.v + V.r - pQ = 0	(63)
dt
sendo v a velocidade vetorial do material. A Equação 63 é aplicável em qualquer ponto do meio poroso.
5.1.1	Conservação de Massa para Fluidos
Considerando-se p = 0; Q = 0; Vr = -Q ; v = vf e p = pf, sendo: Q a vazão;
Vf a velocidade do fluido e pf a densidade do fluido. Substituindo esses termos na Equação 63 tem-se:
d(0pf)
+ vf V(p ) + (0pf )Vvf - Q = 0	(64)
Que pode ser escrita como:
d(0pf)
+V(0pfvf) - Q = 0	(65)
5.1.2	Conservação de Massa para Sólidos
A equação da Conservação de Massa para sólidos pode ser obtida de maneira similar considerando-se p = 1 - 0; Q = 0; Vr = 0; v = vr e p = pr, sendo: vr a velocidade da rocha e pr a densidade do sólido. Substituindo esses termos na Equação 63 tem-se:
31(1	1 + v, V[(1 -&amp;lt;/&gt;)P, ] + (1 -&amp;lt;f)P, Vv, = 0	(66)
Que pode ser escrita como:
31(1 -/1 + V[p, (1 -#)v,) = 0	(67)
dt
sendo vr a incógnita do problema geomecânico.
5.2	Lei de Darcy
Para complementar a equação de balanço de massa das fases fluidas é necessária uma relação entre o gradiente de pressão e a velocidade do fluido. A lei de Darcy rege o transporte de fluido no meio poroso. A lei relaciona a velocidade aparente do fluido com gradientes de pressão através da equação:
v =
k
---(Vp -Pfb)
A f
(68)
onde: k é o tensor de permeabilidade; p é a poro-pressão; p é a viscosidade do fluido e b é o tensor de forças de massa.
Para meios porosos deformáveis, a velocidade na Equação de Darcy (também chamada de velocidade superficial, que é a velocidade do fluido com relação à fase sólida) pode ser relacionada com a velocidade do fluido vf e a velocidade da rocha vr. A velocidade vr é a velocidade da fase sólida com relação à configuração de referência (rocha indeformada).
v = 0{vf - Vr )	(69)
Ou seja, a velocidade de Darcy é uma velocidade relativa (com relação à fase sólida) e a velocidade total do fluido é dada por:
v
Vf = (7 + vr )	(70)
5.3	Fluxo Monofásico em Meios Porosos Deformáveis
Aplicando-se as Equações 69 e 70 na Equação 65 tem-se:
-Vp ~ (VP-Pfè)^ + PVvr + vrV($Pf ) = Qf	(71)
Sendo a derivada material com relação à velocidade do fluido:
■£(♦) = £(•)+ vV(-)	(72)
Dt dt
onde v é a velocidade do meio poroso e é igual a vr.
Usando-se a Equação 72 nos termos da Equação 67 tem-se:
DDt [Pr (1 -0)1 + Pr (1 -^vr = 0
(73)
Com a Equação acima pode se obter o divergente da velocidade da rocha.
DDp (1
Pr (1 ~^)
(74)
Considerando a rocha incompressível (isto é, densidade pr e volume Vr constantes) a Equação 74 pode ser simplificada como:
1 Dó
~ (1 -$) Dt
(75)
Pela definição de porosidade verdadeira (true porosity)'.
$ = Vp-Vb
(76)
Isso conduz a:
Dú	DVp _ Vp DV
Dt	Vb Dt	Vb 2 Dt
Substituindo-se as Equações 76 e 77 na Equação 75 tem-se:
(77)
Vv
± DVl Vb Dt
(78)
r
r
Sabendo-se que a deformação volumétrica é definida como:

(79)
Substituindo-se a Equação 79 na Equação 78 tem-se:
Vvr
D£v
Dt
(80)
A Equação 80 é chamada de equação da compatibilidade na Mecânica dos Sólidos. Substituindo-se a Equação 80 na Equação 71 e aplicando-se a Equação 72 no primeiro e no último termo do lado esquerdo da Equação 71 tem-se:
d	C k
D ) - VPf -V - Pb J	= Qf
(81)
A Equação 81 mostra a interação entre fluido e rocha pela poro-pressão e pela deformação volumétrica ev. Num simulador dito “convencional”, o último termo do lado esquerdo da Equação 81 é ignorado devido a consideração de que o volume total (bulk volume), não varia com o tempo. Sem o acoplamento, a chamada porosidade verdadeira (true porosity) é substituída pela porosidade do reservatório (reservoir porosity):
O’ = 4	(82)
* b
Por definição, a porosidade do reservatório 0* é a razão entre o volume poroso em determinado instante de tempo Vp e o volume total inicial Vb0, enquanto que a porosidade verdadeira é a razão entre o volume poroso em determinado instante de tempo e o volume total calculado no mesmo instante de tempo (Equação 76). Num simulador convencional, a porosidade é estritamente função da pressão e da temperatura, somente porque as tensões não são parte da solução.
A Equação 81 apresenta duas incógnitas: o campo de pressões e o campo de deslocamentos. Para que seja possível a resolução desse sistema se faz necessária a definição de mais uma equação: a equação de equilíbrio em meios porosos deformáveis.
5.4	Equilíbrio em Meios Porosos Deformáveis
A equação de equilíbrio em meios porosos deformáveis pode ser desenvolvida da seguinte forma:
Considerando-se yz = (1 -0)vr; Q = b; r = c; v = vr e p = pr, sendo: c o tensor das tensões e b força de massa, e substituindo-se na Equação 63 tem-se:
d[Pr] + v, V[p, (1 -^)v, ] + [p,(1 -0)v, ]Vv, + Va-prb = 0	(83)
dt
Rearranjando a Equação 83, tem-se:
V° + Pr(1 -0)
Dvr
Dt
d[Pr (1 -1)1
dt
+ V[Pr (1 -^)Vr 1
-Prb = 0
(84)
+ Vr
Na Equação 84 as parcelas que estão entre colchetes, que representam o terceiro termo da equação, já foram deduzidas (Equação 67) e representam a conservação de massa local para um sólido, podem ser retiradas da equação. Ainda, a força de inércia, segundo termo da equação, é considerada somente quando se estuda problemas dinâmicos em escalas de tempo muito pequenas (milisegundos ou microsegundos) como propagação de ondas ou interações fluido-estruturas. Em problemas de fluxo com acoplamento geomecânico, a escala de tempo é maior (dias ou anos); a força de inércia, por sua vez, é muito pequena e pode ser desprezada. Logo, a equação de equilíbrio para um sólido (rocha, por exemplo) pode ser escrita como:
Nc-prb = 0	(85)
O tensor das tensões c representa as tensões totais atuantes no sistema. Entretanto, em um meio poroso, somente uma parte dessa tensão total, chamada tensão efetiva (Terzaghi, 1966) afeta o comportamento mecânico (deformabilidade e resistência). A tensão total c e a tensão efetiva c são relacionadas por (Biot, 1941):
c = C + apI	(86)
onde o parâmetro a é chamado de constante de Biot e possui o valor entre a porosidade e 1 (um); p é a poro-pressão e I a matriz identidade. Em rochas pouco consolidadas, a possui valor próximo de 1 (um) (Terzaghi, 1966).
Substituindo-se a Equação 86 na Equação 85 tem-se:
N(c'+apI) -prb = 0	(87)
A relação constitutiva para um geomaterial pode ser definida por:
C = D£	(88)
sendo tensões e deformações correlacionadas pela matriz constitutiva D. Considera-se ainda que s(u) é pequeno, ou seja, somente pequenas deformações são contempladas.
As Equações 81 e 87 mostram a interação entre sólido e fluido, pelas tensões, deformações e poro-pressões. Essas variáveis podem ser obtidas simultaneamente ou separadamente, dependendo do tipo de acoplamento utilizado.
5.5	Formulação Explícita - CBMEX
A formulação explícita desenvolvida nesta tese e descrita nesta seção é baseada no trabalho desenvolvido por (Inoue and Fontoura, 2009). A sigla CBMEX identifica os simuladores utilizados na metodologia acoplada. O Codebright é um simulador em elementos finitos, desenvolvido por (Olivella et al., 1996), utilizado aqui para o cálculo do campo de deslocamentos, deformações e tensões (simulação geomecânica). O Imex é um simulador comercial (da empresa CMG) largamente utilizado por diversas operadoras e companhias de serviço para o cálculo do fluxo em meios porosos (simulação de reservatórios).
5.5.1	Equações para o Acoplamento Explícito
O termo de armazenamento (primeiro termo do lado esquerdo da Equação 81) é tratado pelos simuladores de reservatórios da seguinte maneira:
Dpf DP +p D0 DP T ’ Dt PfDpT' Dt
D	DPf D0
---(0Pf ) = 0-- + pf — = 0-Dtf Dt---------f Dt	Dp
(89)
Podendo ser reescrita como:
' _1 DP
. Pf DP
D (0Pf ) = 0Pf
Dt
1D0 Dp
0 DPt ) Dt
(90)
Define-se então a compressibilidade da rocha igual a:
1 D0
c =----
r 0 Dp
(91)
Considera-se que essa compressibilidade é constante e que a nova porosidade pode ser descrita como:
0n+1	0 e\p|c (Pn+1 - P0)]
(92)
Ainda, a compressibilidade é considerada pequena e a Equação 92, expandida por série de Taylor e truncada no termo de primeira ordem, se reduz a:
0n+1 =0O[1 + Cr (Pn+1 - P0)]
(93)
+
T
T
Em cada novo passo de tempo do simulador de reservatórios a porosidade é atualizada de acordo com a Equação 93.
Entretanto, considerando-se um material isotrópico linear e elástico num cenário totalmente acoplado, a variação da porosidade é composta de 4 componentes distintos que contribuem para o termo de armazenamento (Zienkiewicz, 1999), são eles:
•	Deformação volumétrica do sólido (-&amp;amp;£v);
•	Compressão do sólido pelo gradiente de pressão ((1-0)Ap / Ks);
•	Compressão do sólido pela tensão efetiva (-KD / Ks (A£v +Áp / Ks));
• Alteração do volume dos fluidos nos poros (fâp / Kf).
Definindo-se:
Kd
E
3(1-2v)
,	Kd
a_ 1—-
Ks
1 _0n a-fl Q ~ ~Kf +
(94)
(95)
(96)
A variação da porosidade passa a ser definida como a soma dos 4 componentes
descritos anteriormente:
_fl +a¿M-e)+Q( p; - pn)
(97)
A Equação 97 corrige a simplificação feita pelo simulador de reservatórios na atualização da porosidade, mas para isso será necessário um processo de restarts na simulação de fluxo que será detalhado na seção 5.5.2.
Desta forma, a Equação 81 tem o seu primeiro termo do lado esquerdo corrigido, mas ainda é necessário incluir o termo da deformação volumétrica (terceiro termo do lado esquerdo da Equação 81) no procedimento realizado durante a simulação de reser-
vatórios (Equação 93). Para isso se faz necessária a definição de uma pseudo compres-sibilidade que será atualizada para cada passo de tempo de acoplamento (Inoue and Fontoura, 2009). Essa pseudo compressibilidade fica definida pela seguinte expressão:
cpseudo =
(Ç+1 -£nv)
&lt;Kpn+1 - pn)
(98)
Com a utilização das Equações 97 e 98 atualiza-se a simulação de fluxo pra cada tempo de acoplamento prescrito e consegue-se uma representação da simulação acoplada.
A próxima seção apresenta o processo de leitura e escrita de arquivos, além da metodologia de troca de informações entre o simulador de reservatórios e o simulador geomecânico.
5.5.2	Metodologia CBMEX
Conforme descrito anteriormente, o simulador Imex realiza os cálculos relacionados ao fluxo em meios porosos (o programa Report, também do pacote CMG, é utilizado para exportação dos mapas de pressões e saturações), cujas incógnitas são pressões e saturações e o simulador Codebright resolve o problema geomecânico cuja incógnita primária é o campo de deslocamentos. Diferentemente do método parcialmente acoplado e iterativo proposto por (Inoue and Fontoura, 2oo9), a metodologia do CBMEX é explícita e não iterativa, o que torna o método menos rigoroso, principalmente pelo efeito da escolha do tamanho do passo de tempo de acoplamento nos resultados. Em contrapartida, o tempo de simulação se reduz drasticamente pelo menor número de execuções do simulador geomecânico. Percebeu-se que para pequenos passos de tempo de acoplamento, os resultados se aproximam da simulação totalmente acoplada (ver seção 5.6).
Um programa em Matlab foi desenvolvido para controle e gerenciamento de arquivos entre o simulador de fluxo e o simulador geomecânico. As principais funções e arquivos necessários para a simulação acoplada são descritas abaixo.
Os arquivos necessários para o Imex são:
• ImexInputFile.dat: arquivo de entrada;
•	ImexRestartFile_00i.dat: arquivo de restart (sendo i variando de 1 até o número total de passos de acoplamento)
•	coord.inc: arquivo de coordenadas x,y para definição do grid do Imex (igual para todos os passos de tempo da simulação acoplada);
•	zcorn.inc: arquivo de coordenadas z para definição do grid do Imex (igual para todos os passos de tempo da simulação acoplada);
•	null.inc: arquivo que indica células ativas e inativas no grid do Imex (igual para todos os passos de tempo da simulação acoplada);
•	permi.inc: arquivo de permeabilidades na direção i (podendo ser atualizado após cada passo de tempo de acoplamento);
•	permj.inc: arquivo de permeabilidades na direção j (podendo ser atualizado após cada passo de tempo de acoplamento);
•	permk.inc: arquivo de permeabilidades na direção k (podendo ser atualizado após cada passo de tempo de acoplamento);
•	ctype.inc: arquivo que atribui um tipo de rocha a cada célula do modelo (igual para todos os passos de tempo de acoplamento);
•	crock.inc: arquivo que atribui para cada tipo de rocha definido em ctype.inc valores de compressibilidade e pressão de referência (este arquivo é modificado em cada passo de tempo de acoplamento);
•	por.inc: arquivo que define as porosidades das células do grid do Imex (este arquivo é modificado em cada passo de tempo de acoplamento)
•	PresRestart_00i.rwd: arquivo que exporta o mapa de pressões para cada passo de tempo de acoplamento (entrada do Reporí);
•	SwRestart_00i.rwd: arquivo que exporta o mapa de saturação de água para cada passo de tempo de acoplamento (entrada do Report)
Os arquivos necessários para o Codebright são:
•	CBFile_gen.dat: arquivo de entrada que define propriedades numéricas e condições de contorno;
•	CBFile_gri.dat: arquivo de entrada que define propriedades geométricas e físicas do grid geomecânico;
•	bcond_user_00i.dat: arquivo de carregamento (Delta P) para cada passo de tempo de acoplamento;
O programa para gerenciamento dos arquivos de simulação contém as principais rotinas de algoritmos:
•	runImex.m: rotina que executa o Imex;
•	runReportPres.m: rotina que executa o Report e extrai o mapa de pressões para determinado passo de tempo de acoplamento;
•	PresMapInterpolant.m: rotina que interpola os mapas de pressão do grid de simulação para o grid geomecânico;
•	runReportSw.m: rotina que executa o Report e extrai o mapa de saturações de água para determinado passo de tempo de acoplamento;
•	SwMapInterpolant.m: rotina que interpola os mapas de saturações do grid de simulação para o grid geomecânico;
•	writebcond.m: rotina que prepara os arquivo de carregamento do Codebright;
•	runCB.m: rotina que executa o Codebright;
•	updProps.m: rotina que calcula os novos valores de porosidade e compressi-bilidade para o restart do simulador de fluxo;
•	writeNewFiles.m: rotina que atualiza os arquivos de simulação para o passo seguinte.
A Figura 21 apresenta o processo de acoplamento e transferência de propriedades entre o simulador de fluxo e o simulador geomecânico para cada passo de acoplamento. Basicamente, executa-se o simulador de reservatórios, extraem-se os mapas de pressão e saturação. Em seguida, preparam-se os arquivos de carregamento do simula
dor geomecânico e executa-se o simulador geomecânico. Posteriormente, extraem-se as deformações volumétricas e calculam-se os novos parâmetros de acoplamento. Finalmente, o processo é reiniciado para um novo passo de tempo de acoplamento.
I ___________________________________________________________t
Simulação de Reservatórios (Imex)	1 1	Simulação de Reservatórios (Imex)
AP e AS	1	AP e AS
Interpolação entre grids	1 1	Interpolação entre grids
Reservatório —► Geomecânico	1	Reservatório —► Geomecânico
AP’ e AS’	1 1	AP’ e AS’
Simulação Geomecânica (Codebright)	1	Simulação Geomecânica (Codebright)
Au, As e Ac	1	Au, As e Ac
Atualização dos parâmetros de acoplamento	1 1	Atualização dos parâmetros de acoplamento
Cpe 9	1	Cpe 9
Interpolação entre grids	1 1	Interpolação entre grids
Geomecânico —► Reservatório	1	Geomecânico —► Reservatório
Cp’ e 9’	1 1	Cp’ e 9’
1------------------------------ I
T=1
T=2
Figura 21: Diagrama representativo do gerenciamento da simulação acoplada.
5.5.3	Função de Interpolação para Malhas não Coincidentes
A principal vantagem da metodologia CBMEX, além da redução do custo computacional, é a capacidade de trabalhar com malhas não coincidentes, ou seja, para uma mesma rodada acoplada, as malhas de simulação de fluxo e simulação geomecânica podem ser diferentes. Essa é uma vantagem considerável, pois é comum encontrarmos modelos de fluxo (diferenças finitas) com elementos bastante distorcidos (pinch-outs e células inativas, por exemplo), o que, na maioria das vezes, inviabiliza o aproveitamento da mesma malha de fluxo para a simulação geomecânica (elementos finitos).
A função de interpolação de propriedades entre as malhas de fluxo e geomecâni-ca utilizada é uma função interna do Matlab (denominada scatteredInterpolant) capaz de interpolar e extrapolar propriedades num espaço previamente amostrado. A Figura 22 demonstra o principio básico deste tipo de interpolação. Maiores detalhes do algoritmo de interpolação podem ser encontrados em (Watson, 1994).
Figura 22: Interpolação “Natural Neighbor”. A área dos círculos verdes definem os pesos da interpolação. A região roxa é chamada nova célula de Voronoi após a inserção do ponto a ser interpolado (ponto preto). Os pesos representam a área de interseção da célula roxa com cada uma das 7 células circundantes.
A seção 5.7 apresenta um caso em que as malhas de simulação de fluxo e geo-mecânica não são coincidentes, de qualquer forma, esta mesma função é utilizada no exemplo da seção 5.6 para interpolar as propriedades entre os centros de células (informação de pressão e saturação do modelo de fluxo) e os nós da malha de elementos finitos (carregamento do modelo geomecânico).
5.6	Caso de Validação - Dean 2006
A formulação apresentada na seção anterior será avaliada em um caso clássico da literatura relacionado ao acoplamento geomecânico na simulação de reservatórios. Tal caso foi sugerido por (Dean et al., 2006) baseado no problema descrito por (Gutierrez and Lewis, 1998).
Esse caso inclui a produção de um reservatório pouco consolidado, confinado por uma rígida camada adjacente, e mostra os efeitos geomecânicos presentes no fenômeno de fluxo em meios porosos que só podem ser avaliados por metodologias que incluem cálculos geomecânicos. Além da geometria e das propriedades físicas do problema, os gráficos e mapas comparativos apresentados nesta seção serão plotados no sistema inglês de unidades (field) em congruência com os resultados obtidos por (Dean et al., 2006).
A malha de simulação possui 21 células na direção x, 21 células na direção y e 12 células na direção z (um total de 5292 células) e inclui tanto o reservatório quanto as rochas adjacentes. As dimensões das células na direção x foram definidas da seguinte maneira: 4000 pés para as primeiras 5 células; 2000 pés para as 11 células seguintes e 4000 pés para as últimas 5 células. Na direção y, os valores são a metade dos definidos para a direção x. O topo do grid está na profundidade “zero” e as espessuras das células (direção z) foram definidas por: 4000, 3000, 2000, 800 e 200 pés para as primeiras 5 células que representam o overburden. As próximas 5 células possuem espessura de 50 pés cada e representam o reservatório e as últimas duas camadas tem espessura de 100 pés cada e representam o underburden. As Figuras 23 e 24 mostram, respectivamente, a geometria do modelo e a malha de simulação que possui 6292 nós e 5292 elementos.
No reservatório, as permeabilidades horizontal e vertical são iguais a 100 e 10 milidarcy, respectivamente (células 6-16, 6-16, e 6-10). A porosidade foi definida constante e igual a 25% para todo o modelo. O fluxo é considerado monofásico com um fator volume de formação igual a hum em pressão atmosférica (14,7 psi). O fluido tem viscosidade 1 cP, densidade igual a 62,4 lbm/ft3 e compressibilidade de 3x10 6 psi1. A pressão inicial na superfície é igual a 14,7psi e o gradiente hidrostático é igual a 0,437 psi/ft.
O módulo de elasticidade é igual a 1x104 psi no reservatório e 1x106 psi nas rochas adjacentes. O coeficiente de Poisson é definido em todo o domínio igual a 0,25. As condições de contorno mecânicas são de deslocamentos prescritos nulos nas laterais e base do modelo. Pela pequena espessura definida para o underburden, tais condições de contorno afetaram o resultado no que tange os campos de deslocamentos calculados. Mas, como o objetivo foi comparar com os resultados obtidos pelo artigo, foram mantidas as mesmas condições de contorno propostas. O exemplo seguinte, apresentado na
seção 5.7, elimina esse efeito da condição de contorno estendendo-se a malha de forma adequada.
a)
s 00 Tf O V	Reservatório	
3352 m ◄	►	^Poço Produtor	
	6705,80 m ◄	►&lt;	
Vista Superior
6098,00 m
----------------►
y
x
18897,60 m
b)
Vista Frontal
z
À
----kx
76,2 m
18897,60 m
Figura 23: Geometría da malha geomecanica. a) vista superior, b) vista frontal, respectivamente.
Figura 24: Malha de simulação geomecânica. a) vista superior, b) vista lateral e c) vista 3D. O grid de simulação de reservatórios é definido pelas células 6-16, 6-16, 6-10 nas direções x, y e z, respectivamente.
Um poço vertical com um raio de 0,25 ft é completado no centro do reservatório em todas as suas 5 camadas (células 11, 11, 6-10). A vazão de 50000 STB/D é imposta
por 2000 dias. No caso original do artigo os passos de tempo foram definidos em 20 dias para os primeiros 400 dias e iguais a 200 dias até o fim da simulação.
A Figura 25 apresenta a variação da pressão média do reservatório ao longo dos 2000 dias de simulação. Foram plotados gráficos utilizando-se diferentes simuladores. Os resultados obtidos com os simuladores da CMG (Gem, Gem-Geomech e Imex) utilizaram arquivos de “template'’” disponibilizados juntamente com a instalação do software. O arquivo de simulação do Abaqus reproduz os resultados descritos por (Inoue and Fontoura, 2009) em que foram comparados resultados do software comercial Abaqus e a formulação iterativa proposta. Os resultados Dean-Explicity e Dean-Full foram apenas reproduzidos graficamente do artigo (Dean et al., 2006).
A pressão média calculada pela metodologia desenvolvida nesta tese (ver seção 5.5.2) foi considerada satisfatória, principalmente quando comparada à simulação de reservatórios convencional e ao método explícito apresentado em (Dean et al., 2006).
Outros aspectos importantes a serem apresentados são os relacionados à deformação do maciço. Esta informação só através de uma simulação acoplada. As Figuras 26 e 27 mostram, respectivamente, a compactação do reservatório e subsidência da superfície ao longo do tempo.
A compactação calculada pela metodologia estudada nesta tese apresentou valores bastante próximos daqueles obtidos via simulação totalmente acoplada, o que fortalece a escolha da abordagem explicita nos problemas em que cálculos geomecânicos devam ser incorporados a simulação de reservatórios.
Da mesma maneira, o resultado da subsidência calculada se aproximou efetivamente dos resultados obtidos via simulação totalmente acoplada (Figura 27).
A diferença nos gradientes de pressão identificada na Figura 25 pode ser observada também na Figura 28, que apresenta um mapa de distribuição de pressões no topo do reservatório após 2000 dias de simulação, comparando-se os resultados do modelo Imex e da metodologia acoplada CBMEX. No mapa da Figura 29 observa-se que a consideração do efeito geomecânico provoca uma menor redução da porosidade na região do poço. Entretanto, a variação de porosidade acontece numa área maior quando se considera efeitos geomecânicos na simulação de reservatórios. A Figura 29 mostra o mapa de porosidade ao final de 2000 dias de simulação.
Figura 25: Pressão média do reservatório ao longo do tempo comparando-se os resultados de diferentes simuladores.
Figura 27: Subsidência ao longo do tempo de um reservatório depletado.
Esta diferença de comportamento é coerente com a maior variação de pressão mostrada na simulação acoplada, pois a menor variação da porosidade indica que a rigidez do sistema é maior ao longo do tempo de simulação o que é reproduzido na metodologia CBMEX pela redução do valor da compressibilidade ao longo do tempo.
Os carregamentos impostos ao Codebright são variações de pressão (Delta P) entre dois passos de acoplamento. Desta forma, os campos de deslocamentos calculados também são “deltas” de deslocamento entre os mesmos passos de acoplamento. Os resultados da metodologia CBMEX apresentados nas Figuras 26 e 27 foram obtidos acumulando-se os incrementos de cada passo de acoplamento. Sendo assim, as Figuras 30 a 33 mostram diferentes visões do campo de deslocamentos verticais e o carregamento no último passo de acoplamento. Percebe-se que tais resultados estão de acordo com a solução analítica descrita na seção 2.3 onde, durante a depleção, o topo do reservatório tende a descer e a base do reservatório tende a subir. Esse resultado só é possível de ser obtido em processos que contemplem o acoplamento geomecânico.
I
I
4.470
4.347
4.223
4.100
3.976
3.853
3.729
3.606
3.482
3.359
3.235
Figura 28: Mapa de pressões (psi) no topo do reservatório ao final de 2000 dias de simulação. simulação de reservatórios tradicional e b) metodologia acoplada.
a)
□ 0,250
0,243
— 0,236
0,229
0,222
— 0,215
0,209
0,202
— 0,195
I 0,188
■ 0,181
Figura 29: Mapa de porosidades no topo do reservatório ao final de 2000 dias de simulação. a) simulação de reservatórios tradicional e b) metodologia acoplada.
Figura 31: Deslocamento vertical no topo do reservatório (compactação) entre passos de acoplamento.
Figura 33: Variação de pressão (Delta P) aplicada no último passo de acoplamento.
Com os resultados apresentados conclui-se que a metodologia desenvolvida se aproxima satisfatoriamente dos resultados obtidos através da formulação totalmente acoplada. Na próxima seção serão apresentados resultados de uma simulação WAG (water alternating gas) comparando-se a simulação de reservatórios tradicional com a metodologia desenvolvida.
5.7	Modelo de Injeção Wag - Caso SPE 05
No final da década de 80 e início da década de 90 a SPE (Society of Petroleum Engineers) apresentou para a comunidade cientifica de simulação de reservatórios diferentes projetos comparativos com o objetivo de avaliar a concordância ou não dos diversos simuladores de reservatórios presentes até então. O quinto projeto comparativo trata de uma simulação WAG (water alternating gas) (Killough and Kossack, 1987). Este projeto apresentou resultados de comparações entre simuladores black-oil e com-posicionais para três diferentes cenários. Nesta tese, reproduziu-se os resultados do cenário 1 utilizando-se o programa Imex e comparou-se com a metodologia acoplada descrita na seção 5.5.2. Tais resultados serão apresentados no sistema inglês (field) de unidades em congruência com o estudo feito por (Killough and Kossack, 1987). As Figuras 34 e 35 apresentam, respectivamente, a geometria o grid de simulação de reservatórios e o modelo geomecânico que considera as camadas adjacentes.
Neste caso, as malhas não são coincidentes e a função de interpolação descrita na seção 5.5.3 é utilizada para interpolação de propriedades. Outra contribuição dada a este modelo é a camada de transição na espessura utilizada para reduzir o número de elementos do modelo geomecânico. Este método é pouco utilizado na geração de modelos geomecânicos a partir de malhas de simulação de reservatórios. A Figura 36 apresenta esquematicamente a metodologia de transição proposta.
Em uma malha de reservatórios de um campo real esse procedimento reduziria significativamente o número de elementos do modelo geomecânico sem nenhum dano ao resultado, pois o grande número de camadas, necessário para a representatividade do reservatório não se faz necessário nas rochas adjacentes. Neste exemplo, a malha atual possui 3440 nós e 2799 elementos, enquanto que sem a camada de transição na espessura esta mesma malha possuiria um acréscimo de 280 elementos (aproximadamente 10%
a mais na quantidade de elementos). Num modelo real de reservatórios em que o número de camadas é grande esse ganho seria ainda mais expressivo.
a)
b)
Vista Superior
Poço Produtor
s ▲
o
oo
SO
SO
O ,,
Poço Injetor
z
x
Vista Frontal
30,48 m
1066,80 m
◄---------►
y
▲
x
5066,80 m
Figura 34: Geometría da malha geomecanica do caso a) vista superior, b) vista frontal, respectivamente.
Figura 35: Malhas de simulação a) malha de simulação de reservatórios; b) corte malha geomecânica representando a posição do reservatório e o underburden e c) malha geomecânica completo.
O cenário estudado envolve um poço injetor WAG localizado numa extremidade do reservatório e completado na primeira camada e um poço produtor perfurado na outra extremidade do reservatório e completado na última camada. O poço produtor é restrito a produzir uma vazão de óleo máxima de 12000 stb/d. No inicio da simulação abre-se o poço produtor com a pressão de fundo mínima igual a 1000 psi por dois anos com o poço injetor fechado. Ao final do segundo ano, inicia-se a injeção WAG com ciclos de 1 ano. A pressão máxima de fundo do poço injetor é restrita em 10000 psi
A pressão média do reservatório cai rapidamente abaixo da pressão de saturação e se mantem assim em grande parte dos 20 anos de simulação. Os detalhes descritivos da formulação de fluidos necessários para a representação via modelo de 4 componentes está descrita em (Todd and Longstaff, 1972).
No caso do simulador Imex, um modelo black-oil adaptativo implícito é utilizado com a opção “pseudo-miscível”. Esta opção assume que o solvente pode se dissolver na fase água mas não na fase óleo. A abordagem de mistura de parâmetros descrita em (Todd and Longstaff, 1972) foi utilizada.
Figura 36: Processo de transição do número de camadas entre grids para redução do número de elementos no modelo geomecânico.
A Figura 37 apresenta a variação da pressão média do reservatório com o tempo. Conforme esperado o comportamento segue o esperado. Por atualizar os valores da porosidade e compressibilidade em cada passo de acoplamento, a metodologia CBMEX faz com que a variação de pressão seja mais expressiva, pois a variação da porosidade é menor ao longo do tempo de simulação.
Neste caso, o impacto na variação de pressão foi pequeno pela característica do reservatório de ser uma rocha pouco compressível, ou seja, bastante rígida. O valor inicial da compressibilidade da rocha é igual a 5,0 x 10-6 psi1. Sendo assim, percebe-se que o impacto da formulação acoplada no resultado final da simulação é tão maior quanto mais compressível é o comportamento da rocha reservatório.
Analisando-se a Figura 37, observa-se ainda a retomada da pressão de forma mais acelerada a partir do ponto em que o reservatório recupera a tendência de pressuri-zação (após 5000 dias aproximadamente). Isto se deve também à correção do valor da porosidade e compressibilidade através da parcela de deformação volumétrica, não contemplada na simulação de reservatórios tradicional.
O gráfico da variação da porosidade nos poços injetor e produtor é apresentado na Figura 38.
A menor variação dos valores de porosidade representa a maior rigidez do sistema incorporada pela metodologia acoplada. Esse resultado esta em concordância com o anteriormente obtido no caso de validação apresentado na seção 5.6.
Outros resultados interessantes a serem comparados por estas metodologias (simulação tradicional x simulação acoplada) é o comportamento da produção acumulada de óleo e da razão água-óleo. As Figuras 39 e 40 consolidam tais resultados, respectivamente.
Tempo (dias)
Figura 37: Comportamento da pressão média do reservatório ao longo do tempo.
Figura 38: Comportamento das porosidades nas células dos poços injetor e produtor comparando-se a metodologia CBMEX e a simulação de reservatórios tradicional.
X 107
Tempo (dias)
Figura 39: Produção acumulada de óleo ao longo do tempo.
7	i	---1----------1---------1----------1---------r
0	1000	2000	3000	4000	5000	6000	7000
Tempo (dias)
Figura 40: Razão água-óleo ao longo do tempo.
A menor produção de óleo acumulada pela metodologia CBMEX pode ser explicada, também, pela alteração na rigidez do sistema ao longo da simulação. Como a queda de pressão é maior na modelagem acoplada, a vazão de produção imposta se mantem por menos tempo no patamar desejado.
Neste caso (ver Figura 40), a chegada da água ao poço produtor sofreu um atraso de aproximadamente 250 dias ao se incorporar os efeitos geomecânicos.
A Figura 41 apresenta o mapa de delta de pressão no último passo de acoplamento. Este mapa é pouco representativo para resultados globais, pois, como já dito anteriormente, os resultados da metodologia CBMEX devem ser acumulados já que o carregamento é levado ao simulador geomecânico no formato de deltas de pressão. De qualquer maneira, percebe-se a frente de pressurização se movimentando do poço inje-tor em sentido do poço produtor.
Desta forma, conclui-se que o uso de uma metodologia acoplada deve ser avaliado em cada caso. Para geração de malhas de modelos geomecânicos, considerado um gargalho tecnológico, foi proposta uma metodologia de transição de malhas que reduz significativamente o número de nós e elementos dos modelos.
Figura 41: Delta de pressão no último passo de acoplamento.
A próxima seção apresenta uma discussão sobre o tempo computacional gasto em simulações de reservatórios acoplada à geomecânica.
5.8	Tempo de CPU em Modelos Acoplados
Para problemas acoplados em escala de campo, entre 70 e 80% do tempo de simulação é gasto no modelo geomecânico para se resolver o sistema de equações. Essa demanda computacional se deve a dois principais motivos. O primeiro diz respeito ao número de incógnitas no modelo geomecânico, que é de 3 por nó, enquanto que o número de incógnitas na formulação IMPES (IMplicit Pressure Explicit Saturation) em diferenças finitas do modelo de simulação de reservatórios é igual a 1 para cada célula. O segundo motivo é o número de elementos usados no modelo geomecânico devido à inclusão das camadas adjacentes (overburden, sideburden e underburden).
Comparando-se os resultados apresentados nas seções anteriores, comprova-se que a modelagem acoplada encarece significativamente o tempo de simulação. Por exemplo, no modelo “Dean”, apresentado na seção 5.6, o modelo de reservatórios é executado em 40 segundos enquanto que o tempo computacional gasto na simulação acoplada foi de aproximadamente 50 minutos. No caso do modelo “SPE5”, apresentado na seção 5.7, a simulação de reservatórios tradicional leva 3 minutos para ser completada enquanto que a modelagem acoplada gastou um tempo aproximado de 2 horas para encerrar sua execução.
A quantificação de incertezas em modelos que demandam longos tempos de execução se torna inviável. Normalmente, modelos substitutos (conhecidos como proxy) são utilizados para tornar eficaz uma análise de incertezas. Alguns exemplos de modelos substitutos são: superfícies de resposta; krigagem e redes neurais. Ainda, conforme discutido anteriormente, a aplicação da teoria da evidencia possui um alto custo computacional relacionado ao processo de otimização necessário para se criar as curvas CBF e CPF (ver seções 3.4.3 e 4.2). Desta forma, perde-se o sentido prático a realização de um processo de quantificação de incertezas em um modelo de simulação acoplada.
Para exemplificar o uso da teoria da evidência utilizando-se modelos numéricos, o processo de reativação de falhas foi avaliado (ver Apêndice A). Uma rede neural foi treinada para viabilizar a quantificação de incertezas via teoria da evidência.
6.	CONCLUSÕES E RECOMENDAÇÕES
Esta tese se propôs a avaliar e integrar dois temas: quantificação de incertezas e geomecânica de reservatórios. Para isso, foi realizada uma revisão bibliográfica sobre os principais problemas relacionados à geomecânica de reservatórios, tais como: injeção acima da pressão de fratura, reativação de falhas geológicas durante o processo de explotação de um campo de petróleo, compactação de reservatórios e injeção de CO2. Esta revisão contou com a dedução e implementação de soluções analíticas disponíveis na literatura relatas aos fenômenos descritos acima. Desta forma, a primeira contribuição desta tese foi agrupar diferentes soluções analíticas relacionadas à geomecânica de reservatórios em um único documento.
O processo de quantificação de incertezas foi amplamente discutido. Desde a definição de tipos de incertezas - aleatórias ou epistêmicas, até a apresentação de diferentes metodologias para quantificação de incertezas. A teoria da evidência, também conhecida como Dempster-Shafer theory, foi detalhada e apresentada como uma generalização da teoria da probabilidade. Apesar de vastamente utilizada em diversas áreas da engenharia, pela primeira vez a teoria da evidência foi utilizada na engenharia de reservatórios, o que torna tal fato uma contribuição fundamental desta tese. Cenários com diferentes tipos de evidência e exemplos de aplicação foram expostos na tentativa de elucidar conceitos.
O conceito de decisões sob incerteza foi introduzido e catapultou a integração desses dois temas extremamente relevantes na engenharia de reservatórios. Diferentes cenários inerentes à tomada de decisão foram descritos e discutidos, entre eles: a ausência de dados de entrada disponíveis, a situação em que os parâmetros de entrada são conhecidos, a inferência de novos dados ao longo do projeto e, por fim, uma modelagem híbrida, onde alguns parâmetros são considerados aleatórios e outros epistêmicos. Como resultado desta integração entre os conceitos de geomecânica de reservatórios e quantificação de incertezas via teoria da evidência foram submetidos 3 artigos a revistas indexadas. O primeiro, já aceito e publicado, enquanto que os demais ainda estão em
processo de revisão. Estes artigos foram incluídos nos apêndices presentes ao final desta tese.
Por fim, foi deduzida a equação de fluxo em meios porosos deformáveis e proposta uma metodologia explícita para incorporação dos efeitos geomecânicos na simulação de reservatórios tradicional. Esta metodologia apresentou resultados bastante efetivos quando comparada a métodos totalmente acoplados ou iterativos presentes na literatura. Em seguida, utilizou-se esta mesma metodologia num cenário mais realista onde foram discutidos processos de geração de malhas geomecânicas e impactos em algumas respostas de interesse provenientes da simulação de reservatórios, como por exemplo, produção de óleo acumulada e chegada de água no poço produtor. Em suma, a metodologia para acoplamento geomecânico na simulação de reservatórios desenvolvida nesta tese se mostrou eficiente e promissora.
Os seguintes aspectos podem ser considerados como continuidade da presente pesquisa e são, consequentemente, sugestões para trabalhos futuros:
•	Utilizar a teoria da evidência em modelos numéricos mais realistas e comparar tempos de simulação em diferentes cenários de disponibilização de dados de entrada;
•	Propor técnicas de combinação de evidência para problemas especificamente relacionados à geomecânica de reservatórios;
•	Analisar o impacto do refinamento dos grids de simulação no resultado final da simulação acoplada;
•	Inserir a modelagem de transporte reativo na metodologia desenvolvida e avaliar o impacto da alteração da permeabilidade pelo efeito de dissolução da rocha reservatório.
APÊNDICE A - Artigo publicado na Computers
&amp;amp; Geotechnics
Provided for non-commercial research and education use. Not for reproduction, distribution or commercial use.
This article appeared in a journal published by Elsevier. The attached copy is furnished to the author for internal non-commercial research and education use, including for instruction at the authors institution and sharing with colleagues.
Other uses, including reproduction and distribution, or selling or licensing copies, or posting to personal, institutional or third party websites are prohibited.
In most cases authors are permitted to post their version of the article (e.g. in Word or Tex form) to their personal website or institutional repository. Authors requiring further information regarding Elsevier's archiving and manuscript policies are encouraged to visit:
http://www.elsevier.com/authorsrights
Computers and Geotechnics 56 (2014) 202-215
Contents lists available at ScienceDirect
Computers and Geotechnics
journal homepage: www.elsevier.com/locate/compgeo
Coupled hydro-mechanical fault reactivation analysis incorporating evidence theory for uncertainty quantification
Leonardo C. Pereira a,b,c, Leonardo J.N. Guimarães b, Bernardo Horowitz b, Marcelo Sánchez a'*
aZachry Department of Civil Engineering, Texas A&amp;amp;M University, USA
b Department of Civil Engineering, Federal University of Pernambuco, Recife, Brazil
cPETROBRAS - Petróleo Brasileiro S.A., Brazil
article info	abstract
Article history: Received 9 September 2013 Received in revised form 4 December 2013 Accepted 12 December 2013 Available online 3 January 2014	The injection of water (or CO2) at high pressure is a common practice to enhance oil production. A crucial component of this activity is the estimation of the maximum pressure at which the fluids can be injected without inducing the reactivation of pre-existing faults that may exist in the formation. The damage zones typically formed around the geological faults are highly heterogeneous. The materials involved in the damage zones are characterized by the huge variation of their properties and high uncertainties
Keywords: Fault reactivation Maximum injection pressure Coupled hydro-mechanical analysis Evidence theory Uncertainty quantification	associated with them. To estimate the maximum allowable injection pressure this paper presents a novel approach based on: a coupled hydro-mechanical formulation (for the numerical analyses); a criterion based on the total plastic work (for the fault reactivation); and the evidence theory (for uncertainty quantification). A case study based on information gathered from an actual field is presented to illustrate the capabilities of the proposed framework. © 2013 Elsevier Ltd. All rights reserved.
1.	Introduction
Reservoir seals are natural traps that prevent the upward migration of hydrocarbons. Fault reactivation may induce seal rupture with the possibility of connecting the reservoir with other rock blocks and finally with the surface. This process may lead to oil release and the associated environmental accident. Fault reactivation can be triggered by different factors, including the injection of fluids (i.e. H20; C02) at high pressure generally used to enhance oil production. Injection of fluids at high pressure represents a central activity around oil production strategies. Perhaps one of the biggest challenges of engineers and geologists involved in H20/ C02 injection projects is to estimate the maximum allowable injection pressure that will not hampered the seal condition of the fault. In this work a methodology based on coupled numerical analysis and uncertainty quantification (using evidence theory) is proposed to estimate the maximum injection pressure in oil reservoir systems.
The appropriate representation of a geological fault in a numerical model requires a good understanding of the so-called damage zones. Damage zones are formed by fractured rocks associated with the deformation process near the less permeable
* Corresponding author. Address: Zachry Department of Civil Engineering, Texas A&amp;amp;M University, 3136 TAMU, College Station, TX 77843-3136, USA. Tel.: +1 979 862 6604; fax: +1 979 862 7696.
E-mail address: msanchez@civil.tamu.edu (M. Sanchez).
fault core. A number of recent modeling efforts have been made to better understand the mechanical and flow behavior in the damage zones. For example Goodarzi et al. [1] performed a deterministic modeling for C02 storage in Nisku aquifer in Wabamun Lake area in Canada. They concluded that the injection in the Nisku aquifer may cause small surface heave only, with (very likely) no environmental impact associated with surface deformations. The hydro-mechanical behavior of a fault running across a reservoir during a C02 injection scenario was modeled by Ducellier et al. [2]. Particular attention was paid to the influence of some model parameters on the fluid flow response through the fracture. They showed that the parameter with the major influence on the flow along the fault is the permeability of the filling material, and the following ones (in a lower extent): the reservoir permeability, the permeability of the joint elements and the initial opening of the joint elements (these last two are strongly related). The main aim of this work [2] was not to estimate the injection pressure that reactivate the fault, but the effects of the flow-deformation couplings on the behavior of the fault zone. Perera et al. [3] developed a 2D model using the Comsol Multiphysics simulator to obtain the optimum injection pressure in coal seam without breaking its sealant. They concluded that during C02 injection, the cap rock is vertically deformed by C02 pressure. Moreover, the maximum vertical deformation of the cap rock and the C02 spread rate along the interface increase with the increment of the C02 injecting pressure. The problem of fluid flow coupled with the deformation of the natural formation has been discussed extensively in the literature. For
0266-352X/S - see front matter © 2013 Elsevier Ltd. All rights reserved. http://dx.doi.Org/10.1016/j.compgeo.2013.12.007
Nomenclature			
r.	divergence operator	RHO	Specific rock weight
R	total stress tensor	Ko	earth pressure coefficient at rest
b	body force vector	kh	horizontal permeability
	porosity	kv	vertical permeability
Pi	liquid density	E	Young’s modulus
Pi	liquid pressure	t	Poisson’s ratio
qi	Darcy’s flow	IDZ	internal damage zone
u	solid phase velocity	EDZ	external damage zone
Ps	solid density	C2	external damage zone cohesion
ev	volumetric strain	NIRES	reservoir Poisson’s ratio
R	effective stress tensor	ERES	reservoir Young’s modulus
m	auxiliary vector	AA2	external damage zone friction angle
Fy	yield surface	NI2	external damage zone Poisson’s ratio
e	lode angle	E2	external damage zone Young’s modulus
J	2nd stress invariant	NIRIG	defines Ko = NIRIG/(1 - NIRIG)
p	effective mean stress	KRES	reservoir permeability
c0	effective cohesion	KDAMAGE2	external damage zone permeability
/'	effective friction angle	KCORE	core permeability
EP	plastic strain tensor	CCORE	core cohesion
	stress tensor at yielding	NICORE	core Poisson’s ratio
Wc	plastic energy dissipated per unit volume	ECORE	core Young’s modulus
fa	initial porosity	AACORE	core friction angle
ki	intrinsic initial permeability tensor		
example, Kojic and Cheatham [4] presented a treatment of the plasticity theory in porous media with fluid flow. Huang et al. [5] used the random finite element method to investigate the influence of the standard deviation on various measures of the equivalent coefficients of consolidation.
A key characteristic of the fault reactivation problem is the huge range of variation of hydraulic and mechanical parameters in this highly heterogeneous zone. Furthermore, the lack of reliable experimental data associated with the materials in the damage zone is quite common. Therefore, a formal framework is essential for handling the uncertainties associated with this kind of analysis. Uncertainty quantification is the process of determining the effect of input uncertainties on response metrics of interest. The input uncertainties can be characterized as either aleatory uncertainty, which has irreducible variabilities inherent in nature; or epistemic uncertainties, which are reducible uncertainties resulting from a lack of knowledge. Since sufficient data is generally available for aleatory uncertainties, probabilistic methods are commonly used for computing response distribution statistics based on input probability density functions. Conversely, for epistemic uncertainties data is generally scarce, making the use of probability theory questionable and resulting sometimes in non-accurate predictions. When dealing with epistemic variables, non-probabilistic methods generally based on the specification of intervals data are more appropriate.
In the analyses presented in this work, the parameters uncertainties are mainly related to the lack of (or limited) information about them. So, the uncertainty is not aleatory but epistemic and, therefore, a framework based on the evidence theory (a non-probabilistic method) has been adopted in this work to deal with the uncertainties associated with the main variables involved in this problem.
Uncertainty quantification has been an important topic in almost all the engineering areas. Probability theory based on the Bayesian school has been widely used in the past (e.g. [6,7]); but more recently fuzzy methods and evidence theory are regarded as ones of the legitimate extension of classical probability theory (e.g. [8,9]). Dodagoudar and Venkatachalam [10] presented an
alternative approach, called fuzzy point estimate method, to estimate the reliability of a finite earth slope. The approach incorporates the degree of membership value associated with each of the uncertain input parameters used in the proposed model. Schweiger and Peschl [11] developed a methodology based on evidence theory and compared the results obtained with this technique against random field finite element analysis by means of assessing the probability of failure of a simple slope. They showed that in some cases both methods give very similar results but in other cases important differences are observed, in particular for small spatial correlation lengths where the averaging procedure adopted is less accurate. That study concentrated on the mechanical problem only, without considering the hydro-mechanical (HM) coupling in the analysis.
This paper focuses on the evaluation of the maximum allowable injection pressure to assist oil production in pressurized reservoirs involving a fault zone. Deterministic, probabilistic and non-proba-bilistic analyses have been performed based on a typical fault reactivation problem. This kind of analysis is generally solved analytically or numerically and the criterion adopted to decide whether the fault will be reactivated or not is based on the position of the current stress state with respect to a yield surface or envelope (e.g. Mohr Coulomb envelope). In this work, a fully coupled formulation has been used in the numerical analyses and for the first time the evidence theory has been used to quantify the uncertainties typically associated with this complex problem. Another novel contribution of this work is the adoption of the total plastic work criterion in the fault zone to estimate the maximum injection pressure. A quality of this criterion is that the assessment of the fault reactivation is based on the behavior in a certain volume (near the fault) and not in isolated points, as it happens in other criteria adopted in the pass (e.g. the ones based on the yield surface criterion). One disadvantage of the evidence theory is the high CPU-time demand generally associated with the numerical analysis. In this paper we propose the use of Artificial Neural Networks (ANN) to reduce the computational cost.
The paper is organized as follows: first, concepts related to the fault zone and the fault reactivation processes are introduced.
Then, the main equations of the model and relevant HM couplings are briefly described. Afterwards, the case study is analyzed using probabilistic and non-probabilistic methods. Finally the main conclusions of this research are presented.
2.	Fault reactivation process
The loss of the reservoir seal is one of the most critical situations in the oil industry. The uncontrolled influx and leak-out by pre-existing or induced discontinuities in the overlying rock mass can lead to huge economic loss and environmental impact. The trap is the stratigraphic or structural feature that ensures the juxtaposition of reservoir and seals such that hydrocarbons remain trapped in the subsurface, rather than escaping to the surface due to their natural buoyancy. Fig. 1 presents (schematically) a structural view of a trapped reservoir.
Geological discontinuities, such as faults, are inherent in most petroleum formations. A fault plane (or fault zone) is a discontinuity in the rock mass, quite common in most sedimentary basins. It may be formed, amongst other, by tectonism or halokinesis processes characterized by temporal distortion of evaporites. Fig. 2 shows a typical disposition of materials in a fault zone. The fault plane is an intensely fractured region basically formed by a central core and the damage zones. This set of materials is called fault rock. The core comprises a filler material located between the fault planes. The filler is composed of fine particles sizes (mainly formed due to shear deformations), which may or may not be diageneti-cally cemented. In geomechanical studies, it can be assumed that the fault core is initially impermeable. The damage zone encompasses the mass of deformed rocks around the fault surface, which results from the initiation, propagation and build-up of slip along faults (e.g. [12,13]). The damage zone may extend up to several meters beyond the fault core.
Fractures in fault zones may develop in a variety of angles with respect to the direction of the maximum principal stresses generated during the frictional sliding of the adjacent faulted blocks [14,15]. Due to the presence of the fractures, this region may have different mechanical properties respect to the host rock. These fractures are considered sealed at the beginning of the analyses, and they act as a barrier for the migration of fluids. In fact, the sealant condition of the fault in both regions (i.e. central core and the damage zone) is an essential requirement for the existence of the oil reservoir.
However this sealant condition of the fault may change due to geological processes or human actions. For example, typical operations associated with oil production, such as H2O or CO2 injections, generally result in the re-distribution of in situ stresses, which may lead to the reactivation of nearby faults and subsequent slips [16]. The mechanical integrity of the reservoir depends on a number of
factors, amongst others: shear and tensile strengths of the formation rock and changes in fluid pressures (which in turn induce changes in effective stresses). If the injection of fluids (i.e. H2O or CO2) at high pressures is such that the associated changes in effective stresses induce yielding, significant stress re-distribution and plastic deformations are anticipated in the reservoir and fault zone. Those changes generally lead to the reactivation of pre-existing discontinuities (and/or the formation of new fractures) and the loss of the fault sealing capability. Once the fault is reactivated, fluids may migrate between blocks and also from the reservoir to shallower depths through the reopened discontinuity. The fault reactivation can also pose a problem for well casing integrity that eventually intersects these reactivated regions.
From above, it can be concluded that this problem is characterized by the strong coupling between the mechanical and the flow problems [17]. For a particular case study, the injection pressure that may trigger the fault reactivation will depend on the coupled hydro-mechanical processes described above and on the materials properties involved in the analysis. The formal approach used in this work to analyze this type of problems is presented in the next section.
3.	Mathematical formulation and computer code
The fully coupled formulation proposed by Olivella et al. [18] and the associated finite element program CODE_BRIGHT [19] have been adopted for the numerical analysis. This approach is able to deal with non-isothermal multiphase fluid flow in deformable porous media. For sake of simplicity, isothermal conditions have been assumed in this work. Furthermore, only one fluid has been considered in the analysis. This decision has been taken to reduce the number of constitutive laws (and related model parameters) involved in the uncertainty quantification analysis. This study can be also considered as the necessary preliminary step before performing a multiphase analysis.
The formulation is composed of balance equations (established for the whole porous medium) and constitutive equations (related to the intrinsic materials behavior). In this work three balance equations have been adopted as follows: momentum balance, mass balance of water and solid. The specific constitutive equations are described below and the adopted model parameters for the analyses are presented in Section 4. More details about the mathematical formulation can be found elsewhere (e.g. [18,19]).
3.1.	Balance equations
Inertia effects are not considered in this study; therefore momentum balance is reduced to the equilibrium equation:
V.c + b = 0	(1)
Fig. 1. Main components in a petroleum reservoir in a sedimentary basin.
Reservoir
External Damage Zone (EDZ) (~25 meters)
Internal
Damage Zone Core (IDZ) (~5 meters) (~15 meters)
Fig. 2. Main fault zones. Damage zones and fault core.


where a is the symmetric tensor of total stresses and b is the body force vector.
Considering a porous medium saturated with a single fluid (e.g. water), the mass balance of liquid is represented by: ^V.^ + /p,u )- 0	(2)
where / is the porosity and pl is the liquid density. The liquid is considered compressible and its density depends on the liquid pressure Pi (e.g. [19]). The Darcy's flow is defined by ql and u is the solid phase velocity.
The mass balance of solid can be expressed as:
@ [(1 - /)ps] + V.[(1 - /)PsU]= 0	(3)
where ps is the density of the solid phase, which depends on solid grains compressibility. Applying the concept of material derivative, the variation in porosity can be expressed by [18]:
D/	(1 - /) DPs
Dt	Ps	Dt
+ (1 — /)ev
(4)
where ev is the volumetric strain.
3.2.	Constitutive equations
It is assumed that the mechanical behavior is controlled by the effective stresses (a'), defined by:
a' - a — mpl	(5)
where r_ is the total stress tensor and m is an auxiliary vector (mT = 1, 1, 1, 0, 0, 0). The mechanical constitutive model adopted in this work is based on the Drucker-Prager criterion, where the yield surface (FY) can be expressed as:
Fy - J — U? +	G(h)- 0	(6)
where G is a function of the lode angle (h):
C(h -—30»)- f2p3sin	(7)
3 — sin /'
where J is the 2nd stress invariant of deviatoric stress tensor, p0 is the effective mean stress, c0 is the effective cohesion and /0 the effective friction angle.
The mechanical model presented above has been combined with a criterion to predict fault reactivation. A ductile fracture criterion to predict the onset of fracture opening based on the ‘‘total plastic work” was first suggested by Gillemot [20]. This criterion assumes that the crack will start (and will then propagate) when the plastic energy dissipated per unit volume reaches a critical value (Wc). The energy absorbed per unit volume during yielding can be calculated as [20]:
Wc -J Ry : d£pdV	(8)
where ep is the tensor of plastic strains and is the stress tensor at yielding. This study proposes to use the total plastic work criterion to define the maximum injection pressure. In this work the plastic work has been calculated for the volumes associated with each one of the materials involved in the damage zone (i.e. the plastic work integrated (Eq. (8)) for the volumes comprising: the core, the external and internal damage zones). The plastic work values calculated in that way have been used to evaluate the maximum injection pressure (as explained in Section 4). Such kind of criterion is recommendable because the prediction of the mobilization of the geological fault is not based on what happens in isolated points, but on the plastic behavior of the materials in the fault zone.
As for the hydraulic problem, this work assumes that the intrinsic permeability depends on porosity through the following exponential law [21] as follows:
k - kieb(/—^i’	(9)
where /i is the initial porosity, ki is the intrinsic initial permeability tensor and the parameter b controls the effect of porosity on permeability.
3.3.	Computer code
The finite element program CODE_BRIGTH solves the equations described above (i.e. mass balance, momentum balance, and constitutive equations) in a fully coupled way. One unknown is associated with each one the equations presented above (i.e. u is associated with (1); liquid pressure is associated with (2); and (4) is used to update /). It is a 3D code that adopts the Newton-Raphson method to solve the non-linear problem and finite differences to solve the evolution in time. The code has been extensively validated in different simulations involving coupled geomechanical problems (e.g. [19,22,23]).
4.	Numerical modeling of a fault reactivation problem
The numerical modeling of problems associated with fault reactivation has received increasing attention in the last few years (e.g. [24-26]). As mentioned in Section 2, the analyses associated with the reactivation of geological faults in oil reservoir are generally related to the injection of fluids (i.e. H2O or CO2) at high pressure. Numerical modeling in this area is aimed at estimating the maximum injection pressure that can be applied to assist oil production without inducing the reactivation of pre-existing faults or the generation of new fractures.
The numerical analyses presented in this paper focus on the behavior near a fault zone. Fig. 3 shows a typical two-dimensional geological cross-section associated with an oil reservoir. The rectangle in this figure delimits the domain studied in this paper.
The problem has been solved assuming 2D-plain-strain conditions, with a rectangular domain 1560 m (wide) and 750 m (deep), as shown in Fig. 4. In this problem, the fault is quite long and with rather constant gradient, so 2D-plain-strain conditions are appropriate. The reservoir is a 50 m thick consolidated sandstone, overlaid by a 400 m thick deposit of shale. The sea bed is at 130 m
Fig. 3. Typical 2D geological cross-section. The red square indicates the area of influence of the model analyzed in this study. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)
Fig. 4. Geometry of the model: the reservoir is a 50 m thick consolidated sandstone embedded 400 m and being located in a 130 m water depth region.
below the water level. The adopted boundary conditions are shown in the same figure. The initial stress state was defined assuming geostatic conditions, considering specific rock weight (RHO) equal to 2300 kg/cm3 and Ko equal to 0.45.
The geological fault is represented by three materials: core, internal damage zone and external damage zone (Fig. 2). The case study is based on an actual field located in ‘‘Campos Basin”, Rio de Janeiro, Brazil. The properties adopted in this model were obtained from both: log results (to define flow properties); and triaxial tests (to determine mechanical properties). The reservoir average horizontal intrinsic permeability (kh) is equal to 50 milidarcy (i.e. 4.934 x 10 14 m2) and the average porosity is equal to 0.2. Following common practice in reservoir simulation, the vertical permeability (kv) has been estimated according to the following ratio: kh/10 = kv. Based on laboratory tests, the elastic properties (i.e. Young's modulus - E - and Poisson's ratio - v -) presented in Table 1 have been adopted for the reservoir, overburden and underburden rocks. The mechanical properties associated with the fault zone (i.e. core, internal damage zone and external damage zone) were estimated through 1st and 2nd after rupture (AR) tests [27]. The main mechanical properties (i.e. E; v; C; and /') are also presented in Table 1. It is worth mentioning that the uncertainty associated with these parameters is high and the adopted values for the deterministic case presented in this section correspond to best estimate values.
The finite element mesh is composed of 6335 nodes and 12,550 triangular quadratic elements. Fig. 5b shows the adopted finite element mesh. Fig. 5a presents a detail of the fault region where it can be observed the high mesh refinement adopted there, anticipating the high gradients to be expected in this part of the domain. Fig. 5c illustrates the adopted physical model associated with the damage zones used to define the numerical model. The fault zone has the following characteristics: a core 5 m thick; an internal damage zone 15 m thick (at each side of the core); and an external damage zone 25 m thick. The core generally tends to be more rigid and less permeable than the adjacent damage zones.
It has been assumed that an injection well is located at the left of the fault zone and a production well is located at the right (Fig. 4). This well disposition corresponds to a plausible scenario in this kind of problem. It has been considered that injection well operates with a well bottom hole pressure (BHP) up to 3.8 MPa above the original pressure. It has been assumed that the production well operates with a delta pressure of -4.0 MPa.
After some time pressurizing the reservoir, the effective stresses will reach the yield criterion, jeopardizing the entire mechanical stability of the formation and leading to the reactivation of the fault zone. The development of (dilatant) plastic strains facilitates the migration of fluids into the fault zone; which would compromise the sealing capabilities of this zone. If this happens, fluids will migrate from the reservoir up to the surface. The challenge is to estimate the maximum allowed increase in pore pressure that will not trigger the exudation process. The follow figures illustrate the process of fault reactivation explained above.
Table 1
Mechanical and flow properties for each region considered in the model.
	Horizontal permeability (mD)	Porosity	Young modulus (GPa)	Poisson ratio	Cohesion (MPa)	Friction angle (°)
Reservoir	50	0.2	30	0.3	-	-
Overburden	le-5	0.01	42	0.37	-	-
Underburden	le-5	0.01	26	0.26	-	-
Fault						
Core	le-5	0.1	8	0.3	1.0	14
1DZ	le-5	0.2	8	0.3	0.8	16
EDZ	le-5	0.3	6	0.25	0.8	16
Fig. 5. Mesh adopted for the numerical analysis: (a) detail of the mesh at the fault zone; (b) mesh for the whole analyzed domain; and (c) illustration of the adopted physical model associated with the damage zones used to define the numerical model.
Fig. 6. Typical results associated with fluid pressure distribution in the reservoir: (a) beginning of the reservoir pressurization; (b) just before the fault reactivation phenomenon; and (c) just after the fault reactivation.
Fig. 6 shows typical results associated with changes in pore pressure in the fault zone for the case that there is a lack of control on reservoir pressurization. Fig. 6a shows the beginning of the reservoir pressurization. Fig. 6b presents the adopted scale for contours plots. Fig. 6c shows the pressure distribution in the reservoir before the fault reactivation phenomenon. Fig. 6d illustrates the pressure distribution after the fault reactivation.
Fig. 7 shows the fluid flow (module) in the area under study after fault reactivation. It can be observed that leakage takes place predominantly in the internal damage zone.
The reactivation of faults may induce important changes in the displacement field. The associated subsidence can damage seabed infrastructure and (if applicable) surface facilities. Localized deformations can induce the collapse of well casing located close to the fault planes. Fig. 8 shows the displacement (module) field predicted by the model after the fault reactivation process.
Once the activities associated with the exploitation of the reservoir start, the changes in the pore pressure (i.e. Fig. 6) modify the initial stress state. Fig. 9 shows the concentration of shear stress in the damage zone. As expected, a concentration of stresses is predicted near the fault plane; which is mainly induced by the contrast between the stiffness of the different materials involved in the analysis.
The significant changes in the stress states may lead to yielding and the development of plastic strains. Fig. 10 shows the concentration of shear strains in the fault zone.
As mentioned before, a criterion based on the total plastic work (8) has been adopted in this work to define the maximum injection pressure. Fig. 11 shows the evolution of the plastic work in the three different materials of the fault zone (i.e. core, internal damage zone and external damage zone). The maximum injection pressure is defined when some of those materials reach a maximum
Fig. 7. Module of fluid flow in the area under study just after fault reactivation.
Y-Displacements
	0 059233 0 052586		-5.7182e-l7 0.0047779	H
	0 045939		0.0095558	■
	0 039292		■0.014334	
	0 032645		-0.019112	•
	0 025998		0.023889	■
	0019351		■0.028667	
	0012705		-0.033445	
u	00060576 ■0 00058938	kJ	0.038223 -0.043001	u
[Displacements!
0.060874 0.05411 0.047347 0.040583 0.033819 0.027055 0.020291 0.013528 0.0067638 2.1659e-15
X-Displacements
Fig. 8. Displacements field just after fault reactivation: (a) horizontal displacement; (b) vertical displacement; and (c) displacement module.
Fig. 9. Distribution of shear stresses in the damage zone.
:ic Shear ns [-]
10.0311B3
0.027718
. 0.024253
0.020789
I 0.017324
| 0.013859 0.010394
I 0.0069295
0.0034648
0
Fig. 10. Distribution of plastic shear strains in the damage zone.
value of the total plastic work equal to 10 2 [kg(m/s)2]. It can be seen (Fig. 11) that, after this value is attained, the plastic work increases significantly, so it can be assumed that the fault is reactivated.
For this specific case, the maximum allowed pore pressure increment obtained was 3.2 MPa. However this result is related to the particular parameters adopted in this analysis. A more general study accounting for the uncertainties associated with the different parameters involved in this kind of problems is presented later on the paper (Section 7). Next section presents some background information related to uncertainty quantification.
5.	Uncertainty quantification
Uncertainty Quantification (UQ) is becoming an essential component of analyses associated with complex problems dealing with limited information. The uncertainty dual nature is described by Helton [8] as:
•	Aleatory uncertainty is the uncertainty that results from the fact that a system can behave randomly. It is also known as: stochastic uncertainty; Type A uncertainty; or irreducible uncertainty.
•	Epistemic uncertainty is the uncertainty that results from the lack of knowledge about a system and/or its properties. It is also known as: subjective uncertainty; Type B uncertainty; or reducible uncertainty. Evidence theory is one of the possible approaches for dealing with epistemic uncertainty, as it is explained later on.
Traditionally, the probability theory has been used to analyze problems involving these two types of uncertainties. It is known that the probability theory is a proper framework to quantify uncertainties when dealing with aleatory variables. However, recent contributions have shown that probabilistic methods may not be the more convenient ones for analysis involving epistemic uncertainties [8]. Other alternative methods, based for example on evidence theory, seem more suitable for this kind of analysis [8].
Probabilistic analysis requires information about the probable events. When this information is not available, the uniform distribution function is generally used based on the ‘‘Principle of Insufficient Reason” [28]. This principle can be interpreted as follows: in simple events, where the probability density function is not known, it can be assume that the data is equiprobable. This assumption is very appropriate for random variables, but when dealing with epistemic ones, it is perhaps too approximate.
The evidence theory is an alternative to the probability theory when attempting to quantify epistemic uncertainties (i.e., when model input variables are considered reducible). The potential benefit of the evidence theory is that it allows a less restrictive description of the uncertainty, when compared with probability theory. The uncertainty quantification through the evidence theory is computationally more demanding, than the probabilistic method [8]. The additional computation effort comes mainly from the numerical optimization process needed to compute the belief and plausibility functions (details in Section 6.1). This paper proposes the use of ANNs to speed up this process, as it is explained later on.
A further assumption in classical probability theory is inherent in the additivity axiom. This axiom implies that information about a particular event is associated with the knowledge of the complement of this event (i.e. the probability associated with the occurrence of a given event also informs about the probability of no occurrence of this event [8]). Although the additivity assumption and the principle of insufficient reason may be appropriate when modeling random events associated with random uncertainty,
Fig. 11. Evolution of the plastic work in the three different materials of the fault zone.
these restrictions are questionable when dealing with situations involving epistemic uncertainty [8]. Examples of these types of situations are: (a) little information about the model input parameters is available; (b) the available information is ambiguous or contradictory.
In cases where the available information is not enough to quantify the uncertainty with a probability function, it is sensible to consider a possible variation range of the variable defined by intervals, or sets of possible values. The definition of a range (or a set) of probable values associated with a given variable implies that the Principle of Insufficient Reason is not enforced. A priori, probabilities can be established to sets, without having pre-established assumptions about the probabilities of the individual events. Furthermore, the additive axiom is not enforced, therefore the complementary sets of probabilities measures do not need to sum 1 (when this occurs, this framework converges to the traditional probabilistic representation) [8].
According to Helton et al. [29,30], there are three main theories from which the uncertainty representation has been approached by intervals: (i) imprecise probabilities; (ii) possibility theory; and (iii) evidence theory, also known as Dempster-Shafer theory (e.g. [31-33]). This study adopts the evidence theory to quantify the uncertainties associated with the fault reactivation problem. The main motivations behind the selection of the evidence theory are: (a) it is a sound theory; (b) it is easy to correlate evidence theory and the probability theory; (c) the large number of successful applications reported in the last few years (e.g. [34-36]); and (d) the versatility of the theory to represent and combine different types of evidences obtained from multiple sources. More information about evidence theory can be found elsewhere [37-39].
6.	Uncertainty quantification in fault reactivation
In this work, typical results obtained from a UQ study using the well-known probability theory are compared against the results obtained from the analyses based on the evidence theory. The case presented in Section 4 is used in the analyses. In complex problems (like this one) involving a large number of variables, it is highly recommended to perform a sensitivity analysis to identify the key variables that will control the problem [40-42]. The sensitivity study will allow focusing the uncertainty analysis on those variables that have a major impact on the model response. In this work the sensitivity analysis has been based on the Monte-Carlo experimental design technique proposed by Martens et al. [43]. Using this kind of approach it is possible to reveal the more influential
input parameters without running the full factorial design. The subset is chosen so as to exploit the sparsity-of-effects principle to expose information about the most important features of the fault reactivation problem, while using a fraction of the effort required in a full factorial design in terms of experimental runs and resources. The Table 2 presents some of the 15 variables considered in the sensitivity analysis. The Monte Carlo numerical simulations show that eight parameters were the most influential ones in terms of the maximum injection pressure. The tornado plot presented in Fig. 12 shows the impact of the main variables on this problem according to the sensitivity analysis.
The vertical axis presents the variables considered in this study and the horizontal axis represents the weighted influence of those variables on the final result. The sum of those coefficient for all the variables considered in the analysis is equal to one. The sign is dictated by the way in which this variable impact on the final result. For example, the relative weight of C2 (cohesion of the external damage zone) is +0.3. This means that an increase of C2 is associated with an increment (due to the + sign) of the maximum allowable injection pressure and the relative impact (when compared with the other variables) is 30%. A negative sign implies that an increase in the value of this variable is related with a decrease of the maximum injection pressure. The other variables presented in this plot are as follows NIRES (the reservoir Poisson's ratio); ERES (the reservoir Young's modulus); AA2 (the friction angle of the external
Table 2
Mechanical and flow properties considered in the sensitivity analysis.
Property	Symbol	Uncertainty Characterization
Reservoir Poisson's ratio	NIRES	Uniform (0.2-0.4)
Reservoir Young's modulus (GPa)	ERES	Uniform (20-50)
External damage zone /' (°)	AA2	Uniform (14-30)
External damage zone cohesion	C2	Uniform (0.8-1.2)
(MPa)		
External damage zone Poisson's ratio	NI2	Uniform (0.2-0.4)
External damage zone Young's	E2	Uniform (5-10)
modulus (GPa)		
Defines Ko = NIRIG/(1 - NIRIG)	NIRIG	Uniform (0.3-0.34)
Specific rock weight (kg/m3)	RHO	Uniform (2200-2600)
Reservoir permeability (mD)	KRES	Uniform (20-80)
External damage zone k (mD)	KDAMAGE2	Uniform (1e-5-2e-5)
Core permeability (mD)	KCORE	Uniform (1e-5-2e-5)
Core cohesion (MPa)	CCORE	Uniform (0.2-0.8)
Core Poisson's ratio	NICORE	Uniform (0.2-0.4)
Core Young's modulus (GPa)	ECORE	Uniform (5-10)
Core / (o)	AACORE	Uniform (14-30)
Fig. 12. Tornado diagram showing the results of the sensitivity analysis of the most influential parameters respect to the pore pressure increment.
damage zone); NI2 (the external damage zone Poisson's ratio); E2 (the external damage zone Young's modulus); RHO (the specific rock weight, used to define the initial vertical stress); KRES, KDAM-AGE2 and KCORE are the permeabilities of the reservoir, damage zone 2 and core zones, respectively. NIRIG defines the Ko (i.e. the coefficient of earth pressure at rest is used to define the initial horizontal stress). Fig. 12 presents the eight more influential ones only (i.e. with a relative impact of |0.1%| or higher), plus the three permeabilities mentioned above.
Interestingly, the core and internal damage zone properties were not very influential on the model response. This is because the external damage zone (initially sealant) controls the failure of the fault zone. It is also remarkable that some variables with high uncertainties (such as permeability) have a minor impact on the maximum pore water pressure that can be injected. It can be observed (Fig. 12) that the impact of the permeability of the different materials is around 0.02%. This can be attributed to the fact that the maximum injection pressure that may induce the fault reactivation is mainly controlled by the mechanical problem. Variations on the permeability values may have an impact on the time at which the fault reactivation process may start, but its influence on the maximum predicted injection pressure is rather marginal.
The sensitivity analysis presented above was used to select the variables to be included in the uncertainty quantification study using both probabilistic and evidence theories.
There are different alternatives to combine evidence [29]. Typically, they can be subdivided in 4 groups: (i) consonant evidence, when multiples sources agree in all subsets ranges; (ii) consistence evidence, where at least there is one element that is common to all subsets; (iii) arbitrary evidence, there is no element common to all subsets, but some subsets may have elements in common; and (iv) disjoint evidence, where any two subsets have no elements in common. In this wok, it was assumed arbitrary evidence based on experts knowledge. Geologists and reservoir engineers working in this topic were asked to define the ranges of each uncertainty parameter and also the weights of each interval. Using the Dempster's rule of combination [32], the weights (or basic probability assignment, BPA) presented in Fig. 13 were obtained. This information is used in the evidence theory to calculate the belief and plausibility functions as explained in the following sections.
Uniform density functions were assumed for the analysis using the probability theory. Fig. 13 shows that the same values of the input parameters are adopted when applying the probability and evidence theories. However, it is different to say that a given model
Ranges •+• Weights (BPA's)
		Probability Theory		Evidence Theory	
Parameter	Symbol	Ranges	Density	Ranges	Weights
Reservoir Poisson's ratio	HIRES	(0.2-0.4)	Uniform	(0.2-0.28/ 0.28-0.34/ 0.34-0.4)	0.4/0.3/0.3
Reservoir Young's modulus (MPa)	ERES	(20000-50000)	Uniform	(20000-35000/ 35000-50000)	0.5/0.5
External damage zone friction angle (°)	AA2	(14-30)	Uniform	(14-22/22-30)	0.5/0.5
External damage zone cohesion (MPa)	C2	(0.8-1.2)	Uniform	(0.8-1.0/1.0-1.2)	0.5/0.5
External damage zone Poisson's ratio	NI2	(0.2-0.4)	Uniform	(0.2-0.28/ 0.28-0.32/0.32-0.4)	0.4/0.2/0.4
External damage zone Young's modulus (MPa)	E2	(5000-10000)	Uniform	(5000-8000/ 8000-10000)	0.6/0.4
KO = NIRIG/(1-NIRIG)	NIRIG	(0.3-0.34)	Uniform	(0.3-0.32/0.32-0.34)	0.5/0.5
Specific rock weight (Kg/m3)	RHO	(2300-2600)	Uniform	(2200-2300/ 2300-2600)	0.25/0.75
Fig. 13. Range of variation of the different model parameters incorporated in the uncertainty quantification analysis. For the case of evidence theory the weight are also presented. This data was defined after experts' opinions.
parameter falls in the interval [a,b], than to say that the value has an uniform distribution in [a,b].
6.1.	Belief and plausibility functions
The first work in evidence theory was done by Shafer [32] as an expansion of Dempster [31] approach. Ina finite discrete space, the evidence theory or Dempster-Shafer theory (DST) can be interpreted as the generalization of the probability theory for the case in which the probabilities are assigned to sets of values. In traditional probability theory, evidence is associated with only one possible event. In the DST, the evidence may be associated with multiple events (i.e. sets of events). As a result, the DST evidence is presented with a higher level of abstraction, i.e. without inferring a prior probability distribution of the input variables. When evidence is sufficient to assign probabilities to single events, the Dempster-Shafer model falls in traditional probabilistic formulation.
One of the most important features of the evidence theory is that the model is designed to handle different levels of accuracy (i.e. regarding the available data), and no further assumption is needed to represent them. It also allows the direct representation of the uncertainty responses of the system. For example if the input variable is imprecise, it can be characterized by a set or range of values and, consequently, the resulting output will also be a set (or a range of values).
Two core concepts in evidence theory of Basic Probability Assignment (BPA) and focal elements are introduced as follows:
(i)	the BPA for a set is the amount of probability that can be assigned to that set but cannot be decomposed into additional probabilities for subsets of that set; and (ii) focal elements are those sets that have nonzero BPAs. This then leads naturally to a clear distinction between an evidence space for a set XE of possible outcomes and a probability space for a set XP of possible outcomes.
Suppose a model represented by:
y=f (x)	(io)
where, x = [x1, x2, x3 xnX] is the vector of the input variables and y = [y1, y2, y3 ynY] is the vector associated with the model results. The probability theory requires that uncertainty associated with x should be defined by a density function (e.g. normal, triangular or uniform). To characterize the uncertainty in x is necessary to establish a probability space (XP,XP, mPX) where XP is the sample space of possible values of x; XP is the set of subsets of XP chosen appropriately (r-algebra); and mPX is a probability factor [29].
Thus, the uncertainty iny is estimated by a cumulative distribution function (CDF).
nS
prob(y 6 y) =	d(f (x)jy)dXdX ~ Xd(f(x)|y)/nS	(11)
XP	i=1
where yy is the expected value; nS is the number of samples; d(f(x)|y = 1) if f (x) 6 y and ¿Cf(x)|y) = 0 if fx) &gt; y.
Conversely the evidence theory does not impose a rigid structure on the uncertainty characterization. The evidence theory is a more flexible approach. To characterize the uncertainty in x it is necessary to define an evidence space: (XP,XP, mPX) — (XE,XE, mEX) where XP = XE is the sample space of possible values of x but XP—XE. In probability theory, if a subset A belongs to XP the probabilities associated to its complement Ac can be obtained from the additive axiom (i.e. the complementary sets of probabilities have to sum 1); while in evidence theory, the uncertainty in y is estimated by two uncertainty functions: belief and plausibility functions [29].
Bely(y 6 y) = X mEY(A)	(12)
AcAy
Ply(y 6 y) =	mEy(A)	(13)
AnAy-0
At an intuitive level, belief is a measure of the amount of information that indicates that a statement is true, and plausibility is a measure of the amount of information that indicates that a statement could be true.
If one considers the universe of all probability distributions that are consistent with the focal elements and BPAs that define an evidence space on a set XE. Then, for a subset U (i.e. y 6 y) of X, Bel(U) is the smallest possible probability that could occur for U over the set of indicated distributions and Pl(U) is the largest possible probability that could occur for U over the set of indicated distributions. Central to this is the idea that all probability distributions that do not violate the assumed properties of the evidence space are under consideration.
These concepts can be easily explained using the simple example as follows. Suppose that for the proposition that says ‘‘the fault is reactivated” there are a belief of 0.5 and a plausibility of 0.8. This means that the existing evidence allows to strongly state that the proposition is true with a confidence of 0.5. However, the evidence contrary to that hypothesis (i.e. ‘‘the fault is not reactivated”) only has a confidence of 0.2. The remaining mass of 0.3 (i.e. the gap between the 0.5 supporting evidence on the one hand, and the 0.2 contrary evidence on the other) means that the fault could either be reactivated or not. This interval represents the level of uncertainty based on the evidence in the system.
The definition of the belief and plausibility functions for the model output has been done through an optimization process. The main component of the optimization analysis is presented in the following section.
6.2.	Optimization algorithm to compute CPF and CBF
The belief and plausibility functions are defined for the input variables, x. For each input variable it is necessary to specify intervals (and the associated bounds) and Basic Probability Assignments (BPA). A brief description of the necessary steps to compute these two functions of the response is given below (based e.g. in the data presented in Fig. 13).
(1)	Combine intervals of each input variable into combination cells (for example, if a case has 2 input variables and each of these is defined by 3 intervals, the number of combination cells is equal to 9). The total number of such cells is given by:
niv
ncell = JJ nint(i)	(14)
i=i
where: ncell = total number of combination cells, nint(i) = number of intervals of input variable i, niv = number of input variables. The basic probability assigned to each cell is the product of the BPA's of the involved intervals.
(2)	Perform the realizations involving all the possible combinations defined above. For each cell solve the following two optimization problems have to be solved to find the lower (lb) and upper (ub) bounds of the response:
(lb) DPminj = Minimize DP(x)	(15)
(ub) DPmaXj = Maximize DP(x)	(16)
where DP(x) = reactivation pore pressure increment modeled through a previously trained neural network; (xl)j and (xu) are the bounds of the input variables intervals associated with combination cell j. The adopted neural network is explained in Section 6.3.
The ub and lb pressures obtained after the optimization process (i.e. Eqs. (15) and (16)) are used to plot the plausibility and belief functions. These results are not organized in a particular order. They need to be organized in ascending order, as it is explained below.
(3)	Sort in ascending order the lower bounds of the output response intervals obtained in Step 2. Once the data is organized, to obtain the plausibility function the lb pressure (Step 2) calculated per each combination (Step 1) is plotted against the corresponding cumulative probability. The belief function is plotted in a similar way, except that one should use the up pressures (also obtained in Step 2).
6.3.	Neural network
The optimization process explained in Section 6.2 involves a large number of realizations. Different options are available to perform these simulations. A numerical code can be used (e.g. CODE_BRIGHT for the analysis presented in this paper), or alternatively, any properly calibrated proxy (capable of performing the numerical analyses) can be used to reduce the high CPU-demand typically associated with the optimization process involving multiple variables. In this work an artificial neuronal network has been adopted as proxy.
Neural networks are composed of simple elements operating in parallel. Commonly, neural networks are adjusted in the way that a particular input leads to a specific target output. The network is adjusted based on a comparison between the output and the target until these values become similar. Different types of ANNs have been proposed for forecasting and modeling engineering problems (e.g. [44-47]). A feedforward neural network with back propagation was trained in this study. The ANN has one hidden layer of
10 sigmoid neurons followed by an output layer of linear neurons. The linear output layer is most often used for function fitting (or non-linear regression) problems.
When training multilayer networks, the general practice is to first divide the data into three subsets. The first subset is the training set, which is used for computing the gradient and updating the network weights and biases. The second subset is the validation set. The error on the validation set is monitored during the training process. When the network begins to over fit the data, the error on the validation set typically begins to rise. The network weights and biases are saved at the minimum of the validation set error. The test set error is not used during training, but it is used to compare different models. It is also useful to plot the test set error during the training process. If the error on the test set reaches a minimum at a significantly different iteration number than the validation set error, this might indicate a poor division of the data set.
As discussed earlier, 2000 coupled simulations had been performed to create an extensive database. The ratios for training, testing and validation are 0.7, 0.15 and 0.15, respectively. Fig. 14 presents the fit for these three subsets and also the global fitting: (a) results of the training; (b) validation fitting; (c) test results and (d) the global fitting. Based on these results, the performance of ANN for this problem can be considered very satisfactory and, therefore, this ANN can be used as a proxy to save CPU-time.
7.	Results
This section focuses on the use of the evidence theory for estimating the maximum injection pressure in problems related to oil production. Considering the limited (and in some cases lack of) information regarding many parameters involved in this kind of analysis, it is highly recommendable to define via an uncertainty
Test: R=0.98535
All: R=0.98744
analysis the maximum pressure at which a fluid can be injected in a reservoir.
Fig. 15a presents the main results comparing the use of probability and evidence theories for uncertainty quantification associated with the fault reactivation problem presented in Section 5. The cumulative distribution function (CDF, Eq. (13)) was obtained after the application of the probability theory, computed according to [6]. The evidence theory was used to compute the cumulative belief function (CBF, Eq. (14)) and the cumulative plausibility function (CPF, Eq. (15)). Complementary cumulative distribution function (CCDF), complementary cumulative belief function (CCBF) and complementary cumulative plausibility function (CCPF) were directly obtained from the CDF, CBF and CPF, respectively. Fig. 15a shows the results in terms of probability of fault reactivation (i.e. cumulative probability to be lower) and it should be used when one wants to look at the probability or belief or plausibility that a give increment of the injection pressure will reactive the fault. Fig. 15b presents the outputs of the same analyses but in terms of probability of fault reactivation (i.e. complementary cumulative results, probability to be higher), and it should be used to analyze the complementary case, that is, when one wants to check that the fault will not reactivate for a given increment of the injection pressure. The complementary cumulative results are more appropriate when the likelihood of exceeding a value is under consideration.
In this work the activation criterion is a function of the total plastic work (Eq. (8)). When in any of fault regions the plastic work exceeds a given maximum value (set as a tolerance in this
analysis), it is considered that the sealing capability of the fault is breached and any additional pore pressure increment could trigger the fault reactivation process. For example, if the maximum injection pressure obtained from the deterministic case (i.e. Pl = 3.2 MPa, Section 4) is analyzed in the context of the results presented in Fig. 15, according to the classical probability theory, it can be concluded that the probability of a fault reactivation when the fluid is injected at 3.2 MPa is around 0.20. However, according to the evidence theory the probability of a fault reactivation can be between 0.65 (as a maximum value) and 0.00 (as a lower bound). According to the classical probability theory, the probability of the fault to remain stable is 0.8 if an increment of the injection pressure equal to 3.2 is applied, and the associated degree of belief obtained with the epistemic theory is 0.35 (Fig. 15b). It can be seen that the evidence theory provides more complete information to assist the decision maker.
One can also use these plots to analyze the maximum pressure that can be applied based on a given probability, for example let's assume that 0.10 is the largest possible probability that an injection project manager can take respect to the reactivation of a fault. It is important to remember (see Section 6.1) that, Bel(U) is the smallest possible probability that could occur for U over the set of indicated distributions and Pl(U) is the largest possible probability that could occur for U over the set of indicated distributions. Therefore, considering first the CDF, the increment in the injection pressure can be as higher as 3.0 MPa (i.e. intersection of 0.10 probability with CDF). Looking at the evidence theory (i.e. CPF and CBF
curves), one may conclude that for the same given probability (i.e. 0.10) for pressure increments equal or lower than 2.0 MPa the fault will reactivate with a plausibility of 0.10. Note that for that increment of injection pressure (i.e. 2.0 MPa) the degree of belief that the fault will reactivate is equal to 0.0. Similar analyses can be done looking at the complementary cumulative probabilities (i.e. Fig. 15b). It can be interpreted that the design of the injection pressure based on the combination of CBF and CPF curves will be, initially, conservative (and perhaps safer) when compared respect to the ones based on CDF curve.
It is worth to mention that this has been a first attempt in the application of this methodology to assess the maximum injection pressure in reservoirs. Obviously improvements can be introduced in this kind of analysis, amongst others: establishing correlations between the parameters adopted in the study; include the influence of more complex initial stress conditions; and performing multiphase analysis involving two fluids. In any case, the results presented in this section are very promising and encourage the use of this kind of approach to design the maximum injection pressure when assisting oil production or storing CO2.
8.	Conclusions
This paper focuses on the estimation of the maximum allowable injection pressures such the mechanical integrity of a pressurized reservoir is not compromised. In particular, the problem of fault reactivation during oil production assisted by injection of a fluid at high pressure has been studied in detail. Relievable analysis in this area should account for the inherent uncertainties associated with the materials involved in these kinds of problem. In this work, a framework based on the evidence theory has adapted to the particular condition of this problem. When dealing with epistemic variables (as in this case) it has been shown that this methodology is an excellent alternative to more traditional methods based on probabilistic method. Probabilistic methods require the definition of prior density functions related to the uncertainties associated with the parameters contemplated in the numerical analysis. This requirement may be difficult to meet in cases dealing with no (or limited) information about these density functions. As shown, the evidence theory only requires instead the definition set of values for the variables and the corresponding weighing factor.
The case analyzed in this paper is based on information gathered from an actual oil production field. A fully coupled hydromechanical formulation together with a trained ANN have been adopted for performing the analyses involved in the different stages of a UQ study. Two distinct curves are obtained from this analysis: the cumulative plausibility function (CPF) and the cumulative belief function (CBF). In addition to the analysis based in the evidence theory, a conventional probabilistic analysis and a deterministic study (using the best estimate values for all the variables) have been performed.
It can be concluded that information provided by the evidence theory is more general and complete when compared against the probabilistic and deterministic results. The proposed approach it has been shown very reliable and promising for its application in this type of problems. More complex analyses involving multiphase flow, correlation between parameters, 3D, and more complex geological conditions are underway.
Acknowledgements
The authors acknowledge the financial support from PETROBRAS, CNPq (Brazilian National Research Consul) and Foundation CMG (Chair on Reservoir Simulation at UFPE). It is also gratefully acknowledged by the authors the assistance provided by
the ‘Reservoir Geomechanics Group’ at PETROBRAS to perform this work. The expert opinion of the members of this group has been instrumental to develop the data base use in this study; special thanks to (amongst others): Alvaro Maia da Costa, Luis Carlos de Sousa Jr., Claudio Amaral, Claudio Lima and Erick Slis. Finally, we would like to thank the reviewers of this paper for their comments and suggestions.
References
[1]	Goodarzi S, Settari A, Keith D. Geomechanical modeling for CO2 storage in Nisku aquifer in Wabamun Lake area in Canada. Int J Greenh Gas Control 2012;10:113-22.
[2]	Ducellier A, Seyedi D, Foerster E. A coupled hydromechanical fault model for the study of the integrity and safety of geological storage of CO2. Energy Proc 2011;4:5138-45.
[3]	Perera MSA, Ranjith PG, Choi SK, Bouazza A. A parametric study of coal mass and cap rock behaviour and carbon dioxide flow during and after carbon dioxide injection. Fuel 2013;106:129-38.
[4]	Kojic MC, Cheatham JB. Theory of plasticity of porous media with fluid flow. R Soc Petrol Eng J 1974;14(3):263-70 [Int J Rock Meeh Min Sci Geomech Abstr].
[5]	Huang J, Griffiths D, Fenton G. Probabilistic analysis of coupled soil consolidation. J Geotech Geoenviron Eng 2010;136(3):417-30.
[6]	Pugachev VS. Probability theory and mathematical statistics for engineers. Pergamon Press; 1984.
[7]	Blanchard BS, Fabrycky WJ. Systems engineering and analysis. Pearson Prentice Hall; 2006.
[8]	Helton JC, Oberkampf WL, Johnson JD. Competing failure risk analysis using evidence theory. Risk Anal 2005;25(4):973-95.
[9]	Bai YC, Han X, Jiang C, Liu J. Comparative study of metamodeling techniques for reliability analysis using evidence theory. Adv Eng Softw 2012;53:61-71.
[10]	Dodagoudar GR, Venkatachalam G. Reliability analysis of slopes using fuzzy sets theory. Comput Geotech 2000;27(2):101-15.
[11]	Schweiger HF, Peschl GM. Reliability analysis in geotechnics with the random set finite element method. Comput Geotech 2005;32(6):422-35.
[12]	Cowie PA. Growth of faults by accumulation of seismic slip. J Geophys Res 1992;97(B7):11085-95.
[13]	McGrath A. Damage zone geometry around fault tips. J Struct Geol 1995;17(7):1011-24.
[14]	Petit JP. Criteria for the sense of movement on fault surfaces in brittle rocks. J Struct Geol 1987;9(5-6):597-608.
[15]	Storti F, Salvini F. The evolution of a model trap in the central Apennines, Italy: fracture patterns, fault reactivation and development of cataclastic rocks in carbonates at the Narni anticline. J Petrol Geol 2001;24(2):171-90.
[16]	Chapman RE. Petroleum geology. Amsterdam, New York: Elsevier; 1983.
[17]	Cappa F, Rutqvist J. Modeling of coupled deformation and permeability evolution during fault reactivation induced by deep underground injection of CO2. Int J Greenh Gas Control 2011;5(2):336-46.
[18]	Olivella S, Carrera J, Gens A, Alonso E. Nonisothermal multiphase flow of brine and gas through saline media. Trans Porous Media 1994;15(3):271-93.
[19]	Olivella S. Numerical formulation for a simulator (CODE_BRIGHT) for the coupled analysis of saline media. Eng Comput 1996;13(7):87-112.
[20]	Gillemot L. Criterion of crack initiation and spreading. Eng Fract Mech 1976;8(1):239-53.
[21]	Sanchez M, Gens A, Guimarães L. Thermal-hydraulic-mechanical (THM) behaviour of a large-scale in situ heating experiment during cooling and dismantling. Can Geotech J 2012;49(10):1169-95.
[22]	Sanchez M, Gens A, Guimarães L, Olivella S. Implementation algorithm of a generalised plasticity model for swelling clays. Comput Geotech 2008;35(6):860-71.
[23]	Gens A. A full-scale in situ heating test for high-level nuclear waste disposal: observations, analysis and interpretation. Geotechnique 2009;59(4):377-99.
[24]	Rutqvist J. Status of the TOUGH-FlAC simulator and recent applications related to coupled fluid flow and crustal deformations. Comput Geosci 2011;37(6):739-50.
[25]	Neves MC, Paiva LT, Luis J. Software for slip-tendency analysis in 3D: a plug-in for Coulomb. Comput Geosci 2009;35(12):2345-52.
[26]	Gambolati G, Teatini P, Tomasi L. Stress-strain analysis in productive gas/oil reservoirs. Int J Numer Anal Methods Geomech 1999;23(13):1495-519.
[27]	Costa AM, et al. Setting the maximum water injection pressures. PETROBRAS; 2007.
[28]	Savage L. The foundations of statistics. J Consult Psychol 1955;19(3):237-45.
[29]	Helton JC, Johnson JD, Oberkampf WL, Storlie CB. A sampling-based computational strategy for the representation of epistemic uncertainty in model predictions with evidence theory. Comput Methods Appl Mech Eng 2007;196(37-40):3980-98.
[30]	Helton JC, Johnson JD, Oberkampf WL, Storlie CB. A sampling-based computational strategy for the representation of epistemic uncertainty in model predictions with evidence theory. SAND2006-5557. Albuquerque, NM: Sandia National Laboratories; 2006.
[31]	Dempster AP. Upper and lower probabilities induced by a multivalued mapping. Ann Math Stat 1967;38(2):325-39.
[32]	Shafer G. A mathematical theory of evidence. Princeton University Press; 1976.
[33]	Yager RR. Arithmetic and other operations on Dempster-Shafer structures. Int J Man-Machine Stud 1986;25(4):357-66.
[34]	Curcuru G, Galante GM, La Fata CM. Epistemic uncertainty in fault tree analysis approached by the evidence theory. J Loss Prevent Proc Ind 2012;25(4):667-76.
[35]	Khalaj M, Makui A, Tavakkoli-Moghaddam R. Risk-based reliability assessment under epistemic uncertainty. J Loss Prevent Proc Ind 2012;25(3):571-81.
[36]	Wang P, Lu Z, Tang Z. A derivative based sensitivity measure of failure probability in the presence of epistemic and aleatory uncertainties. Comput Math Appl 2013;65(1):89-101.
[37]	Helton JC, Johnson JD, Oberkampf WL. An exploration of alternative approaches to the representation of uncertainty in model predictions. Reliab Eng Syst Saf 2004;85(1-3):39-71.
[38]	Helton JC, Johnson JD, Oberkampf WL, Sallaberry CJ. Representation of analysis results involving aleatory and epistemic uncertainty. Int J Gen Syst 2010;39(6):605-46.
[39]	HeltonJC,JohnsonJD. Quantification of margins and uncertainties: alternative representations of epistemic uncertainty. Reliab Eng Syst Saf 2011;96(9):1034-52.
[40]	Saltelli A, Chan K, Scott EM. Sensitivity analysis. New York, NY: Wiley; 2000.
[41]	Helton JC, Johnson JD, Sallaberry CJ, Storlie CB. Survey of sampling-based methods for uncertainty and sensitivity analysis. Reliab Eng Syst Saf 2006;91(10-11):1175-209.
[42]	Saltelli A, Ratto M, Tarantola S, Campolongo F. Sensitivity analysis for chemical models. Chem Rev 2005;105(7):2811-28.
[43]	Martens H, Dijksterhuis GB, Byrne DV. Power of experimental designs, estimated by Monte Carlo simulation. J Chemomet 2000;14(5-6):441-62.
[44]	Chen S, Cowan CFN, Grant PM. Orthogonal least squares learning algorithm for radial basis function networks. IEEE Trans Neural Networks 1991;2(2):302-9.
[45]	Wasserman PD. Advanced methods in neural computing. New York: Van Nostrand Reinhold; 1993.
[46]	Firat M, Turan ME, Yurdusev MA. Comparative analysis of neural network techniques for predicting water consumption time series. J Hydrol 2010;384(1-2):46-51.
[47]	Obrzud RF, Vulliet L, Truty A. Optimization framework for calibration of constitutive models enhanced by neural networks. Int J Numer Anal Methods Geomech 2009;33(1):71-94.
APENDICE B - Manuscrito submetido ao JPSE
Ouatií'icacdo de Incertezas Aplicada a Geomecanica de Reservatórios
Página 129
SUBMISSION TO:
“Journal of Petroleum Science and Engineering”
TITLE:
Defining Maximum Injection Pressures in an Offshore Petroleum Reservoir Using the
Evidence Theory for Uncertainty Quantification
AUTHORS:
12 3	1	2
Leonardo C. Pereira ’ ’ , Marcelo Sánchez , Leonardo J. N. Guimarães , Erick S. R.
32
Santos and Bernardo Horowitz
AFFILIATION:
1	Zachry Department of Civil Engineering. Texas A&amp;amp; M University, USA.
2	Department of Civil Engineering, Federal University of Pernambuco, Recife, Brazil
3	PETROBRAS - Petróleo Brasileiro S.A, Research and Development Center (CENPES), Brazil
CORRESPONDING AUTHOR:
Mr. Leonardo Cabral Pereira: leonardocabral@petrobras.com.br
Petroleum Engineer
Research and Development Center (CENPES).
Petróleo Brasileiro S.A., Brazil.
Cidade Universitaria - Ilha do Fundao | Rio de Janeiro, RJ 21941-915
Tel. +1 979 587-8056 | (+55 21) 3865-6550 | Fax: (+55 21) 3865-6828
Submission Date:
August 28, 2014
Abstract
The injection of water or carbon dioxide at high pressures is a common practice to sustain oil production. An important piece of information required by the reservoir manager is the evaluation of the maximum pressure upon which fluids can be injected without inducing fault reactivation. Different approaches can be used to determine that maximum allowable injection pressure, ranging from empirical to numerical analyses, and including analytical solutions. A key aspect related to these types of analyses is the estimation of the in-situ stresses (which depend mainly on the density and thickness of the different materials involved in the analysis), and strength properties of the materials (i.e. friction angle and cohesion). Huge uncertainties are generally associated with these two components of the analysis (i.e. in-situ stresses and rocks strength). We propose a relatively simple analytical method to estimate the maximum allowable injection pressure that handles the uncertainties associated with this problem using a non-probabilistic method based on the evidence theory. In this work we have combined: i) real data from an offshore oil field located in Campos Basin (Brazil), ii) with a criterion to define the fault mechanical properties based on the information provided by the well logs, and iii) with the use of the evidence theory for uncertainty quantification. The final outcome of this study is a risk chart that assists during the decision making process to estimate maximum allowable injection pressure and the potential risk associated with it. This tool can also help to asses in which situations a more sophisticated geomechanical (numerical) model is necessary to make a decision.
Keywords: Maximum injection pressure; Fault Reactivation; Uncertainty quantification; Evidence Theory.
1. Introduction
Reservoir seals are natural traps that prevent the upward migration of hydrocarbons and thus ensure the accumulation of these hydrocarbons in the reservoir rock. The correct understanding of how a reservoir unit is hydraulically bounded (or sealed) is crucial for the development of exploitation projects. Robust production plans should not allow reservoir seal violations, with the associated uncontrolled fluid loss and its impacts on production costs and on environmental damage. Natural discontinuities in the rock mass in the form of fault planes (or fault zone) can jeopardize the hydraulic integrity of oil reservoirs. These fractures are very common in sedimentary basins, they are naturally sealed and act as a barrier for the migration of fluids. However, natural events or human actions may induce seal rupture with the possibility of connecting the reservoir with other rock blocks and finally with the surface.
The reactivation of geological faults induced by natural events has been reported in the literature. As explained by Cappa et al., 2009 (Cappa and Rutqvist, 2011) natural CO2 degassing through active fault zones was observed during the 1965-1967 Matsushiro earthquake swarm in Central Japan. A similar phenomenon was observed during the 1997 Umbria-Marche seismic sequence in northern Italy (e.g. Miller et al., 2004). In those two event it was suggested that a pressurized CO2 source was the main triggering and driving mechanism for the earthquake ruptures. Therefore, it is important to assess the potential for seismic events and to estimate how such an event might impact sequestration efficiency (e.g. Hawkes et al., 2004; Rutqvist et al., 2007; Chiaramonte et al., 2007; Morris et al.,
2009). The reactivation of geological faults induced by human actions has also been discussed in several publications. As for example, the sequence of unusual maximal magnitude and seismicity rate that struck offshore Spain during September and October 2013 (e.g. Cesca al. 2014). These events were detected and considered to be related to a coincident gas injection operation on Castor platform. Reactivated natural faults near the injected reservoir are believed to be a source of such a
short-term seismic activity (e.g. Cesca al. 2014). Pereira et al. (2014) also studied the link between fluid injection at high pressure and fault reactivation. Fully coupled hydro-mechanical analyses were performed using the finite element program CODE_BRIGHT (Olivella et al., 1996). Synthetic cases aimed at reproducing oil-production problems were modeled, sensitivity analyses were performed and the uncertainties associated the material parameters were investigated using probabilistic and no-probabilistic methods investigating the effect of different assumptions and material properties, looking in particular at the effect of permeability, initial stresses and strength parameters. During these analyses we realized that was essential to incorporate a measure of the uncertainties in the numerical analyses. The main results of this study were recently published in Pereira et al., (2004), as we stated in the original manuscript.
In this work we propose a methodology based on the use of real data and analytical solutions to suggest a preliminary scheme able to assess the risk associated with fluid injection projects.
Recent developments in numerical modeling have allowed a better understanding of the mechanical and hydraulic behavior of oil reservoirs when water or carbon dioxide are injected. For example, Rutqvist (2011) presented the modeling of crustal deformations caused by deep underground fluid movements and pressure changes as a result of both industrial activities and natural events. From the geomechanical analysis it was concluded that the cause of the Matsushiro Earthquake Swarm was an overpressure resulting from the upwelling CO2-rich fluid. Earthquake mechanisms were attributed to shear failure initiated by reduced effective stress on pre-existing fracture planes within and near the two main faults. Teatini et al., (2014) assessed the maximum CO2 that can be sequestered into a 2,000 meters deep multi-compartment reservoir seated in the off-shore northern Adriatic sedimentary basin. They used a 3D finite element geomechanical model to simulate the ground surface displacement, the fault reactivation, and the possible mechanical failure of both: the injected
formation and the cap rock. From this work it can be concluded that the evaluation of the Mohr-Coulomb failure criterion and the geomechanical properties of faults are very important factors for the correct prediction of the mechanical behavior of the injected formation.
The study of the fault reactivation process by means of analytical and semi-analytical solutions has been discussed extensively in the literature. For example, Soltanzadeh and Hawkes (2008) presented semi-analytical solutions for reactivation analysis based on Coulomb's failure criterion for horizontal and dipping reservoirs of finite depth with rectangular and elliptical cross-sections assuming plane strain conditions. It was also shown that fault reactivation potential is dependent on reservoir geometry and dip angle. Neves et al. (2009) developed an interactive graphic tool to perform sliptendency analysis. They presented focal mechanism solution associated with the predicted slip direction for a given fault.
A key characteristic of the fault reactivation problem is the huge range of variation of material parameters in the highly heterogeneous fault zone. Furthermore, the lack of reliable experimental data associated with the materials in the fault zone is quite common. Therefore, a formal framework is essential for handling the uncertainties associated with this kind of analysis. Uncertainty quantification is the process of determining the effect of input uncertainties on response metrics of interest. The input uncertainties can be characterized as either aleatory uncertainty, which has irreducible variabilities inherent in nature, or epistemic uncertainties, which are reducible uncertainties resulting from a lack of knowledge. When dealing with epistemic variables, non-probabilistic methods generally based on the specification of intervals data are more appropriate.
In the analyses presented in this work, the parameters uncertainties are mainly related to the lack of (or limited) information about them. So, the uncertainty is not aleatory but epistemic. Therefore, in
this work we adopted a framework based on the evidence theory (i.e. a non-probabilistic method) to deal with the uncertainties associated with the main variables involved in this problem.
Probability theory based on the Bayesian school has been widely used in the past (e.g. (Pugachev, 1984), (Blanchard and Fabrycky, 2011)), but more recently fuzzy methods and evidence theory are regarded as those of the legitimate extension of classical probability theory (e.g. (Helton et al., 2005), (Bai et al., 2012)). In this field, Althuwaynee et al. (2012) used the Dempster-Shafer theory of evidence model to prepare landslide susceptibility maps. The results of this study indicated that the evidential belief function model can be effectively used in preparation of landslide susceptibility maps. Feizizadeh et al. (2014) developed a methodology based on evidence theory to assess the uncertainties associated with two multicriteria decision analysis techniques, namely analytical hierarchical process and ordered weighted averaging implemented in GIS. They concluded that the multicriteria decision analysis methods can provide an effective tool for spatial decision making systems. Pereira et al. (2014) used evidence theory and coupled hydro-mechanical finite element analysis to estimate the maximum injection pressure in a hypothetical oil production problem.
In this paper we focus on the evaluation of the maximum allowable injection pressure to assist oil production in pressurized oil reservoirs. Deterministic, probabilistic, and non-probabilistic analyses have been performed based on a typical fault reactivation problem. The case study is based on actual data obtained from a real oil field in Brazil. We used the evidence theory to quantify the uncertainties typically associated with this complex problem. We propose a novel risk chart that help the decision making process when assessing the maximum injection pressure in problems involving enhanced oil production by fluid injection. The methodology we are proposing does not replace the information provided by numerical models, but complement it. It can also be used to know in what cases and
under which situations (e.g. high risk of fault reactivation) more refined numerical models are necessary to make a decision.
The paper is divided in three main sections. Firstly, the field data and the analytical model are briefly described. Secondly, the case study is analyzed using probabilistic and non-probabilistic methods. Finally, the main conclusions of this research are presented.
2. Field Data
The case study corresponds to an Oligocene-Miocene turbidite located in the Campos Basin in Brazil. Figure 1 shows the region under study together with the main reservoirs. In this study, the reservoir is a turbidite deposit mostly associated with the slope and continental rise. The eastern portion is dominated by lobe-type deposits whose successive coalescence resulted in sand bodies with great lateral continuity, composed mainly of fine to medium-grained massive sandstones. Figure 2 shows a schematic stratification related to the reservoir under study. The porosity of this unconsolidated reservoir is around 0.3 and the horizontal permeability can be higher than 1 [Darcy] (i.e. 9.87e-13 m2) (Sousa et al., 2010). The reservoir is located in a prevalent normal faulting regime area, but strike-slip regime can occur in some specific regions. The cap rock is primarily formed by shale (Sousa et al., 2010).
The water depth in this section ranges from 800 [m] up to 2000 [m] and the total vertical depth of the reservoir under study is between 2800 and 3500 [m]. The reservoir is initially oil sub-saturated with 21oAPI, and the average oil viscosity under reservoir conditions is about 3 [centipoises]. Seawater injection is used for secondary oil recovery. In this work we focus on the evaluation of the maximum allowable fluid pressure that can be injected without inducing fault reactivation.
Figure 1 - Campos basin reservoirs (after Bruhn 2003).
2900m	Gamma Ray		
	-I	■	
3000m			
3050m	p		
□ Calcilutite
Marls
Shale
Sand
Figure 2 - Schematic representation of the reservoir stratigraphy.
The sealant condition of a fault may change because of geological processes or human actions. For example, typical operations associated with oil production, such as H2O or CO2 injections, generally result in the redistribution of in-situ stresses, which may lead to the reactivation of nearby faults and subsequent slips (Chapman, 2000). The mechanical integrity of the reservoir depends on a number of factors including shear and tensile strengths of the formation rock and changes in fluid pressures (which in turn induce changes in effective stresses), amongst others. If the injection of fluids (i.e. H2O or CO2) at high pressures is such that the associated changes in effective stresses induce yielding, significant stress re-distribution and plastic deformations are anticipated in the reservoir and fault zone. Those changes may lead to the reactivation of pre-existing discontinuities (and/or the
formation of new fractures) and the loss of the fault sealing capability. Once the fault is re-activated, fluids may migrate between blocks and also from the reservoir to shallower depths through the reopened discontinuity.
In some occasions the geomechanical data associated with this kind of problems can be quite limited (i.e. lack of information or few well log data or laboratory data from the area of interest). In this case the quantification of in-situ stresses and rock properties relies on data from similar environments and expert judgment. The other extreme corresponds to the situation in which extensive geomechanical information about the site is available, and need to be reduced to extract meaningful data. In any of these two cases (or intermediate ones), a consistent method for the data compilation, quantification of uncertainties and the subsequent decision making process is needed. The case study analyzed in this work is closer to the second scenario (i.e. good field data is available), and we have adopted a method based on the evidence theory (explained in Section 4) to deal with the uncertainties associated with this problem. A similar approach was adopted by us before (i.e. Pereira et al., 2014) for a case study in which limited data was available.
We reduced well log data from 20 wells to determine the in-situ stresses and rock properties. Figure 3 presents in a local (normalized) coordinate system in the horizontal plane, the position of the wells and the fault under study.
We chose this fault because it crosses the overburden from the reservoir until the sea bed. An indispensable previous step to perform the fault reactivation analysis is to estimate the in-situ stresses and the strength parameters (i.e. cohesion and friction angle) of the materials involved in the problem. The estimation of in-situ stresses and strength parameters was based on well log correlations. As explained in sections 2.1 and 2.2 respectively.
25
5
O'-------------1--------------1-----------------1--------------1
0	5	10	15	20
Normalized ’X’ (km)
Figure 3 - Normalized coordinate system showing wells location and fault plane under study
2.1	In-situ stress
Our calculations are based on the common assumption that one principal stress (the major one in the case of normal fault regime) is vertical and the others two are horizontal. This is a valid hypothesis in tectonically inactive areas (as this one) or areas which have been relaxed from previous tectonic activity. In active areas this assumption may not be valid, and the stress field can be significantly distorted (Fj^r et al., 1992).
To perform the fault reactivation study the estimation of the overburden stress and minimum horizontal stress is required. The estimation of the maximum horizontal stress (i.e. intermediate principal stress) is not crucial when dealing with a normal fault regime, as in this case. In the following sections the methodology we adopted to estimate those variables is presented in detail.
The density of the sediments and rocks is the key property to estimate the overburden stress. In this type of analysis, the materials density is generally estimated from indirect methods. The density tools are active gamma ray techniques that use the Compton scattering of gamma rays to measure the electron density of the formation. By appropriate lithology corrections, the electron density is converted to mass density with reasonable accuracy. The density integrated over the vertical depth of the well is usually considered to give a good estimate of the vertical (principal) stress (at least in areas of low tectonic activity).
A good mechanical contact between the tool pad and the wellbore wall is necessary in order to avoid fluid-induced gamma ray attenuation (Ellis and Singer, 2007). The conventional tools provide a limited extension when pushing the pads towards the wellbore walls, and therefore, are designed for certain drill bit ranges typically below 31.12 [cm] (12 %”). As a consequence, the shallow sections drilled with larger bits are rarely logged. Wellbore wall condition and enlarged holes intervals tend to affect the density data, therefore information from these shallow zones should be disregarded as well. These limitations often lead to incomplete, and sometimes, short density log data. The extension of density data towards the unavailable zones followed two main steps (Sousa et al., 2010):
i)	first, for those intervals in which we have information of the compressional wave velocity (Vp [ft/^s]), we correlate it with the density (pb in [g/cc]) using the following power law:
Pb = aVP	(1)
where a and b were the fitting parameters. We performed this ‘cross plot' fitting for each one of the analyzed well.
ii)	second, for those intervals in which neither density nor acoustic data were available (i.e. mainly shallow intervals) we extrapolated the density value following the power law presented in Eq. 2. We
adopted p = 1.7 as the mud line bulk density and the coefficients c and d were fitted for each well. In the reservoir under study, the fitted d value was d = 1.2.
pb = p+cDd = 1.7+cD1.2	(2)
where D is the depth.
The procedure outlined above was applied to each one of the 20 wells presented in Figure 2. It is a common practice to use all the available data to obtain an expression to estimate the vertical stress in the area of study. Eq. 3 fits the integrated overburden data after down sampling it to equal intervals.
(Tv = 0.006895 (1.4566 zM + 2.5634 D/06 zM"0-036 )	(3)
where&amp;lt;5v [MPa] is the total vertical stress and zM [m] is the mud line depth .
The first term in Eq. 3 stands for the sea water pressure at the mud line. The second term adds the contribution of each layer according to the overburden thickness (Ds) in a power fit. The water depth correction (i.e. zM”°'036) accounts for the huge variations in water depth observed throughout the field. Figure 4 illustrates these variations by means of contours of the water depth in the area under study.
Normalized ’X’ (km)
Figure 4 - Water depth (in meters) superimposed on normalized coordinate system including wells
and fault plane under study
Figures 5 a) and b) presents the variation of stresses (total and effective) and pore-pressures along the total vertical depth for two different wells, 1 and 8, respectively. It is worth mentioning that the variation of the seabed level affects the effective vertical stress acting in the oil reservoir.
Total Vertical Depth (meters)	Total Vertical Depth (meters)
Stresses I Porepressure (MPa)
Figure 5 - Calculated vertical stresses and porepressure for Well 01 (a), and Well 08 (b)
In tectonically inactive areas or in regions which have been relaxed from previous tectonic activity or extensional basins under gravitational load, the minimum horizontal stress (ah) can be fairly well estimated as follows:
U
=----m -aPp) + aPP
1-u
(4)
where Pp is the pore-pressure, a is the Biot's coefficient and U is the Poisson's ratio. We estimated the Poisson's ratio based on well log correlation (Montmayeur, 1985) founded on the theory of elastic wave propagation, as follows:
U = 1-
1
2 - 2(VS / Vp )2
(5)
where Vs and Vp are the shear and compressional wave velocities , respectively.
Equation 4 assumes a linear-elastic body undergoing a one-dimensional (vertical) compaction. If tectonic stresses are acting, this equation represents a lower bound for the minimum horizontal stress (Ch ; which is the minor principal stress in this problem). The horizontal stress could be better
estimated based on microfracs, minifracs or leakoff tests (LOTs) data (Montmayeur, 1985). Generally, LOTs are much more abundant than micro and minifracs. Microfracs are more reliable because they are performed in a smaller pressure chamber, as opposed to the whole cased well section which is left open in the LOTs. Microfracs produce small fractures in desirable zones (generally in the reservoir section), while LOTs are performed just below the casing shoe; which commonly lies in the reservoir cap rock in the deep casings. For the region under study, only 3
minifracs are available, whereas more than 50 LOTs were reported in the daily rig summaries. The
analysis of these data provides an average effective K0 around 0.64 (i.e.
).
Figure 6 presents the variation of minimum horizontal stresses/pore-pressure in depth for two different wells, 13 and 20 respectively. The proposed correlation presented above in Equation 6 for the total minimum horizontal stress is also shown in Figure 6.
(yh =(1.4436 zM+2.5057 Ds) 0.006895	(6)
where&amp;lt;5h [MPa] is the total minimum horizontal stress.
Total Vertical Depth (meters)	Total Vertical Depth (meters)
Stresses I Porepressure (MPa)
Figure 6 - Calculated horizontal stresses and porepressure for Well 13 (a), and Well 20 (b)
We adopted the elastoplastic Mohr-Coulomb model to perform the fault reactivation analysis (more details about the analysis are provided in Section 3). The following strength parameters are associated with this model: the intercept cohesion (c') and the internal effective friction angle (^').We estimated these two parameters based on well log correlations (Montmayeur, 1985). We adopted Lal (1999) method to estimate the effective friction angle of the different rocks involved in
this problem by using the equation as follows:
0
' = arcsin
7
(7)
where Vp is in [Km/s]. In the reservoir under study sandstones appear in thin layers with different clay contents. Carbonates also appear in thin sections and its influence on the overall rock behavior can be considered insignificant. To estimate the cohesion we adopted the MECHPRO (computer program for determine the mechanical properties of the formation) expression (Bagheri and Settari, 2005). In this approach the unconfined compressive strength (qu) is calculated first through,
q, = 1.884X10"20pb Vp‘ [j-U] (1 -2u)(1 + 0.78	)	(8)
where qu is in [MPa], pb in [kg/m3], and Vp in [m/s]. Vclay is the compressional velocity in the layer with clay contents. Then the effective cohesion is calculated through,
c
Qu
2tan (z]
(9)
We used the correlations above to estimate c' and 0’ for all the wells considered in this study. Figures 7 and 8 show typical distribution in depth of c ’ and O found in two different wells for each strength property (i.e. wells # 7 and 10 for cohesion (Figure 7) and wells # 1 and 20 for friction angle (Figure 8)). It is noticeable the great variability of the strength parameters observed in this problem.
Total Vertical Depth (meters)	Total Vertical Depth (meters)
Cohesion (MPa)
Figure 7 - Plot showing the variation of cohesion in depth according to Eq. 9
Total Vertical Depth (meters)	Total Vertical Depth (meters)
Figure 8 - Plot showing the variation of friction angle in depth according to Eq. 6
The injection pressure that may trigger the fault reactivation will depend on the initial stress state and strength parameters of the rocks involved in this study. From the figures above (i.e. Figures 5 to 8) it can be observed the great level of uncertainty associated with the strength parameters computed in this case study. It is clear that a formal framework for dealing with the uncertainties associated with this problem is needed. In the next section we present the approach adopted in this study.
3. Analytical modeling of a fault reactivation problem
Different failure criteria have been proposed for rocks (e.g., (Fj^r et al., 1992)). In the analytical models commonly used to assess the stress changes induced during oil production or injection, the compaction or expansion of the reservoir is assumed to be uniaxial (e.g., (Fj^r et al., 1992) and (Zoback and Zinke, 2002)).
A proper estimation of the sustainable pore pressure increment to prevent fault reactivation will depend on stress and fluid paths inside the reservoir. Since the vertical stress is strongly related to the overburden weight and the crust is not vertically constrained, this stress component is slightly affected by the pore pressure changes in the reservoir. On the other hand, the horizontal stresses can be largely influenced by pore pressure changes
It is assumed that the mechanical behavior of the reservoir rock is controlled by the effective stresses (o ), defined by:
G = G - ip	(10)
where G is the total stress tensor, Pi is the liquid pressure, and I is the identity matrix. The mechanical constitutive model adopted in this work is based on the Mohr-Coulomb criterion in which the yield surface (Fy) can be expressed as:
where O1 and 03 are the principal stresses. The trace of the yield surface on the shear stress normal stress plane is shown in Figure 8.
Following the approach discussed by (Soltanzadeh and Hawkes, 2008), the pore pressure changes resulting from fluid production or injection in a reservoir will generate changes in effective stresses. The effective stress change Ao. is related to the pore pressure change AP and the total stress change Ao as follows:
Ao. = Aoj + a.AP	(12)
where AP is considered positive during injection and a is assumed equal to 1 for the fault reactivation analyses presented as follows.
Increasing fluid pressure induce a progressive movement of the Mohr's circle to the left, inducing the failure of the material when the circle touches the failure envelope (Figure 9). Pre-existing faults are reactivated at that moment. The mode of failure depends on which principal stress is closest to the vertical. An assumption made here is that the fault plane has the same strength parameters of the reservoir. We understand that it may not be true and agreed that this is another reason to support the importance of an uncertainty quantification study as presented in Section 5.
3.1	Assessment of fault reactivation using the field data
To illustrate the concepts discussed above to predict the fault reactivation we will consider the fault plane presented in Figure 3, which crosses the overburden from the reservoir until the sea bed. It is assumed that the state of stresses and strength parameters are being calculated only from well 16 (the closest well to the fault plane).
The properties adopted in this example (a deterministic model) were obtained from log correlations as described in section 2 (i.e. c’ = 8 [MPa] and 0’ = 30o). The initial effective vertical stress are: O] ’ = 22 [MPa] and 03’ = 15 [MPa].
For this specific case, the maximum allowed pore pressure increment obtained was 16 [MPa]. This implies that fluid pressures equal or above this value will lead to fault reactivation with the potential loss of the sealing capabilities of the fault zone. However, this result is related to the particular parameters adopted in this analysis. A more general study accounting for the uncertainties associated with the different parameters involved in these kinds of problems is presented in Section 5. Before this, in the next section some background information on uncertainty quantification will be discussed.
4.	Uncertainty Quantification:
Uncertainty quantification (UQ) is a key component when analyzing problems with limited data and/or high variability of the existing information. The development of new and more efficient numerical techniques, the increase in computational power, and the massive use of parallel computing, among others, have recently assisted the incorporation of UQ analysis in routinely studies. The uncertainty dual nature is described by (Helton et al., 2007) as:
i)	Aleatory uncertainty is the uncertainty that results from the fact that a system can behave randomly. It is also known as stochastic uncertainty, ‘Type A' uncertainty, or irreducible uncertainty.
ii)	Epistemic uncertainty is the uncertainty that results from the lack of knowledge about a system and/or its properties. It is also known as subjective uncertainty, ‘Type B' uncertainty, or reducible uncertainty. Evidence theory is one of the possible approaches for dealing with epistemic uncertainty, as it is explained later on.
The probability theory is commonly used to evaluate both types of uncertainties. However, recent studies have shown that probabilistic methods may not be the most convenient ones for analyses involving epistemic uncertainties. Alternative approaches, based for example on evidence theory, seem more appropriate (Helton et al., 2007).
The prior distribution of a variable is a basic information required in analysis based on probabilistic methods. The “Principle of Insufficient Reason (Savage, 2012)” is generally used when this information is not available. This implies that when the density functions associated with the model parameters are not known, uniform distribution functions could be assumed. This assumption is valid for random variables, but is perhaps inaccurate when dealing with epistemic ones.
An alternative approach to quantify epistemic uncertainties is based on the evidence theory. This theory is particularly useful in cases where there is not enough information to quantify the uncertainty with a known density function. In the evidence theory the range of the variables is defined by intervals or sets of possible values. The definition of a range (or a set) of probable values linked with a given variable has three important implications:
i)	it is possible to incorporate in the analysis experimental information from different sources that we can weight according to our confidence on the particular test result;
ii)	the principle of insufficient reason is not enforced, furthermore there is not a pre-establish structure for the distribution of the input variables. Initial probabilities can be arranged in sets, without having pre-established assumptions about the probabilities of individual events;
iii)	the additivity axiom is not mandatory. Therefore, the complementary sets of probabilities measures do not have to add up to one. When this occurs, this framework converges to the traditional probabilistic representation (Helton et al., 2007).
If we compare the probability theory against the evidence theory, we can see that the benefit of the last one is that it allows a less restrictive description of the uncertainty. However a drawback of the evidence theory is that uncertainty analyses are computationally more demanding than the ones used in probabilistic method. The additional computation effort comes mainly from the numerical optimization process needed to calculate the belief and plausibility functions (details are presented in Section 4.1). In the following section we discuss some background information related to the evidence theory, more details can be found elsewhere (e.g. (Helton et al., 2004), (Helton et al., 2005) and (Pereira et al., 2014)).
4.1	Belief and Plausibility Functions:
The two core concepts in evidence theory are: i) the Basic Probability Assignment (BPA), and ii) focal elements. The BPA (or weight) for a given set, it is the probability that can be assigned to it, but cannot be decomposed into additional probabilities for subsets of that set. The focal elements are those sets that have nonzero BPAs. The basic probability assignment is different from the classical definition of probability. It is defined by mapping over an interval in which the basic assignment of the null set is ‘0’ and the summation of basic assignments in a given set is ‘1’. The BPA is called a focal point for each element for those sets that have nonzero BPAs.
The first work in evidence theory was done by Shafer (1976) as an expansion of the one previously performed by Dempster (1967). In a finite discrete space, the evidence theory or the Dempster-Shafer theory (DST), can be interpreted as the generalization of probability theory for the case in which the probabilities are assigned to sets of values. When evidence is sufficient to assign probabilities to single events, the Dempster-Shafer model falls into the traditional probabilistic formulation.
If we consider a model represented by:
y = f (x)
where, x — Jx2,x3,...,xnX] is the vector of the input variables and y — [y15y2,y3,...,ynY] is the
vector associated with the model results. Thus, the uncertainty in y can be estimated in the framework of the probabilistic theory by the Cumulative Distribution Function (CDF).
prob(y &amp;lt;y) —	8(f(x) | y^dX = EX 3(f(x) | y) /
nS
(14)
where y is the expected value; nS is the number of samples; 8( f (x) | y) =1 if f (x) &amp;lt;y and
8(f (x)| y) = 0 if f (x) &gt; y .
Conversely the evidence theory does not impose a rigid structure for the characterization of the uncertainties. It is a more flexible approach. The uncertainty in y is estimated by means of two uncertainty functions, the belief and plausibility functions (Helton et al., 2007),
Bel(y &amp;lt;y)— E mEY(A)
Ac Ay
piy (y &amp;lt;y)— E mEY (A)
A^Ay ^0
(15)
(16)
At an intuitive level, belief is a measure of the amount of information that indicates that a statement is true, and plausibility is a measure of the amount of information that indicates that a statement could be true. Central to this concept is the idea that all the probability distributions that do not violate the assumed properties of the evidence space are under consideration. More details can be found elsewhere (e.g. (Helton et al., 2005); (Helton et al., 2007); and (Pereira et al., 2014)).
The definition of the belief and plausibility functions for the model output can be done through different optimization processes. The main component of the optimization analysis is presented in the following section.
4.2	Optimization Algorithm
The belief and plausibility functions are defined for the input variables (x). For each input variable it is necessary to specify intervals (and the associated bounds) and BPAs. To calculate the belief and plausibility measures, it is necessary to calculate the minimum and maximum of the response function in each interval cell combination. This implies that epistemic analyses require the implementation of an optimization algorithm to find the maximum and minimum for all the possible combinations considered. Waltz et al. (2006), Byrd et al. (2000), and Helton et al. (2004), amongst others, provide more details about the implementation of typical optimization methods that can be used to perform Dempster-Shafer calculations. The intervals and their associated BPAs are then propagated to obtain cumulative distribution functions on belief (CBF) and plausibility (CPF). As mentioned above, belief is the lower bound on a probability estimate that is consistent with the evidence, and plausibility corresponds to the upper bound on a probability estimate that is consistent with the evidence.
Based on the data presented in Fig. 11, we followed the approach presented by (Pereira et al., 2014) to compute the CBF and CPF; which consist of the main following steps:
1)	Combine intervals of each input variable into combination cells.
2)	For each cell, the following two optimization problems have to be solved to find the lower (lb) and upper (ub) bounds of the response:
(lb) APminj = Minimize	AP (x)
subject to: (xl). &amp;lt;x&amp;lt;(xu
(ub) APmaxj = Maximize	AP (x)
subject to: (xl). &amp;lt;x&amp;lt;(xu)
(18)
where: AP(x) corresponds to the increment of pore pressure that indices reactivation obtained through the analytical approach presented in Section 3; (xl)j and (xu)j are the bounds of the input variables intervals associated with the combination cell j.
3)	Sort the lower bounds of the output response intervals obtained in Step 2 in ascending order.
We compared typical results of a UQ study obtained from the well-known probability theory against the results obtained from the evidence theory. The description of the case study is presented in Section 5.
5.	Case description, results and discussion
Fault reactivation analyses are normally conducted to prevent scenarios with adverse consequences. In the present study we used the simple analytical approach presented in Section 3 to estimate the maximum injection pressure. In section 5.1, we introduce the uncertainty specifications provided for the required parameters to conduct the analyses. Figures 5, 6, 7 and 8 show the typical behavior of these parameters along the depth. In section 5.2 the results of the fault reactivation analyses will be discussed. Section 5.3 focuses on the possible consequence associated with a failure. In section 5.4 the results associated with the risk interval are described. Finally, section 5.5 summarizes the results.
5.1	Specification of uncertain input data
This section discusses a criterion to define the mechanical properties of the fault and the initial stress state based on the information provided by the well logs. The methodology proposed in this work defines the variables subsets and the Basic Probability Assignments (BPAs) for each subset based on the distance of each well to the fault plane under study (see Figure 10). It is proposed that the weights for a given interval will depend on the distance between the fault plane and the well, associating higher weights to those wells that are closer to the fault plane.
o'---------------1--------------1---------------1-----------------1
0	5	10	15	20
Normalized ’X’ (km)
Figure 10 - Plot showing the distance between the wells and the fault plane
The data associated with the evidence theory is developed by deducing weights for each subset of the uncertain variables as a function of the likelihood associated with each subset. These subsets then become the focal elements of the evidence theory for this variable, where focal element refers to a subset of the sample space that is assigned a nonzero BPA (Helton et al., 2005). Afterwards, the BPAs are assigned to the focal elements in a function of the well distance to the fault plane as follows:
dt
mj (U) “ #wellS X di i=1
(19)
The function mj (u) defines the BPA for the subset U, and dj is the least distance from well “j” to the
have a different number of wells to be considered in the BPA calculation due to the type of well log information available for each well.
The ranges for each variable used in the analyses were defined by the calculations presented in Section 2. Due to the great variance in the friction angle values (see Fig.8), the range was normalized varying now between 25o and 30o.
The representation of the probability theory follows the method presented by Helton et al. (2004), and it is based on the same subsets of the variable ranges used in the evidence theory for uncertainty quantification regarding the variable values. Specifically, rather than assigning BPAs to these subsets, the probability theory representation for uncertainty quantification is obtained by assigning uniform distributions functions on these subsets and then weighting these distributions by probabilities that are the same as the BPAs assigned to these subsets. Because of the introduction of the uniform distributions, the probability representation is requiring more structure on the specified uncertainty than the evidence theory representation does.
The details of the evidence theory representations for the individual variables are summarized in Figure 11. The indicated BPA assignments in Fig. 11 are certainly not unique and other experts using the same uncertainty information could propose different assignments. As explained above, uniform density functions were assumed for the analyses using the probability theory. Fig. 11 also presents the input parameters adopted when applying probability and evidence theories. It should be noticed that there is a difference in stating that a parameter falls in the interval [a,b] than assuming uniform distribution in [a,b].
Probability Density Functions	Ranges + Weights (BPA’s)
1 I-------------------1--------1---------1
Parameter	Ranges	Probability Theory (Density functions)	Evidence Theory (Weights)
Vertical Effective Stress (Mpa) (information available in 20 wells)	(22-24)7(25-27)7(20-22)7(19-20)7 (19-21)7(20-22)7(12-13)7(15-16)/ (12-14)7(17-19)7(27-29)7(23-25)7 (22-24)7(24-26)7(23-25)7(24-26)/ (21-22)7(20-22)7(22-24)7(20-22)	Uniform in all ranges	0.041/0.102/0.071/0.026/ 0.061/0.015/0.112/0.082/ 0.107/0.056/0.031/0.031/ 0.025/0.031/0.020/0.010/ 0.025/0.031/0.041/0.082
Minimum Horizontal Effective Stress(MPa) (information available in 20 wells)	(13-15)7(1 5-16)7(1 2-14)7(1 2-13)/ (12-14)7(12-14)7(8-10)7(10-11)/ (8-10)7(11-12)7(17-19)7(15-16)7 (13-15)7(15-16)7(14-16)7(15-17)7 (13-14)7(12-14)7(14-15)7(13-14)	Uniform in all ranges	0.041/0.102/0.071/0.026/ 0.061/0.015/0.112/0.082/ 0.107/0.056/0.031/0.031/ 0.025/0.031/0.020/0.010/ 0.025/0.031/0.041/0.082
Friction Angle (°) (information available in 12 wells)	(25-30)7(25-27)7(25-27)7(25-28)7 (25-30)7(25-2 7)7(25-29)7(25-28)7 (25-26)7(25-30)7(25-27)7(25-29)	Uniform in all ranges	0.052/0.131/0.09270.033 0.07870.02070.14470.104 0.13770.07270.03370.104
Cohesion (MPa) (information available in 12 wells)	(2-17)7(4-6)7(3-5)7(3-6)7 (2-10)7(3-6)7(1-11)7(2-9)7 (1-6)7(2-15)7(3-7)7(2-10)	Uniform in all ranges	0.052/0.131/0.092/0.033 0.078/0.020/0.144/0.104 0.13770.072/0.03370.104
Figure 11 - Uncertainty characterization for the analyses involving evidence and probability theories
5.2	Results for the fault reactivation analysis
Figure 12 presents the comparison between the main results from the probability and evidence theories for uncertainty quantification. The cumulative distribution function (CDF, Eq.14) was obtained through the use of probability theory, computed according to Pugachev (1984). Evidence theory was used to compute the cumulative belief function (CBF, Eq.15) and the cumulative plausibility function (CPF, Eq.16). Figure 11 shows the results in terms of fault reactivation probability (i.e. cumulative probability to be lower), and it should be used when looking at the probability or belief or plausibility that a given increment of the injection pressure may reactive the fault.
Belief and plausibility can be interpreted as the lower (i.e., minimum) and upper (i.e., maximum) probabilities. For example, for a subset U, Bel(U) is the smallest possible probability that could occur for U over the set of indicated distributions and Pl(U) is the largest possible probability that could occur for U over the set of indicated distributions. For example, suppose that for the proposition that says ‘the fault is reactivated' there are a belief of 0.5 and a plausibility of 0.8. This means that the existing evidence allows to strongly state that the proposition is true with a confidence of 0.5. However, the evidence contrary to that hypothesis (i.e. ‘the fault is not reactivated') only has a confidence of 0.2. The remaining mass of 0.3 (i.e. the gap between the 0.5 supporting evidence on the one hand, and the 0.2 contrary evidence on the other) means that the fault could either be reactivated or not. This interval represents the level of uncertainty based on the evidence in the system.
When assessing the maximum allowable injection pressure, it is also necessary to estimate not only the probability of a fault reactivation but also the possible consequences associated with this failure. The next section presents a simple way to account them.
Figure 12 - Main curves related to the probability and evidence theories coded as follows: cumulative
distribution function (CDF); cumulative belief function (CBF) and cumulative plausibility function
(CPF)
5.3	Calculation associated with the consequences of a failure
Once a geological fault is reactivated, the oil may migrate through the fault zone to upper levels. The movement of oil can be induced by three main mechanisms: advection (which is mainly controlled by Darcy's law), capillary forces, and buoyancy (which is related to the difference in fluid density). A comprehensive analysis of the oil migration under the scenario of a fault reactivation generally requires quite complex numerical simulations, which are out of the scope of this work. However, it can be relatively easy to find the equivalent oil column height in the case that a straight path is open for the oil after the fault reactivates.
We used a simple analytical approach to compute the oil heights considering that the fault plane is already reactivated and became a straight path available to the oil leakage from the reservoir up to the seabed. The results obtained from the method we are proposing here can be considered appropriate to provide a guidance on the possible risks associated with different injection pressures and are complementary to the more general numerical analyses. In a simplified but conservative scheme, the oil column height (hoii [m]) in the fault can be calculated as follows:
h =
AP106
Po g
(20)
where AP is the pore pressure increment [MPa], Po is the oil volumetric mass density [kg/m3], and g is the gravity [m2/s].
We have categorized the consequence of failure into three main categories: Moderate (M), in which the fault reactivates but the oil column height is less than 50% of the overburden thickness; High (H), when the magnitude of the oil column height is in between 50% and 100% of the overburden thickness; and Severe (S), when the oil exudation will happen very likely because the oil column height is higher than the overburden thickness.
Figure 13 shows contours maps of overburden thickness including the location of the fault plane. The numbers associated with the dots in the fault plane represent the overburden thickness at that position. Regarding the increment of pore pressure equals to 5 [MPa] all the 5 points in the fault plane fell in the range of moderate consequence of failure (i.e. consequence type “M”), as discussed above. Increasing up to 10 [MPa] of pore pressure increment, we can see that 3 out of 5 points in the fault plane are in the range of moderate consequence of failure and the other 2 points in the range of high consequence of failure (i.e. type “M-H” ). If the fluid pressure is increased up to 15 [MPa], 3 out of 5 points in the fault plane fell in the range of high consequence of failure and the other 2 points in the range of severe consequence of failure (i.e. type “H-S”). Finally, if the fluid pressure is
increased up to 20 MPa of pore pressure increment in all the 5 points are in the range of severe
consequence of failure (i.e., type “S”).
In section 5.4 we discuss the risk interval result that will correlate the probability of fault reactivation calculations with the consequence of failure design.
Normalized ’X’ (km)
Figure 13 - Normalized coordinate system showing fault plane locations selected for the consequence
of failure analyses (contours and points on the fault plane are plotted in meters)
5.4	Risk interval results
Once the intervals of fault reactivation probability (defined by the bounds of the cumulative belief and cumulative plausibility functions) and the magnitude of the consequent impact are determined, it is possible to use a risk assessment analysis to find the level of risk for different fluid pressure
increments. For this purpose, a simple model was defined to describe oil exudation risk; which is defined as the product of the probability of fault reactivation time the consequence of a failure. The results presented in sections 5.2 and 5.3 were combined and a two-dimensional risk diagram was prepared in which the horizontal axis is related to the consequence of failure, while the vertical axis corresponds to the likelihood of the occurrence of fault reactivation (Figure 14).
If for example we analyze the case associated with a pore pressure increment of 15 [MPa], we can conclude from the belief and plausibility functions (Fig. 12) that the minimum and maximum fault reactivation probability is defined as 0.06 and 0.40, respectively. Fig. 14 also presents the classification of the associated impact. In the risk diagram for a fluid pressure increment of 15 [MPa], we can see that the consequence of failure was considered as H-S (i.e. high-severe, see Section 5.3) and the associated risk (probability of fault reactivation multiplied by the consequence of failure) can be considered as large. If we have enough adequate data available, we may use probabilistic methods to provide quantitative and precise numbers for calculation of a probability. However, in this case study, due to lack of information regarding specifically the fault plane strength parameters and state of stress (i.e. the information comes from the wells and in this case they are not crossing the fault plane), we can only determine ranges for the fault reactivation probability and magnitudes of consequence.
Figure 14 - Risk diagram proposed to define the fault reactivation risk
5.5	Results summary
This section discusses some differences between the results obtained by three different approaches: deterministic, uncertainty quantification based on probability theory and uncertainty quantification based on evidence theory. It also deliberates about the flexibility of the evidence theory to incorporate new data (when they are available).
As presented in Section 3.1, from the deterministic approach it was obtained that 16 [MPa] is the maximum injection pressure. Pressure equal or above this value will lead to fault reactivation with the potential loss of the sealing capabilities of the fault zone. However, this result is related to the particular parameters adopted in this analysis (which correspond to the ones calculated from the borehole closer to the fault). In Figure 14, this value would be placed at the “large” (or “very large”) risk category.
A more general result accounting for the uncertainties associated with the different parameters involved in this kind of problem was presented in Figure 12. It can be observed that according to the classical probability theory (CDF curve), the probability of a fault reactivation when the fluid is injected at 15 [MPa] is around 0.18. However, according to the evidence theory; the probability of a fault reactivation can be between 0.40 (as a maximum value - CPF) and 0.06 (as a lower bound -CBF). It can be seen that the evidence theory provides more information to assist the decision maker.
For example, if a profitable injection project requires an increase in the pore pressure increment above 10 [MPa] for the scenario presented in Section 5.3, we can conclude from this simple analysis that the risk of fault reactivation is large. Under these circumstances, a more detailed geomechanical numerical model (ideally 3-D) would be advisable before making a final decision.
The flexibility of the evidence theory to incorporate new data is related with the BP As (see Section 4.1). For instance, if a new well is perforated or new laboratory tests are performed, the new BPAs will consider this fresh information and upgraded results can be obtained incorporating the new data.
6.	Conclusions
In this paper we proposed an approach to estimate the maximum allowable injection pressures, such that the mechanical integrity of a pressurized reservoir is not compromised by a fault reactivation
phenomenon. We adapted a framework based on the evidence theory to the particular conditions of this problem. This methodology is an excellent alternative to traditional methods based on the probabilistic theory when dealing with epistemic variables (as in this case). We proposed a novel methodology that allows estimating the maximum injection pressure, considering the related epistemic uncertainties and also deal with the associated risk. The method we are proposing is based on the main following components: i) analytical solutions to determine the maximum allowable injection pressure, ii) parameters identification and determination of intervals and weights based on field data and expert judgment; iii) an optimization scheme to obtain the cumulative plausibility function (CPF) and the cumulative belief function (CBF), which are the two main outcomes associated with the evidence theory; and iv) a risk analysis to propose an integrated diagram to assist handling the risk of a fault reactivation during the decision making procedure. We also performed a deterministic study and one analysis based on the conventional probabilistic theory
The case we analyzed in this paper is based on information gathered from an actual oil production field located in Campos Basin, Brazil. We study the problem of the maximum fluid injection by means of three different approaches, namely, deterministic, probabilistic and epistemic approaches. We compared the main outcomes obtained from these different theories. We showed that the information provided by the epistemic method is more complete and provide additional information that can be crucial for the decision making process. Furthermore, the expert opinion (a key component in this kind of problem) is an integral part of the epistemic approach,
The proposed methodology does not obviously intent to replace detailed 2-D or 3-D numerical analysis, but to complement and support the decision regarding the importance of a numerical model for a specific region. It also could help the process of accounting for material parameter uncertainties and inclusion of expert opinion. It can also see as a preliminary step in this complex
problem to evaluate in a quick manner the risk associated with the fluid injection at high pressure in oil reservoirs. We think that the results presented in this work are very promising and encourage the use of this kind of approach to design the maximum injection pressure when assisting oil production or storing CO2.
7.	References
Althuwaynee, O.F., Pradhan, B., Lee, S., 2012. Application of an evidential belief function model in landslide susceptibility mapping. Computers &amp;amp; Geosciences 44, 120-135.
Bagheri, M.A., Settari, A., 2005. Modeling of Geomechanics in Naturally Fractured Reservoirs.
Bai, Y.C., Han, X., Jiang, C., Liu, J., 2012. Comparative study of metamodeling techniques for reliability analysis using evidence theory. Advances in Engineering Software 53, 61-71.
Blanchard, B.S., Fabrycky, W.W.J., 2011. Systems Engineering and Analysis. Pearson Education, Limited.
Byrd, R.H., Gilbert, J.C., Nocedal, J., 2000. A trust region method based on interior point techniques for nonlinear programming. Mathematical Programming 89, 149-185.
Chapman, R.E., 2000. Petroleum Geology. Elsevier Science.
Dempster, A.P., 1967. Upper and Lower Probabilities Induced by a Multivalued Mapping. The
Annals of Mathematical Statistics 38 (2), 325-339.
Ellis, D.V., Singer, J.M., 2007. Well Logging for Earth Scientists. Springer.
Feizizadeh, B., Jankowski, P., Blaschke, T., 2014. A GIS based spatially-explicit sensitivity and uncertainty analysis approach for multi-criteria decision analysis. Computers &amp;amp; Geosciences 64, 8195.
Fj^r, E., Horsrud, P., Raaen, A.M., Risnes, R., Holt, R.M., 1992. Petroleum Related Rock Mechanics. Elsevier Science.
Helton, J.C., Johnson, J.D., Oberkampf, W.L., 2004. An exploration of alternative approaches to the representation of uncertainty in model predictions. Reliability Engineering &amp;amp; System Safety 85, 3971.
Helton, J.C., Johnson, J.D., Oberkampf, W.L., Storlie, C.B., 2007. A sampling-based computational strategy for the representation of epistemic uncertainty in model predictions with evidence theory. Computer Methods in Applied Mechanics and Engineering 196, 3980-3998.
Helton, J.C., Oberkampf, W.L., Johnson, J.D., 2005. Competing Failure Risk Analysis Using Evidence Theory. Risk Analysis: An International Journal 25, 973-995.
Lal, M., 1999. Shale Stability: Drilling Fluid Interaction and Shale Strength. Society of Petroleum Engineers.
Montmayeur, H., 1985. Prediction of static elastic/mechanical properties of consolidated and unconsolidated sands from acoustic measurements. Colorado School of Mines.
Neves, M.C., Paiva, L.T., Luis, J., 2009. Software for slip-tendency analysis in 3D: A plug-in for Coulomb. Computers &amp;amp; Geosciences 35, 2345-2352.
Pereira, L.C., Guimarães, L.J.N., Horowitz, B., Sánchez, M., 2014. Coupled hydro-mechanical fault reactivation analysis incorporating evidence theory for uncertainty quantification. Computers and Geotechnics 56, 202-215.
Pugachev, V.S., 1984. Probability theory and mathematical statistics for engineers. Pergamon Press.
Rutqvist, J., 2011. Status of the TOUGH-FLAC simulator and recent applications related to coupled fluid flow and crustal deformations. Computers &amp;amp; Geosciences 37, 739-750.
Savage, L.J., 2012. The Foundations of Statistics. Dover Publications.
Shafer, G., 1976. A Mathematical Theory of Evidence. Princeton University Press.
Soltanzadeh, H., Hawkes, C.D., 2008. Semi-analytical models for stress change and fault reactivation induced by reservoir production and injection. Journal of Petroleum Science and Engineering 60, 7185.
Sousa, L.C., Jr., Santos, E.S.R., Ferreira, F.H., 2010. Geomechanical Data Acquisition And Modeling Applied to an Offshore Sandstone Petroleum Reservoir. American Rock Mechanics Association.
Teatini, P., Castelletto, N., Gambolati, G., 2014. 3D geomechanical modeling for CO2 geological storage in faulted formations. A case study in an offshore northern Adriatic reservoir, Italy. International Journal of Greenhouse Gas Control 22, 63-76.
Waltz, R.A., Morales, J.L., Nocedal, J., Orban, D., 2006. An interior algorithm for nonlinear optimization that combines line search and trust region steps. Mathematical Programming 107, 391408.
Zoback, M.D., Zinke, J.C., 2002. Production-induced Normal Faulting in the Valhall and Ekofisk Oil Fields. pure and applied geophysics 159, 403-420.
APENDICE C - Manuscrito submetido ao SPE Paper Contest 2015
SPE Paper Contest 2015 - PHD category


Society of Petroleum Engineers
Uncertainty Quantification for Reservoir Geomechanics
Leonardo Cabral Pereira, Petrobras - Petróleo Brasileiro S.A.
Leonardo José do Nascimento Guimarães, Federal University of Pernambuco
Marcelo Sanchez Castilla, Texas A&amp;amp;M University
Copyright 2015, Society of Petroleum Engineers
This paper was prepared for presentation at the SPE Annual Technical Conference and Exhibition held in Houston, Texas, USA, 28-30 September 2015.
This paper was selected for presentation by an SPE program committee following review of information contained in an abstract submitted by the author(s). Contents of the paper have not been reviewed by the Society of Petroleum Engineers and are subject to correction by the author(s). The material does not necessarily reflect any position of the Society of Petroleum Engineers, its officers, or members. Electronic reproduction, distribution, or storage of any part of this paper without the written consent of the Society of Petroleum Engineers is prohibited. Permission to reproduce in print is restricted to an abstract of not more than 300 words; illustrations may not be copied. The abstract must contain conspicuous acknowledgment of SPE copyright.
Abstract
Reservoir geomechanics encompasses aspects related to rock mechanics, structural geology and petroleum engineering. The geomechanics of reservoirs must be understood in order to better explain critical aspects present in petroleum reservoirs exploration and production phases, such as: pore pressure prediction, geological fault seal potential, well design, fracture propagation, fault reactivation, reservoir compaction, CO2 injection, among others.
In this paper we discussed topics in reservoir geomechanics related to reservoir engineering. Geological and geophysical aspects of this discipline can be found in the literature (e.g. (Zoback, 2010) and (Fjar et al., 2008)). The goal of this paper is to reproduce analytical calculations and illustrate different examples about the importance of uncertainty quantification in reservoir geomechanics. This contribution also covers typical issues related to reservoir geomechanics.
Introduction
In general terms, uncertainty can be defined as the lack of exact knowledge, regardless the cause of this ignorance (Uusitalo et al., 2015). Each decision or set of decisions is associated to several factors and thus is highly uncertain (Fenton and Neil, 2012). Normally, the uncertainty is present in all stages of reservoir geomechanics projects, from the definition of the input parameters to the characterization of simplifying assumptions and constitutive models to be used. Only very recently, the scientific community has begun to recognize the need to define types of uncertainties. Different classifications of uncertainty can be found in the literature, some of then are very similar, but others are quite different (e.g., (Regan et al., 2002), (Skinner et al., 2013) and (Walker et al., 2003)). The following definitions have been adopted in this paper.
The dual nature of uncertainty can be described as follows: Aleatory Uncertainty - type of uncertainty that results from the fact that a system may behave in a random shape; this type of uncertainty is also known as: stochastic, type A or irreducible; and Epistemic Uncertainty -type of uncertainty that results from lack of knowledge about a system and/or its properties; also known as: subjective, type B or reducible uncertainty.
We present in this work a methodology based on evidence theory aimed at helping the decision maker to opt for the optimal decision, considering the intrinsic uncertainties associated with typical problems in reservoir geomechanics. We also compare the results obtained with different methods, namely, deterministic analysis, conventional probabilistic theory and non-probabilistic methods.
Injection above Fracture Propagation Pressure
Water injection is one of the most common methods to maintain the production of petroleum reservoirs. Such technique is quite effective after the production of the reservoir by primary mechanisms is exhausted, and also as a production strategy in the early stages of the field exploitation.
During the water injection process, the injectivity can decline due to specific rock and fluids features, geometry of the injectors and producers wells, precipitation of salts or by the presence of solid particles in the injection water. One of the best ways to avoid injectivity loss is to inject above the fracture propagation pressure (de Souza et al., 2005).
In 1980, a semi-analytical model to predict fracture propagation during waterflooding processes was proposed (Hagoort et al., 1980). However, one of the most relevant factors, the thermally induced stress parcel, was not considered in this model. Consequently, the transfer of heat between the injected fluid and the formation was not discussed. Then, in 1985, a three model regions engaging the behavior of water flow together with the fracture mechanics was developed (Perkins and Gonzalez, 1985). Such a model is widely cited in the literature (e.g. (Saripalli et al., 1999), (Suri and Sharma, 2009) and (Rahman and Khaksar, 2012)), however, no clear improvements respect to previous techniques can be highlighted.
The calculations of the reservoir fracture pressure can be done in different ways. Later on this paper, the formulations outlined by (de Souza et al., 2005) and (Perkins and Gonzalez, 1985) will be presented. The importance of uncertainty quantification for the fracture pressure calculation will be also discussed.
Fracture Pressure
The fracture pressure in vertical wells where a normal fault regime is predominat, considering penetrating fluid and thermally induced stress changes, can be described by (Yew and Weng, 2014):

c&gt;h	\Pf'
Pfrat	max &amp;lt;(3&amp;amp;h	eTa Pe +	) _ \gPfrat
(1)
(l + 0	eTa)
k
where:&amp;lt;Jh is the minimum horizontal stress in the reservoir;&amp;lt;JH defines the maximum horizontal stress; eTa is a elastic parameter calculated as a(1 _ 2u/(1 _u)) where a and v are the Biot and Poisson coefficients, respectively; Pe is the reservoir initial pressure; 0 is the reservoir porosity;&amp;lt;JT is the tensile strength of the reservoir rock and \cf' is the thermally induced stress due to the difference between the injected fluid and the fluid formation temperatures.
To calculate the fracture pressure from Equation 1 it must be defined the stress state and the mechanical properties of the reservoir rock. Equations 2 to 4 propose a possible way to define such features. The vetical stress can be calculated as:
c&gt;v =yw.LA + Y Sot
(2)
where: Yw defines the sea water pressure gradient; yr is the lithostatic pressure gradient; LA is the water depth e Sot the overburden thickness. The minimum horizontal stress can be estimated by the following expression:
= K(ov _Pe) + Pe	(3)
where: K0 correlates the minimum horizontal stress with the vertical stress. The thermally induced stress change can be calculated as follows:
J(Tr _T )Eff
1_u
where: ft is the termal expansion coefficient; ff the elastic therm form fator ranging between 0,5 and 1 depending on the temperature front (Perkins and Gonzalez, 1985); E sets the reservoir Young modulus; Tr is the reservoir temperature and T} the injected fluid temperature.
Application Example
To illustrate the use and significance of the previous equations we present here an application example using the properties listed in Table 1. The calculated fracture pressure using Equation 1 is equal to 616,14 kgf/cm2. Table 1: Input data necessary for the calculation of the reservoir fracture pressure.	
Geomechanical Data	
Young Modulus (E)	25 GPa
Biot's Coefficient («)	0,8
Tensile Strength (cT)	41 kgf/cm2
Poisson's Coefficient (o)	0,25
Thrust Coefficient (Ko)	0,8
Rock and Fluid Properties	
Water Viscosity (ww)	0,85 cP
Porosity (w)	7,3 %
Permeability (k)	55 mD
Residual Oil Saturation (Sor)	0,25
Water Specific Heat (Cw)	5 kJ/kg.K
Oil Specific Heat (Co)	2 kJ/kg.K
Density x Grain Specific Heat (pgrCgr)	2000 kJ/m3.K
Water density (pw)	1000 kg/m3
Oil Density (po)	881 kg/m3
Length Data	
Water Depth (LA)	2100 m
Overburden Thickness (Sot)	2450 m
Reservoir Thickness (hR)	100 m
Pressure Data	
Reservoir Initial Pressure (Pe)	490 kgf/cm2
Sea Water Pressure Gradient (yw)	0,101kgf/cm2/m
Lithostatic Pressure Gradient (yr)	0,209kgf/cm2/m
Temperature Data	
Reservoir Temperature (Tr)	60 oC
Injected Water Temperature (T})	35 oC
Injection Data	
Flow Rate (Qinj)	4000 m3/d
Injection Time (t)	5 years
Reservoir Compaction
The terms subsidence and compaction have different meanings. Compaction is related to the thickness reduction in a given formation, while subsidence refers to a downward movement in the surface. The phenomenon of subsidence occurs over much larger areas than the compaction, which is associated with the reservoir thickness.
The pore pressure reduction leads to changes not only in effective stresses, but also in the total stresses present in the reservoir and adjacent rocks. Such changes affect directly the compaction and subsidence phenomena, as they can significantly impact the fluid flow in the reservoir.
Several authors report the importance of understanding the phenomena of reservoir compaction and subsidence during the exploitation of an oil field (e.g. (Zimmerman, 2000), (Bruno, 2002), (Holt et al., 2004) and (Schutjens et al., 2004)). In particular, more recently, (Ferreira, 2014) did an extensive review on cases of subsidence around the world and compared some analytical models with numerical solutions via the commercial software FL AC.
This paper will present deterministic calculations of compaction and subsidence of a petroleum reservoir under depletion. The equations presented are based on the formulation described by (Geertsma, 1966). The importance of uncertainty quantification for problems associated with compaction and subsidence of petroleum reservoirs will be discussed later in this paper.
Vertical Displacements
The vertical displacement field calculated by (Geertsma, 1966) considers the geometry of the diskshaped reservoir, as illustrated in Figure 1.
The displacement field is defined as follows:
uz (0, z) = -
CmhR
C(Z-1)
~	2-11/2
2 |_[1+C2(Z-1)2]
(3 - 4u)C(Z+1)
[1+C2(Z+1)2]V2 +
2CZ
+ [1+C2(Z+1)2]3/2
+(3 - 4v+e)
Ap
(5)
where: Z = z/c; C = c/R e e = -1 para z &gt; c, e e = +1 para z &amp;lt;c (Figure 1). The elastic constant cm is defined by:
c
m
a(1- 2u)
2G(1-u)
(6)
where: G is the shear modulus.
Application Example
An example of application using the properties listed in Table 2 is presented in this section to illustrate the use of Equation 5. The maximum compaction value calculated is equal to 1,40 meters.
Table 2: Required input data for the estimation of the vertical displacement field in a depleted reservoir._
Geomechanical Data
Shear Modulus (G)	1500 kgf/cm2
Biot's Coefficient («)	1,0
Poisson's Coefficient (o)	0,286
Reservoir Properties
Reservoir Thickness (hR)	100	m
Top (c)	2900	m
Radius (R)	1000	m
Total Depth (z)	4000	m
2
Porepressure variation (depletion) (Zp)	133,5 kgf/cm
Figure 2 presents the vertical displacement field along the depth. Note that the top of the reservoir goes down and the base rises. This arc effect is discussed by (Fjar et al., 2008) and (Ferreira, 2014) and it can be physically explained by the contrast in the mechanical properties between the reservoir and the adjacent rocks. The reservoir thickness reduction is almost three meters while the subsidence is in the order of 20 centimeters.
This analysis can be considered deterministicThe inference of new sampled data for the reservoir compaction problem and their impact on the uncertainty quantification is discussed in the following sections.
Figure 2: Vertical displacement field along the depth. The reservoir is located at 2900 meters depth.
Uncertainty Quantification
Uncertainty quantification (UQ) is a key component when analyzing problems with limited data and/or high variability of the avaiable information. The development of new and more efficient numerical techniques, the increase in computational power, and the massive use of parallel computing, among others, have recently assisted the incorporation of UQ analysis in routinely studies. The uncertainty dual nature is described by (Helton et al., 2007) as:
i)	Aleatory uncertainty is the uncertainty that results from the fact that a system can behave randomly. It is also known as stochastic uncertainty, ‘Type A’ uncertainty, or irreducible uncertainty.
ii)	Epistemic uncertainty is the uncertainty that results from the lack of knowledge about a system and/or its properties. It is also known as subjective uncertainty, ‘Type B’ uncertainty, or reducible uncertainty. Evidence theory is one of the possible approaches for dealing with epistemic uncertainty, as it is explained later on.
The probability theory is commonly used to evaluate both types of uncertainties. However, recent studies have shown that probabilistic methods may not be the most convenient ones for analyses involving epistemic uncertainties. Alternative approaches, based for example on evidence theory, seem more appropriate (Helton et al., 2007).
The prior distribution of a variable is a basic information required in analysis based on probabilistic methods. The “Principle of Insufficient Reason (Savage, 2012)” is generally used when this information is not available. This implies that when the density functions associated with the model parameters are not known, uniform distribution functions could be assumed. This assumption is valid for random variables, but is perhaps inaccurate when dealing with epistemic ones.
An alternative approach to quantify epistemic uncertainties is based on the evidence theory. This theory is particularly useful in those cases where there is not enough information to quantify the uncertainty with a known density function. In the evidence theory the range of the variables is defined by intervals or sets of possible values. The definition of a range (or a set) of probable values linked with a given variable has three important implications:
i)	it is possible to incorporate in the analysis experimental information from different sources that we can weight according to our confidence on particular test results;
ii)	the principle of insufficient reason is not enforced. Furthermore there is not a pre-establish structure for the distribution of the input variables. Initial probabilities can be arranged in sets, without having pre-established assumptions about the probabilities of individual events;
iii)	the additivity axiom is not mandatory. Therefore, the complementary sets of probabilities measures do not have to add up to one. When this occurs, this framework converges to the traditional probabilistic representation (Helton et al., 2007).
If one compare the probability theory against the evidence theory, one can see that the benefit of the last one is that it allows a less restrictive description of the uncertainty. However a drawback of the evidence theory is that uncertainty analyses are computationally more demanding than the ones used in probabilistic method. The additional computation effort comes mainly from the numerical optimization process needed to calculate the belief and plausibility functions. In the following section we discuss some background information related to the evidence theory, more details can be found elsewhere (e.g. (Helton et al., 2004), (Helton et al., 2005) and (Pereira et al., 2014)).
Belief and Plausibility Functions:
The two core concepts in evidence theory are: i) the Basic Probability Assignment (BPA), and ii) focal elements. The BPA (or weight) for a given set, it is the probability that can be assigned to it, but cannot be decomposed into additional probabilities for subsets of that set. The focal elements are those sets that have nonzero BPAs. The basic probability assignment is different from the classical definition of probability. It is defined by mapping over an interval in which the basic assignment of the null set is ‘0’ and the summation of basic assignments in a given set is ‘1’. The BPA is called a focal point for each element for those sets that have nonzero BPAs.
The first work in evidence theory was done by (Shafer, 1976) as an expansion of the one previously performed by (Dempster, 1967). In a finite discrete space, the evidence theory or the Dempster-Shafer theory (DST), can be interpreted as the generalization of the probability theory for the case in which the probabilities are assigned to sets of values. When evidence is sufficient to assign probabilities to single events, the Dempster-Shafer model falls into the traditional probabilistic formulation.
If we consider a model represented by:
y=f (x)	(7)
where, x = [xp x2, x3,..., xnX ] is the vector of the input variables and y = [y1, y2, y3,..., ynY ] is the vector associated with the model results. Thus, the uncertainty in y can be estimated in the framework of the probabilistic theory by the Cumulative Distribution Function (CDF).
Prob( y &amp;lt;y) =	3(f (x) | y)dxdX = Si=i	(x) | y)/ ns	(8)
where y is the expected value; nS is the number of samples; S( f (x) | y) =1 if f (x) &amp;lt;y and $(f (x) 1 y) = 0 if f (x) &gt; y.
Conversely the evidence theory does not impose a rigid structure for the characterization of the uncertainties. It is a flexible approach. The uncertainty in y is estimated by means of two uncertainty functions, the belief and plausibility functions as follows (Helton et al., 2007):
BelY (y &amp;lt;y) = S mEY (A)	(9)
A^Ay
At an intuitive level, belief is a measure of the amount of information that indicates that a statement is true, and plausibility is a measure of the amount of information that indicates that a statement could be true. Central to this concept is the idea that all the probability distributions that do not violate the assumed properties of the evidence space are under consideration. More details can be found elsewhere (e.g. (Helton et al., 2005); (Helton et al., 2007); and (Pereira et al., 2014)).
The definition of the belief and plausibility functions for the model output can be done through different optimization processes. The main component of the optimization analysis is presented in the following section.
Optimization Algorithm
The belief and plausibility functions are defined for the input variables (x). For each input variable it is necessary to specify intervals (and the associated bounds) and BPAs. To calculate the belief and plausibility measures, it is necessary to calculate the minimum and maximum of the response function in each interval cell combination. This implies that epistemic analyses require the implementation of an optimization algorithm to find the maximum and minimum for all the possible combinations considered. The contributions by (Waltz et al., 2006), (Byrd et al., 2000), and (Helton et al., 2004) provide more details about the implementation of typical optimization methods that can be used to perform Dempster-Shafer calculations. The intervals and their associated BPAs are then propagated to obtain cumulative distribution belief and plausibility functions (CBF and CPF, respectively). As mentioned above, belief is the lower bound on a probability estimate that is consistent with the evidence, and plausibility corresponds to the upper bound on a probability estimate that is consistent with the evidence.
We followed the approach presented by (Pereira et al., 2014) to compute the CBF and CPF; which consist of the main following steps:
1)	Combine intervals of each input variable into combination cells.
2)	For each cell, two optimization problems have to be solved to find the lower (lb) and upper (ub) bounds of the response.
3)	Sort the lower bounds of the output response intervals obtained in Step 2 in ascending order.
Later on this paper we compared typical results of a UQ study obtained from the well-known probability theory against the UQ results obtained from the evidence theory, but before we do this, in the following section we discuss about different types of evidence.
Types of Evidence
To establish the type of analysis to be performed one must pay attention not only to the availability of data but also to the provenance and reliability of this information. There are two critical aspects related to the combination of evidence obtained from different sources. The first one relates to the type of evidence involved and the second aspect is based on how to handle conflicting evidence. Figure 3 presents four different types of evidence that impact on the choice of how such evidence must be combined. They are called: inclusive evidence, consistent evidence, arbitrary evidence and disjointed evidence (Sentz, 2002).
The “inclusive” evidence can be represented by subsets that is contained in a larger subsets, which in turns can be part of a larger one (Figure 3a). For example, this type of evidence can be related to the case where the information for a given set is obtained from a larger period of time by reducing the intervals of evidence. In the case of “consistent” evidence at least one element is common to all subsets of the sample space (Figure 3b). The “arbitrary” evidence corresponds to the situation where there is no element common to all subsets (Figure 3c), although there may be some subsets of elements in common. Finally, the disjointed evidence is associated with the case in which any two subsets of the sample space there are no elements in common. (Figure 3d).
Each of these possible configurations of evidence from different sources has different implications on the level of conflict associated with each situation. It is intuitive that for the case of disjointed evidence, all sources of evidence are conflicting. With the arbitrary evidence there is some agreements between arbitrary sources, but there is no consensus on any of the elements. Consistent evidence implies an agreement between at least one subset of evidence. Inclusive evidence represents a situation in which every subset is included in a larger one, i.e., it is guaranteed that there is a correlation between the evidences. However, there is inherent conflict in the additional evidence supported by the larger subsets.
“7
Figure 3: Types of Evidence: a) inclusive; b) consistent; c) arbitrary and d) disjointed. The letters A, B, C, D e E represent diferente sources of evidence.
X	/
The traditional probability theory cannot deal with inclusive, consistent or arbitrary evidence, without applying the definition of probability density functions for all the elements of the sets, as well as, it is not able to express the level of conflict between these sets of evidence. The following examples show how the evidence theory is more flexible when dealing with the scenarios discussed above. Furthermore, in the evidence theory, there are many ways to incorporate imminent conflicts in the process of combining multiple sources of information.
Decisions Making Process under Uncertainty
Decision-making is certainly the most important task of a manager (decision maker) and often tends to be rather difficult. The field of decision analysis models lies between two extreme cases, depending on the degree of knowledge about the outcome of a particular action. One end of this scale defines the deterministic problems while the opposite end defines the pure uncertainty. Between these two extremes are defined the problems in that the risk can be assessed (Ben-Haim, 2006).
In reservoir geomechanics there is a predisposition to develop increasingly sophisticated models without first assessing the impact of parameters variations in simpler analyses, as for example those ones based
on semi-analytical solutions. This is the purpose of this section, i.e. to study a decision making case based on simple solution incorporating the uncertainty quantification via the evidence theory
There are mainly two different types of decision-making models that help to analyze different scenarios. Depending on the amount and degree of knowledge available, different methodologies can be applied:
1)	Decision making under pure uncertainty (lack of input data);
2)	Decision-making with inference of new input data, reducing the uncertainty over time due to the availability of new data;
Lack of Input Data
The lack of data is a recurring problem associated with reservoir geomechanics. One of the main reasons has been the high cost associated with the gathering of the data from the field. Another factor is the reduction of the geomechanical data (gathered via seismic logs or cores), because it is quite complex and require high technical expertise. It is alse worth mentioning the lack of credibility that still persists in the industry, related to the importance of reservoir geomechanics studies during an oil field exploration and production phases.
The calculation of the fracture pressure will be used as an example of the decision making process incorporating uncertainty quantification in a case in which the input data is quite scarce. Afterwards, the discussion will focus on the comparisons between the results obtained via probability and evidence theories. In this example, three experts were consulted to define the ranges of parameters to be used in the UQ analysis. These experts defined the weights required for the use of the evidence theory. The ranges of the parameters were defined by properties found in analogous reservoirs. The uncertainty parameters are the Youngs modulus (E), the Poisson’s coefficient (o), Biot’s coefficient (a) and the thrust coefficient (Ko). Table 3 presents the experts opinions.
Table 3: Ranges of possible values characterizing the uncertainty of input variables. Each interval is defined by different experts' opinions.
Parameter	Expert 1	Expert 2	Expert 3
E (GPa)	35-40	35-45	30-40
o	0,2-0,3	0,2-0,3	0,2-0,3
a	0,6-0,8	0,6-0,8	0,5-0,7
Ko	0,7-0,9	0,75-0,8	0,7-0,8
As discussed before, the variables quoted above are considered epistemic and the evidence theory is an efficient framework for the treatment of this kind of uncertainty. Figure 4 presents the CBF and CPF of the fracture pressure calculation using the parameters shown in Table 3.
One of the main dilemmas associated with the use of the probability theory for uncertainty quantification of epistemic variables is related to the choice of the probability density function. Figure 3 shows three CDF's associated with the choice of an uniform probability density function (in dark blue), normal (in light blue) and a distribution that considers the BPA's as weights (in green) in the respective intervals.
Figure 4: Fracture pressure calculation. Uniform probability density function (in dark blue), normal (in light blue) and a distribution that considers the BPA's as weights (in green) in the respective ranges. Via theory of evidence it is estimated the CBF (in red) and the CPF (in black).
New Data Inference
Table 4 corresponds to a possible scenario of information avaiability over time for the problem at stake. Initially, there is no information about the mechanical variables of the reservoir, being possible only a representation of the model input properties through the data observed in similar reservoirs. In this way, the associated uncertainty tends to be meaningful. After the first well drilled, there is information available through log correlations. A crucial point right now is the definition of the form of introducing the new information in the process of uncertainty quantification. As discussed previously, the evidence theory is more flexible because it is able to define probabilities to sets, unlike the requirement of the “prior” definition by the probability theory. Then, new laboratories tests results are avaiable and the expert should decide how to measure the significance of each piece of information (definition of BPAs). Finally, as new wells are drilled new ranges of parameters become available.
Table 4: Information (evidence) over time. The uncertainty quantification process is modified with each new piece of evidence.
T = 0 - Similar Reservoirs		
Parameter	T = 0	
a	0,6-1,0	
V	0,2-0,4	
G (kgf/cm2)	500-3000	
BPA0
1,0
T = 1 - Well Drilled				
Parameter a o G (kgf/cm2)	T = 0 0,6-1,0 0,2-0,4 500-3000	T = 1 0,7-0,8 0,32-0,36 2500-3200		
BPA1	0,2	0,8		
T = 2 - Lab Test				
Parameter	T = 0	T = 1	T = 2	
a	0,6-1,0	0,7-0,8	0,95-1,0	
o	0,2-0,4	0,32-0,36	0,35-0,4	
G (kgf/cm2)	500-3000	2500-3200	2000-2200	
BPA2	0,1	0,7	0,2	
T = 3 - New Well Drilled				
Parameter	T = 0	T = 1	T = 2	T = 3
a	0,6-1,0	0,7-0,8	0,95-1,0	0,75-0,9
o	0,2-0,4	0,32-0,36	0,35-0,4	0,3-0,35
G (kgf/cm2)	500-3000	2500-3200	2000-2200	2600-3000
BPA3	0,02	0,4	0,18	0,4
The results presented in Figure 5 meet the changes observed in the process of uncertainty quantification via the evidence theory over the avaiability of new pieces of information. One can see that the range of uncertainty decreases in the analysis when new information is incorporated.
In the case of new data inference, the evidence theory provides the reduction of the associated uncertainty with the response of the through the reduction between the CPF and CBF curves, i.e., after every new available "evidence", the minimum and maximum likelihood is closer to each other (see Figure 5).
Figure 5: Incorporation of new data over time. Uniform probability density function (green curves), and a distribution that considers the BPA's as weights (blue curves) in their respective ranges. Via theory of evidence it is estimated the CBF (in red) and the CPF (in black).
Conclusions
In the case of lack of data, the use of the traditional probability theory stablishing the probability density function “a priori” tends to inhibit the decision maker.
The use of the evidence theory based on CBFs and CPFs, guarantee that the information is consistent with all the known data associated with the input parameters of the model. In the limit, these two curves (i.e CBF and CPF) converge to a single "real" cumulative distribution function curve, when the data that completely describes the model is fully avaiable.
The flexibility of the evidence theory for new data inference is clear in the evaluation of the results. Not surprisingly, new information about the parameters reduces the uncertainty range about the subsidence value. Initially, with little information (based mainly in previous experiences in similar reservoirs), the range of possible values of subsidence was quite wide (0.05 to 0.9 meters). After the first well was drilled the uncertainty decreased, and the subsidence was between 0.05 to 0.7 meters. Once the information from laboratory testing became available, it was observed a similar trend to reduce the uncertainty, with a minimum and maximum possible subsidence in the range of 0.05 to 0.52 m. Finally, after drilling the second well, the uncertainty reduced even more, and the possible values were between 0.05 to 0.18 meters.
It is worth mentioning that the use of evidence theory in this case allowed the inference of new data without the requirement of “priors” definition. It was observed that the curve related to the probability theory considering uniform distribution kept constant during the analysis because the range that defines the minimum and maximum values of the parameters were not modified as new information became available (and thus the uniform distribution did not be change). The evidence theory managed to properly incorporate in the analyses the information provided by the experts, which is instrumental in assisting the decision-making process.
References
BEN-HAIM, Y. 2006. Info-Gap Decision Theory: Decisions Under Severe Uncertainty, Elsevier Science.
BRUNO, M. S. 2002. Geomechanical and Decision Analyses for Mitigating Compaction-Related Casing Damage. SPE Annual Technical Conference and Exhibition. New Orleans, USA: Society of Petroleum Engineers.
BYRD, R. H., GILBERT, J. C. &amp;amp; NOCEDAL, J. 2000. A trust region method based on interior point techniques for nonlinear programming. Mathematical Programming, 89, 149-185.
DE SOUZA, A. L. S., FERNANDES, P. D., MENDES, R., ROSA, A. J. &amp;amp; FURTADO, C. J. A. 2005. The Impact of Fracture Propagation on Sweep Efficiency During a Waterflooding Process. SPE Latin American and Caribbean Petroleum Engineering Conference held in Rio de Janeiro, Brazil, 20 - 23 June 2005. Rio de Janeiro: Society of Petroleum Engineers.
DEMPSTER, A. P. 1967. Upper and Lower Probabilities Induced by a Multivalued Mapping. The Annals of Mathematical Statistics, 38 (2), 325-339.
FENTON, N. &amp;amp; NEIL, M. 2012. Risk Assessment and Decision Analysis with Bayesian Networks, Taylor &amp;amp; Francis.
FERREIRA, F. H. 2014. Subsidence due to Reservoir Compaction: From Theory to Well Integrity Problems. DSc, Federal University of Rio de Janeiro.
FJAR, E., HOLT, R. M., RAAEN, A. M., RISNES, R. &amp;amp; HORSRUD, P. 2008. Petroleum Related Rock Mechanics: 2nd Edition, Elsevier Science.
GEERTSMA, J. 1966. Problems of Rock Mechanics In Petroleum Production Engineering. 1st ISRM Congress. Lisbon, Portugal: International Society for Rock Mechanics.
HAGOORT, J., WEATHERILL, B. D. &amp;amp; SETTARI, A. 1980. Modeling the Propagation of Waterflood-Induced Hydraulic Fractures. SPE Journal, 20(4), 293-303.
HELTON, J. C., JOHNSON, J. D. &amp;amp; OBERKAMPF, W. L. 2004. An exploration of alternative approaches to the representation of uncertainty in model predictions. Reliability Engineering &amp;amp; System Safety, 85, 39-71.
HELTON, J. C., JOHNSON, J. D., OBERKAMPF, W. L. &amp;amp; STORLIE, C. B. 2007. A sampling-based computational strategy for the representation of epistemic uncertainty in model predictions with evidence theory. Computer Methods in Applied Mechanics and Engineering, 196, 3980-3998.
HELTON, J. C., OBERKAMPF, W. L. &amp;amp; JOHNSON, J. D. 2005. Competing Failure Risk Analysis Using Evidence Theory. Risk Analysis: An International Journal, 25, 973-995.
HOLT, R. M., FLORNES, O., LI, L. &amp;amp; FJAER, E. 2004. Consequences Of Depletion-Induced Stress Changes On Reservoir Compection And Recovery. 6th North America Rock Mechanics Symposium (NARMS): Rock Mechanics Across Borders and Disciplines. Houston, Texas, June 5 - 9, 2004: American Rock Mechanics Association.
PEREIRA, L. C., GUIMARÃES, L. J. N., HOROWITZ, B. &amp;amp; SÁNCHEZ, M. 2014. Coupled hydro-mechanical fault reactivation analysis incorporating evidence theory for uncertainty quantification. Computers and Geotechnics, 56, 202-215.
PERKINS, T. K. &amp;amp; GONZALEZ, J. A. 1985. The Effect of Thermoelastic Stresses on Injection Well Fracturing. OLD SPE Journal.
RAHMAN, K. &amp;amp; KHAKSAR, A. 2012. Fracture Growth and Injectivity Issues for Produced Water Reinjection Wells - Case Studies with Fields from offshore Australia and UK North Sea. SPE Asia Pacific Oil and Gas Conference and Exhibition held in Perth, Australia, 22-24 October 2012. Society of Petroleum Engineers.
REGAN, H. M., COLYVAN, M. &amp;amp; BURGMAN, M. A. 2002. A Taxonomy and Treatment of Uncertainty for Ecology and Conservation Biology. Ecological Applications, 12, 618-628.
SARIPALLI, K. P., BRYANT, S. L. &amp;amp; SHARMA, M. M. 1999. Role of Fracture Face and Formation Plugging in Injection Well Fracturing and Injectivity Decline. SPE/EPA Exploration and Production Environmental Conference held in Austin, Texas, 28 February-3 March 1999. Society of Petroleum Engineers.
SAVAGE, L. J. 2012. The Foundations of Statistics, Dover Publications.
SCHUTJENS, P. M. T. M., HANSSEN, T. H., HETTEMA, M. H. H., MEROUR, J., DE BREE, P., COREMANS, J. W. A. &amp;amp; HELLIESEN, G. 2004. Compaction-Induced Porosity/Permeability Reduction in Sandstone Reservoirs: Data and Model for Elasticity-Dominated Deformation. SPE Annual Technical Conference and Exhibition. New Orleans, USA: Society of Petroleum Engineers.
SENTZ, K. A. F., S. 2002. Combination of Evidence in Dempster-Shafer Theory. Sandia National Laboratories.
SHAFER, G. 1976. A Mathematical Theory of Evidence, Princeton University Press.
SKINNER, D. J. C., ROCKS, S. A., POLLARD, S. J. T. &amp;amp; DREW, G. H. 2013. Identifying Uncertainty in Environmental Risk Assessments: The Development of a Novel Typology and Its Implications for Risk Characterization. Human and Ecological Risk Assessment: An International Journal, 20, 607-640.
SURI, A. &amp;amp; SHARMA, M. M. 2009. Fracture Growth in Horizontal Injectors. SPE Hydraulic Fracturing Technology Conference held in The Woodlands, Texas, USA, 19-21 January 2009. Society of Petroleum Engineers.
UUSITALO, L., LEHIKOINEN, A., HELLE, I. &amp;amp; MYRBERG, K. 2015. An overview of methods to evaluate uncertainty of deterministic models in decision support. Environmental Modelling &amp;amp; Software, 63, 24-31.
WALKER, W. E., HARREMOES, P., ROTMANS, J., VAN DER SLUIJS, J. P., VAN ASSELT, M. B. A., JANSSEN, P. &amp;amp; KRAYER VON KRAUSS, M. P. 2003. Defining Uncertainty: A Conceptual Basis for Uncertainty Management in Model-Based Decision Support. Integrated Assessment, 4, 5-17.
WALTZ, R. A., MORALES, J. L., NOCEDAL, J. &amp;amp; ORBAN, D. 2006. An interior algorithm for nonlinear optimization that combines line search and trust region steps. Mathematical Programming, 107, 391-408.
YEW, C. H. &amp;amp; WENG, X. 2014. Mechanics of Hydraulic Fracturing, Elsevier Science.
ZIMMERMAN, R. W. 2000. Implications of Static Poroelasticity for Reservoir Compaction. Pacific Rocks 2000, Girard, Liebman, Breeds &amp;amp; Doe (eds) Rotterdam, Netherlands: American Rock Mechanics Association.
ZOBACK, M. D. 2010. Reservoir Geomechanics, Cambridge University Press.
REFERENCIAS
AVEN, T., ZIO, E., BARALDI, P. &amp;amp; FLAGE, R. 2013. Uncertainty in Risk Assessment: The Representation and Treatment of Uncertainties by Probabilistic andNon-Probabilistic Methods, Wiley.
BEN-HAIM, Y. 2006. Info-Gap Decision Theory: Decisions Under Severe Uncertainty, Elsevier Science.
BIOT, M. A. 1941. General Theory of Three-Dimensional Consolidation. Journal of Applied Physics, 12, 155-164.
BRUNO, M. S. 2002. Geomechanical and Decision Analyses for Mitigating Compaction-Related Casing Damage. SPE Annual Technical Conference and Exhibition. New Orleans, USA: Society of Petroleum Engineers.
BYRD, R. H., GILBERT, J. C. &amp;amp; NOCEDAL, J. 2000. A trust region method based on interior point techniques for nonlinear programming. Mathematical Programming, 89, 149-185.
CHIN, L. Y., RAGHAVAN, R. &amp;amp; THOMAS, L. K. 2000. Fully Coupled Geomechanics and Fluid-Flow Analysis of Wells With Stress-Dependent Permeability. SPE Journal, 5 (1), 32-45.
DE SOUZA, A. L. S., FERNANDES, P. D., MENDES, R., ROSA, A. J. &amp;amp; FURTADO, C. J. A. 2005. The Impact of Fracture Propagation on Sweep Efficiency During a Waterflooding Process. SPE Latin American and Caribbean Petroleum Engineering Conference held in Rio de Janeiro, Brazil, 20 - 23 June 2005. Rio de Janeiro: Society of Petroleum Engineers.
DEAN, R. H., GAI, X., STONE, C. M. &amp;amp; MINKOFF, S. E. 2006. A Comparison of Techniques for Coupling Porous Flow and Geomechanics. SPE Reservoir Simulation Symposium. Houston: Society of Petroleum Engineers.
DEMPSTER, A. P. 1967. Upper and Lower Probabilities Induced by a Multivalued Mapping. The Annals of mathematical statistics, 38, 325-339.
DUNN, W. L. &amp;amp; SHULTIS, J. K. 2011. Exploring Monte Carlo Methods, Elsevier Science.
ERINGEN, A. C. 1980. Mechanics of continua, R. E. Krieger Pub. Co.
FANG, K. T., LI, R. &amp;amp; SUDJIANTO, A. 2005. Design and Modeling for Computer Experiments, CRC Press.
FELLER, W. 1971. An introduction to probability theory and its applications, Wiley.
FENTON, N. &amp;amp; NEIL, M. 2012. Risk Assessment and Decision Analysis with Bayesian Networks, Taylor &amp;amp; Francis.
FERREIRA, F. H. 2014. Subsidence due to Reservoir Compaction: From Theory to Well Integrity Problems. DSc, Federal University of Rio de Janeiro.
FJAR, E., HOLT, R. M., RAAEN, A. M., RISNES, R. &amp;amp; HORSRUD, P. 2008. Petroleum Related Rock Mechanics: 2nd Edition, Elsevier Science.
GEERTSMA, J. 1966. Problems of Rock Mechanics In Petroleum Production Engineering. 1st ISRM Congress. Lisbon, Portugal: International Society for Rock Mechanics.
GHANEM, R. &amp;amp; RED-HORSE, J. 1999. Propagation of probabilistic uncertainty in complex physical systems using a stochastic finite element approach. Physica D: Nonlinear Phenomena, 133, 137-144.
GOLUB, A. L. 1997. Decision Analysis: An Integrated Approach, Wiley.
GOODWIN, P. &amp;amp; WRIGHT, G. 2009. Decision Analysis for Management Judgment, Wiley.
GUTIERREZ, M. &amp;amp; LEWIS, R. W. 1998. The Role of Geomechanics in Reservoir Simulation. SPE/ISRM Eurock. Trondheim, Norway: Society of Petroleum Engineers.
HAGOORT, J., WEATHERILL, B. D. &amp;amp; SETTARI, A. 1980. Modeling the Propagation of Waterflood-Induced Hydraulic Fractures. SPE Journal, 20(4), 293-303.
HALDAR, A. &amp;amp; MAHADEVAN, S. 2000. Probability, reliability, and statistical methods in engineering design, John Wiley.
HELTON, J. C., JOHNSON, J. D. &amp;amp; OBERKAMPF, W. L. 2004. An exploration of alternative approaches to the representation of uncertainty in model predictions. Reliability Engineering &amp;amp; System Safety, 85, 39-71.
HELTON, J. C., JOHNSON, J. D., SALLABERRY, C. J. &amp;amp; STORLIE, C. B. 2006. Survey of sampling-based methods for uncertainty and sensitivity analysis. Reliability Engineering &amp;amp; System Safety, 91, 1175-1209.
HOLM, L. W. 1959. Carbon Dioxide Solvent Flooding for Increased Oil Recovery. Oil Recovery Conference of the Permian Basin Midlan, Texas, USA: Society of Petroleum Engineers.
HOLM, L. W. 1976. Status of CO2 and Hydrocarbon Miscible Oil Recovery Methods. SPE-AIME 50th Annual Fall Technical Conference and Exhibition. Dallas, Texas, USA: Society of Petroleum Engineers.
HOLT, R. M., FLORNES, O., LI, L. &amp;amp; FJAER, E. 2004. Consequences Of Depletion-Induced Stress Changes On Reservoir Compection And Recovery. 6th North America Rock Mechanics Symposium (NARMS): Rock Mechanics Across Borders and Disciplines. Houston, Texas, June 5 - 9, 2004: American Rock Mechanics Association.
HOWARD, G. C. &amp;amp; FAST, C. R. 1957. Optimum Fluid Characteristics for Fracture Extension. Drilling and Production Practice. New York, New York, USA: American Petroleum Institute.
HUBBERT, M. K. &amp;amp; WILLIS, D. G. 1972. Mechanics of hydraulic fracturing. Transactions of Society of Petroleum Engineers of AIME, 210, 153-168.
HUSTEDT, B., ZWARTS, D., BJOERNDAL, H.-P., AL-MASFRY, R. A. &amp;amp; VAN DEN HOEK, P. J. 2008. Induced Fracturing in Reservoir Simulations: Application of a New Coupled Simulator to a Waterflooding Field Example. SPE Reservoir Evaluation &amp;amp; Engineering, 11(3), 569-576.
INOUE, N. &amp;amp; FONTOURA, S. 2009. Answers to Some Questions About the Coupling Between Fluid Flow and Rock Deformation in Oil Reservoirs. SPE/EAGE Reservoir Characterization and Simulation Conference. Abu Dhabi, UAE: Society of Petroleum Engineers.
JAULIN, L. 2001. Applied Interval Analysis: With Examples in Parameter and State Estimation, Robust Control and Robotics, Springer London.
JI, L., SETTARI, A. &amp;amp; SULLIVAN, R. B. 2009. A Novel Hydraulic Fracturing Model Fully Coupled With Geomechanics and Reservoir Simulation. SPE Journal.
KEARFOTT, R. B. &amp;amp; KREINOVICH, V. 1996. Applications of Interval Computations, Springer.
KILLOUGH, J. E. &amp;amp; KOSSACK, C. A. 1987. Fifth Comparative Solution Project: Evaluation of Miscible Flood Simulators. SPE Reservoir Simulation Symposium. San Antonio, Texas: Society of Petroleum Engineers.
KROESE, D. P., TAIMRE, T. &amp;amp; BOTEV, Z. I. 2013. Handbook of Monte Carlo Methods, Wiley.
LIN, C. C. &amp;amp; SEGEL, L. A. 1988. Mathematics Applied to Deterministic Problems in the Natural Sciences, Society for Industrial and Applied Mathematics.
LIN, G., ENGEL, G.W., ESLINGER, P.W. 2012. Survey and Evaluate Uncertainty Quantification Methodologies.
MAtTRE, O. P. L. &amp;amp; KNIO, O. M. 2010. Spectral Methods for Uncertainty Quantification: With Applications to Computational Fluid Dynamics, Springer.
MINSSIEUX, L. 1994. WAG Flow Mechanisms in Presence of Residual Oil. SPE Annual Technical Conference and Exhibition. New Orleans, USA: Society of Petroleum Engineers.
NAGEL, N. B. 2001. Compaction and subsidence issues within the petroleum industry: From wilmington to ekofisk and beyond. Physics and Chemistry of the Earth, Part A: Solid Earth and Geodesy, 26, 3-14.
NEVES, M. C., PAIVA, L. T. &amp;amp; LUIS, J. 2009. Software for slip-tendency analysis in 3D: A plug-in for Coulomb. Computers &amp;amp; Geosciences, 35, 2345-2352.
NORDBOTTEN, J., CELIA, M. &amp;amp; BACHU, S. 2005. Injection and Storage of CO2 in Deep Saline Aquifers: Analytical Solution for CO2 Plume Evolution During Injection. Transport in Porous Media, 58, 339-360.
OLIVELLA, S., GENS, A., CARRERA, J. &amp;amp; ALONSO, E. E. 1996. Numerical formulation for a simulator (CODE_BRIGHT) for the coupled analysis of saline media. Engineering Computations, 13, 87-112.
PEGORARO, R. T. 2012. Three Phase Flow in Porous Media: Oil-Gas-Water Relative Permeability. MSc, Federal University of Rio de Janeiro.
PERKINS, T. K. &amp;amp; GONZALEZ, J. A. 1985. The Effect of Thermoelastic Stresses on Injection Well Fracturing. OLD SPE Journal.
RAHMAN, K. &amp;amp; KHAKSAR, A. 2012. Fracture Growth and Injectivity Issues for Produced Water Reinjection Wells - Case Studies with Fields from offshore Australia and UK North Sea. SPE Asia Pacific Oil and Gas Conference and Exhibition held in Perth, Australia, 22-24 October 2012. Society of Petroleum Engineers.
REGAN, H. M., COLYVAN, M. &amp;amp; BURGMAN, M. A. 2002. A Taxonomy and Treatment of Uncertainty for Ecology and Conservation Biology. Ecological Applications, 12, 618-628.
RUTQVIST, J. 2011. Status of the TOUGH-FLAC simulator and recent applications related to coupled fluid flow and crustal deformations. Computers &amp;amp; Geosciences, 37, 739-750.
SARIPALLI, K. P., BRYANT, S. L. &amp;amp; SHARMA, M. M. 1999. Role of Fracture Face and Formation Plugging in Injection Well Fracturing and Injectivity Decline. SPE/EPA Exploration and Production Environmental Conference held in Austin, Texas, 28 February-3 March 1999. Society of Petroleum Engineers.
SAVAGE, L. 1955. The foundations of statistics. Journal of consulting psychology, 19, 237-237.
SCHUTJENS, P. M. T. M., HANSSEN, T. H., HETTEMA, M. H. H., MEROUR, J., DE BREE, P., COREMANS, J. W. A. &amp;amp; HELLIESEN, G. 2004. Compaction-Induced Porosity/Permeability Reduction in Sandstone Reservoirs: Data and Model for Elasticity-Dominated Deformation. SPE Annual Technical Conference and Exhibition. New Orleans, USA: Society of Petroleum Engineers.
SENTZ, K. A. F., S. 2002. Combination of Evidence in Dempster-Shafer Theory. Sandia National Laboratories.
SETTARI, A. 1988. General Model of Fluid Flow (Leakoff) From Fractures Induced in Injection Operations. 63rd Annual Technical Conference and Exhibition of the Society of Petroleum Engineers held in Houston. TX, October 2-5, 1988.: Society of Petroleum Engineers.
SHAFER, G. 1976. A Mathematical Theory of Evidence, Princeton University Press.
SKINNER, D. J. C., ROCKS, S. A., POLLARD, S. J. T. &amp;amp; DREW, G. H. 2013. Identifying Uncertainty in Environmental Risk Assessments: The Development of a Novel Typology and Its Implications for Risk Characterization. Human and Ecological Risk Assessment: An International Journal, 20, 607-640.
SOLTANZADEH, H. &amp;amp; HAWKES, C. D. 2008. Semi-analytical models for stress change and fault reactivation induced by reservoir production and injection. Journal of Petroleum Science and Engineering, 60, 71-85.
SURI, A. &amp;amp; SHARMA, M. M. 2009. Fracture Growth in Horizontal Injectors. SPE Hydraulic Fracturing Technology Conference held in The Woodlands, Texas, USA, 19-21 January 2009. Society of Petroleum Engineers.
TEATINI, P., CASTELLETTO, N. &amp;amp; GAMBOLATI, G. 2014. 3D geomechanical modeling for CO2 geological storage in faulted formations. A case study in an offshore northern Adriatic reservoir, Italy. International Journal of Greenhouse Gas Control, 22, 63-76.
TERZAGHI, K. 1966. Theoretical Soil Mechanics, Wiley.
TODD, M. R. &amp;amp; LONGSTAFF, W. J. 1972. The Development, Testing, and Application Of a Numerical Simulator for Predicting Miscible Flood Performance. Journal of Petroleum Technology, 24.
UUSITALO, L., LEHIKOINEN, A., HELLE, I. &amp;amp; MYRBERG, K. 2015. An overview of methods to evaluate uncertainty of deterministic models in decision support. Environmental Modelling &amp;amp; Software, 63, 24-31.
VAN GIGCH, J. P. 2003. Metadecisions: Rehabilitating Epistemology, Springer US.
WALKER, W. E., HARREMOES, P., ROTMANS, J., VAN DER SLUIJS, J. P., VAN ASSELT, M. B. A., JANSSEN, P. &amp;amp; KRAYER VON KRAUSS, M. P. 2003. Defining Uncertainty: A Conceptual Basis for Uncertainty Management in Model-Based Decision Support. Integrated Assessment, 4, 5-17.
WALTZ, R. A., MORALES, J. L., NOCEDAL, J. &amp;amp; ORBAN, D. 2006. An interior algorithm for nonlinear optimization that combines line search and trust region steps. Mathematical Programming, 107, 391-408.
WATSON, D. 1994. Nngridr: an Implementation of Natural Neighbor Interpolation, D. Watson.
XIU, D. 2010. Numerical Methods for Stochastic Computations: A Spectral Method Approach, Princeton University Press.
YAGER, R. R. 1986. Arithmetic and other operations on Dempster-Shafer structures. International Journal of Man-Machine Studies, 25, 357-366.
YEW, C. H. &amp;amp; WENG, X. 2014. Mechanics of Hydraulic Fracturing, Elsevier Science.
ZHOU, X., ZENG, Z., LIU, H. &amp;amp; BOOCK, A. 2009. Laboratory Testing On Geomechanical Properties of Carbonate Rocks For CO2 Sequestration. 43rd US Rock Mechanics Symposium and 4th U.S.-Canada Rock Mechanics Symposium. Asheville, NC American Rock Mechanics Association.
ZIENKIEWICZ, O. C. 1999. Computational geomechanics with special reference to earthquake engineering, John Wiley.
ZIMMERMAN, R. W. 2000. Implications of Static Poroelasticity for Reservoir Compaction. Pacific Rocks 2000, Girard, Liebman, Breeds &amp;amp; Doe (eds) Rotterdam, Netherlands: American Rock Mechanics Association.
ZOBACK, M. D. 2010. Reservoir Geomechanics, Cambridge University Press.</field>
	</doc>
</add>