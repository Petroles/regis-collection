<?xml version="1.0" encoding="utf-8"?>
<add>
	<doc>
		<field name="docid">BR-TU.17800</field>
		<field name="filename">24607_Dorini_FabioAntonio_D.pdf</field>
		<field name="filetype">PDF</field>
		<field name="text">


ii



iii



Instituto de Matema?tica, Estat??stica e Computac?a?o Cient??fica

Universidade Estadual de Campinas

Me?todos para Equac?o?es do Transporte

com Dados Aleato?rios

Fabio Antonio Dorini1

Dezembro de 2007

Banca Examinadora:

• Profa. Dra. Maria Cristina de Castro Cunha (Orientadora)

• Prof. Dr. He?lio Pedro Amaral Souto - IPRJ/UERJ

• Prof. Dr. Jose? Alberto Cuminato - ICMC/USP

• Prof. Dr. Marcelo Martins dos Santos - IMECC/UNICAMP

• Prof. Dr. Lu?cio Tunes dos Santos -IMECC/UNICAMP

1Suporte financeiro de: bolsa do CNPq (processo 140406/2004-2) Mar/2004–Dez/2007;
e bolsa de Doutorado Sandu??che do CNPq (processo 210132/2006-0) Jun/2007–Nov/2007.

v



Resumo

Modelos matema?ticos para processos do mundo real frequ?entemente te?m a forma de sis-

temas de equac?o?es diferenciais parciais. Estes modelos usualmente envolvem para?metros

como, por exemplo, os coeficientes no operador diferencial, e as condic?o?es iniciais e de

fronteira. Tipicamente, assume-se que os para?metros sa?o conhecidos, ou seja, os mode-

los sa?o considerados determin??sticos. Entretanto, em situac?o?es mais reais esta hipo?tese

frequ?entemente na?o se verifica dado que a maioria dos para?metros do modelo possui uma

caracter??stica aleato?ria ou estoca?stica. Modelos avanc?ados costumam levar em consi-

derac?a?o esta natureza estoca?stica dos para?metros. Em vista disso, certos componentes

do sistema sa?o modelados como varia?veis aleato?rias ou func?o?es aleato?rias. Equac?o?es di-

ferenciais com para?metros aleato?rios sa?o chamadas equac?o?es diferenciais aleato?rias (ou

estoca?sticas). Novas metodologias matema?ticas te?m sido desenvolvidas para lidar com

equac?o?es diferenciais aleato?rias, entretanto, este problema continua sendo objeto de estudo

de muitos pesquisadores. Assim sendo, e? importante a busca por novas formas (nume?ricas

ou anal??ticas) de tratar equac?o?es diferenciais aleato?rias. Durante a realizac?a?o do curso de

doutorado, vislumbrando a possibilidade de aplicac?o?es futuras em problemas de fluxo de

fluidos em meios porosos (dispersa?o de poluentes e fluxos bifa?sicos, por exemplo), desen-

volvemos trabalhos relacionados a? equac?a?o do transporte linear unidimensional aleato?ria

e ao problema de Burgers-Riemann unidimensional aleato?rio. Nesta tese, apresentamos

uma nova metodologia, baseada nas ide?ias de Godunov, para tratar a equac?a?o do trans-

porte linear unidimensional aleato?ria e desenvolvemos um eficiente me?todo nume?rico para

os momentos estat??sticos da equac?a?o de Burgers-Riemann unidimensional aleato?ria. Para

finalizar, apresentamos tambe?m novos resultados para o caso multidimensional: mostra-

mos que algumas metodologias propostas para aproximar a me?dia estat??stica da soluc?a?o

da equac?a?o do transporte linear multidimensional aleato?ria podem ser va?lidas para todos

os momentos estat??sticos da soluc?a?o.

vii



Abstract

Mathematical models for real-world processes often take the form of systems of partial

differential equations. Such models usually involve certain parameters, for example, the

coefficients in the differential operator, and the initial and boundary conditions. Usually,

all the model parameters are assumed to be known exactly. However, in realistic situati-

ons many of the parameters may have a random or stochastic character. More advanced

models must take this stochastic nature into account. In this case, the components of

the system are then modeled as random variables or random fields. Differential equations

with random parameters are called random (or stochastic) differential equations. New

mathematical methods have been developed to deal with this kind of problem, however,

solving this problem is still the goal of several researchers. Thus, it is important to look

for new approaches (numerical or analytical) to deal with random differential equations.

Throughout the realization of the doctorate and looking toward future applications in

porous media flow (pollution dispersal and two phase flows, for instance) we developed

works related to the one-dimensional random linear transport equation and to the one-

dimensional random Burgers-Riemann problem. In this thesis, based on Godunov’s ideas,

we present a new methodology to deal with the one-dimensional random linear transport

equation, and develop an efficient numerical scheme for the statistical moments of the

solution of the one-dimensional random Burgers-Riemann problem. Finally, we also pre-

sent new results for the multidimensional case: we have shown that some approaches to

approximate the mean of the solution of the multidimensional random linear transport

equation may be valid for all statistical moments of the solution.

ix



Agradecimentos

A? minha esposa Leyza, por estar sempre ao meu lado, compartilhando momentos alegres

e dif??ceis e, acima de tudo, me incentivando a sempre acreditar em mim mesmo.

A? minha orientadora, Profa. Cristina Cunha, pelo incentivo e apoio prestados.

Ao Prof. Fred Furtado, University of Wyoming, pelo apoio e incentivo durante a realizac?a?o

do Programa de Doutorado Sandu??che no Exterior.

Ao Prof. Lu?cio Tunes dos Santos pelo apoio e ajuda durante a realizac?a?o deste trabalho.

Ao IMECC e a? UNICAMP, pela estrutura e ambiente.

A todos que de alguma forma contribu??ram para este trabalho: fam??lia, professores, colegas

e funciona?rios do IMECC.

Ao CNPq pelo excelente suporte financeiro.

xi



Suma?rio

Resumo vii

Abstract ix

Agradecimentos xi

1 Introduc?a?o 1

1.1 Motivac?a?o e objetivos . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1

1.2 Organizac?a?o da tese . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3

1.2.1 Cap??tulo 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3

1.2.2 Cap??tulo 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3

1.2.3 Cap??tulo 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3

1.2.4 Cap??tulo 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4

1.2.5 Cap??tulo 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4

1.2.6 Cap??tulo 7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4

2 A note on the Riemann problem for the random transport equation 7

2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7

2.2 The Riemann problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9

2.3 Monte Carlo simulations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16

2.4 Concluding remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18

References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18

3 A finite volume method for the mean of the solution of the random

linear transport equation 21

3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21

3.2 The numerical scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23

3.3 Numerical analysis of the scheme . . . . . . . . . . . . . . . . . . . . . . . 27

3.4 Numerical examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29

3.5 Concluding remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32

xiii



References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33

4 A numerical scheme for the variance of the solution of the random linear

transport equation 35

4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35

4.2 The numerical scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36

4.3 Numerical analysis of the scheme . . . . . . . . . . . . . . . . . . . . . . . 40

4.4 Numerical examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41

4.5 Concluding remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43

References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44

5 Statistical moments of the random linear transport equation 47

5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47

5.2 The numerical scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49

5.3 Numerical analysis of the scheme . . . . . . . . . . . . . . . . . . . . . . . 52

5.3.1 The Normal case . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53

5.4 The system of partial differential equations for the central moments . . . . 53

5.5 Computational tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56

5.6 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60

References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60

6 Statistical moments of the solution of the random Burgers-Riemann

problem 63

6.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63

6.2 The random solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66

6.3 The algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69

6.4 Computational tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72

6.5 Concluding remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76

References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76

7 On the evaluation of moments for solute transport by random velocity

fields 79

7.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79

7.2 Main result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80

7.3 First application: Gaussian processes . . . . . . . . . . . . . . . . . . . . . 81

7.3.1 The probability density function . . . . . . . . . . . . . . . . . . . . 82

7.4 Second application: Telegraph processes . . . . . . . . . . . . . . . . . . . 83

References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84

xiv



8 Concluso?es e trabalhos futuros 87

8.1 Concluso?es . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87

8.2 Trabalhos futuros . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87

8.2.1 Problema 1. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88

8.2.2 Problema 2. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88

8.2.3 Problema 3. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90

Refere?ncias Bibliogra?ficas 91

xv



Lista de Tabelas

6.1 Illustration of the first step of Algorithm 1 . . . . . . . . . . . . . . . . . . 70

6.2 Absolute errors and CPU times; h = 0.01. . . . . . . . . . . . . . . . . . . 73

6.3 Absolute errors and CPU times; h = 0.01 (600 subintervals). . . . . . . . . 74

xvii



Lista de Figuras

2.1 Interval of dependence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10

2.2 ?U (x, T )?, fixed T . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
2.3 A is normal, A ? N (1, 0.6), and T = 0.4. . . . . . . . . . . . . . . . . . . . 17
2.4 A is normal, A ? N (1, 0.6), and T = 0.8. . . . . . . . . . . . . . . . . . . . 17
2.5 A is lognormal, A = exp (?), ? ? N (0.5, 0.15), and T = 0.4. . . . . . . . . . 17
2.6 A is lognormal, A = exp (?), ? ? N (0.5, 0.15), and T = 0.8. . . . . . . . . . 17

3.1 ?x = 0.016, ?t = 0.002 (a), and ?t = 0.00065 (b). . . . . . . . . . . . . . 30

3.2 ?x = 0.016, ?t = 0.005 (a), and ?t = 0.0022 (b). . . . . . . . . . . . . . . 30

3.3 ?x = 0.004, ?t = 0.003 (a), and ?t = 0.001 (b). . . . . . . . . . . . . . . . 30

3.4 ?x = 0.016, ?t = 0.002 (a), and ?t = 0.00065 (b). . . . . . . . . . . . . . 31

3.5 ?x = 0.016, ?t = 0.005 (a), and ?t = 0.0022 (b). . . . . . . . . . . . . . . 32

3.6 ?x = 0.004, ?t = 0.003 (a), and ?t = 0.001 (b). . . . . . . . . . . . . . . . 32

4.1 ?x = 0.02 and T = 0.3. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42

4.2 ?x = 0.02 and T = 0.5. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42

4.3 ?x = 0.01 and T = 0.3. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42

4.4 ?x = 0.01 and T = 0.5. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43

4.5 ?x = 0.02 and T = 0.3. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44

4.6 ?x = 0.02 and T = 0.5. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44

4.7 ?x = 0.01 and T = 0.3. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45

4.8 ?x = 0.01 and T = 0.5. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45

5.1 Schematic diagram of the algorithm. . . . . . . . . . . . . . . . . . . . . . 49

5.2 A ? N (1.0, 0.8), ?x = 0.01, ?t = 0.000195, and tf = 0.4. . . . . . . . . . . 57
5.3 A = exp(?), ? ? N (0.5, 0.35), ?x = 0.01, ?t = 0.000312, and tf = 0.4. . . . 58
5.4 A ? N (?0.5, 0.6), ?x = 0.02, ?t = 0.000138, and tf = 0.4. . . . . . . . . . 59

6.1 Integration regions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67

6.2 Discretization scheme of the ?M square. . . . . . . . . . . . . . . . . . . . 69

6.3 Mean at t = 0.4 (left) and t = 0.8 (right). . . . . . . . . . . . . . . . . . . . 72

xix



6.4 Approximations to the statistical moments using the Monte Carlo method

(with 50 000 realizations), and Algorithm 1 (with N=601). . . . . . . . . . 74

6.5 Approximations to the statistical moments using the Monte Carlo method

(with 50 000 realizations), and Algorithm 1 (with N=601). . . . . . . . . . 75

6.6 Approximations to the statistical moments using the Monte Carlo method

(with 50 000 realizations), and Algorithm 1 (with N=601). . . . . . . . . . 75

7.1 Mean (left), variance (middle), and third central moment (right) of the

solution to (7.5); ? = 0.1. . . . . . . . . . . . . . . . . . . . . . . . . . . . 82

7.2 Mean (left), variance (middle), and third central moment (right) of the

solution to (7.5); ? = 1.0. . . . . . . . . . . . . . . . . . . . . . . . . . . . 82

xx



Cap??tulo 1

Introduc?a?o

1.1 Motivac?a?o e objetivos

Modelos matema?ticos para problemas f??sicos frequ?entemente te?m a forma de sistemas de

equac?o?es diferenciais parciais. Estes modelos usualmente envolvem para?metros como, por

exemplo, os coeficientes no operador diferencial, e as condic?o?es iniciais e de fronteira.

Tipicamente, assume-se que os para?metros sa?o conhecidos, ou seja, os modelos sa?o consi-

derados determin??sticos. Entretanto, em situac?o?es mais reais esta hipo?tese frequ?entemente

na?o se verifica dado que a maioria dos para?metros do modelo possui uma caracter??stica

aleato?ria ou estoca?stica. Modelos mais avanc?ados costumam levar em considerac?a?o esta

natureza estoca?stica dos para?metros. Em vista disso, certos componentes do sistema sa?o

modelados como varia?veis aleato?rias ou func?o?es aleato?rias. Equac?o?es diferenciais com

para?metros aleato?rios sa?o chamadas equac?o?es diferenciais aleato?rias (ou estoca?sticas).

Novas metodologias matema?ticas te?m sido desenvolvidas para lidar com equac?o?es diferen-

ciais aleato?rias (veja [13, 17, 18, 22, 28, 30, 44], por exemplo); entretanto, este problema

continua sendo objeto de estudo de muitos pesquisadores.

Dentre as va?rias metodologias para tratar equac?o?es diferenciais aleato?rias podemos

citar os me?todos para equac?o?es dos momentos (veja [28, 44], por exemplo). Nestes

me?todos o objetivo e? obter equac?o?es diferenciais determin??sticas que governem os mo-

mentos estat??sticos da soluc?a?o do problema aleato?rio. A mais relevante destas equac?o?es e?

a equac?a?o diferencial para a me?dia (esperanc?a matema?tica) da soluc?a?o, chamada equac?a?o

efetiva. Estas equac?o?es diferenciais determin??sticas sa?o enta?o resolvidas numericamente

ou analiticamente. Conve?m ressaltar que tal estrate?gia na?o e? simples de ser aplicada e

os me?todos oriundos desta metodologia possuem inu?meras restric?o?es de validade, con-

sequ?e?ncias de va?rias aproximac?o?es que sa?o necessa?rias durante o processo (veja [44], por

exemplo).

Na maioria dos casos as soluc?o?es nume?ricas de equac?o?es diferenciais parciais com co-

1



2 Cap??tulo 1

eficientes aleato?rios sa?o calculadas (aproximadas) usando o conhecido me?todo de Monte

Carlo (veja [10, 35], por exemplo). Neste caso, os para?metros aleato?rios do modelo sa?o

amostrados repetidamente reduzindo o problema a? soluc?a?o de uma equac?a?o diferencial de-

termin??stica para cada amostra. As propriedades estoca?sticas da soluc?a?o sa?o subsequ?ente-

mente determinadas pela ana?lise estat??stica do conjunto de soluc?o?es obtidas. De um modo

geral, este me?todo demanda geradores de nu?meros aleato?rios e um nu?mero excessivo de

simulac?o?es nume?ricas, ou seja, o custo computacional e? alto.

Assim sendo, e? importante a busca por novas te?cnicas (nume?ricas ou anal??ticas) de

tratar equac?o?es diferenciais aleato?rias. Durante a realizac?a?o do curso de doutorado e

vislumbrando a possibilidade de futuras aplicac?o?es em problemas de fluxo de fluidos em

meios porosos (dispersa?o de poluentes e fluxos bifa?sicos, por exemplo), desenvolvemos

trabalhos relacionados aos seguintes problemas:

• a equac?a?o do transporte linear unidimensional aleato?ria:

?

?t
Q(x, t) + A(t, x)

?

?x
Q(x, t) = 0, t &gt; 0, x ? R,

Q(x, 0) = Q0(x), (1.1)

onde a velocidade A(t, x) e a condic?a?o inicial sa?o func?o?es aleato?rias. Esta equac?a?o

e? frequ?ente, por exemplo, em problemas de dispersa?o de poluentes no qual a va-

riabilidade da permeabilidade do meio poroso tem como consequ?e?ncia um campo

aleato?rio de velocidades, utilizado para calcular a concentrac?a?o do poluente.

• o problema de Burgers-Riemann unidimensional aleato?rio:

?

?t
Q(x, t) +

1

2

?

?x
Q2(x, t) = 0, t &gt; 0, x ? R,

Q(x, 0) =

{
QL, if x &amp;lt;0,

QR, if x &gt; 0,
(1.2)

onde QL and QR sa?o varia?veis aleato?rias. Aqui a aleatoriedade aparece somente

na condic?a?o inicial. A versa?o determin??stica de (1.2) foi introduzida por Burgers

[1] como um modelo simplificado para capturar caracter??sticas ba?sicas em dina?mica

dos gases. Mas, em vez de modelar um processo f??sico, a Equac?a?o de Burgers

inv??scida tem sido usada no desenvolvimento de me?todos nume?ricos e teo?ricos para

equac?o?es hiperbo?licas determin??sticas. A investigac?a?o dos momentos estat??sticos da

soluc?a?o deste problema foi nosso primeiro passo na direc?a?o do tratamento de leis de

conservac?a?o na?o-lineares com para?metros aleato?rios.



1.2. Organizac?a?o da tese 3

1.2 Organizac?a?o da tese

O texto desta tese esta? organizado de modo a agrupar (cronologicamente) os principais

artigos, publicados e/ou submetidos para publicac?a?o, que foram resultados da pesquisa

realizada. A seguir apresentamos uma breve relato sobre o conteu?do de cada cap??tulo.

1.2.1 Cap??tulo 2

A note on the Riemann problem for the random transport equation

(Ref. [4]; publicado no journal: Computational &amp;amp; Applied Mathematics).

Apresentamos uma expressa?o expl??cita para a soluc?a?o do problema (1.1), com A(t, x) = A

uma varia?vel aleato?ria, e com condic?a?o inicial dada por

Q(x, 0) =

{
Q+0 , x &gt; 0,

Q?0 , x &amp;lt;0,

onde os estados iniciais, Q?0 e Q
+
0 , sa?o varia?veis aleato?rias. Este problema e? conhecido

como problema de Riemann aleato?rio. Sua soluc?a?o e? fundamental no desenvolvimento

de esquemas nume?ricos com condic?a?o inicial mais geral, via me?todo de Godunov [14, 27]

ou me?todo de Glimm [12] (random choice method). Mostramos que esta soluc?a?o (func?a?o

aleato?ria) e? de similaridade e, admitindo independe?ncia estat??stica entre a velocidade, A,

e os estados iniciais, Q?0 e Q
+
0 , obtemos uma expressa?o para os momentos estat??sticos.

1.2.2 Cap??tulo 3

A finite volume method for the mean of the solution of the random linear

transport equation (Ref. [8]; publicado no Journal of Applied Mathematics and Com-

putation).

Utilizando as ide?ias do trabalho [4] e o me?todo de Godunov [14], constru??mos um es-

quema nume?rico expl??cito para a me?dia da soluc?a?o da Equac?a?o (1.1), onde A(t, x) = A

e? uma varia?vel aleato?ria e a condic?a?o inicial, Q(x, 0) = Q0(x), e? uma func?a?o aleato?ria.

Sob algumas hipo?teses na discretizac?a?o obtemos condic?o?es de estabilidade do me?todo e

mostramos sua consiste?ncia com uma equac?a?o do tipo advecc?a?o-difusa?o determin??stica.

Va?rios exemplos computacionais mostram uma boa concorda?ncia dos resultados quando

comparados com o me?todo de Monte Carlo.

1.2.3 Cap??tulo 4

A numerical scheme for the variance of the solution of the random linear

transport equation (Ref. [5]; publicado no Journal of Applied Mathematics and Com-

putation).



4 Cap??tulo 1

Avanc?ando no conhecimento dos momentos estat??sticos da soluc?a?o de (1.1), com A(t, x) =

A uma varia?vel aleato?ria e Q0(x) uma func?a?o aleato?ria, propomos um esquema nume?rico

expl??cito para a varia?ncia da soluc?a?o. Obtemos as condic?o?es de estabilidade do me?todo

proposto e mostramos sua consiste?ncia com um sistema determin??stico (para a me?dia e

a varia?ncia) de equac?o?es do tipo advecc?a?o-difusa?o na?o-homoge?neo desacoplado. Testes

computacionais sa?o apresentados para avaliar a proposta.

1.2.4 Cap??tulo 5

Statistical moments of the random linear transport equation

(Ref. [6]; submetido para o Journal of Computational Physics).

Neste trabalho generalizamos as ide?ias de [5, 8] e apresentamos um esquema nume?rico

para os momentos da soluc?a?o da equac?a?o do transporte linear unidimensional aleato?ria

(1.1), com A(t, x) = A uma varia?vel aleato?ria e Q0(x) uma func?a?o aleato?ria. O esquema

e? baseado na soluc?a?o de problemas de Riemann e no me?todo de Godunov. Mostramos

que o esquema e? consistente e esta?vel com uma equac?a?o do tipo advecc?a?o-difusa?o. Ale?m

disso, no caso em que a velocidade e? normalmente distribu??da, obtemos um sistema de

equac?o?es diferenciais parciais para os momentos e momentos centrais da soluc?a?o. Testes

computacionais sa?o apresentados para avaliar a proposta.

1.2.5 Cap??tulo 6

Statistical moments of the solution of the random Burgers-Riemann problem

(Ref. [7]; submetido para o Journal of Mathematics and Computers in Simulation ).

Neste trabalho apresentamos uma expressa?o para a soluc?a?o da Equac?a?o de Burgers aleato?ria

(1.2). A soluc?a?o aleato?ria permite expresso?es integrais para os momentos estat??sticos da

soluc?a?o. Usando ide?ias de integrac?a?o nume?rica, propomos um algoritmo eficiente para

calcular os momentos estat??sticos da soluc?a?o. Testes computacionais sa?o apresentados

para validar a proposta.

1.2.6 Cap??tulo 7

On the evaluation of moments for solute transport by random velocity fields

(Ref. [9]; submetido para o Journal of Applied Numerical Mathematics).

Apresentamos um u?til resultado para a equac?a?o do transporte linear aleato?ria multidi-

mensional. Basicamente, provamos que algumas metodologias baseadas em averaging

approach para aproximar a me?dia estat??stica da soluc?a?o da equac?a?o do transporte linear

aleato?ria (Equac?a?o 1.1, multidimensional) podem ser va?lidas para todos os momentos



1.2. Organizac?a?o da tese 5

estat??sticos da soluc?a?o. Com este resultado podemos obter mais informac?o?es estat??sticas

sobre a soluc?a?o aleato?ria, como ilustrado em dois exemplos particulares.



Cap??tulo 2

A note on the Riemann problem for

the random transport equation

Abstract

We present an explicit expression to the solution of the random Riemann problem for the

one-dimensional random linear transport equation. We show that the random solution is a

similarity solution and the statistical moments have very simple expressions. Furthermore,

we verify that the mean, variance, and 3rd central moment agree quite well with the Monte

Carlo method. We point out that our approach could be useful in designing numerical

methods for more general random transport problems.

Keyword: random linear transport equation, Riemann problem, statistical moments.

2.1 Introduction

Conservation laws are differential equations arising from physical principles of the con-

servation of mass, energy or momentum. The simplest of these equations is the one-

dimensional advective equation and its solution plays a role in more complex problems

such as the numerical solution of nonlinear conservation laws [6]. This linear initial value

problem can, for instance, model the concentration, or density, of a chemical substance

transported by a one dimensional fluid that flows with a known velocity. The deterministic

problem is to find u(x, t) such that
{

ut + a(x)ux = 0, t &gt; 0, x ? R,
u(x, 0) = u0(x).

(2.1)

It is well known that the solution to (2.1) is the initial condition transported along the

characteristic curves. The characteristic system associated to (2.1) is defined by ordinary

7



8 Cap??tulo 2

differential equations:
?
???
???

dx

dt
= a(x), x(0) = x0,

d[u(x(t), t)]

dt
= 0, u(x, 0) = u0(x),

(2.2)

where the last equation is along the characteristic curve, x(t), given by the first equation.

If a is constant, the characteristics are straight lines and the analytic solution is u(x, t) =

u0(x ? at).
The complexity of natural phenomena compels us to study partial differential equa-

tions with random data. For example, (2.1) may model the flux of a two phase equal

viscosity miscible fluid in a porous media. The total velocity is obtained from Darcy’s law

and it depends on the geology of the porous media. Thus, the external velocity is defined

by a given statistics. Also, the prediction of the initial state of the process is obtained

from data acquired with a few number of exploratory wells using geological methods.

Our aim in this paper is to study the random Riemann problem:
?
??
??

Ut + AUx = 0, t &gt; 0, x ? R,

U (x, 0) = U0(x) =

{
U +0 , x &gt; 0,

U?0 , x &amp;lt;0,

(2.3)

where A, U?0 and U
+
0 are random variables.

Several numerical methods which were developed to solve the deterministic problem

(2.1) use solutions of Riemann problems. For instance, the Random Choice Method,

developed by Glimm [2], and the Godunov’s method [4, 6]. These methods suggest that

the random Riemann solutions can be used for designing numerical methods to random

transport equations, where the velocity and the initial condition are random fields. Our

preliminary results in this direction, i.e., using Godunov’s method with random Riemann

solutions in the averaging step, are promising.

Besides the well-developed theoretical methods such as Ito integrals, Martingales and

Wiener measure [5, 7, 9, 10] to deal with stochastic differential equations, two types of

methods are normally used in the construction of solutions for random partial differential

equations. The first is based on the Monte Carlo method which, in general, demands

massive numerical simulations (see [8], for example), and the second is based on effective

equations (see [3], for example), deterministic differential equations whose solutions are

the statistical means of (2.3).

It is well known that, for each realization A(?) and U0(x, ?), of A and U0(x), re-

spectively, one has a deterministic problem that can be solved analytically using the

characteristics’ method. Therefore, if the probability of the realizations is known then

the random solution U (x, t, ?), and its probability, can be found analytically.



2.2. The Riemann problem 9

On the other hand, if we have precise information about the velocity we may consider

a mixed deterministic-random version for (2.2):

?
???
???

dx

dt
= a, x(0) = x0,

d[U (x(t), t)]

dt
= 0, U (x, 0) = U0(x).

(2.4)

This mixed formulation gives the characteristic straight lines x(t) = x0 + at and a

random ordinary differential equation along these straight lines. The formulation (2.4) is

convenient to our future arguments because, for each realization U0(x, ?) of U0(x), the

random function (x, t) 7? U (x, t, ?) = U0(x ? at, ?) solves (2.4). This means that for
precise values of the velocity the random initial conditions are “transported” along the

straight lines.

In this paper, we use (2.4) to find the random Riemann solution to (2.3). The pro-

cedure and the theoretical consequences are presented in Section 2.2. In Section 2.3 we

assess our results by comparing them with the Monte Carlo method.

2.2 The Riemann problem

In this section we study the random Riemann initial value problem:

?
????
????

dX

dt
= A, X(0) = x0,

d[U (X, t)]

dt
= 0, U (x, 0) =

{
U +0 , x &gt; 0,

U?0 , x &amp;lt;0,

(2.5)

where A, U?0 and U
+
0 are random variables. We assume the statistical independence of

A and both U?0 and U
+
0 , and that their cumulative probability functions, FA and FU?U + ,

are known.

In our approach we focus on partial realizations in (2.5), i.e., we consider only A(?),

letting the data U?0 and U
+
0 out of the realizations. This kind of decoupling of the system

(2.5) allows us to use the solution of (2.4). To simplify, let us consider A continuously

varying in some interval [am, aM ], am &amp;lt;aM .

We recall that each realization A(?) yields the random function (x, t) 7? U0(x?A(?)t),
i.e., the initial condition at x0 = x ? A(?)t. Also, as illustrated in Figure 2.1, for a fixed
(x?, t? ) we have x? ? aM t? ? x0 ? x? ? amt?. Hence, the solution at (x?, t? ) depends upon
the initial data in the interval [ x? ? aM t?, x? ? amt? ]. As shown in Figure 2.1, this interval
is determined by two characteristics x ? aM t = constant and x ? amt = constant, both



10 Cap??tulo 2

x ? aM t = constant

x? ? amt? xx? ? aM t?
0

1

?
1

aM

1

am

(x?, t?)

x ? amt = constant

Figure 2.1: Interval of dependence

passing through (x?, t? ). From now on the interval [ x? ? aM t?, x? ? amt? ] will be referred to
as the interval of dependence of the point (x?, t? ).

To separate the contributions of the left state, U?0 , and right state, U
+
0 , to the solution

at (x?, t? ), we shall call ? = x?/t? and define the following disjoint subsets of [am, aM ]:

M? = {a; xa = x? ? at? &amp;lt;0} and M + = {a; xa = x? ? at? &gt; 0} .

Comparing the slopes of the characteristics (see Figure 2.1), we can rewrite these sets

as

M? =
{

a;
1

aM
? 1

a
&lt;

1

?

}
= {a; ? &amp;lt;a ? aM}

and

M + =

{
a;

1

?
&lt;

1

a
? 1

am

}
= {a; am ? a &amp;lt;?} .

Thus, the probability of occurrence of the sets M + and M? can be calculated using
the cumulative probability function of the velocity:

P (M +) = FA(?) = ? and P (M
?) = 1 ? FA(?) = 1 ? ?. (2.6)

The solution to (2.5) is given by the following result:

Proposition 2.1. Let (x?, t? ), t? &gt; 0, be an arbitrary point and ? = x?/t?. The solution to

(2.5) at (x?, t? ) is the random variable

U (x?, t? ) = (1 ? X)U?0 + XU +0 = U?0 + X
(
U +0 ? U?0

)
, (2.7)

where X is the Bernoulli random variable with P (X = 0) = 1 ? FA(?) and P (X = 1) =
FA(?).



2.2. The Riemann problem 11

Proof. To prove this proposition we use the characteristics x ? amt = 0 and x ? aM t =
0 to divide the semi-plane t ? 0 into three regions, R1 = {(x, t); x &amp;lt;amt}, R2 =
{(x, t); amt ? x ? aM t}, and R3 = {(x, t); x &gt; aM t}, and we demonstrate (2.7) for each
one of this regions.

If (x?, t? ) ? R2, we may divide the interval of dependence into two sub-intervals: I? =
[ x? ? aM t?, 0 ) and I+ = [ 0, x? ? amt? ]. In a realization such that x0 = x? ? A(?)t? ? I?,
only the left state will contribute to the solution. On the other hand, we also conclude

that x0 = x? ? A(?)t? ? I? if and only if A(?) ? M?, and therefore the probability
of occurrence of I? is equal to the probability of occurrence of M?. Thus, from (2.6)
it follows that P (I?) = P (M?) = 1 ? FA(?). Otherwise, in a realization such that
x0 = x? ? A(?)t? ? I+, the contribution will be due only to the right state and we use
the same arguments above to conclude that P (I+) = P (M +) = FA(?). Finally, taking in

account the probability of occurrence of U?0 and U
+
0 , the solution is obtained “weighting”

their respective probabilities, i.e., U (x?, t? ) = (1 ? X)U?0 + XU +0 , where X is the Bernoulli
random variable with P (X = 1) = FA(?) and P (X = 0) = 1 ? FA(?).
If (x?, t? ) ? R1 then x? ? amt? &amp;lt;0 and all the points of the interval of dependence,
[ x? ? aM t?, x? ? amt? ], are negatives. Therefore, only the left state contributes to the so-
lution, i.e., U (x?, t? ) = U?0 with probability one. In this case the solution is (2.7) with
FA(?) = 0. On the other hand, if (x?, t? ) ? R3 only the right state contributes to the
solution and we have U (x?, t? ) = U +0 with probability one, i.e., (2.7) with FA(?) = 1.

Corollary 2.1. The solution of (2.5) is constant along the rays x/t = constant, i.e., the

random solution is a similarity function.

Proof. This result follows directly from (2.7) since if x/t = constant then FA (x/t) =

constant.

Proposition 2.2. If (x, t) is fixed, n ? N, n ? 1, and we assume the statistical indepen-
dence of A and both U?0 and U

+
0 , then the nth moment of the random solution (2.7) is

given by:

?U n(x, t)? =
?
(U?0 )

n
?

+ FA

(x
t

) {?
(U +0 )

n
?
?

?
(U?0 )

n
?}

. (2.8)

Proof. From Proposition 2.1,

U (x, t) = U?0 + X
(
U +0 ? U?0

)
= (1 ? X)U?0 + XU +0 ,



12 Cap??tulo 2

where X = X(x, t) is the Bernoulli random variable:

X =

{
1, P (X = 1) = FA

(
x
t

)
= ?

0, P (X = 0) = 1 ? FA
(

x
t

)
= 1 ? ?.

It is easy to see that ?Xj? = FA
(

x
t

)
= ?, for all j = 1, 2, 3, ....

To prove (2.8) we first need the following results:

• For all n ? 1,
n?

j=0

(?1)j
(

n

j

)
= 0, (2.9)

where

(
n

j

)
is the binomial coefficient.

• For n ? 1 and 1 ? j ? n ? 1,

?
(1 ? X)n?j Xj

?
= (2.10)

=

?
Xj

n?j?
m=0

(?1)m
(

n ? j
m

)
Xm

?
=

?
n?j?
m=0

(?1)m
(

n ? j
m

)
Xm+j

?
=

=

n?j?
m=0

(?1)m
(

n ? j
m

) ?
Xm+j

?
? ?? ?

?

= ?

n?j?
m=0

(?1)m
(

n ? j
m

)

? ?? ?
zero by (2.9)

= 0.

• For n ? 1,

?(1 ? X)n? =
?

n?
j=0

(?1)j
(

n

j

)
Xj

?
= (2.11)

= 1 +
n?

j=1

(?1)j
(

n

j

) ?
Xj

?
? ?? ?

?

= 1 + ?
n?

j=1

(?1)j
(

n

j

)

? ?? ?
?1 by (2.9)

= 1 ? ?.



2.2. The Riemann problem 13

Now, assuming the independence of A and both U?0 and U
+
0 , we have:

?U n(x, t)? =
?[

(1 ? X)U?0 + XU +0
]n?

=

=

?
n?

j=0

(
n

j

)
(1 ? X)n?jXj

(
U?0

)n?j (
U +0

)j
?

=

= ?(1 ? X)n?? ?? ?
1?? by (2.11)

?(
U?0

)n?
+ ?Xn?? ?? ?

?

?(
U +0

)n?
+

+
n?1?
j=1

(
n

j

) ?
(1 ? X)n?jXj

?
? ?? ?

zero by (2.10)

?(
U?0

)n?j (
U +0

)j?
=

= (1 ? ?)
?(

U?0
)n?

+ ?
?(

U +0
)n?

.

Corollary 2.2. For a fixed (x, t), ? = FA (x/t), and considering the independence of A

and both U?0 and U
+
0 , the mean of the solution (2.5) is

?U (x, t)? = (1 ? ?)?U?0 ? + ??U +0 ? = ?U?0 ? + ?
[
?U +0 ? ? ?U?0 ?

]
. (2.12)

Proof. The expression (2.12) follows from (2.8) with n = 1.

Proposition 2.3. Let (x1, t1) and (x2, t2) be fixed. Define ?j = xj/tj, ?j = FA(?j), and

H(?j) = U (xj, tj) as in (2.7) (j = 1, 2). Also, consider the statistical independence of A

and both U?0 and U
+
0 . If ?1 6= ?2 then the covariance between H(?1) and H(?2) is

Cov[H(?1), H(?2)] = (1 ? ?1)(1 ? ?2)V ar[U?0 ] + ?1?2V ar[U +0 ]+
+ {?1(1 ? ?2) + ?2(1 ? ?1)} Cov[U?0 , U +0 ]. (2.13)

On the other hand, if ?1 = ?2 then the variance is given by

V ar[H(?1)] = V ar[U (x1, t1)] = V ar[U
?
0 ] + ?1

{
V ar[U +0 ] ? V ar[U?0 ]

}
+

+ ?1(1 ? ?1)[?U +0 ? ? ?U?0 ?]2. (2.14)

Proof. At first, from (2.7) and (2.12) we can observe that

?H(?1)??H(?2)? =
{
?U?0 ? + ?1[?U +0 ? ? ?U?0 ?]

} {
?U?0 ? + ?2[?U +0 ? ? ?U?0 ?]

}
=

= ?U?0 ?2 + (?1 + ?2)?U?0 ?[?U +0 ? ? ?U?0 ?] + ?1?2[?U +0 ? ? ?U?0 ?]2, (2.15)



14 Cap??tulo 2

and

?H(?1)H(?2)? =
?{

U?0 + X(?1)[U
+
0 ? U?0 ]

} {
U?0 + X(?2)[U

+
0 ? U?0 ]

}?
=

= ?(U?0 )2? +
?
{X(?1) + X(?2)}

{
U?0 [U

+
0 ? U?0 ]

}?
+

+ ?X(?1)X(?2)??[U +0 ? U?0 ]2?,
where X(?j) (j=1,2) is the Bernoulli random variable defined in (2.7).

Since

V ar[U +0 ? U?0 ] = V ar[U?0 ] + V ar[U +0 ] ? 2Cov[U?0 , U +0 ] =
= ?[U +0 ? U?0 ]2? ? [?U +0 ? ? ?U?0 ?]2,

we have

?H(?1)H(?2)? = ?(U?0 )2? + (?1 + ?2)?U?0 [U +0 ? U?0 ]? + ?X(?1)X(?2)?[?U +0 ? ? ?U?0 ?]2+
+ ?X(?1)X(?2)?

{
V ar[U?0 ] + V ar[U

+
0 ] ? 2Cov[U?0 , U +0 ]

}
. (2.16)

From (2.15) and (2.16) it follows that

Cov[H(?1), H(?2)] = ?H(?1)H(?2)? ? ?H(?1)??H(?2)? =
= V ar[U?0 ] + (?1 + ?2)

{
?U?0 [U +0 ? U?0 ]? ? ?U?0 ?[?U +0 ? ? ?U?0 ?]

}
+

+ ?X(?1)X(?2)?
{
V ar[U?0 ] + V ar[U

+
0 ] ? 2Cov[U?0 , U +0 ]

}
+

+ {?X(?1)X(?2)? ? ?1?2} [?U +0 ? ? ?U?0 ?]2 =
= V ar[U?0 ] + (?1 + ?2)

{
Cov[U?0 , U

+
0 ] ? V ar[U?0 ]

}
+

+ ?X(?1)X(?2)?
{
V ar[U?0 ] + V ar[U

+
0 ] ? 2Cov[U?0 , U +0 ]

}
+

+ {?X(?1)X(?2)? ? ?1?2} [?U +0 ? ? ?U?0 ?]2,
or, equivalently,

Cov[H(?1), H(?2)] = {1 ? ?1 ? ?2 + ?X(?1)X(?2)?} V ar[U?0 ] + ?X(?1)X(?2)?V ar[U +0 ]+
+ {?1 + ?2 ? 2?X(?1)X(?2)?} Cov[U?0 , U +0 ]+
+ {?X(?1)X(?2)? ? ?1?2} [?U +0 ? ? ?U?0 ?]2. (2.17)

Now we can observe that

?X(?1)X(?2)? =
{

?1?2 if ?1 6= ?2,
?1 if ?1 = ?2.

(2.18)

Therefore, if ?1 6= ?2 we have from (2.17) and (2.18):
Cov[H(?1), H(?2)] = {1 ? ?1 ? ?2 + ?1?2?} V ar[U?0 ] + ?1?2V ar[U +0 ]+

+ {?1 + ?2 ? 2?1?2} Cov[U?0 , U +0 ] =
= (1 ? ?1)(1 ? ?2)V ar[U?0 ] + ?1?2V ar[U +0 ]+
+ {?1(1 ? ?2) + ?2(1 ? ?1)} Cov[U?0 , U +0 ].



2.2. The Riemann problem 15

Otherwise, if ?1 = ?2 it follows that

V ar[H(?1)] = [1 ? ?1]V ar[U?0 ] + ?1V ar[U +0 ] + ?1(1 ? ?1)[?U +0 ? ? ?U?0 ?]2
= V ar[U?0 ] + ?1

{
V ar[U +0 ] ? V ar[U?0 ]

}
+ ?1(1 ? ?1)[?U +0 ? ? ?U?0 ?]2.

As an illustration we plot in Figure 2.2 the mean of the solution at t = T , ?U (x, T )?,
using (2.12). We can observe a diffusive behavior in the interval [amT, aM T ], called by

some authors the mixing zone. In this mixing zone, ?U (x, T )? is the mean of the left state
added to the product of the cumulative probability function of the velocity and the jump

between the means of right and left states. This illustration also shows that the shape of

the cumulative probability function of A controls the mixing zone: only symmetric density

functions will produce antisymmetrical mixing zones. Our computational tests will make

clear this remark (see Figures 2.3-2.4 (symmetric) and Figures 2.5-2.6 (nonsymmetric)).

?U
?
0
?

amT 0 aM T x

?U
+

0
?

Figure 2.2: ?U (x, T )?, fixed T

The length of this mixing zone is studied by some authors (see [1, 3, 11, 12], for

example) using the effective equation methodology. For instance, the effective equation

for the linear transport with random velocity, ?(x), is

??c?
?t

+ ??(x)? ??c?
?x

? D(t) ?
2?c?
?x2

= 0,

with the dissipation coefficient given by

D(t) =

? t
0

???(x ? st)??(x)?ds.

If the random velocity is constant then

D(t) =

? t
0

???2?ds = ?2t,



16 Cap??tulo 2

where ? is the standard deviation of ?.

We shall compare a particular solution of the effective equation methodology with our

expression for the mean, (2.12). If we take the initial condition

?c(x, 0)? = U0(x) =
{

1, x &amp;lt;0,

0, x &gt; 0,

for both the effective equation and problem (2.5), we can show that the analytical expres-

sions for the mean are:

(i) using the effective equation:

?c(x, t)? = 1
2

{
1 ? 2?

?

? x????t
l(t)

0

e??
2

d?

}
,

where l(t) = 2

[? t
0

D(?) d?

] 1
2

is the mixing length;

(ii) using (2.12) with a normally distributed random velocity, A ? N (???, ?):

?U (x, t)? = 1
2

{
1 ? 2?

?

? x????t?
2?t

0

e??
2

d?

}
.

Comparing these expressions, they will be equal only if the mixing length satisfies

l(t) =
?

2?t or, equivalently, if the diffusion coefficient of the effective equation is D(t) =

?2t, i.e., the same dissipation coefficient for the constant velocity case.

2.3 Monte Carlo simulations

To assess our results we compare the expressions for the mean, variance and 3rd central

moment with the Monte Carlo method. We use suites of realizations of A, U?0 and U
+
0

considering: the independence of A and both U?0 and U
+
0 ; U

?
0 and U

+
0 have a bivariate

normal distribution with ?U?0 ? = 1, ?U +0 ? = 0, V ar
[
U?0

]
= 0.16, V ar

[
U +0

]
= 0.25 and

Cov
(
U?0 , U

+
0

)
= 0.12. We plot the results in T = 0.4 and T = 0.8. In order to investigate

the influence of the velocity randomness we use two models: (i) A is normally distributed,

A ? N (1, 0.6), in Figures 2.3 and 2.4; (ii) A is lognormally distributed, A = exp (?),
? ? N (0.5, 0.15), in Figures 2.5 and 2.6. All Monte Carlo simulations were performed
with 1 500 realizations and recalling that the solution to (2.5), at (x, t), for a single

realization
(
A(?), U?0 (?), U

+
0 (?)

)
of

(
A, U?0 , U

+
0

)
, is

U (x, t) = U0(x ? A(?)t) =
{

U?0 (?), x ? A(?)t &amp;lt;0,
U +0 (?), x ? A(?)t &gt; 0.

All the numerical experiments presented in this section were computed in double precision

with some MATLAB codes on a 3.0Ghz Pentium 4 with 512Mb of memory.



2.3. Monte Carlo simulations 17

?2 0 2 4 6
?0.5

0

0.5

1

1.5
mean of solution

Monte Carlo
proposed

?2 0 2 4 6
0.1

0.2

0.3

0.4

0.5
variance of solution

?2 0 2 4 6
?0.2

?0.1

0

0.1

0.2
3rd central moment of solution

Figure 2.3: A is normal, A ? N (1, 0.6), and T = 0.4.

?2 0 2 4 6
?0.5

0

0.5

1

1.5
mean of solution

Monte Carlo
proposed

?2 0 2 4 6
0.1

0.2

0.3

0.4

0.5
variance of solution

?2 0 2 4 6
?0.2

?0.1

0

0.1

0.2
3rd central moment of solution

Figure 2.4: A is normal, A ? N (1, 0.6), and T = 0.8.

?2 0 2 4 6
?0.5

0

0.5

1

1.5
mean of solution

Monte Carlo
proposed

?2 0 2 4 6
0.1

0.2

0.3

0.4

0.5
variance of solution

?2 0 2 4 6
?0.2

?0.1

0

0.1

0.2
3rd central moment of solution

Figure 2.5: A is lognormal, A = exp (?), ? ? N (0.5, 0.15), and T = 0.4.

?2 0 2 4 6
?0.5

0

0.5

1

1.5
mean of solution

Monte Carlo
proposed

?2 0 2 4 6
0.1

0.2

0.3

0.4

0.5
variance of solution

?2 0 2 4 6
?0.2

?0.1

0

0.1

0.2
3rd central moment of solution

Figure 2.6: A is lognormal, A = exp (?), ? ? N (0.5, 0.15), and T = 0.8.



18 References

2.4 Concluding remarks

In this article we present an expression to the solution of the random Riemann problem

for the linear transport equation with random velocity. As far as we know, this approach

does not appear in the literature and we believe that it can be useful in the development of

numerical procedures for more general random partial differential equations. Expression

(2.7) shows us that if the statistics of the velocity is known then the local behavior of the

solution is independent of the physical mechanisms governing the process. The procedure

also shows agreement with the effective equation methodology when the velocity is a

normal random variable; however, it seems to us that the random expression to the

solution yields more information about the process.

Acknowledgments

Our acknowledgments to the Brazilian Council for Development of Science and Technology

(CNPq) through the grants 5551463/02-3 and 140406/2004-2.

References

[1] F. Furtado, F. Pereira, Scaling analysis for two-phase immiscible flow in heteroge-

neous porous media. Computational and Applied Mathematics, 17(3):237–263 (1998).

[2] J. Glimm, Solutions in the large for nonlinear hyperbolic systems of equations. Comm.

Pure Appl. Math., 18:695–715 (1965).

[3] J. Glimm, D. Sharp, Stochastic partial differential equations: Selected applications

in continuum physics, in Stochastic Partial Differential Equations: Six Perspecti-

ves, (Edited by R. A. Carmona and B. L. Rozovskii), Mathematical Surveys and

Monographs, American Mathematical Society, No. 64, p.03–44, Providence, 1998.

[4] S. K. Godunov, A difference method for numerical calculation of discontinuous solu-

tion of the equations of hydrodynamics. Mat. Sb., 47:271–306 (1959).

[5] P. E. Kloeden, E. Platen, Numerical Solution of Stochastic Differential Equations.

Springer, New York, 1999.

[6] R. J. LeVeque, Finite Volume Methods for Hyperbolic Problems. Cambridge Univer-

sity Press, Cambridge, 2002.



References 19

[7] B. Oksendal, Stochastic Differential Equations: an introduction with applications.

Springer, New York, 2000.

[8] H. Osnes, H. P. Langtangen, A study of some finite difference schemes for a uni-

diretional stochastic transport equation. SIAM Journal on Scientific Computing,

19(3):799–812 (1998).

[9] K. Sobczyk, Stochastic Wave Propagation. Elsevier-PWN Polish Scientific Pub., New

York, 1985.

[10] T. T. Soong, Random Differential Equations in Sciences and Engineering. Academic

Press, New York, 1973.

[11] Q. Zhang, The asymptotic scaling behavior of mixing induced by a random velocity

field. Adv. Appl. Math., 16:23–58 (1995).

[12] Q. Zhang, The transient behavior of mixing induced by a random velocity field.

Water Research Resources, 31:577–591 (1995).



Cap??tulo 3

A finite volume method for the mean

of the solution of the random linear

transport equation

Abstract

We present a numerical scheme, based on Godunov’s method (REA algorithm), for the

statistical mean of the solution of the one-dimensional random linear transport equation,

with homogeneous random velocity and random initial condition. Numerical examples

are considered to validate our method.

Keyword: random linear transport equation, finite volume schemes, Riemann problem,

statistical mean, Godunov’s method (REA algorithm).

3.1 Introduction

Conservation laws are differential equations arising from physical principles of the con-

servation of mass, energy or momentum. The simplest of these equations is the one-

dimensional advective equation and its solution plays a role in more complex problems

such as the numerical solution for nonlinear conservation law. This linear initial value

problem can, for instance, model the concentration, or density, of a chemical substance

transported by a one-dimensional fluid that flows with a known velocity. In the determi-

nistic case, we want to find q(x, t) such that:

{
qt + a(x)qx = 0, t &gt; 0, x ? R,
q(x, 0) = q0(x).

(3.1)

21



22 Cap??tulo 3

It is well known that the solution to (3.1) is the initial condition transported along

the characteristic curves.

The complexity of natural phenomena compels us to study partial differential equa-

tions with random data. For example, (3.1) may model the flux of a two phase equal

viscosity miscible fluid in a porous media. The total velocity is obtained from Darcy’s law

and it depends on the geology of the porous media. Thus, the external velocity is defined

by given statistics. Also, the prediction of the initial state of the process is obtained from

data acquired from a small number of exploratory wells using geological methods.

In this work, we are concerned with the numerical solution of the random version of

the problem (3.1), i.e., the stochastic transport equation,

Qt(x, t) + AQx(x, t) = 0, t &gt; 0, x ? R, (3.2)

with a homogeneous random transport velocity, A, and stochastic initial condition, Q(x, 0) =

Q0(x).

A mathematical basis for the solution of stochastic, or random, partial differential

equations has not been complete yet. Besides the well-developed theoretical methods

such as Ito integrals, Martingales and Wiener measure [7, 8], two types of methods are

normally used in the construction of solutions for random partial differential equations.

The first method is based on the Monte Carlo method, which in general demands massive

numerical simulations using high resolution methods (see [6]), and the second is based on

the effective equations (see [2]), the deterministic differential equations whose solutions

are the statistical means of (3.2).

The solution of (3.2) is a random function. For a particular case when the initial

condition is given by

Q(x, 0) =

{
Q+0 , x &gt; 0,

Q?0 , x &amp;lt;0,
(3.3)

with Q?0 and Q
+
0 random variables, we have shown in [1] that the solution of the Riemann

problem (3.2)-(3.3) is

Q(x, t) = Q?0 + X
(
Q+0 ? Q?0

)
, (3.4)

where X is the Bernoulli random variable with P (X = 0) = 1?FA(x/t) and P (X = 1) =
FA(x/t); here FA(x) is the cumulative probability function of the random variable A.

Also, according to [1], considering the independence of A and both Q?0 and Q
+
0 , the

statistical mean of the solution of the Riemann problem (3.2)-(3.3) for a fixed (x, t) is

?Q(x, t)? = ?Q?0 ? + FA
( x

t

) [
?Q+0 ? ? ?Q?0 ?

]
. (3.5)

Besides the formal verification of the explicit expression (3.4), in [1] we compare (3.5)

with the mean given by an effective equation to (3.2) and also show that the Monte Carlo



3.2. The numerical scheme 23

method agree quite well with (3.5). We can observe that (3.5) gives the mean, ?Q(x, t)?,
without considering either the effective equation or the Monte Carlo method.

In this paper, we use these results to design a numerical scheme to approximate the

statistical mean for (3.2) with a more general initial condition. The method is based on

the Riemann problems solution, Godunov’s ideas, and finite volume methods – widely

used in high-resolution methods for deterministic conservation laws (see [5], Ch. 4).

The outline of this paper is as follows. In Section 3.2 we deduce the explicit numerical

scheme using the ideas of Godunov’s reconstruct-evolve-average algorithm. The analysis of

stability and convergence of the method is presented in Section 3.3. Finally, in Section 3.4

we present and compare some numerical examples.

3.2 The numerical scheme

In this section, we present the numerical method for the mean of the solution of (3.2).

Initially, we discretize both space and time assuming uniform mesh spacing with ?x

and ?t, respectively. We denote the spatial and time grid points by xj = j?x and

tn = n?t, respectively. In a context of finite volume methods, denoting the jth grid cell

by Cj = (xj?1/2, xj+1/2), where xj±1/2 = xj±?x/2, the value denoted by Qnj approximates
the average value of the random function Q(x, tn) over the jth grid cell:

Qnj ?
1

?x

?

Cj

Q(x, tn)dx =
1

?x

? xj+1/2
xj?1/2

Q(x, tn)dx. (3.6)

We follow the basic ideas of REA algorithm, Reconstruct-Evolve-Average, a finite

volume algorithm originally proposed by [3] as a method for solving the nonlinear Euler

equations of gas dynamics.

Assuming that the cell averages at time tn, Q
n
j , are known, we summarize the REA

algorithm (see [5], Ch. 4) in three steps:

Step 1. Reconstruct a piecewise polynomial function Q?(x, tn), defined for all x, from the

cell averages Qnj . In our case we use the piecewise constant function with Q
n
j in the

jth grid cell, i.e., Q?(x, tn) = Q
n
j , for x ? Cj.

Step 2. Evolve the equation exactly, or approximately, with this initial data to obtain

Q?(x, tn+1) a time ?t later. In our case we can evolve exactly using (3.4).

Step 3. Average Q?(x, tn+1) over each grid cell to obtain the new cell averages, i.e.,

Q n+1j =
1

?x

?

Cj

Q?(x, tn+1)dx.



24 Cap??tulo 3

The piecewise constant function, step 1, defines a set of Riemann problems in each

x = xj?1/2: differential equation (3.2) with the initial condition

Q(x, tn) =

{
Qnj?1, x &amp;lt;xj?1/2,
Qnj , x &gt; xj?1/2.

(3.7)

Therefore, we may use (3.4) to solve each Riemann problem:

Q(x, tn+1/2) = Q
n
j?1 + X

(
x ? xj?1/2

?t/2

) [
Qnj ? Qnj?1

]
, (3.8)

where, for x fixed, X(x) is the Bernoulli random variable:

X(x) =

{
1, P (X(x) = 1) = FA(x),

0, P (X(x) = 0) = 1 ? FA(x).
(3.9)

As in the deterministic case the solution at tn+1/2, Q?(x, tn+1/2), can be constructed by

piecing together the Riemann solutions, provided that the half time step ?t/2 is short

enough such that adjacent Riemann problems have not started to interact yet. This

requires that ?x and ?t must be chosen satisfying:

Q(xj?1, tn+1/2) ? Qnj?1 and Q(xj, tn+1/2) ? Qnj ,

where the symbol “ ? ” means “sufficiently near to”.
Substituting the above conditions into (3.8) we must have X (??x/?t) = 0 and

X (?x/?t) = 1 both with probability sufficiently near to 1. From (3.9) this means the

following conditions:

FA

(
??x

?t

)
? 0 and FA

(
?x

?t

)
? 1. (3.10)

Remark 3.1. We may regard (3.10) as a kind of CFL condition for the method: the inter-

val [??x/?t, ?x/?t] must contain the effective support of the density probability function
of A. The word effective support means that outside [??x/?t, ?x/?t] the probability of
A is sufficiently near to zero, i.e., it can be disregarded. The existence of the effective

support is ensured by Chebyshev’s inequality: for any k &gt; 0, P{|A??A?| ? k?A} ? 1/k2,
where ?A is the standard variation of A.

Under hypothesis (3.10) we may finish the step 2 taking

Q?(x, tn+1/2) =
?

j

Q(x, tn+1/2) 1[xj?1, xj ], (3.11)

where 1[xj?1, xj ] is the characteristic function of [xj?1, xj].



3.2. The numerical scheme 25

In step 3 of REA algorithm we use (3.11) to calculate Q
n+1/2
j?1/2 :

Q
n+1/2
j?1/2 =

1

?x

? xj
xj?1

Q?(x, tn+1/2)dx

=
1

?x

? xj
xj?1

{
Qnj?1 + X

(
x ? xj?1/2

?t/2

) [
Qnj ? Qnj?1

]}
dx

= Qnj?1 +
1

?x

{? xj
xj?1

X

(
x ? xj?1/2

?t/2

)
dx

}
[
Qnj ? Qnj?1

]

= Qnj?1 +
?t

2?x

{? ?x
?t

??x
?t

X(x) dx

}
[
Qnj ? Qnj?1

]
. (3.12)

The cell averages, Q
n+1/2
j?1/2 , define new Riemann problems at xj. We repeat the proce-

dure above to obtain the solution in Cj at tn+1:

Qn+1j =
1

?x

? xj+1/2
xj?1/2

{
Q

n+1/2
j?1/2 + X

(
x ? xj
?t/2

) [
Q

n+1/2
j+1/2

? Qn+1/2
j?1/2

]}
dx =

= Q
n+1/2
j?1/2 +

1

?x

{? xj+1/2
xj?1/2

X

(
x ? xj
?t/2

)
dx

} [
Q

n+1/2
j+1/2

? Qn+1/2
j?1/2

]
=

= Q
n+1/2
j?1/2 +

?t

2?x

{? ?x
?t

??x
?t

X(x)dx

} [
Q

n+1/2
j+1/2

? Qn+1/2
j?1/2

]
. (3.13)

Lemma 3.1. Let Y =

? ?
??

X(x)dx be a random variable with ? &gt; 0 and X(x) the random

field defined in (3.9). Then P{Y = ?Y ?} = 1.

Proof. Since ?Y ? =
?? ?

??
X(x)dx

?
=

? ?
??
?X(x)?dx =

? ?
??

FA(x)dx, we have

?Y 2? =
?[? ?

??
X(x)dx

]2?
=

?? ?
??

? ?
??

X(x1)X(x2)dx1dx2

?

=

? ?
??

? ?
??
?X(x1)X(x2)?dx1dx2 =

? ?
??

? ?
??
?X(x1)??X(x2)?dx1dx2

=

? ?
??

? ?
??

FA(x1)FA(x2)dx1dx2 =

[? ?
??

FA(x)dx

]2
= ?Y ?2.

Therefore, V ar(Y ) = ?Y 2? ? ?Y ?2 = 0 and thus P{Y = ?Y ?} = 1.



26 Cap??tulo 3

From this result we can conclude that
? ?x

?t

??x
?t

X(x)dx =

?? ?x
?t

??x
?t

X(x)dx

?
=

? ?x
?t

??x
?t

?X(x)?dx =
? ?x

?t

??x
?t

FA(x)dx,

and thus rewrite (3.12)–(3.13) as

Q
n+1/2
j?1/2 = Q

n
j?1 +

?t

2?x

{? ?x
?t

??x
?t

FA(x) dx

}
[
Qnj ? Qnj?1

]
(3.14)

and

Qn+1j = Q
n+1/2
j?1/2 +

?t

2?x

{? ?x
?t

??x
?t

FA(x)dx

} [
Q

n+1/2
j+1/2

? Qn+1/2
j?1/2

]
. (3.15)

Lemma 3.2. Let A be a random variable and [??, ?] an effective support of the density
probability function, fA, of A. Then

? ?
??

FA (x) dx ? ? ? ?A?. (3.16)

Proof. Using the hypothesis and integration by parts in the definition of the statistical

mean of A we have:

?A? =
? ?
??

x fA (x) dx ?
? ?
??

x fA (x) dx = x FA(x)|??? ?
? ?
??

FA (x) dx.

Since FA(??) ? 0 and FA(?) ? 1 we obtain the result.

Using (3.16) as an approximation of the integral in (3.14) and (3.15), and denoting

? = (?t?A?)/?x, we define the two-step numerical scheme:

Q
n+1/2
j?1/2 =

1

2

[
Qnj?1 + Q

n
j

]
? ?

2

[
Qnj ? Qnj?1

]

and

Qn+1j =
1

2

[
Q

n+1/2
j?1/2 + Q

n+1/2
j+1/2

]
? ?

2

[
Q

n+1/2
j+1/2

? Qn+1/2
j?1/2

]
.

Joining these expressions we can summarize the two-step scheme above in the explicit

method:

Qn+1j = Q
n
j ?

?

2

[
Qnj+1 ? Qnj?1

]
+

1

4

(
1 + ?2

) [
Qnj+1 ? 2Qnj + Qnj?1

]
. (3.17)

Taking the statistical mean in (3.17) we obtain the explicit scheme for the mean of

the solution to (3.2):

?Qn+1j ? = ?Qnj ? ?
?

2

[
?Qnj+1? ? ?Qnj?1?

]
+

1

4

(
1 + ?2

) [
?Qnj+1? ? 2?Qnj ? + ?Qnj?1?

]
, (3.18)

where ? = (?t?A?)/?x.



3.3. Numerical analysis of the scheme 27

Remark 3.2. The numerical method (3.18) is conservative, in the sense that it can be

rewritten as

?Qn+1j ? = ?Qnj ? ?
?t

?x

[
F nj+1/2 ? F nj?1/2

]
,

where F nj?1/2 = (1/2)?A?[?Qnj?1? + ?Qnj ?] ? (1/4)?A? (1? + ?) [?Qnj ???Qnj?1?] is an appro-
ximation to the average flux at x = xj?1/2.

3.3 Numerical analysis of the scheme

In this section, we analyze the convergence of the method (3.18) and show its stability

and consistency with an advective-diffusive equation.

Proposition 3.1. For (?x2/?t) = ? fixed the numerical scheme (3.18) is an O(?x2)
approximation for u(x, t), solution of the deterministic differential equation

ut + ?A?ux =
?

4
uxx. (3.19)

Proof. Let u(x, t) be a smooth function such that u(xj, tn) = ?Qnj ?. Thus, by (3.18) we
have

u(x, t + ?t) = u(x, t) ? ?t
2?x

?A? [u(x + ?x, t) ? u(x ? ?x, t)] +

+
1

4

[
1 +

(
?t

?x
?A?

)2]
[u(x + ?x, t) ? 2u(x, t) + u(x ? ?x, t)] ,

and using Taylor’s expansion it follows
{

ut +
?t

2
utt +

?t2

6
uttt + ...

}
+ ?A?

{
ux +

?x2

6
uxxx + ...

}
=

=
1

4

(
?x2

?t
+ ?t?A?2

) {
uxx +

?x2

2
uxxxx + ...

}
.

Since (?x2/?t) = ? is fixed, we have ?t = (?x2/?) = O(?x2). Thus, grouping the
terms of the same order we arrive at the expression:

ut + ?A?ux =
?

4
uxx + O(?x2).

Proposition 3.2. The numerical method (3.18) is stable under the conditions (3.10) and

?t

?x
|?A?| ? 1. (3.20)



28 Cap??tulo 3

Proof. Using the von Neumann analysis (see [9]) it follows that the amplification factor

associated to (3.18) is

g(?) = 1 ? ?
2

(
ei? ? e?i?

)
+

1

4
(1 + ?2)

(
ei? ? 2 + e?i?

)

= 1 +
1

2
(1 + ?2)(cos ? ? 1) ? i ? sin ?

= 1 ? (1 + ?2) sin2
(

?

2

)
? i 2? sin

(
?

2

)
cos

(
?

2

)
,

for ? ? [??, ?].
The magnitude of the amplification factor g(?) is given by,

|g(?)|2 =
{

1 ? (1 + ?2) sin2
(

?

2

)}2
+ 4?2 sin2

(
?

2

)
cos2

(
?

2

)

= 1 ?
[
2(1 + ?2) ? 4?2

]
sin2

(
?

2

)
+

[
(1 + ?2)2 ? 4?2

]
sin4

(
?

2

)

= 1 ? 2(1 ? ?2) sin2
(

?

2

)
+ (1 ? ?2)2 sin4

(
?

2

)

=

[
1 ? (1 ? ?2) sin2

(
?

2

)]2
, ? ? [??, ?].

Therefore, if |?| ? 1 we have |g(?)| ? 1, for all ? ? [??, ?].

Remark 3.3. We can show that the conditions in (3.10) are sufficient for (3.20). In

fact, using Lemma 3.2:

0 ?
? ?x

?t

??x
?t

FA(x)dx ?
?x

?t
? ?A? ? 2?x

?t
.

Thus ??x/?t ? ?A? ? ?x/?t or |?A?| ? ?x/?t, i.e., ?t |?A?| /?x ? 1. With this
remark we conclude that the conditions (3.10) ensure the stability of the proposed scheme.

Remark 3.4. Under the stability conditions (3.10) and the consistency (Proposition 3.1)

we have the convergence of the means calculated by (3.18) to the solution of equation

(3.19).

Proposition 3.3. Under the conditions (3.10), the numerical scheme (3.18) is total va-

riation diminishing (TVD), i.e., T V (Qn+1) ? T V (Qn).



3.4. Numerical examples 29

Proof. We observe that (3.18) can be rewritten as

?Qn+1j ? = ?Qnj ? ?
(1 + ?)2

4? ?? ?
?

[
?Qnj ? ? ?Qnj?1?

]
+

(1 ? ?)2
4? ?? ?
?

[
?Qn+1j ? ? ?Qnj ?

]
.

According to Harten’s theorem [4] the sufficient conditions to ensure the TVD property

of a method are: ? ? 0, ? ? 0 and ? + ? ? 1. From Remark 3.3 we have |?| ? 1. Thus,
these three conditions are satisfied under hypothesis (3.10).

3.4 Numerical examples

To assess our method for the mean of the linear advective equation with random data we

present two numerical examples. In Example 3.1 we solve a Riemann problem with ran-

dom velocity and deterministic initial condition; in this case the exact solution, ?Q(x, t)?,
is known. In Example 3.2 we apply our method in a problem with random velocity and

initial condition being a correlated random field. In both examples we use A normally,

lognormally, and uniformly distributed, respectively, to compare the effects of the velocity

distribution.

Example 3.1.

Let us consider the PDE (3.2) with the deterministic initial condition

Q(x, 0) =

{
1, x &amp;lt;0,

0, x ? 0.

In Figures 3.1 – 3.3 we compare the approximations of the mean calculated using

(3.18) with the exact values given by (3.5): ?Q(x, t)? = 1 ? FA(x/t). We plot the results
at T = 0.1 and T = 0.3 (figures (a) and (b), respectively). To observe the influence of the

velocity variation we use three models: [i] A is normally distributed, A ? N (1.0, 0.8), in
Figure 3.1; [ii] A is lognormally distributed, A = exp (?), ? ? N (0.5, 0.25), in Figure 3.2;
[iii] A is uniformly distributed in [0.75, 1.25], in Figure 3.3. The values of ?t and ?x are

presented in the captions of the figures. The figures in this example, especially Figure

3.3, also help us in the verification of the “high-resolution” of the proposed method in the

sense that the numerical dispersion of the method does not give a false appearance to the

mixing zone derived from the variability of the velocity.



30 Cap??tulo 3

?0.5 0 0.5 1 1.5
?0.2

0

0.2

0.4

0.6

0.8

1

1.2

Mean of solution

Proposed Method
Exact Solution

?0.5 0 0.5 1 1.5

0

0.2

0.4

0.6

0.8

1

1.2

Mean of solution

Proposed Method
Exact Solution

Figure 3.1: ?x = 0.016, ?t = 0.002 (a), and ?t = 0.00065 (b).

?0.5 0 0.5 1 1.5
?0.2

0

0.2

0.4

0.6

0.8

1

1.2

Mean of solution

Proposed Method
Exact Solution

?0.5 0 0.5 1 1.5

0

0.2

0.4

0.6

0.8

1

1.2

Mean of solution

Proposed Method
Exact Solution

Figure 3.2: ?x = 0.016, ?t = 0.005 (a), and ?t = 0.0022 (b).

?0.5 0 0.5 1 1.5
?0.2

0

0.2

0.4

0.6

0.8

1

1.2

Mean of solution

Proposed Method
Exact Solution

?0.5 0 0.5 1 1.5
?0.2

0

0.2

0.4

0.6

0.8

1

1.2

Mean of solution

Proposed Method
Exact Solution

Figure 3.3: ?x = 0.004, ?t = 0.003 (a), and ?t = 0.001 (b).



3.4. Numerical examples 31

Example 3.2.

Here we consider the PDE (3.2) with a normal random initial condition, Q0(x). The

mean of Q0(x) is

?Q0(x)? =
{

1, x ? (1.4, 2.2),
e?20(x?0.25)

2

, otherwise,
(3.21)

and the covariance is defined by Cov(x, x?) = ?2 exp (??|x ? x?|), where Var[Q0(x)] = ?2
is constant and ? &gt; 0 governs the decay rate of the spatial correlation. In the tests we use

? = 40 and ?2 = 0.2. The numerical results are compared with the Monte Carlo method

using suites of realizations of A and Q0(x), with A and Q0(x) statistically independents.

The solution of (3.2)–(3.21) for a single realization A(?) and Q0(x, ?), of A and Q0(x),

respectively, is given by Q(x, t, ?) = Q0(x ? A(?)t, ?). The realizations of the correlated
random field Q0(x) are generated using the matriz decomposition method, a direct method

for generating correlated random fields (see [10], Ch. 3, for example). We use the Monte

Carlo method with 1500 realizations, and plot the results at T = 0.1 and T = 0.3 (figures

(a) and (b), respectively). Again, we use three models of velocity: [i] A is normally

distributed, A ? N (1.0, 0.8), in Figure 3.4; [ii] A is lognormally distributed, A = exp (?),
? ? N (0.5, 0.25), in Figure 3.5; [iii] A is uniformly distributed in [0.75, 1.25], in Figure 3.6.
The values of ?t and ?x are the same used in Example 3.1. In fact, the known solution

of the Riemann problem allows to choose good values for ?t and ?x. Once these values

were calibrated, they are used in the general initial condition problem with success, as

show the results presented here. The numerical tests have shown that a good choice for

? in (3.19) is ? = 2Var[A]T .

0 1 2 3
?0.2

0

0.2

0.4

0.6

0.8

1

1.2

Mean of solution

0 1 2 3
?0.2

0

0.2

0.4

0.6

0.8

1

1.2
Mean of solution

Mean of Initial Condition
Proposed Method
Monte Carlo Simulations

Figure 3.4: ?x = 0.016, ?t = 0.002 (a), and ?t = 0.00065 (b).



32 Cap??tulo 3

0 1 2 3
?0.2

0

0.2

0.4

0.6

0.8

1

1.2

Mean of solution

0 1 2 3
?0.2

0

0.2

0.4

0.6

0.8

1

1.2

Mean of solution
Mean of Initial Condition
Proposed Method
Monte Carlo Simulations

Figure 3.5: ?x = 0.016, ?t = 0.005 (a), and ?t = 0.0022 (b).

0 1 2 3
?0.2

0

0.2

0.4

0.6

0.8

1

1.2

Mean of solution

0 1 2 3
?0.2

0

0.2

0.4

0.6

0.8

1

1.2

Mean of solution
Mean of Initial Condition
Proposed Method
Monte Carlo Simulations

Figure 3.6: ?x = 0.004, ?t = 0.003 (a), and ?t = 0.001 (b).

3.5 Concluding remarks

In this article, we present a numerical scheme for the statistical mean of the solution of the

random linear transport equation. The random data are the velocity (random variable)

and the initial condition (random function). To design the method we use the basic

ideas of Godunov’s method (REA algorithm) with a known expression for the random

Riemann problem solution. We obtain the stability condition of the method and show its

consistency with a deterministic advective-diffusive equation, which means convergence

of the method. The examples show good agreement of the results with the Monte Carlo

method.

As far as we know, this methodology has not been studied yet. The advantages of

the algorithm are: it does not require an effective equation and it does not demand the

great number of realizations necessary in the Monte Carlo method. We believe that

this methodology can also be applied to solve more general problems, and also to obtain

information about other statistical moments of the solution.



References 33

Acknowledgments

Our acknowledgments to the Brazilian Council for Development of Science and Technology

(CNPq) through the grants 5551463/02-3 and 140406/2004-2 (doctoral scholarship).

References

[1] M. C. C. Cunha, F. A. Dorini, A note on the Riemann problem for the random

transport equation. Computational and Applied Mathematics 26(3):323–335 (2007).

[2] J. Glimm, D. Sharp, Stochastic partial differential equations: Selected applications

in continuum physics, in Stochastic Partial Differential Equations: Six Perspecti-

ves, (Edited by R. A. Carmona and B. L. Rozovskii), Mathematical Surveys and

Monographs, American Mathematical Society, No. 64, p.03–44, Providence, 1998.

[3] S. K. Godunov, A difference method for numerical calculation of discontinuous solu-

tions of the equations of hydrodynamics. Mat. Sb. 47:271–306 (1959).

[4] A. Harten, High resolution schemes for hyperbolic conservation laws. J. Comput.

Phys. 49(2):357–393 (1983).

[5] R. J. LeVeque, Finite Volume Methods for Hyperbolic Problems. Cambridge Univer-

sity Press, Cambridge, 2002.

[6] H. Osnes, H. P. Langtangen, A study of some finite difference schemes for a uni-

diretional stochastic transport equation. SIAM Journal on Scientific Computing

19(3):799–812 (1998).

[7] K. Sobczyk, Stochastic Wave Propagation. Elsevier-PWN Polish Scientific Pub., New

York, 1985.

[8] T. T. Soong, Random Differential Equations in Sciences and Engineering. Academic

Press, New York, 1973.

[9] J. C. Strikwerda, Finite Difference Schemes and Partial Differential Equations.

Wadsworth &amp;amp; Brooks/Cole, California, 1989.

[10] D. Zhang, Stochastic Methods for Flows in Porous Media - Coping with Uncertainties.

Academic Press, 2002.



Cap??tulo 4

A numerical scheme for the variance

of the solution of the random linear

transport equation

Abstract

We present a numerical scheme, based on Godunov’s method (REA algorithm), for the

variance of the solution of the one-dimensional random linear transport equation with

homogeneous random velocity and stochastic initial condition. We obtain the stability

conditions of the method and we also show its consistency with a deterministic nonho-

mogeneous advective-diffusive equation, which means convergence. Numerical results are

considered to validate our scheme.

Keyword: random linear transport equation, finite volume schemes, Godunov’s method.

4.1 Introduction

In this work, we are concerned with the variance of the solution of the random transport

equation, {
Qt(x, t) + AQx(x, t) = 0, t &gt; 0, x ? R,
Q(x, 0) = Q0(x),

(4.1)

with a homogeneous random transport velocity, A, and stochastic initial condition, Q0(x).

The solution, Q(x, t), is a random function. For the particular case, Riemann problem

(4.1) with

Q(x, 0) =

{
Q?0 if x &amp;lt;0,
Q+0 if x &gt; 0,

(4.2)

35



36 Cap??tulo 4

where Q?0 and Q
+
0 are random variables, we presented in [1] the expression for its solution:

QR(x, t) = Q
?
0 + X

(
Q+0 ? Q?0

)
, (4.3)

where X is the Bernoulli random variable with P (X = 0) = 1?FA(x/t) and P (X = 1) =
FA(x/t); here FA(x) is the cumulative probability function of the random variable A.

Also, according to [1], and considering the statistical independence between A and

both Q?0 , Q
+
0 , the mean and variance of the solution are given by

?QR(x, t)? = ?Q?0 ? + FA
(x

t

) [
?Q+0 ? ? ?Q?0 ?

]
(4.4)

and

V ar[QR(x, t)] =V ar[Q
?
0 ] + FA

(x
t

) [
V ar[Q+0 ] ? V ar[Q?0 ]

]
+

FA

(x
t

) [
1 ? FA

(x
t

)] [
?Q+0 ? ? ?Q?0 ?

]2
, (4.5)

respectively.

In our point of view, the special attraction of (4.3), (4.4), and (4.5) is their utilization

in discretizations of stochastic equations, like (4.1). In [2] we present an explicit method

to calculate the mean of Q(x, t), the solution of (4.1) with Q(x, 0) = Q0(x) a random

function. In that report, we show that Godunov’s method provides a numerical scheme

for the statistical mean which is, under certain assumptions on the discretization, stable

and consistent with an advective-diffusive equation. Therefore, besides the scheme itself,

the numerical approach also gives an effective equation compatible with one published in

the literature.

The aim of this paper is to improve the knowledge of the random solution of (4.1).

Basically, we present a numerical method to calculate the variance of Q(x, t), which is

the quantity most commonly used to specify the dispersion of the distribution around its

mean.

The outline of this paper is as follows. In Section 4.2 we deduce the explicit numerical

scheme using Godunov’s ideas. Consistency, stability and convergence are analyzed in

Section 4.3. Finally, in Section 4.4 we present some numerical examples.

4.2 The numerical scheme

In this section, we present the numerical scheme for the variance of the solution of (4.1).

We denote the spatial and time grid points by xj = j?x and tn = n?t, respectively, and

the jth grid cell is Cj = (xj?1/2, xj+1/2), xj±1/2 = xj ±?x/2. Let Qnj be an approximation



4.2. The numerical scheme 37

of the cell average of Q(x, tn):

Qnj '
1

?x

?

Cj
Q(x, tn)dx =

1

?x

? xj+1/2
xj?1/2

Q(x, tn)dx. (4.6)

Assuming that the cell averages at time tn, Q
n
j , are known, we summarize the REA,

Reconstruct-Evolve-Average, algorithm [4, 5] in three steps:

Step 1. Reconstruct a piecewise polynomial function, Q?(x, tn), from the cell averages

Qnj . In our case we use the piecewise constant function with Q
n
j in the jth cell, i.e.,

Q?(x, tn) = Q
n
j for all x ? Cj.

Step 2. Evolve the equation exactly, or approximately, with this initial data to obtain

Q?(x, tn+1) a time ?t later.

Step 3. Average Q?(x, tn+1) over each grid cell to obtain the new cell averages, i.e.,

Q n+1j =
1

?x

?

Cj
Q?(x, tn+1)dx.

At a time tn, the piecewise constant function, step 1, defines a set of Riemann problems

in each x = xj?1/2: the differential equation (4.1) with the initial condition

Q(x, tn) =

{
Qnj?1 if x &amp;lt;xj?1/2
Qnj if x &gt; xj?1/2.

(4.7)

We may use (4.3) to find a local solution to each Riemann problem at a time ?t/2

later:

Q(x, tn+1/2) = Q
n
j?1 + X

(
x ? xj?1/2

?t/2

) [
Qnj ? Qnj?1

]
, (4.8)

where, for x sufficiently close to xj?1/2, X(x) is the Bernoulli random variable:

X(x) =

{
1, P (X(x) = 1) = FA(x)

0, P (X(x) = 0) = 1 ? FA(x).
(4.9)

Also, according to (4.4) and (4.5), and denoting ?j?1/2(x) = FA
(

x?xj?1/2
?t/2

)
, we have:

?Q(x, tn+1/2)? = ?Qnj?1? + ?j?1/2(x)
[
?Qnj ? ? ?Qnj?1?

]
(4.10)

and

V ar[Q(x, tn+1/2)] =V ar[Q
n
j?1] + ?j?1/2(x)

[
V ar[Qnj ] ? V ar[Qnj?1]

]
+

?j?1/2(x)
(
1 ? ?j?1/2(x)

) [
?Qnj ? ? ?Qnj?1?

]2
. (4.11)



38 Cap??tulo 4

Therefore, the variance of the solution at tn+1/2, V ar[Q?(x, tn+1/2)], can be constructed

by piecing together the local values of the variance, (4.11), provided that the half time

step ?t/2 is short enough such that adjacent Riemann problems do not interact between

themselves. This requires that ?x and ?t must be chosen satisfying:

V ar[Q(xj?1, tn+1/2)] ? V ar[Qnj?1] and V ar[Q(xj, tn+1/2)] ? V ar[Qnj ],

where the symbol “ ? ” means “sufficiently near to”. Substituting these conditions in
(4.11), the following conditions must be satisfied:

FA

(
??x

?t

)
? 0 and FA

(
?x

?t

)
? 1. (4.12)

Remark 4.1. We may regard (4.12) as a kind of CFL condition for the method. The

interval [??x/?t, ?x/?t] must contain the “effective support” of the density probabil-
ity function of A. This means that outside [??x/?t, ?x/?t] the probability of A is
sufficiently near to zero, i.e., it can be disregarded. The existence of an effective sup-

port is ensured by Chebyshev’s inequality: P{|A ? ?A?| ? k?A} ? 1/k2, for all k &gt; 0,
where ?A is the standard variation of A. Therefore, if we take 1/k

2 sufficiently close to

zero, to escape from the interaction between solutions of Riemann problems we must take

(|?A?| + k?A) ?t/?x ? 1.

Under the hypothesis (4.12), the expression (4.11) defines V ar[Q?(x, tn+1/2)], x ?
[xj?1, xj]; its cell average will be denoted by

V
n+1/2
j?1/2 =

1

?x

? xj
xj?1

V ar[Q?(x, tn+1/2)] dx.

Therefore, using (4.11) we have the cell average of the variance in [xj?1, xj] at t =
tn+1/2:

V
n+1/2
j?1/2 =

1

?x

? xj
xj?1

{
V nj?1 + ?j?1/2(x)

[
V nj ? V nj?1

]}
dx +

1

?x

? xj
xj?1

?j?1/2(x)
(
1 ? ?j?1/2(x)

)
dx

? ?? ?
?

[
?Qnj ? ? ?Qnj?1?

]2
.

Preliminary computational tests have shown that ? reduces excessively the contribu-

tion of
[
?Qnj ? ? ?Qnj?1?

]2
to V

n+1/2
j?1/2 . The following approximation provides better results:

? =
1

?x

? xj
xj?1

?j?1/2(x)
[
1 ? ?j?1/2(x)

]
dx ' ?j?1/2(?)

[
1 ? ?j?1/2(?)

]
,



4.2. The numerical scheme 39

where ? ? [xj?1, xj] is such that

?j?1/2(?)
[
1 ? ?j?1/2(?)

]
= max

x?[xj?1, xj ]
?j?1/2(x)

[
1 ? ?j?1/2(x)

]
.

It is straightforward to show that ? must satisfy ?j?1/2(?) = 1/2.
Thus, ?j?1/2(?)

[
1 ? ?j?1/2(?)

]
= 1/4 and, changing variables in the other integral,

we obtain

V
n+1/2
j?1/2 = V

n
j?1 +

?t

2?x

{? ?x
?t

??x
?t

FA(x) dx

}
[
V nj ? V nj?1

]
+

1

4

[
?Qnj ? ? ?Qnj?1?

]2
. (4.13)

Lemma 4.1. Let A be a random variable and [??, ?] an effective support of the density
probability function, fA, of A, i.e., FA(??) ? 0 and FA(?) ? 1. Then

? ?
??

FA (x) dx ? ? ? ?A?. (4.14)

Proof. See [2].

Substituting (4.14) in (4.13), and denoting ? = ?t?A?/?x, we have:

V
n+1/2
j?1/2 =

1

2

[
V nj + V

n
j?1

]
? ?

2

[
V nj ? V nj?1

]
+

1

4

[
?Qnj ? ? ?Qnj?1?

]2
. (4.15)

Now we can repeat the same procedure to obtain approximations of the solution in

[xj?1/2, xj+1/2] at tn+1:

V n+1j =
1

2

[
V

n+1/2
j+1/2

+ V
n+1/2
j?1/2

]
? ?

2

[
V

n+1/2
j+1/2

? V n+1/2
j?1/2

]
+

1

4

[
?Qn+1/2

j+1/2
? ? ?Qn?1/2

j?1/2 ?
]2

. (4.16)

The ideas of the Godunov method were also used in [2] to design a scheme for appro-

ximations of the statistical means of (4.1):

?Qn+1/2
j?1/2 ? =

1

2

[
?Qnj ? + ?Qnj?1?

]
? ?

2

[
?Qnj ? ? ?Qnj?1?

]
(4.17)

and

?Qn+1j ? =
1

2

[
?Qn+1/2

j+1/2
? + ?Qn+1/2

j?1/2 ?
]
? ?

2

[
?Qn+1/2

j+1/2
? ? ?Qn+1/2

j?1/2 ?
]
, (4.18)

or, joining these expressions,

?Qn+1j ? = ?Qnj ? ?
?

2

[
?Qnj+1? ? ?Qnj?1?

]
+

1

4

(
1 + ?2

) [
?Qnj+1? ? 2?Qnj ? + ?Qnj?1?

]
. (4.19)



40 Cap??tulo 4

Using (4.15) and (4.17) in (4.16), we can summarize the two-step scheme for the

variance in the explicit form:

V n+1j =V
n
j ?

?

2

[
V nj+1 ? V nj?1

]
+

1

4

(
1 + ?2

) [
V nj+1 ? 2V nj + V nj?1

]
+

1

8
(1 ? ?)

[
?Qnj+1? ? ?Qnj ?

]2
+

1

8
(1 + ?)

[
?Qnj ? ? ?Qnj?1?

]2
+ (4.20)

1

16

{[
?Qnj+1? ? ?Qnj?1?

]
? ?

[
?Qnj+1? ? 2?Qnj ? + ?Qnj?1?

]}2
,

where ? = ?t?A?/?x.

4.3 Numerical analysis of the scheme

In this section, we analyze some numerical aspects of the method (4.19)–(4.20), for the

mean and variance of the solution of (4.1). We obtain the stability conditions of the

scheme and we also show its consistency with a deterministic nonhomogeneous advective-

diffusive system.

Proposition 4.1. For ?x2/?t = ? fixed, the numerical scheme (4.19)-(4.20) is an

O(?x2) approximation for u(x, t) and v(x, t), solutions of the deterministic system of
partial differential equations:

{
ut + ?A?ux = ?4 uxx
vt + ?A?vx = ?4 vxx + ?2 u2x.

(4.21)

Proof. Let v(x, t) and u(x, t) be smooth functions such that v(xj, tn) = V
n
j and u(xj, tn) =

?Qnj ?. We have shown in [2] that if ?x2/?t = ? is fixed then the numerical scheme
(4.19) gives an O(?x2) approximation for u(x, t), solution of the differential equation
ut + ?A?ux = (?/4)uxx.
Also, using Taylor’s expansion in (4.20) we have

[
vt +

?t

2
vtt + O(?t2)

]
+ ?A?

[
vx + O(?x2)

]
=

?

4

(
1 + ?2

) [
vxx + O(?x2)

]
+

?

8
(1 ? ?)

[
ux +

?x

2
uxx + O(?x2)

]2
+

?

8
(1 + ?)

[
ux ?

?x

2
uxx + O(?x2)

]2
+

?

4

{[
ux + O(?x2)

]
? ?

2

[
uxx + O(?x2)

]}2
.

Since ?x2/?t = ? is fixed, we have ? = ?t?A?/?x = ?x?A?/? = O(?x) and ?t =
O(?x2).



4.4. Numerical examples 41

Thus, grouping the terms of the same order, we obtain

vt + ?A?vx =
?

4
vxx +

?

2
u2x + O(?x2).

Remark 4.2. Although the consistency result is for any ?, computational tests have shown

that a good choice for ? = ?x2/?t, in (4.21), to calculate ?Q(x, T )? and Var(Q(x, T )),
is ? = 2Var[A]T .

Remark 4.3. The modified equations in (4.21) constitute a decoupled deterministic non-

homogeneous advective-diffusive system whose transport terms are the mean of the velocity,

and the diffusive terms are the same. The source term in the second equation involves the

spatial derivative of the mean, given by the first equation.

Remark 4.4. In [2] we have shown that the stability condition of (4.19) is (4.12). On

the other hand, since the terms corresponding to the mean can be considered source terms,

the method for the variance, (4.20), has the same stability conditions, i.e., (4.12). As a

linear problem, we have convergence.

4.4 Numerical examples

To assess our method for the variance we present two numerical examples. In Example 4.1

we solve a Riemann problem in which the exact values of ?Q(x, t)? and Var[Q(x, t)] are
known. In Example 4.2 we apply the method in a problem with random velocity and a

correlated random field as the initial condition. In both examples we use A normally and

lognormally distributed. The value of ?x is presented in the caption of the figures. The

value of ?t was chosen based on Remark 4.2, i.e., we used ? = ?x2/?t = 2Var[A]T . All

the numerical experiments presented in this section were computed in double precision

with some MATLAB codes on a 3.0Ghz Pentium 4 with 512Mb of memory.

Example 4.1.

Let us consider the problem (4.1)–(4.2) where the random variables U?0 and U
+
0 have

a bivariate normal distribution defined by: ?U?0 ? = 1 (mean of UL); ?U +0 ? = 0 (mean
of UR); V ar[U

?
0 ] = 0.16 (variance of U

?
0 ); V ar[U

+
0 ] = 0.25 (variance of U

+
0 ); and ? =

0 (correlation coefficient between U?0 and U
+
0 ). In Figures 4.1 – 4.4 we compare the

approximations of the mean and variance calculated using (4.19) and (4.20), respectively,

with the exact values given by (4.4) and (4.5). We plot the results at T = 0.3 and T = 0.5.

To observe the influence of the velocity variation we use two models: [i] A is normally

distributed, A ? N (1.0, 0.6), in Figures 4.1 and 4.2; [ii] A is lognormally distributed,
A = exp (?), ? ? N (0.5, 0.25), in Figures 4.3 and 4.4.



42 Cap??tulo 4

?1 0 1 2 3
?0.2

0

0.2

0.4

0.6

0.8

1

1.2

Mean of solution

Proposed method
Exact solution

?1 0 1 2 3

0.15

0.2

0.25

0.3

0.35

0.4

0.45

0.5

Variance of solution

Figure 4.1: ?x = 0.02 and T = 0.3.

?1 0 1 2 3
?0.2

0

0.2

0.4

0.6

0.8

1

1.2

Mean of solution

Proposed method
Exact solution

?1 0 1 2 3

0.15

0.2

0.25

0.3

0.35

0.4

0.45

0.5

Variance of solution

Figure 4.2: ?x = 0.02 and T = 0.5.

?1 0 1 2 3
?0.2

0

0.2

0.4

0.6

0.8

1

1.2

Mean of solution

Proposed method
Exact solution

?1 0 1 2 3

0.15

0.2

0.25

0.3

0.35

0.4

0.45

0.5

Variance of solution

Figure 4.3: ?x = 0.01 and T = 0.3.



4.5. Concluding remarks 43

?1 0 1 2 3
?0.2

0

0.2

0.4

0.6

0.8

1

1.2

Mean of solution

Proposed method
Exact solution

?1 0 1 2 3

0.15

0.2

0.25

0.3

0.35

0.4

0.45

0.5

Variance of solution

Figure 4.4: ?x = 0.01 and T = 0.5.

Example 4.2.

In this example we consider the random partial differential equation (4.1) with initial

condition being the normal random field with mean

?Q0(x)? =
{

1, x ? (1.4, 2.2),
e?20(x?0.25)

2

, otherwise,
(4.22)

and covariance Cov(x, x?) = ?2 exp (??|x ? x?|), where Var[Q0(x)] = ?2 is constant. The
parameter ? &gt; 0 governs the decay rate of the spatial correlation. In the tests we use

? = 40 and ?2 = 0.12. The numerical results are compared with the Monte Carlo method

using suites of realizations of A and Q0(x), with A and Q0(x) statistically independent.

The analytical solution of each realization A(?) and Q0(x, ?) is given by Q(x, t, ?) =

Q0(x?A(?)t, ?). The 2000 realizations of the correlated random field Q0(x) are generated
using the matriz decomposition method, a direct method for generating correlated random

fields (see [9], Ch. 3, for example). As in the previous example, we use two models of

velocity: [i] A is normally distributed, A ? N (?0.5, 0.6), in Figures 4.5 and 4.6; [ii] A is
lognormally distributed, A = exp (?), ? ? N (0.15, 0.25), in Figures 4.7 and 4.8.

4.5 Concluding remarks

In this paper, we extend the ideas presented in our previous work [2] to obtain more

information about the statistical properties of the solution to the one-dimensional random

linear transport equation. We show that the ideas of the Godunov method can also be used

to design a numerical scheme to calculate the variance of the solution: (4.19) and (4.20).

We also present the stability conditions and the consistency of the numerical scheme with

the decoupled system of advective-diffusive equations (4.21). Computational results are



44 References

?1 0 1 2 3
?0.2

0

0.2

0.4

0.6

0.8

1

1.2

Mean of solution
Mean of initial condition
Proposed method
Monte Carlo simulations

?1 0 1 2 3
0.1

0.15

0.2

0.25

0.3

0.35

0.4

Variance of solution

Figure 4.5: ?x = 0.02 and T = 0.3.

?1 0 1 2 3
?0.2

0

0.2

0.4

0.6

0.8

1

Mean of solution Mean of initial condition
Proposed method
Monte Carlo simulations

?1 0 1 2 3
0.1

0.15

0.2

0.25

0.3

0.35

0.4

0.45

Variance of solution

Figure 4.6: ?x = 0.02 and T = 0.5.

compared with the exact solution, in the Riemann problem, and with the Monte Carlo

method in a more general situation. As far as we know, this kind of methodology has not

been used to deal with differential equations with uncertainties in the parameters.

Acknowledgments

Our acknowledgments to the Brazilian Council for Development of Science and Technology

(CNPq) through the grants 5551463/02-3 and 140406/2004-2 (doctoral scholarship).

References

[1] M. C. C. Cunha, F. A. Dorini, A note on the Riemann problem for the random

transport equation. Computational and Applied Mathematics 26(3):323–335 (2007).



References 45

?0.5 0 0.5 1 1.5 2 2.5 3 3.5
?0.2

0

0.2

0.4

0.6

0.8

1

1.2

Mean of solution Mean of initial condition
Proposed method
Monte Carlo simulations

?0.5 0 0.5 1 1.5 2 2.5 3 3.5

0.1

0.15

0.2

0.25

0.3

0.35

0.4

0.45

Variance of solution

Figure 4.7: ?x = 0.01 and T = 0.3.

?0.5 0 0.5 1 1.5 2 2.5 3 3.5
?0.2

0

0.2

0.4

0.6

0.8

1

1.2

Mean of solution Mean of initial condition
Proposed method
Monte Carlo simulations

?0.5 0 0.5 1 1.5 2 2.5 3 3.5

0.1

0.15

0.2

0.25

0.3

0.35

0.4

0.45

Variance of solution

Figure 4.8: ?x = 0.01 and T = 0.5.

[2] F. A. Dorini, M. C. C. Cunha, A finite volume method for the mean of the solution

of the random transport equation. Appl. Math. and Comput. 187(2):912–921 (2007).

[3] J. Glimm, D. Sharp, Stochastic partial differential equations: Selected applications

in continuum physics, in Stochastic Partial Differential Equations: Six Perspecti-

ves, (Edited by R. A. Carmona and B. L. Rozovskii), Mathematical Surveys and

Monographs, American Mathematical Society, No. 64, p.03–44, Providence, 1998.

[4] S. K. Godunov, A difference method for numerical calculation of discontinuous solu-

tions of the equations of hydrodynamics. Mat. Sb. 47:271–306 (1959).

[5] R. J. LeVeque, Finite Volume Methods for Hyperbolic Problems. Cambridge Univer-

sity Press, Cambridge, 2002.

[6] H. Osnes, H. P. Langtangen, A study of some finite difference schemes for a uni-

diretional stochastic transport equation. SIAM Journal on Scientific Computing

19(3):799–812 (1998).



46 References

[7] J. C. Strikwerda, Finite Difference Schemes and Partial Differential Equations.

Wadsworth &amp;amp; Brooks/Cole, California, 1989.

[8] J. W. Thomas, Numerical Partial Differential Equations: finite difference methods.

Springer-Verlag, New York, 1995.

[9] D. Zhang, Stochastic Methods for Flows in Porous Media - Coping with Uncertainties.

Academic Press, 2002.



Cap??tulo 5

Statistical moments of the random

linear transport equation

Abstract

This paper deals with a numerical scheme to approximate the mth moment of the solution

of the one-dimensional random linear transport equation. The initial condition is assumed

to be a random function and the transport velocity is a random variable. The scheme

is based on local Riemann problem solutions and Godunov’s method. We show that the

scheme is consistent and stable with an advective-diffusive equation. Furthermore, in the

case where the velocity is normally distributed we obtain partial differential equations for

the moments and the central moments. Numerical examples are added to illustrate our

approach.

Keyword: random linear transport equation, Riemann problem, statistical moments, Go-

dunov’s method, numerical methods for random partial differential equations.

5.1 Introduction

Partial differential equations have been important models during the last centuries, mainly

because they have the fundamental support of differential calculus, numerical methods,

and computers. However, the formulation of a physical process as a partial differential

equation demands experiments to measure the data, for example, the diffusion coefficient,

permeability of a porous media, initial conditions, boundary conditions and so on. This

means that the interpretation of the data as random variables is more realistic in some

practical situations. Differential equations with random parameters are called Random

Differential Equations; new mathematical methods have been developed to deal with this

kind of problems (see [6, 8, 12, 15], for example).

47



48 Cap??tulo 5

We are interested in the solution of random linear transport equations given by
{

Qt(x, t) + AQx(x, t) = 0, t &gt; 0, x ? R,
Q(x, 0) = Q0(x),

(5.1)

where A is a random variable and Q0(x) is a random function.

The solution for the random Riemann problem to (5.1),

Q0(x) =

{
QL, if x &amp;lt;0,

QR, if x &gt; 0,
(5.2)

with QL and QR being random variables is given by [1]:

Q(x, t) = QL + X
(x

t

)
(QR ? QL) . (5.3)

In (5.3) X is the Bernoulli random variable with P {X(?) = 1} = FA(?), the cumulative
probability function of the random variable A. Furthermore, in case of independence

between A and both QL and QR, the mth moment of Q(x, t), ?Qm(x, t)?, m ? N, m ? 1,
is given by

?Qm(x, t)? = ?QmL ? + FA
(x

t

)
[?QmR? ? ?QmL ?] . (5.4)

The closed solution (5.3) and Godunov’s ideas [7, 9, 10] are used in [4] and [2] to

design numerical methods to compute the mean and the variance of the solution to (5.1).

The methods are explicit and neither demand generation of random numbers (as does

the Monte Carlo method [5, 11, 14, 17]), nor require differential equations governing the

statistical moments (as in the effective equations methodology [6, 17]). Moreover, the

schemes are stable and consistent with an advective-diffusive equation which agrees with

the effective equation to the expectation presented in the literature (see [6], for example).

In [3] we use the idea of collecting deterministic realizations through their probability

functions to solve the nonlinear random Riemann-Burgers equation.

In this paper, we deal with the general moments of the solution to (5.1). The outline of

this paper is as follows. In Section 5.2 we use (5.3) and (5.4) to design a numerical method

to the mth statistical moment of the solution to the general problem (5.1). We present

the CFL condition under which the local solutions do not interact between themselves.

In Section 5.3 we show the stability of the numerical scheme and its consistency with an

advective-diffusive equation. We show that the diffusion coefficient is related with the

probability density function of the velocity by Eq. (5.18), which has a simple solution

in the normal velocity case. In Section 5.4 we present a decoupled system of partial

differential equations to be satisfied by the central moments of the random solution. All

the partial differential equations in this paper are linear. In fact, denoting by L(u) =
ut + ?A?ux ? ?uxx, the equations are of the form: L(u) = 0, for the moments, and
L(u) = f , for the central moments. Computational experiments and comparisons with
the Monte Carlo method are presented in Section 5.5.



5.2. The numerical scheme 49

5.2 The numerical scheme

In this section, we present the numerical method for the mth statistical moment of the

solution to (5.1). The method is based on the juxtaposition of Riemann problems whose

solutions are given by (5.3). We discretize both space and time assuming a uniform mesh

spacing: xj = j?x, xj±1/2 = xj ±(?x/2), tn = n?t, tn±1/2 = tn ±(?t/2), for ?x, ?t &gt; 0.
In Figure 5.1 we present a schematic diagram of the algorithm. Let us assume that the

random variables Qnj and the mth moments ?Qm,nj ? = ?Qm(xj, tn)? are known at t = tn.

xj+3/2

Qnj?1 Q
n
j Q

n
j+1

Q
n+1/2

j?1/2
Q

n+1/2

j+1/2

Q
n+1
j

xj?3/2 xj?1 xj?1/2 xj xj+1/2 xj+1

tn

tn+1/2

tn+1

Figure 5.1: Schematic diagram of the algorithm.

In the following we use the ideas of REA, Reconstruct-Evolve-Average, algorithm

[7, 10] to approximate ?Qm,n+1j ? = ?Qm(xj, tn+1)?.
Step 1. We reconstruct the piecewise random constant function Q?(x, tn) from Q

n
j ,

i.e, Q?(x, tn) = Q
n
j for x ? [xj?1/2, xj+1/2].

The piecewise constant random function Q?(x, tn) defines a set of local random Riemann

problems, each one centered at x = xj?1/2,

Qt(x, t) + AQx(x, t) = 0, t &gt; tn, x ? R,

Q(x, tn) =

{
Qnj?1, if x &amp;lt;xj?1/2,
Qnj , if x &gt; xj?1/2.

(5.5)

Step 2. From (5.3) and (5.4), the local solutions of (5.5) and the respective statistical

moments are given by

Gj?1/2(x, tn+1/2) = Q
n
j?1 + X

(
x ? xj?1/2

?t/2

) [
Qnj ? Qnj?1

]
(5.6)



50 Cap??tulo 5

and

?Gmj?1/2(x, tn+1/2)? = ?Qm,nj?1? + FA
(

x ? xj?1/2
?t/2

) [
?Qm,nj ? ? ?Qm,nj?1?

]
. (5.7)

The global solution at t = tn+1/2, Q?(x, tn+1/2), can be constructed by piecing together

the local random Riemann solutions (5.6), provided that ?t/2 is sufficiently small such

that adjacent local random Riemann solutions do not interact. Therefore, taking into

account the similarity property of the random Riemann solutions, ?x and ?t must be

chosen such that:

Gj?1/2(x, tn+1/2)
??
x=xj?1

? Qnj?1, Gj?1/2(x, tn+1/2)
??
x=xj

? Qnj ,

where the symbol “ ? ” means “sufficiently near to”. By substituting these conditions in
(5.6) we must have

FA

(
??x

?t

)
? 0 and FA

(
?x

?t

)
? 1. (5.8)

Remark 5.1. We may regard (5.8) as the CFL condition for the method: the interval

[??x/?t, ?x/?t] must contain an effective support of the density probability function
of A. This means that the probability of A outside of the interval [??x/?t, ?x/?t]
is sufficiently near to zero, and then may be disregarded. The existence of an effective

support is ensured by Chebyshev’s inequality: P{|A ? ?A?| ? k?A} ? 1/k2, for all k &gt; 0,
where ?A is the standard variation of A. Therefore, if we take 1/k

2 sufficiently close to

zero, to escape from the interaction between solutions of Riemann problems we must take

(|?A?| + k?A) ?t/?x ? 1.

Under condition (5.8), we conclude Step 2 by taking

Q?(x, tn+1/2) =
?

j?1/2
Gj?1/2(x, tn+1/2) 1[xj?1,xj ]

where 1[a,b] denotes the characteristic function of the interval [a, b]. From (5.7) it follows

that

?Q?m(x, tn+1/2)? =
?

j?1/2
?Gmj?1/2(x, tn+1/2)? 1[xj?1,xj ]. (5.9)

In a similar way, using the values at t = tn+1/2, we obtain

?Q?m(x, tn+1)? =
?

j

?Gmj (x, tn+1)? 1[xj?1/2,xj+1/2]. (5.10)



5.2. The numerical scheme 51

Step 3. We use (5.10) to approximate ?Qm,n+1j ? as the average value of ?Q?m(x, tn+1)?
over the interval [xj?1/2, xj+1/2]:

?Qm,n+1j ? '
1

?x

? xj+1/2
xj?1/2

?Q?m(x, tn+1)?dx =
1

?x

? xj+1/2
xj?1/2

?Gmj (x, tn+1)?dx

=
1

?x

? xj+1/2
xj?1/2

{
?Qm,n+1/2

j?1/2 ? + FA
(

x ? xj
?t/2

) [
?Qm,n+1/2

j+1/2
? ? ?Qm,n+1/2

j?1/2 ?
]}

dx

= ?Qm,n+1/2
j?1/2 ? +

?t

2?x

{? ?x
?t

??x
?t

FA(x)dx

} [
?Qm,n+1/2

j+1/2
? ? ?Qm,n+1/2

j?1/2 ?
]
. (5.11)

Likewise, we use (5.9) to approximate ?Qm,n+1/2
j?1/2 ?:

?Qm,n+1/2
j?1/2 ? '

1

?x

? xj
xj?1

?Q?m(x, tn+1/2)?dx =
1

?x

? xj
xj?1

?Gmj?1/2(x, tn+1/2)?dx

=
1

?x

? xj
xj?1

{
?Qm,nj?1? + FA

(
x ? xj?1/2

?t/2

) [
?Qm,nj ? ? ?Qm,nj?1?

]}
dx

= ?Qm,nj?1? +
?t

2?x

{? ?x
?t

??x
?t

FA(x)dx

}
[
?Qm,nj ? ? ?Qm,nj?1?

]
. (5.12)

The following result is proved in [4]:

Lemma 5.1. Let Y be a random variable and [??, ?] an effective support of the density
probability function, fY , of Y , i.e., FY (??) ? 0 and FY (?) ? 1. Then

? ?
??

FY (x)dx ? ? ? ?Y ?. (5.13)

Inserting (5.13) in (5.11) and (5.12), and denoting ? = ?t?A?/?x, gives

?Qm,n+1j ? =
1

2

[
?Qm,n+1/2

j?1/2 ? + ?Q
m,n+1/2
j+1/2

?
]
? ?

2

[
?Qm,n+1/2

j+1/2
? ? ?Qm,n+1/2

j?1/2 ?
]

(5.14)

and

?Qm,n+1/2
j?1/2 ? =

1

2

[
?Qm,nj?1? + ?Qm,nj ?

]
? ?

2

[
?Qm,nj ? ? ?Qm,nj?1?

]
. (5.15)

Grouping these expressions we summarize the two-step scheme (5.14)–(5.15) in the

one-step explicit method:

?Qm,n+1j ? = ?Qm,nj ? ?
?

2

[
?Qm,nj+1? ? ?Qm,nj?1?

]
+

+
1

4

(
1 + ?2

) [
?Qm,nj+1? ? 2?Qm,nj ? + ?Qm,nj?1?

]
. (5.16)



52 Cap??tulo 5

Remark 5.2. The numerical scheme (5.16) is conservative, i.e., it can be rewritten as

?Qm,n+1j ? = ?Qm,nj ? ?
?t

?x

[
F

m,n
j+1/2

? F m,n
j?1/2

]
,

where F
m,n
j?1/2 = (1/2)?A?[?Q

m,n
j?1? + ?Qm,nj ?] ? (1/4)?A? (1/? + ?) [?Qm,nj ? ? ?Qm,nj?1?] is an

approximation to the average flux at x = xj?1/2.

5.3 Numerical analysis of the scheme

The scheme (5.16) is a generalization of a previously studied scheme to the mean (m = 1)

of the solution to (5.1). Therefore, we can use the same arguments used in [4] to show

• consistency: if ? = ?x2/(4?t) is fixed then the numerical scheme (5.16) yields an
O(?x2) approximation for the solution of the partial differential equation

ut + ?A?ux = ?uxx; (5.17)

• stability: the numerical method (5.16) is stable under the CFL condition (5.8).

As a linear problem, the convergence of (5.16) to the differential equation (5.17) is a

consequence of the Lax Equivalence Theorem, no matter what ? = ?x2/(4?t) is. The

following proposition gives an additional information about the diffusion associated with

the random velocity, A.

Proposition 5.1. The diffusion coefficient in (5.17) must satisfy

?f ?A
( x

t

)
?(x, t) = fA

(x
t

)
(x ? ?A?t) , (5.18)

where fA(?) = d[FA(?)]/d? is the density probability function of A.

Proof. As a general differential equation, (5.17) must be satisfied by a particular solu-

tion. The random Riemann problem (5.1)–(5.2) is a particular case of (5.1) with known

moments given by (5.4):

?Qm(x, t)? = ?QmL ? + FA
(x

t

)
[?QmR? ? ?QmL ?] .

Direct derivations and substitution of this solution in (5.17) gives (5.18), a necessary

condition to ?(x, t).



5.4. The system of partial differential equations for the central moments 53

5.3.1 The Normal case

Let A ? N (?A?, ?A). Using the normal probability density function in (5.18) we obtain
? = ?2At. In this case, the differential equation (5.17) turns to be

ut + ?A?ux = (?2At)uxx, t &gt; 0, (5.19)
which agrees with the effective equation for the statistical mean presented by some authors

(see [6], for example). We emphasize that our convergence results show that the differential

equation which describes the evolution of all the moments is the same. Using (5.18) we

may also show that if ?(x, t) depends only on t then A is normally distributed.

Now let t = tf be fixed, and select ?t and ?x such that

?x2

4?t
= ? =

1

2
(?2Atf ). (5.20)

The convergence results show that our method converges to the solution of the differential

equation

ut + ?A?ux =
1

2
(?2Atf )uxx. (5.21)

The solutions of (5.19) and (5.21), u1(x, t) and u2(x, t), respectively, both with u(x, 0) =

g(x), are equal at t = tf . Indeed, according to [13] we have

u1(x, tf ) =
1?

??1(tf )

? +?
??

exp

[
?

(
x ? ?A?tf ? ?

?1(tf )

)2]
g(?) d?, (5.22)

where

?1(tf ) = 2

[? tf
0

(?2As) ds

]1/2
=
?

2?Atf .

On the other hand, the solution to (5.21) is also given by (5.22) with

?2(tf ) = 2

[? tf
0

[(?2Atf )/2] ds

]1/2

instead of ?1(tf ). Since ?1(tf ) = ?2(tf ) then u1(x, tf ) = u2(x, tf ).

The condition (5.20) can be rewritten as ?x/?t = 2?2Atf /?x. Thus, the CFL condi-

tion (5.8) may be satisfied for ?x sufficiently small.

5.4 The system of partial differential equations for

the central moments

Central moments of a given random function Q(x, t) are deterministic functions defined

by µm = ?(Q ? ?Q?)m?, m ? N, m ? 2. The most used central moment is the variance,



54 Cap??tulo 5

m = 2, which was introduced by K. F. Gauss (1777-1855) as a measure of dispersion of

the distribution of Q(x, t). But high order central moments are also useful information

concerning random variables [12, 15].

In the following we show that the central moment µm(x, t), if sufficiently smooth,

satisfies an advective-diffusive equation with the source term defined by the expectation

and the central moments µm?1(x, t) and µm?2(x, t). Here, we may extend the definition
of central moments for m ? 0 since µ0 = 1 and µ1 = 0.

We may use algebraic manipulations to show that

(i) If k ? m ? 2 then
(

m

k + 2

)
(k + 1)(k + 2) =

(
m

k

)
(m ? k)(m ? k ? 1). (5.23)

(ii) If k ? m ? 1 then (
m

k + 1

)
(k + 1) =

(
m

k

)
(m ? k). (5.24)

(iii)

µm = ?Qm? ?
m?1?

k=2

(
m

k

)
µk?Q?m?k ? ?Q?m. (5.25)

Proposition 5.2. Let Z(x,t) be a random function whose statistical moments satisfy

(5.17), i.e., the advective-diffusive equations:

?Zm?t + ?A??Zm?x = ??Zm?xx, (5.26)

m ? N, m ? 1. Then the central moments, µm(x, t) = ?[Z ? ?Z?]m?, m ? N, m ? 2,
satisfy the advective-diffusive equations with source term:

µm,t + ?A?µm,x ? ?µm,xx = 2m?µm?1,x ?Z?x + m(m ? 1)?µm?2?Z?2x, (5.27)

where µ0 = 1 and µ1 = 0.

Proof. The proof is based on the induction principle. Since µ2(x, t) = ?Z2(x, t)? ?
?Z(x, t)?2, µ1(x, t) = 0 and µ0(x, t) = 1, direct substitution and derivations show (5.27)
for k = 2. As the induction hypothesis we assume that (5.27) is true for k = 3 : (m ? 1),
and our task is to prove that (5.27) is true for k = m.

From (5.25) we have µm(x, t) = ?Zm? ?
m?1?

k=2

(
m

k

)
µk?Z?m?k ? ?Z?m. By differentiating

this expression with respect to t and x, grouping conveniently the terms, and using (5.26)



5.4. The system of partial differential equations for the central moments 55

we arrive at

µm,t + ?A?µm,x ? ?µm,xx =

?
m?1?

k=2

(
m

k

)
?Z?m?k {µk,t + ?A?µk,x ? ?µk,xx} +

+ 2 ?
m?1?

k=2

(
m

k

)
(m ? k) µk,x ?Z?m?k?1 ?Z?x+

+ ?
m?2?

k=2

(
m

k

)
(m ? k) (m ? k ? 1) µk ?Z?m?k?2 (?Z?x)2+

+ ? m (m ? 1) ?Z?m?2?Z?2x. (5.28)
Using the induction hypothesis in the first sum in (5.28), and separating the last term of

the second and third sums, we obtain

µm,t + ?A?µm,x ? ?µm,xx =

?
m?1?

k=2

(
m

k

)
?Z?m?k

{
2k?µk?1,x?Z?x + k(k ? 1)?µk?2(?Z?x)2

}
+

+ 2 ?
m?2?

k=2

(
m

k

)
(m ? k) µk,x ?Z?m?k?1 ?Z?x+

+ ?
m?3?

k=2

(
m

k

)
(m ? k) (m ? k ? 1) µk ?Z?m?k?2 ?Z?2x+

+ 2 m ? µm?1,x ?Z?x + m (m ? 1) ? µm?2 ?Z?2x+
+ ? m (m ? 1) ?Z?m?2?Z?2x? ?? ?

equal the first sum with k=2.

,

or, equivalently,

µm,t + ?A?µm,x ? ?µm,xx = 2m?µm?1,x?Z?x + m(m ? 1)?µm?2?Z?2x?

? ?
m?1?

k=3

(
m

k

)
?Z?m?k

{
2kµk?1,x?Z?x + k(k ? 1)µk?2?Z?2x

}
+

+ ?
m?2?

k=2

(
m

k

)
(m ? k) 2 µk,x ?Z?m?k?1 ?Z?x+

+ ?
m?3?

k=2

(
m

k

)
(m ? k) (m ? k ? 1) µk ?Z?m?k?2 ?Z?2x. (5.29)



56 Cap??tulo 5

To show that the three sums on the right side of (5.29) are zero, we open the first one of

them:

m?1?

k=3

(
m

k

)
?Z?m?k

{
2 k µk?1,x?Z?x + k(k ? 1)µk?2?Z?2x

}
=????

µ1=0

=
m?1?

k=3

(
m

k

)
?Z?m?k 2kµk?1,x?Z?x +

m?1?

k=4

(
m

k

)
?Z?m?k k(k ? 1)µk?2?Z?2x =

= 2
m?2?

k=2

(
m

k + 1

)
(k + 1)?Z?m?k?1µk,x?Z?x+

+
m?3?

k=2

(
m

k + 2

)
(k + 1)(k + 2)?Z?m?k?2µk?Z?2x =????

using (5.23) and (5.24)

= 2
m?2?

k=2

(
m

k

)
(m ? k)µk,x?Z?m?k?1?Z?x+

+
m?3?

k=2

(
m

k

)
(m ? k)(m ? k ? 1)µk?Z?m?k?2?Z?2x.

Therefore, from (5.29) we arrive at (5.27).

Remark 5.3. In Section 5.3 we have shown that the numerical method (5.16), for the

moments, is stable and consistent with (5.17). Since we have used the same method (5.16)

to compute the central moments, we conclude that the method for the central moments is

stable and consistent with (5.27), equation (5.17) with a source term.

5.5 Computational tests

In this section, we present some examples to assess our approach. In Examples 5.1

and 5.2 the initial condition allows exact statistical moments of the solution. We use

Riemann initial conditions defined by bivariate normal distributions; in this case the

solutions for the moments are given by (5.4). In order to investigate the influence of the

randomness we use two models: in Example 5.1 the velocity, A, is normally distributed,

and in Example 5.2 the velocity is lognormally distributed. In both cases we compare the

exact solutions, given by (5.4), with the solutions yielded by the numerical scheme (5.16)

for some statistical moments. In Example 5.3 we apply our method in the problem (5.1)

where the initial condition is a normal random function and the transport velocity is a

normal random variable. The numerical experiments presented in this section were done



5.5. Computational tests 57

in double precision with some MATLAB codes on a 3.0Ghz Pentium 4 with 512Mb of

memory.

Example 5.1.

Let us consider the random Riemann problem (5.1)–(5.2) where the random velocity

is normally distributed, A ? N (1.0, 0.8), and the random variables QL and QR have a
bivariate normal distribution defined by: ?QL? = 1.0 (mean of QL); ?QR? = 0.0 (mean
of QR); ?L = 0.4 (standard deviation of QL); ?R = 0.5 (standard deviation of QR); and

? = 0.4 (correlation coefficient between QL and QR). In Figure 5.2 we compare the exact

values for the mean, variance, 3rd central moment, and 4th central moment with the

computations using (5.16) at tf = 0.4, and ?t and ?x satisfying (5.20).

?2 ?1 0 1 2 3 4
?0.2

0

0.2

0.4

0.6

0.8

1

1.2

mean

numerical scheme
exact solution

?2 ?1 0 1 2 3 4

0.15

0.2

0.25

0.3

0.35

0.4

0.45

0.5

variance

?2 ?1 0 1 2 3 4

?0.15

?0.1

?0.05

0

0.05

0.1

3rd central moment

?2 ?1 0 1 2 3 4

?0.1

0

0.1

0.2

0.3

0.4

0.5

0.6

4th central moment

Figure 5.2: A ? N (1.0, 0.8), ?x = 0.01, ?t = 0.000195, and tf = 0.4.

Example 5.2.

To check the influence of the velocity distribution we consider the random Riemann

problem (5.1)–(5.2) in which the random velocity is lognormally distributed, A = exp(?),



58 Cap??tulo 5

? ? N (0.5, 0.35). The initial condition (QL, QR) has a bivariate normal distribution
defined by: ?QL? = 1.0; ?QR? = 0.15; ?L = 0.36; ?R = 0.25; and ? = 0.4 . Taking the
lognormal distribution, A = exp(?), ? ? N (µ?, ??), in (5.18) we obtain

?(x, t) =
?2?

(
x
t

) (
x
t
? ?A?t

)

(?2? ? µ?) + ln
(

x
t

). (5.30)

This mean that it is not possible to find constants ?x and ?t such that (?x2)/(4?t) = ?,

the consistency condition. Moreover, the diffusion coefficient given by (5.30) may be

physically inappropriate since it can assume negative values. If we use (5.20) as in the

previous example the results loose quality as shown in Figure 5.3.

?1 0 1 2 3 4

0

0.2

0.4

0.6

0.8

1

1.2

mean

numerical scheme
exact solution

?1 0 1 2 3 4
0.2

0.25

0.3

0.35

0.4

0.45

0.5

0.55

variance

?1 0 1 2 3 4
?0.04

?0.02

0

0.02

0.04

0.06

0.08

0.1

0.12

0.14

3rd central moment

?1 0 1 2 3 4

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9
4th central moment

Figure 5.3: A = exp(?), ? ? N (0.5, 0.35), ?x = 0.01, ?t = 0.000312, and tf = 0.4.

Example 5.3.

In this example we test our method for the random partial differential equation (5.1)

in which A is normal, A ? N (?0.5, 0.6), and Q0(x) is a normal random function with



5.5. Computational tests 59

mean

?Q0(x)? =
{

1, x ? (1.4, 2.2),
e?20(x?0.25)

2

, otherwise,
(5.31)

and covariance Cov(x, x?) = ?2 exp (??|x ? x?|), where Var[Q0(x)] = ?2 is constant and
? &gt; 0 governs the decay rate of the spatial correlation. We use ? = 0.3 and ?2 =

0.16. The numerical results are compared with Monte Carlo simulations using suites of

realizations of A and Q0(x), where A and Q0(x) are statistically independents. Observe

that each realization A(?) and Q0(x, ?) implies analytical solution given by Q(x, t, ?) =

Q0(x ? A(?)t, ?). To generate the realizations required by Monte Carlo simulations we
use random numbers generator of MATLAB. Comparisons with the Monte Carlo method,

with 30 000 realizations, are plotted in Figure 5.4.

?2 ?1 0 1 2 3

0

0.2

0.4

0.6

0.8

1

1.2

mean

numerical scheme
Monte Carlo
mean of init. cond.

?2 ?1 0 1 2 3

0.15

0.2

0.25

0.3

0.35

0.4

0.45

variance

?2 ?1 0 1 2 3

?0.1

?0.05

0

0.05

0.1

3rd central moment

?2 ?1 0 1 2 3

0

0.1

0.2

0.3

0.4

0.5

4th central moment

Figure 5.4: A ? N (?0.5, 0.6), ?x = 0.02, ?t = 0.000138, and tf = 0.4.



60 References

5.6 Conclusions

In this paper, we have used the Godunov ideas to obtain a numerical scheme for the

statistical moments of the solution of the one-dimensional random linear transport equa-

tion. We consider the velocity as a random variable and the initial condition as a random

function. We have used an explicit solution of the random Riemann problem to evolve

in the REA algorithm. Moreover, we have shown that the scheme is stable and consis-

tent with an advective-diffusive equation. A particular Riemann problem solution is used

to find the diffusion coefficient of the differential equations for the statistical moments.

Also, we have obtained the differential equations for the central moments of the solution.

Computational tests have illustrated our theoretical results.

Acknowledgments

Our acknowledgments to the Brazilian Council for Development of Science and Technology

(CNPq) through the grant 140406/2004-2.

References

[1] M. C. C. Cunha, F. A. Dorini, A note on the Riemann problem for the random

transport equation. Computational and Applied Mathematics 26(3):323–335 (2007).

[2] M. C. C. Cunha, F. A. Dorini, A numerical scheme for the variance of the solution

of the random transport equation. Appl. Math. Comput. 190(1):362–369 (2007).

[3] M. C. C. Cunha, F. A. Dorini, Statistical moments of the solution of the random

Burgers-Riemann problem. Technical Report 11/07, Imecc, Unicamp, Campinas,

Brazil, (2007).

http://www.ime.unicamp.br/rel pesq/2007/rp11-07.html.

[4] F. A. Dorini, M. C. C. Cunha, A finite volume method for the mean of the solution

of the random transport equation. Appl. Math. and Comput. 187(2):912–921 (2007).

[5] G. S. Fishman, Monte Carlo: concepts, algorithms and applications. Springer-Verlag,

New York, 1996.

[6] J. Glimm, D. Sharp, Stochastic partial differential equations: Selected applications

in continuum physics, in Stochastic Partial Differential Equations: Six Perspecti-

ves, (Edited by R. A. Carmona and B. L. Rozovskii), Mathematical Surveys and

Monographs, American Mathematical Society, No. 64, p.03-44, Providence, 1998.



References 61

[7] S. K. Godunov, A difference method for numerical calculation of discontinuous solu-

tions of the equations of hydrodynamics. Mat. Sb. 47:271–306 (1959).

[8] P. E. Kloeden, E. Platen, Numerical Solution of Stochastic Differential Equations.

Springer, New York, 1999.

[9] R. J. LeVeque, Numerical Methods for Conservation Laws. Birkha?user, Berlin, 1992.

[10] R. J. LeVeque, Finite Volume Methods for Hyperbolic Problems. Cambridge Univer-

sity Press, Cambridge, 2002.

[11] P. O’Leary, M. B. Allen, F. Furtado, Groundwater transport with stochastic re-

tardation: numerical results. Computer Methods in Water Resources XII 1:255–261

(1998).

[12] B. Oksendal, Stochastic Differential Equations: an introduction with applications.

Springer, New York, 2000.

[13] A. D. Polyanin, Handbook of Linear Partial Differential Equations for Engineers and

Scientists. Chapman &amp;amp; Hall/CRC, New York, 2002.

[14] G. I. Schue?ller, A state-of-the-art report on computational stochastic mechanics.

Prob. Engrg. Mech. 12(4):197–322 (1997).

[15] T. T. Soong, Random Differential Equations in Sciences and Engineering. Academic

Press, New York, 1973.

[16] J. C. Strikwerda, Finite Difference Schemes and Partial Differential Equations.

Wadsworth &amp;amp; Brooks/Cole, California, 1989.

[17] D. Zhang, Stochastic Methods for Flow in Porous Media - Coping with Uncertainties.

Academic Press, San Diego, 2002.



Cap??tulo 6

Statistical moments of the solution

of the random Burgers-Riemann

problem

Abstract

We solve Burgers’ equation with random Riemann initial conditions. The closed solution

allows simple expressions for its statistical moments. Using these ideas we design an

efficient algorithm to calculate the statistical moments of the solution. Our methodology

is an alternative to the Monte Carlo method. The present approach does not demand a

random numbers generator as does the Monte Carlo method. Computational tests are

added to validate our approach.

Keyword: random Burgers’ equation, Monte Carlo method, Riemann problem, statistical

moments, numerical methods for random partial differential equations.

6.1 Introduction

When the data of a differential equation, the coefficients or the initial conditions, are

random variables its solution is a random function; this kind of mathematical problem

has been called a random differential equation. A great number of practical processes

under current investigations falls on the stochastic modeling; we may quote the models

in control, communications, economic systems, chemical kinetics, biosciences, statistical

mechanics and spatial areas and so on. The methodology to understand and solve diffe-

rential equations with uncertainties has stimulated studies under several points of view.

Since the solution is a random function, one particular solution corresponding to a spe-

63



64 Cap??tulo 6

cific realization is not of concern: it is important to know the statistical properties of the

solution such as its mean, variance, or other statistical moments.

Some methods for random differential equations are categorized as moment equations

methods. In these methods the purpose is to obtain differential equations governing the

statistical moments. The most important of these equations is the differential equation

for the expectation (mean), which is called for some authors as effective equation. As far

as we know, no effective equation is known for the nonlinear problem discussed in this

paper.

The Monte Carlo method is an alternative in solving random differential equations.

Partial differential equations and the Monte Carlo method have been related for more

than a century, since the works developed by Lord Rayleigh (1899), Courant et al (1928),

and Kolmogorov (1931). For instance, Courant et al showed that a particular finite

difference equation for the two dimensional Dirichlet boundary value problem and a two

dimensional random walk produce the same results. In modern terms the Monte Carlo

method originated from Los Alamos and the atomic bomb project. Now it is being used

in many scientific fields [6, 20]. The basic idea is to solve a large number of deterministic

differential equations choosing particular values for the random variables according to their

assumed probabilistic distribution. The statistical information of the random solution is

estimated using these realizations. The Monte Carlo method can be used in either linear

or nonlinear random differential equations. However, the exceptionally large volume of

calculations, and the difficulty for generating random numbers limit the significance of

this method.

In a different direction we have been studying numerical methods for the random

transport equation. In the linear case our ideas were inspired by Godunov’s method

[9, 15] for the deterministic transport equation. In [3] we present an explicit expression for

the random solution to one-dimensional random advective equations where the constant

velocity and the Riemann initial condition are random variables. This closed solution

yields simple expressions for its statistical moments, and computational experiments show

good agreement between our expressions and the Monte Carlo method for the first three

moments. The closed solution for random Riemann problems and Godunov’s ideas are

used in [5] and [4] to design numerical methods to calculate the mean and variance of the

solution to transport equations with more general initial condition (random fields). Our

methods are explicit and do not demand differential equations governing the statistical

moments, the effective equations. Furthermore, our scheme is consistent and stable with

the diffusive effective equation presented in the literature [8]. Computational experiments

have shown good agreements with the Monte Carlo method.

In this paper, we generalize our previous ideas to solve the random Riemann problem



6.1. Introduction 65

for Burgers’ equation

?

?t
U (x, t) +

1

2

?

?x
U 2(x, t) = 0, t &gt; 0, x ? R,

U (x, 0) =

{
UL, if x &amp;lt;0,

UR, if x &gt; 0,
(6.1)

where UL and UR are random variables. Here the randomness appears only because of the

initial condition. The deterministic version of (6.1) was introduced by Burgers [1] as the

simplest model that captures some key features of gas dynamics, the nonlinear hyperbolic

term. But, rather than modeling a physical process, the inviscid Burgers equation has

been widely used for developing both theoretical and numerical methods in the literature

of deterministic hyperbolic equations.

Taking into account that several numerical methods to deal with deterministic con-

servation laws use solutions of Riemann problems (Random Choice Method developed by

Glimm [7], and Godunov’s method [9, 15], for example), we believe that the results of the

current paper may be useful in developing numerical methods for more general random

conservation laws. Moreover, since the mathematical theory of methods to random par-

tial differential equations are difficult and not complete yet (see [13, 16, 19], for example),

numerical methods can be a good alternative to deal with random differential equations.

Kim (2006) presents a scheme to calculate the statistical moments of the random

Burgers’equation in [11]. Nevertheless, the author considers the simple case where the

random initial condition is an explicit function of the spatial variable, and of the normal

random variable with zero mean and unit variance. The author uses Wiener chaos expan-

sion to separate random and deterministic effects, and utilizes the Lax-Wendroff method

to discretize the deterministic system of partial differential equations that governs the

propagation of randomness.

In this paper, we use two basic ideas to construct the solution, and its moments, to

(6.1): (i) the realizations of the probabilistic problem are nonlinear transport equations

whose analytical solutions are known (shock and rarefaction waves); and (ii) the random

solution and its statistical moments, as functions of the initial condition and its joint

density function, are found using geometrical partitions of the phase plane (UL, UR).

Integrations on these sets are the shock and rarefaction averaging process.

The outline of this paper is as follows. In Section 2 we deduce an explicit solution to

problem (6.1). We also show the similarity of the solution as well as present an expression

for its statistical moments. Based on bidimensional midpoint quadrature formula, in Sec-

tion 3 we suggest an efficient algorithm to approximate the statistical moments. Finally,

we present some computational tests and conclusions.



66 Cap??tulo 6

6.2 The random solution

In this section, we construct the solution to (6.1), the one-dimensional Burgers’ equation

with random Riemann initial condition. We assume that the random initial states, UL and

UR, and their joint probability density function, fULUR , are known. For a single realization,

UL(?) and UR(?), of UL and UR, respectively, we have the deterministic Burgers-Riemann

problem:

?

?t
u(x, t, ?) +

1

2

?

?x
u2(x, t, ?) = 0, t &gt; 0, x ? R,

u(x, 0, ?) =

{
UL(?), if x &amp;lt;0,

UR(?), if x &gt; 0.
(6.2)

Physically correct solutions to (6.2), i.e., entropy solutions, are the rarefaction or shock

waves (see [14, 15], for example):

(a) If UL(?) &amp;lt;UR(?) then the solution is the rarefaction wave emanating from

(x, t) = (0, 0)

u(x, t, ?) =

?
?
?

UL(?), if
x
t

&amp;lt;UL(?),
x
t
, if UL(?) ? xt ? UR(?),

UR(?), if
x
t

&gt; UR(?).

(6.3)

(b) If UL(?) &gt; UR(?) then the solution is the shock wave

u(x, t, ?) =

{
UL(?), if

x
t

&amp;lt;s(?),

UR(?), if
x
t

&gt; s(?),
(6.4)

with the shock velocity, s(?) = (1/2) [UL(?) + UR(?)], given by the Rankine-Hugoniot

jump condition.

Thus, holding (x, t) fixed and considering the rarefaction and shock solutions together,

we can join (6.3)-(6.4) to express u(x, t, ?) as

u(x, t, ?) =

?
??????
??????

UL(?), if ? &amp;lt;UL(?) and UL(?) &amp;lt;UR(?),

?, if UL(?) ? ? ? UR(?) and UL(?) &amp;lt;UR(?),
UR(?), if ? &gt; UR(?) and UL(?) &amp;lt;UR(?),

UL(?), if ?&amp;lt;
1
2
[UL(?) + UR(?)] and UL(?) &gt; UR(?),

UR(?), if ? &gt;
1
2
[UL(?) + UR(?)] and UL(?) &gt; UR(?),

(6.5)

where ? = x/t.

To simplify (6.5) we define the following mutually exclusive subsets of the phase plane



6.2. The random solution 67

(UL, UR):

R?r = {(UL, UR) such that UL &amp;lt;UR and ? &amp;lt;UL} ;
R0r = {(UL, UR) such that UL &amp;lt;UR and UL ? ? ? UR} ;
R+r = {(UL, UR) such that UL &amp;lt;UR and ? &gt; UR} ;
R?s =

{
(UL, UR) such that UL &gt; UR and ?&amp;lt;

1
2
[UL + UR]

}
;

R+s =
{
(UL, UR) such that UL &gt; UR and ? &gt;

1
2
[UL + UR]

}
.

(6.6)

In this way, for a fixed ? = x/t, we can rewrite the solution (6.5) as follows:

u(x, t, ?) =

?
?
?

UL(?), if (UL(?), UR(?)) ? R?r
? R?s = R?(?),

?, if (UL(?), UR(?)) ? R0r = R0(?),
UR(?), if (UL(?), UR(?)) ? R+r

? R+s = R+(?).
(6.7)

In Figure 6.1 we illustrate the phase plane as R?(?) ? R0(?) ? R+(?); as we can see this
partition of the phase plane depends exclusively of ? = x/t.

R

?(?)P

UR

UL? 2?

?

R

+(?)

R

0(?)

Figure 6.1: Integration regions.

Let XA be the characteristic function of A, a set in (UL, UR) plane:

XA =
{

1, if (UL, UR) ? A,
0, otherwise.

Using XA in (6.7), the arguments so far summarized prove the proposition:



68 Cap??tulo 6

Proposition 6.1. The solution to the random Burgers-Riemann problem (6.1), in a fixed

(x, t), is the random function

U (x, t) = ULXR? + ?XR0 + URXR+ , (6.8)
where ? = x/t, and XR?, XR0, and XR+ are the characteristic functions of the mutually
exclusive sets defined in (6.7).

Remark 6.1. Expression (6.8) is the same for all (x, t) such that x/t = ?. Therefore,

U (x, t) is a similarity function.

In the following corollary, the expression (6.8) and the joint probability density func-

tion of UL and UR are used to calculate the statistical properties of the random solution.

Corollary 6.1. The mth statistical moment of U (x, t), for a fixed (x, t), ? = x/t, is

?U m(x, t)? =
? ?

R?
umL fULUR (uL, uR)duLduR+

+?m
? ?

R0
fULUR (uL, uR)duLduR +

? ?

R+
umR fULUR (uL, uR)duLduR.

(6.9)

Proof. From (6.8), since R?(?), R0(?) and R+(?) are mutually exclusive sets, we have

?U m(x, t)? =
? ?

R×R
[uLXR? + ?XR0 + uRXR+ ]m fULUR (uL, uR)duLduR =

=

? ?

R×R
[umL XR? + ?mXR0 + umRXR+ ] fULUR (uL, uR)duLduR =

=

? ?

R?
umL fULUR (uL, uR)duLduR + ?

m

? ?

R0
fULUR (uL, uR)duLduR+

+

? ?

R+
umR fULUR (uL, uR)duLduR.

Effective values of the moments (6.9) require the calculations of three double integrals

for each value of ?. In some particular cases we can calculate these integrals exactly. For

instance, if UL and UR are independent random variables and uniformly distributed in

the interval [?a, a], some calculations show that the mean of the solution to (6.1) is given
by

?U (x, t)? =
{ ? ?

4a2
[sign(?)? ? a]2 , if ? a ? ? ? a,

0, otherwise,
(6.10)

where ? = x/t. We will use this solution in Example 6.1 as a test problem to assess the

performances between the Monte Carlo method and our algorithm. However, in general

we must use numerical integration to calculate (6.9). In the following section we describe

a useful way to do that.



6.3. The algorithm 69

6.3 The algorithm

To design an efficient algorithm to calculate the statistical moments using (6.9), we take

a square in the phase plane (UL, UR), ?M = [?M, M ] × [?M, M ], which contains the
effective support of fULUR ; this means that outside ?M the values of the density probability

function, fULUR , are sufficiently near to zero.

As shown in Figure 6.1, the point P = (?, ?), ? = x/t, is critical to define R?,
R0 and R+ regions. This point moves in northeast (southwest) direction as ? increases
(decreases). Without loss of generality, we will take t = 1 and use the similarity property

(Remark 6.1) to obtain the solution for t &gt; 0. Therefore, we may identify xj = ?j and

take the same discretization grid for ?, UL, and UR, as illustrated in Figure 6.2.

? = x2

uLx1 x2 x3 x4 x5 xNx7

0

x2

x3

x1

x4

x5

xN

x8

x7

uR ? = x3

uLx1 x2 x3 x4 x5 xNx7

0

x2

x3

x1

x4

x5

xN

x8

x7

uR

(a) ? = x2 (b) ? = x3

Figure 6.2: Discretization scheme of the ?M square.

Notation:

• {xj = ?M + jh; (j = 1 : N )} is the ?-mesh with ?? = h &gt; 0; x1 = ?M ; xN = M ;
xj+1/2 = xj + h/2 (j = 1 : N ? 1); N is an odd number;

• Il ?
? ?

R?
umL fULUR (uL, uR)duLduR;

• I0 ?
? ?

R0
fULUR (uL, uR)duLduR;

• Ir ?
? ?

R+
umR fULUR (uL, uR)duLduR.



70 Cap??tulo 6

We initiate the calculations taking ? = ?1 = x1. In this case ?M ? R?(?1) and,
consequently, Il = ?U mL ?, I0 = 0 and Ir = 0; these values are used to initiate the algorithm.
To save memory, the temporary calculations to update Il, I0 and Ir in ?j-step, xj =

xj?1 + h (j = 2, 3, . . . , N ), are done in the “S
h
j -strip”: S

h
j = ?M

? {R?(?j) \ R?(?j?1)}.
This strip is a collection of squares (and half squares) with edges h and

?
2h (see Figure

6.2). The integration is performed using the bidimensional midpoint quadrature formula

(see [10]). To clarify the ideas, in Table 6.1 we summarized the first step of our algorithm.

Step 1
? ? x2;

Il ? Il ? h2xm1+1/2
{

N?2?
i=1

f
(
x1+1/2, xN?i+1/2

)
+

1

2
f

(
x1+1/2, x1+1/2

)
}

?h2xm2 f (x2, x1);

I0 ? I0 + h2
N?2?
i=1

f
(
x1+1/2, xN?i+1/2

)
;

Ir ? Ir + 12 h2xm1+1/2f
(
x1+1/2, x1+1/2

)
+ h2xm1 f (x4, x1);

?U (x2, 1)m? ? Il + ?mI0 + Ir.

Table 6.1: Illustration of the first step of Algorithm 1

Repeating this idea in the next ?j-steps, we formulate Algorithm 1.

Remark 6.2. Observe that the ?M -discretization scheme has the recursive advantage:

the solution at ?j can be calculated just by updating the solution at ?j?1.

Remark 6.3. The main advantage of Algorithm 1 is that it does not require a random

numbers generator (massive simulation of data with a known probability distribution) as

does the Monte Carlo method. Furthermore, as we will see in Examples 6.1 and 6.4, its

convergence is faster than the Monte Carlo method.



6.3. The algorithm 71

Algorithm 1

N is an odd number;

Il = ?U mL ?; I0 = 0; Ir = 0;
for k = 1 : N?1

2
do

? ? xk+1;

Il ? Il ? h2xmk+1/2
{

N?2?

i=k

f
(
xk+1/2, xN +k?i?1+1/2

)
+

1

2
f

(
xk+1/2, xk+1/2

)
}

?2h2
2k?1?

i=k+1

xmi f (xi, x2k?i+1) ? h2xm2kf (x2k, x1);

I0 ? I0 + h2
{

N?2?

i=k

f
(
xk+1/2, xN +k?i?1+1/2

)
?

k?1?
i=1

f
(
xi+1/2, xk+1/2

)
}

;

Ir ? Ir + h2xmk+1/2
{

k?1?
i=1

f
(
xi+1/2, xk+1/2

)
+

1

2
f

(
xk+1/2, xk+1/2

)
}

+2h2
2k?1?

i=k+1

xm2k?i+1f (xi, x2k?i+1) + h
2xm1 f (x2k, x1);

?U (xk+1, 1)m? ? Il + ?mI0 + Ir;
end for

for k = N +1
2

: (N ? 1) do
? ? xk+1;

Il ? Il ? h2xmk+1/2
{

N?2?

i=k

f
(
xk+1/2, xN +k?i?1+1/2

)
+

1

2
f

(
xk+1/2, xk+1/2

)
}

?2h2
N?1?

i=k+1

xmi f (xi, x2k?i+1) ? h2xmN f (xN , x2k?N +1);

I0 ? I0 + h2
{

N?2?

i=k

f
(
xk+1/2, xN +k?i?1+1/2

)
?

k?1?
i=1

f
(
xi+1/2, xk+1/2

)
}

;

Ir ? Ir + h2xmk+1/2
{

k?1?
i=1

f
(
xi+1/2, xk+1/2

)
+

1

2
f

(
xk+1/2, xk+1/2

)
}

+2h2
N?1?

i=k+1

xm2k?i+1f (xi, x2k?i+1) + h
2xm2k?N +1f (xN , x2k?N +1);

?U (xk+1, 1)m? ? Il + ?mI0 + Ir;
end for



72 Cap??tulo 6

6.4 Computational tests

In this section, we present some examples to assess and illustrate our approach. In

Example 6.1 we take an initial condition that allows exact calculations for the mean.

In the following examples the initial condition has a bivariate normal distribution. In

these examples the mean, variance, 3rd central moment, and 4th central moment of the

solution are obtained by Algorithm 1 and compared with the Monte Carlo method. To

generate the realizations (UL(?), UR(?)), required by the Monte Carlo method, we use

random numbers generators of MATLAB. The analytical solution for each realization,

(UL(?), UR(?)), is given by (6.3) or (6.4). We compare the performances of the methods.

We also plot the solution of the deterministic problem where the initial condition is the

statistical mean of the random data. Some authors ([17], for example) use the name

“naive” for this solution. The numerical experiments presented in this section were done

in double precision with some MATLAB codes on a 3.0Ghz Pentium 4 with 512Mb of

memory.

Example 6.1.

We use (6.10) to calculate exact values of the mean of the solution to (6.1) with the

Riemann initial condition:

U (x, 0) =

{
UL, if x &amp;lt;0,

UR, if x &gt; 0,

where UL and UR are independent random variables uniformly distributed in the interval

[?1, 1]. The mean, at t = 0.4 and t = 0.8, is plotted in Figure 6.3. Absolute errors of
approximations given by the Monte Carlo method and Algorithm 1 are compared in Table

6.2, using ?U (x, 1)?, x ? [?1, 1]. The CPU times are also presented in this table.

?1 ?0.5 0 0.5 1
?0.06

?0.04

?0.02

0

0.02

0.04

0.06

?1 ?0.5 0 0.5 1
?0.06

?0.04

?0.02

0

0.02

0.04

0.06

Figure 6.3: Mean at t = 0.4 (left) and t = 0.8 (right).



6.4. Computational tests 73

Monte Carlo method Algorithm 1
realizations absolute error CPU time

(Nr) (|| . ||?) (sec)
1 000 0.0268 0.071
5 000 0.0124 0.358
10 000 0.0091 0.718
30 000 0.0048 2.154
50 000 0.0037 3.599
100 000 0.0027 7.223

number of absolute error CPU time
partitions (N) (|| . ||?) (sec)

201 2.49 × 10?5 0.084

Table 6.2: Absolute errors and CPU times; h = 0.01.

Example 6.2.

Let us consider the problem (6.1) with UL and UR having bivariate normal distribution

defined by: ?UL? = 0.1 (mean of UL); ?UR? = 0.9 (mean of UR); ?L = 0.3 (standard
deviation of UL); ?R = 0.2 (standard deviation of UR); and ? = 0.42 (correlation coefficient

between UL and UR). Since the probability density function with these data has the

effective support in the semiplane UL &amp;lt;UR, the rarefaction wave solutions dominate.

Figure 6.4 illustrates the mean (compared with the naive solution), variance, 3rd central

moment, and 4th central moment calculated at t = 1 for x ? [?3, 3]. As we can see, the
randomness of the initial conditions smoothen the edges of the naive solution, as in the

random linear transport equations.

Example 6.3.

To illustrate a shock-dominant case, we changed the data of UL and UR used in the

previews example: ?UL? = 0.9; ?UR? = 0.1; ?L = 0.3; ?R = 0.2; and ? = 0.42. With these
data, the bivariate normal probability density function has the effective support in the

semiplane UL &gt; UR. In Figure 6.5 we plot the mean, variance, 3rd central moment, and

4th central moment calculated at t = 1 for x ? [?3, 3]. Again, the randomness of the
initial conditions smoothen the edges of the naive solution.

Example 6.4.

In this example, we also consider a bivariate normal distribution with data that mix

rarefaction and shock waves in the realizations: ?UL? = 0.2; ?UR? = 0.4; ?L = 0.2;
?R = 0.5; and ? = 0.42. In Figure 6.6 we present approximations to the mean, variance,

3rd central moment, and 4th central moment computed using the Monte Carlo method

and Algorithm 1. We also include the naive solution. Since ?UL? &amp;lt;?UR? the naive
solution is a rarefaction wave. This example emphasizes the difference between the mean

of the solution and the solution computed using means of the data. Here, the effect of

the randomness is more than to smoothen edges: as shown in Figure 6.6 the mean of



74 Cap??tulo 6

?3 ?2 ?1 0 1 2 3

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

mean

Monte Carlo
naive solution
our algorithm

?3 ?2 ?1 0 1 2 3

0

0.02

0.04

0.06

0.08

0.1

0.12

0.14
variance

Monte Carlo
our algorithm

?3 ?2 ?1 0 1 2 3

?5

0

5

10

15

20

x 10
?3 3rd central moment

Monte Carlo
our algorithm

?3 ?2 ?1 0 1 2 3

0

0.005

0.01

0.015

0.02

0.025

0.03

0.035

0.04

4th central moment

Monte Carlo
our algorithm

Figure 6.4: Approximations to the statistical moments using the Monte Carlo method
(with 50 000 realizations), and Algorithm 1 (with N=601).

the solution is a humped function. In Table 6.3 we compare the performances between

the Monte Carlo method and Algorithm 1 in calculating ?U (x, 1)?, x ? [?3, 3], taking
into account the error estimates of each method and the CPU time. For instance, in the

approximations plotted in Figure 6.6 the Monte Carlo method has taken 8.675 sec while

Algorithm 1 has taken 0.991sec.

Monte Carlo method Algorithm 1
realizations estimate of CPU time

(Nr) error O(1/
?

Nr) (sec)
1 000 0.0316 0.185
5 000 0.0141 0.877
10 000 0.0100 1.744
30 000 0.0063 5.210
50 000 0.0044 8.675
100 000 0.0031 17.294

number of estimate of CPU time
partitions (N) error O(1/N 2) (sec)

601 0.00018 0.991

Table 6.3: Absolute errors and CPU times; h = 0.01 (600 subintervals).



6.4. Computational tests 75

?3 ?2 ?1 0 1 2 3

0

0.2

0.4

0.6

0.8

1
mean

Monte Carlo
our algorithm
naive solution

?3 ?2 ?1 0 1 2 3
0

0.05

0.1

0.15

0.2

0.25

0.3

0.35

0.4

variance

Monte Carlo
our algorithm

?3 ?2 ?1 0 1 2 3

?0.1

?0.05

0

0.05

0.1

0.15

0.2

3rd central moment

Monte Carlo
our algorithm

?3 ?2 ?1 0 1 2 3

0

0.05

0.1

0.15

0.2

0.25

0.3

4th central moment

Monte Carlo
our algorithm

Figure 6.5: Approximations to the statistical moments using the Monte Carlo method
(with 50 000 realizations), and Algorithm 1 (with N=601).

?3 ?2 ?1 0 1 2 3

0.15

0.2

0.25

0.3

0.35

0.4

0.45

mean

Monte Carlo
our algorithm
naive solution

?3 ?2 ?1 0 1 2 3

0.05

0.1

0.15

0.2

0.25

0.3
variance

Monte Carlo
our algorithm

?3 ?2 ?1 0 1 2 3

?0.06

?0.04

?0.02

0

0.02

0.04
3rd central moment

Monte Carlo
our algorithm

?3 ?2 ?1 0 1 2 3

0

0.05

0.1

0.15

0.2

4th central moment

Monte Carlo
our algorithm

Figure 6.6: Approximations to the statistical moments using the Monte Carlo method
(with 50 000 realizations), and Algorithm 1 (with N=601).



76 References

6.5 Concluding remarks

We have used the basic solutions to nonlinear conservation laws, the shock and rarefac-

tion waves, to construct the random solution for Burgers’ equation with random Riemann

initial condition. These basic solutions are grouped to deduce simple expressions to cal-

culate the statistical properties of the random solution by integrations in three mutually

exclusive cones in the phase plane (Figure 6.1). We also design an algorithm to calculate

the integrals, in case of difficult analytic expressions of the joint density distribution of

the initial condition. Our approach outperformed the Monte Carlo method in terms of

accuracy and computational cost. We believe that this approach can be also used to solve

more general problems.

Acknowledgments

Our acknowledgments to the Brazilian Council for Development of Science and Technology

(CNPq) through the grant 140406/2004-2, and to the reviewer for the helpful comments.

References

[1] J. M. Burgers, A mathematical model illustrating the theory of turbulance. Ad. Appl.

Mech. 1:171–179 (1948).

[2] R. Courant, K. Friedrichs, H. Lewy, Uber die partiellen differenzengleichungen der

matematischen physik. Math. Annalen. 100:32-74 (1928); translated into English by

P. Fox (1956), Institute of Mathematical Sciences, New York University.

[3] M. C. C. Cunha, F. A. Dorini, A note on the Riemann problem for the random

transport equation. Computational and Applied Mathematics 26(3):323–335 (2007).

[4] M. C. C. Cunha, F. A. Dorini, A numerical scheme for the variance of the solution

of the random transport equation. Appl. Math. Comput. 190(1):362–369 (2007).

[5] F. A. Dorini, M. C. C. Cunha, A finite volume method for the mean of the solution

of the random transport equation. Appl. Math. and Comput. 187(2):912–921 (2007).

[6] G. S. Fishman, Monte Carlo: concepts, algorithms and applications. Springer, 1996.

[7] J. Glimm, Solutions in the large for nonlinear hyperbolic systems of equations. Comm.

Pure Appl. Math. 18:695–715 (1965).



References 77

[8] J. Glimm, D. Sharp, Stochastic partial differential equations: Selected applications

in continuum physics, in Stochastic Partial Differential Equations: Six Perspecti-

ves, (Edited by R. A. Carmona and B. L. Rozovskii), Mathematical Surveys and

Monographs, American Mathematical Society, No. 64, p.03-44, Providence, 1998.

[9] S. K. Godunov, A difference method for numerical calculation of discontinuous solu-

tions the equations of hydrodynamics. Mat. Sb. 47:271–306 (1959).

[10] G. Hammerlin, K. H. Hoffmann, Numerical Mathematics. Springer, 1991.

[11] H. Kim, An efficient computational method for statistical moments of Burger’s equa-

tion with random initial conditions. Mathematical Problems in Engineering. 2006:1–

21, doi:10.1155/MPE/2006/17406 (2006).

[12] A. Kolmogorov, Uber die analytischen methoden in der wahrscheinlichkeitsrech-nung.

Math. Annalen. 104:415–458 (1931).

[13] P. E. Kloeden and E. Platen, Numerical Solution of Stochastic Differential Equations.

Springer, New York, 1999.

[14] R. J. LeVeque, Numerical Methods for Conservation Laws. Birkha?user, Berlin, 1992.

[15] R. J. LeVeque, Finite Volume Methods for Hyperbolic Problems. Cambridge Univer-

sity Press, Cambridge, 2002.

[16] B. Oksendal, Stochastic Differential Equations: an introduction with applications.

Springer, New York, 2000.

[17] P. O’Leary, M. B. Allen, F. Furtado, Groundwater transport with stochastic retar-

dation: numerical results. Computer Methods in Water Resources XII. 1:255–261

(1998).

[18] L. J. W. S. Rayleigh, On James Bernoulli’s theorem in probabilities. Phil. Mag.

47:246–251 (1899).

[19] T. T. Soong, Random Differential Equations in Science and Engineering. Academic

Press, New York, 1973.

[20] D. Zhang, Stochastic Methods for Flow in Porous Media - Coping with Uncertainties.

Academic Press, San Diego, 2002.



Cap??tulo 7

On the evaluation of moments for

solute transport by random velocity

fields

Abstract

In this note, we consider the random linear transport equation. We indicate that standard

averaging approaches to obtain an equation for the evolution of the statistical mean of the

solution may also be valid for all the statistical moments of the solution. With this result

we can obtain more statistical information about the random solution, as illustrated in

two particular examples.

Keyword: random linear transport equation, random velocity field, averaging approach,

statistical moments, Gaussian process, Telegraph process.

7.1 Introduction

In this note, we consider the transport of a passive scalar by an incompressible random

velocity field as described by the equation

Ut(x, t) + ?.[V(x, t)U (x, t)] = 0, U (x, t0) = g(x), (7.1)
where U is the density of a passively advected agent (concentration of a chemical species,

temperature, etc.), V is a random velocity field, and g(x) is the deterministic initial

distribution of the scalar. The subscript t in Ut(x, t) denotes the partial derivative with

respect to this variable. Taking into account the incompressibility of V, i.e., ? · V = 0,
we rewrite equation (7.1) as

Ut(x, t) + Vi(x, t)Uxi (x, t) = 0, U (x, t0) = g(x), (7.2)

79



80 Cap??tulo 7

where repeated indices indicate summation.

Standard approaches (see, e.g., [2, 6, 7, 9, 10, 12]) to derive an equation for the mean

of U use the Reynolds decomposition

Vi(x, t) = ?Vi(x, t)? + V ?i (x, t)
(angle brackets denote ensemble averaging) in (7.2) to obtain the following non-closed

averaged equation

?U?t + ?Vi(x, t)??U?xi + ?V ?i (x, t)Uxi? = 0. (7.3)
The basic difficulty with such approaches lies in the necessity to approximate (model)

the unknown correlation moment between the random velocity fluctuations and U (x, t),

the term ?V ?i (x, t)Uxi (x, t)? in (7.3). Moreover, the knowledge of the mean, ?U (x, t)?, is
not enough to provide a detailed understanding of the random transport process. One

must at least examine higher moments of U (x, t). With that in mind, it is our purpose to

show that for the linear transport equation (7.1), some approaches used to approximate

?V ?i (x, t)Uxi (x, t)? in (7.3) may be also used to approximate all the moments of the solution.
In Section 7.2 we present this result, and in Sections 7.3–7.4 we illustrate the approach

with two examples.

7.2 Main result

Proposition 7.1. Let Vi(x, t) = ?Vi(x, t)? + V ?i (x, t) in (7.2). Then
?U m(x, t)?t + ?Vi(x, t)??U m(x, t)?xi + ?V ?i (x, t)U mxi (x, t)? = 0, (7.4)

where ?U m(x, t)?, m ? Z, m ? 1, is the mth moment of the solution to (7.2).
Proof. Notice that U m(x, t), m ? Z, m ? 1, satisfies an equation like (7.2). Indeed,
differentiating U m(x, t) with respect to t and x, and using (7.2) we obtain

(U m)t + Vi(x, t)(U
m)xi = m U

(m?1)[?Ut(x, t) + Vi(x, t)Uxi (x, t)] = 0.

Averaging this expression and using the Reynolds decomposition of the velocity, Vi(x, t) =

?Vi(x, t)? + V ?i (x, t), yields (7.4).

Remark 7.1. In [1] we have shown that in one-dimensional transport problems with a

constant random velocity, V , if the partial differential equation for the moments is an

advection-diffusion equation with diffusion coefficient ?, then ? must satisfy the equation

?f ?V (x/t)? = fV (x/t)(x??V ?t), where fV (?) is the probability density function of V . For
such problems equation (7.3) is closed with ??V ?Ux? = ??U?xx.



7.3. First application: Gaussian processes 81

7.3 First application: Gaussian processes

Consider the following one-dimensional version of problem (7.1):

Ut(x, t) + V (t)Ux(x, t) = 0, U (x, 0) = H(?x), (7.5)
where H(x) is the Heaviside function, and the random velocity, V (t), is Gaussian with

?V (t)? = V constant and an exponentially decaying covariance function, CovV (t, ? ) =
?2V exp (?|t ? ?|/?). The covariance function is parameterized by the variance, Var[V (t)] =
?2V (which is assumed to be constant), and by the correlation length, ? &gt; 0, which governs

the decay rate of the time correlation.

According to [6, 9, 10], the correlation moment between the random flow-velocity and

the random concentration U can be written in the form

?V ?(t)Ux(x, t)? = ?
(? t

0

CovV (t, ? ) d?

)
?U (x, t)?xx. (7.6)

Thus, the mean concentration is exactly governed by

?U (x, t)?t + V ?U (x, t)?x =
(? t

0

CovV (t, ? ) d?

)
?U (x, t)?xx,

?U (x, 0)? = H(?x).
(7.7)

In view of Proposition 7.1 we can use (7.6) to calculate all the moments, i.e., the mth

moment satisfies the following equation:

?U m(x, t)?t + V ?U m(x, t)?x =
(? t

0

CovV (t, ? ) d?

)
?U m(x, t)?xx,

?U m(x, 0)? = [H(?x)]m = H(?x).
(7.8)

The solution to (7.8) is

?U m(x, t)? = 1
2
erfc

(
x ? V t
?(t)

)
, (7.9)

where erfc(x) is the complementary error function and

?(t) = 2

[? t
0

? ?
0

CovV (?, ? )d? d?

]1/2
.

We now compare the moments (7.9) with those yielded by the Monte Carlo method.

To generate the realizations V (t, ?) required by the Monte Carlo method, we use the

subroutine [mvnrnd.m] of MATLAB. The analytical solution for each realization is

U (x, t, ?) = U

(
x ?

? t
0

V (s, ?) ds, 0

)
= H

(? t
0

V (s, ?) ds ? x
)

.



82 Cap??tulo 7

In our numerical experiments the integration of V (t, ?) is performed using the Simpson’s

quadrature rule (see [3], for example). Figures 7.1 and 7.2 illustrate the mean, variance,

and third central moment of the solution to (7.5) computed using the averaging approach

and the Monte Carlo method (with 50 000 realizations). The plots correspond to the

following data: ?V (t)? = V = ?0.2; ?2V = 0.4; t = 0.6; ?t = 0.001; and ?x = 0.0005. In
Figure 7.1 we use ? = 0.1 and in Figure 7.2 we use ? = 1.0, i.e, a more correlated field.

All the numerical experiments were done in double precision with some MATLAB codes

on a 1.73Ghz Intel Core Duo 2 with 2Gb of memory.

?2 ?1 0 1 2
?0.2

0

0.2

0.4

0.6

0.8

1

1.2

1.4

1.6
init. cond.
Monte Carlo
Averaging

?2 ?1 0 1 2
?0.05

0

0.05

0.1

0.15

0.2

0.25

0.3

0.35

0.4
Monte Carlo
Averaging

?2 ?1 0 1 2

?0.1

?0.05

0

0.05

0.1

0.15

Monte Carlo
Averaging

Figure 7.1: Mean (left), variance (middle), and third central moment (right) of the solution
to (7.5); ? = 0.1.

?2 ?1 0 1 2
?0.2

0

0.2

0.4

0.6

0.8

1

1.2

1.4

1.6
init. cond.
Monte Carlo
Averaging

?2 ?1 0 1 2
?0.05

0

0.05

0.1

0.15

0.2

0.25

0.3

0.35

0.4
Monte Carlo
Averaging

?2 ?1 0 1 2

?0.1

?0.05

0

0.05

0.1

0.15

Monte Carlo
Averaging

Figure 7.2: Mean (left), variance (middle), and third central moment (right) of the solution
to (7.5); ? = 1.0.

7.3.1 The probability density function

For this particular example, we have shown that each moment of the solution to (7.5),

?U m(x, t)? (even in the case of a more general random initial condition, G(x)) satisfies



7.4. Second application: Telegraph processes 83

the following advection-diffusion equation:

?t(x, t) + V ?x(x, t) = ?(t) ?xx(x, t),

?(x, 0) = ?G(x)m?, (7.10)

where ?(t) =

? t
0

CovV (t, ? ) d? . As a consequence, the probability density function for

the random solution U (x, t), fU (u; x, t), also satisfies an initial value problem for the

advection-diffusion equation (7.10), i.e.,

(fU )t + V (fU )x = ?(t) (fU )xx,

fU (u; x, 0) = fG(u; x).
(7.11)

Indeed, the Fourier transform of fU (u; x, t), under the assumption that the probability

density function is uniquely determined by its moments (see, e.g., [5] for conditions for

uniqueness in the problems of moments), is

f?U (?; x, t) =
??

j=0

(i?)j

j!
?U m(x, t)?, (7.12)

where ??U m(x, t)?t + V ?U m(x, t)?x = ?(t)?U m(x, t)?xx. Taking the derivative with respect
to t and x in (7.12), we arrive at

(f?U )t + V (f?U )x = ?(t) (f?U )xx. (7.13)

Since the variable ? does not appear in the derivatives, we can go back to the variable

u and find (7.11). The respective initial condition follows from the probability density

function of G(x). This result for the density probability of U (x, t), fU (u; x, t) agrees with

that presented in [11] on page 247 using a different methodology.

7.4 Second application: Telegraph processes

In this section, we consider the one-dimensional transport with the random telegraph

process (see [4, 8], for example) as a model for the velocity, V (t). According to [10], this

is a convenient model of a function that has finite jumps in random times. The random

telegraph process is a stochastic process V (t) defined by

V (t) = V + ?(?1)N (t), (7.14)

where the state space of V (t) is {V ? ?0, V + ?0}, the times at which the process changes
the values (V ? ?0) and (V + ?0) are distributed according to a Poisson process N (t)



84 References

with intensity rate ?, and ? is a random variable independent of N (t) and such that

P{? = ?0} = 1/2 = P{? = ??0}. This process is stationary (see [4], for more details)
with mean ?V (t)? = V and covariance CovV (t, ? ) = ?20 exp (?2?|t ? ?|).

According to [6, 10], the correlation moment between V ?(t) and U (x, t) is exactly given
by

?V ?(t)U (x, t)? = ?
? t

0

CovV (t, ? )
?

?x
?U (x ? V (t ? ? ), ? )? d?. (7.15)

Using (7.15) in (7.3) we obtain the differential equation for the mean concentration,

?U (x, t)?t + V ?U (x, t)?x =
?

?x

? t
0

CovV (t, ? )
?

?x
?U (x ? V (t ? ? ), ? )? d?. (7.16)

Proposition 7.1 asserts that Equation (7.16) is the same for all statistical moments,

i.e., the mth moment satisfies the equation

?U m(x, t)?t + V ?U m(x, t)?x =
?

?x

? t
0

CovV (t, ? )
?

?x
?U m(x ? V )(t ? ? ), ? )? d?.

The analysis of the exact solution to (7.16) is presented in [10].

Acknowledgments

Our acknowledgments to the Brazilian Council for Development of Science and Technology

(CNPq) for support through grants 140406/2004?2 and 210132/2006?0. We thank Prof.
Lu?cio Tunes dos Santos, IMECC, UNICAMP, for his helpful suggestions used in Section

(7.3.1).

References

[1] F. A. Dorini, M. C. C. Cunha, Statistical moments of the random linear trans-

port equation. Technical Report 15/07, Imecc, Unicamp, Campinas, Brazil, (2007).

http://www.ime.unicamp.br/rel pesq/2007/rp15-07.html.

[2] P. O’Leary, M. B. Allen, F. Furtado, Groundwater transport with stochastic re-

tardation: numerical results. Computer Methods in Water Resources XII 1:255–261

(1998).

[3] G. Hammerlin, K. H. Hoffmann, Numerical Mathematics. Springer, 1991.

[4] D. Kannan, An Introduction to Stochastic Processes. Elsevier North Holland, New

York, 1979.



References 85

[5] M. G. Kendall, Conditions for uniqueness in the problems of moments. The Annals

of Mathematical Statistics 11:402–409 (1940).

[6] V. Klyatskin, Stochastic Equations and Waves in Randomly Inhomogeneous Media.

Nauka, Moscow, 1980.

[7] V. I. Klyatskin, Statistical description of the diffusion of a passive tracer is a random

velocity field. Physics-Uspekhi 37(5):501–513 (1994).

[8] S. Miller, D. Childers, Probability and Random processes: with applications to signal

processing an communications. Elsevier Academic Press, San Diego, California, 2004.

[9] S. M. Rytov, A. Yu, V. I. Tatarskii, Principles of Statistical Radiophysics - Elements

of Random Fields. Springer-Verlag, Berlin, Heidelberg, 1989.

[10] M. Shvidler, K. Karasaki, Exact averaging of stochastic equations for transport in

random velocity field. Transport in Porous Media 50:223–241 (2003).

[11] M. Shvidler, K. Karasaki, Probability density functions for solute in random field.

Transport in Porous Media 50:243–266 (2003).

[12] D. Zhang, Stochastic Methods for Flow in Porous Media - Coping with Uncertainties.

Academic Press, San Diego, 2002.



Me?todos para Equac?o?es do Transporte

com Dados Aleato?rios

Este exemplar corresponde a? redac?a?o final da

Tese devidamente corrigida e defendida por

Fabio Antonio Dorini e aprovada pela Banca

Examinadora.

Campinas, 17 de dezembro de 2007.

Tese apresentada ao Programa de Po?s Gra-

duac?a?o do Instituto de Matema?tica, Estat??stica

e Computac?a?o Cient??fica, unicamp, como re-

quisito parcial para a obtenc?a?o do t??tulo de

Doutor em Matema?tica Aplicada.

i



Cap??tulo 8

Concluso?es e trabalhos futuros

8.1 Concluso?es

A apresentac?a?o desta tese na forma cronolo?gica em que nosso trabalho foi sendo desen-

volvido e submetido a revistas especializadas reflete como fomos nos aprofundando numa

area bastante fe?rtil.

O desafio inicial de conhecer me?todos para lidar com incertezas em para?metros de

equac?o?es diferenciais como modelos matema?ticos foi motivado pela simulac?a?o de fluxo em

meios porosos. Historicamente estes modelos foram incorporando os conceitos de permea-

bilidade, permeabilidade relativa, porosidade, pressa?o capilar, ale?m dos mais tradicionais

na meca?nica dos fluidos, como a viscosidade. Experimentos em laborato?rio sa?o usados na

avaliac?a?o destas propriedades e como tal esta?o sujeitos a erros de medic?a?o e dificuldade

de obtenc?a?o de amostras compat??veis com os reservato?rios em analise. Assim, deve-se

considerar a aleatoriedade dos para?metros que alimentam as va?rias equac?o?es diferenciais

que sa?o usadas como modelo de fluxos em meios porosos.

Os trabalhos apresentados nesta tese representam nossa contribuic?a?o na metodologia

aplica?vel as equac?o?es diferenciais com dados aleato?rios.

8.2 Trabalhos futuros

Nesta sec?a?o resumimos tre?s problemas que esta?o em estudo e nos quais usamos algumas

ide?ias desenvolvidas nesta tese.

87



88 Cap??tulo 8

8.2.1 Problema 1.

As equac?o?es diferenciais parciais que foram objeto de estudo nesta tese sa?o casos particu-

lares da lei de conservac?a?o mais geral:

?

?t
Q(x, t) +

?

?x
f (Q(x, t)) = 0, t &gt; 0, x ? R,

Q(x, 0) = Q0(x). (8.1)

onde Q e? a quantidade conservada e f (Q) e? a func?a?o fluxo. Aplicac?o?es desta equac?a?o

aparecem em problemas de recuperac?a?o de petro?leo, dispersa?o de poluentes, fluxo de

gases, fluxo de tra?fego, dentre outros. Visando a dar continuidade aos nossos estudos,

e baseados nas ide?ias apresentadas em [6, 17], vamos investir na busca das propriedades

estat??sticas da soluc?a?o da Equac?a?o de Buckley-Leverett unidimensional aleato?ria, que

modela fluxos bifa?sicos imisc??veis e incompress??veis em um meio poroso,

?

?t
S(x, t) +

?

?x
f (S(x, t)) = 0, t &gt; 0, x ? 0,

S(0, t) = S?, t &gt; 0, (8.2)

S(x, 0) = S+, x &gt; 0,

onde S(x, t) representa a saturac?a?o de a?gua no meio poroso e a func?a?o fluxo, determinada

usando a Lei de Darcy e a incompressibilidade das duas fases dos fluidos, e? dada por:

f (S) =
S?

S? + ?(1 ? S)? . (8.3)

De acordo com a literatura especializada [2, 17, 18], as principais fontes de incerteza

(aleatoriedade) em (8.2)–(8.3) aparecem em ? (raza?o entre as viscosidades dos fluidos),

?, e nos estados S? e S+. O expoente ? e? oriundo da hipo?tese que a permeabilidade
relativa e? governada por uma lei de pote?ncia; isto e?, assume-se que a permeabilidade (que

e? func?a?o da saturac?a?o S) e? proporcional a S?, onde o expoente ? e? obtido a partir de

resultados experimentais via ajuste de curvas.

8.2.2 Problema 2.

A concentrac?a?o de um soluto na?o-reagente S(x, t) na regia?o ?x? = ?(x1, x2, ..., xN )? &amp;lt;?,
t ? t0 e? descrita pelas equac?o?es

?St(x, t) + ?.[V(x, t)S(x, t)] = 0, S(x, t0) = g(x), (8.4)

onde ? e? a porosidade e V (x, t) e? a velocidade aleato?ria de Darcy. Assumimos que g(x)

e? a concentrac?a?o inicial (na?o aleato?ria) e ? e? uma constante (na?o aleato?ria). Levando em



8.2. Trabalhos futuros 89

considerac?a?o a condic?a?o de incompressibilidade do fluido em (8.4), isto e?, div[V(x, t)] = 0,

temos

?St(x, t) + Vi(x, t)Sxi (x, t) = 0, S(x, t0) = g(x). (8.5)

Outra simplificac?a?o em (8.4) seria considerar a velocidade aleato?ria uma func?a?o apenas

do tempo – essa hipo?tese certamente simplifica o problema, pore?m o mesmo continua su-

ficientemente interessante e complicado. Assim, nosso problema de interesse e? o seguinte:

?St(x, t) + Vi(t)Sxi (x, t) = 0, S(x, t0) = g(x). (8.6)

E? fato que para cada realizac?a?o V(t, ?) da velocidade aleato?ria V(t), a soluc?a?o,

S(x, t, ?), e? constante ao longo das curvas caracter??sticas

?

?t
X(t) = V(t, ?), X(0) = X0, (8.7)

ou, equivalentemente, S(X(t), t, ?) = g(X0), onde X(t) = X0 +

? t
0

V(?, ?)d? .

Deste modo, a soluc?a?o para (8.6) pode ser expressa como

S(x, t) = g

(
x ?

? t
0

V(? )d?

)
. (8.8)

Agora, denotando A(t) (vetor aleato?rio N-dimensional) como

A(t) =

? t
0

V(? )d?, (8.9)

segue que S(x, t) = g (x ? A(t)).
Os resultados ate? o momento apresentados provam o seguinte resultado:

Proposition 8.1. O m-e?simo momento estat??stico, m ? Z, m ? 1, da soluc?a?o para (8.6)
e? dado por

?Sm(x, t)? =
?

RN
g(x ? a)mfA(t)(a) da, (8.10)

onde fA(t)(a) e? a func?a?o de densidade de probabilidade conjunta do vetor aleato?rio A(t).

Remark 8.1. De acordo com (8.10), se g(x) = ?(x) e? uma func?a?o Delta enta?o

?Sm(x, t)? =
?

RN
?(x ? a)mfA(t)(a) da = fA(t)(x). (8.11)

Em vista destes resultados, seria interessante entender as propriedades estat??sticas do

vetor aleato?rio A(t) como func?a?o das propriedades estat??sticas de V(t). Por exemplo, a

partir de [19], pa?gina 162, segue que:



90 Cap??tulo 8

Proposition 8.2. Se V(t) e? um processo aleato?rio Gaussiano enta?o A(t) e? tambe?m um

processo Gaussiano.

Neste caso, as propriedades estat??sticas (me?dia e covaria?ncia) de A(t) sa?o facilmente

obtidas via integrac?a?o das propriedades estat??sticas do processo Gaussiano V(t). Tendo

em vista o exposto, nosso interesse futuro e? entender o processo aleato?rio A(t) a partir

das informac?o?es estat??sticas do processo V(t).

8.2.3 Problema 3.

Outro problema de interesse e? investigar a possibilidade de aplicac?a?o da metodologia

apresentada no Cap??tulo 4 a fim propor esquemas nume?ricos para

• a equac?a?o de advecc?a?o unidimensional aleato?ria:
?

?t
Q(x, t) + A(x)

?

?x
Q(x, t) = 0, t &gt; 0, x ? R,

Q(x, 0) = Q0(x), (8.12)

onde a velocidade de transporte A(x) e a condic?a?o inicial sa?o func?o?es aleato?rias.

• a equac?a?o de advecc?a?o bidimensional aleato?ria:
{

Qt(x, y, t) + A(x, y)Qx(x, y, t) + B(x, y)Qy(x, y, t) = 0, t &gt; 0, x, y ? R,
Q(x, y, 0) = Q0(x, y),

(8.13)

onde as velocidades de transporte, A(x, y) e B(x, y), e a condic?a?o inicial, Q0(x, y),

sa?o campos aleato?rios. Me?todos do tipo direc?o?es alternadas (ADI) [27, 40], que te?m

sido usados com sucesso para tratar equac?o?es diferenciais parciais multidimensionais

determin??sticas, seriam utilizados para dividir o problema bidimensional em dois

problemas unidimensionais.



Refere?ncias Bibliogra?ficas

[1] J. M. Burgers, A mathematical model illustrating the theory of turbulance. Ad. Appl.

Mech. 1:171–179 (1948).

[2] Z. Chen, G. Huan, Y. Ma, Computational Methods for Multiphase Flows in Porous

Media. SIAM - Computational Science and Engineering, Philadelphia, 2006.

[3] R. Courant, K. Friedrichs, H. Lewy, Uber die partiellen differenzengleichungen der

matematischen physik. Math. Annalen. 100:32–74 (1928); translated into English by

P. Fox (1956), Institute of Mathematical Sciences, New York University.

[4] M. C. C. Cunha, F. A. Dorini, A note on the Riemann problem for the random

transport equation. Computational and Applied Mathematics 26(3):323–335 (2007).

[5] M. C. C. Cunha, F. A. Dorini, A numerical scheme for the variance of the solution

of the random transport equation. Appl. Math. and Comput. 190(1):362–369 (2007).

[6] M. C. C. Cunha, F. A. Dorini, Statistical moments of the solution of the random

Burgers-Riemann problem. Technical Report 11/07, Imecc, Unicamp, Campinas,

Brazil, (2007). http://www.ime.unicamp.br/rel pesq/2007/rp11-07.html.

[7] F. A. Dorini, M. C. C. Cunha, Statistical moments of the random linear trans-

port equation. Technical Report 15/07, Imecc, Unicamp, Campinas, Brazil, (2007).

http://www.ime.unicamp.br/rel pesq/2007/rp15-07.html.

[8] F. A. Dorini, M. C. C. Cunha, A finite volume method for the mean of the solution

of the random transport equation. Appl. Math. and Comput. 187(2):912–921 (2007).

[9] F. A. Dorini, F. Furtado, M. C. C. Cunha, On the evaluation of moments for so-

lute transport by random velocity fields. Technical Report 31/07, Imecc, Unicamp,

Campinas, Brazil, (2007). http://www.ime.unicamp.br/rel pesq/2007/rp31-07.html.

[10] G. S. Fishman, Monte Carlo: concepts, algorithms and applications. Springer-Verlag,

New York, 1996.

91



92 Refere?ncias Bibliogra?ficas

[11] F. Furtado, F. Pereira, Scaling analysis for two-phase immiscible flow in heteroge-

neous porous media. Computational and Applied Mathematics 17(3):237–263 (1998).

[12] J. Glimm, Solutions in the large for nonlinear hyperbolic systems of equations. Comm.

Pure Appl. Math. 18:695–715 (1965).

[13] J. Glimm, D. Sharp, Stochastic partial differential equations: Selected applications

in continuum physics, in Stochastic Partial Differential Equations: Six Perspecti-

ves, (Edited by R. A. Carmona and B. L. Rozovskii), Mathematical Surveys and

Monographs, American Mathematical Society, No. 64, p.03–44, Providence, 1998.

[14] S. K. Godunov, A difference method for numerical calculation of discontinuous solu-

tions of the equations of hydrodynamics. Mat. Sb. 47:271–306 (1959).

[15] G. Hammerlin, K. H. Hoffmann, Numerical Mathematics. Springer, 1991.

[16] A. Harten, High resolution schemes for hyperbolic conservation laws. J. Comput.

Phys. 49(2):357–393 (1983).

[17] H. Holden, N. H. Risebro, Stochastic properties of the scalar Buckley-Leverett equa-

tion. SIAM Journal on Applied Mathematics 51(5):1472–1488 (1991).

[18] L. Holden, The Buckley–Leverett equation with spatially stochastic flux function.

SIAM Journal on Applied Mathematics 57(5):1443–1454 (1997).

[19] D. Kannan, An Introduction to Stochastic Processes. Elsevier North Holland, New

York, 1979.

[20] M. G. Kendall, Conditions for uniqueness in the problems of moments. The Annals

of Mathematical Statistics 11:402–409 (1940).

[21] H. Kim, An efficient computational method for statistical moments of Burger’s equa-

tion with random initial conditions. Mathematical Problems in Engineering. 2006:1–

21, doi:10.1155/MPE/2006/17406 (2006).

[22] P. E. Kloeden, E. Platen, Numerical Solution of Stochastic Differential Equations.

Springer, New York, 1999.

[23] V. Klyatskin, Stochastic Equations and Waves in Randomly Inhomogeneous Media.

Nauka, Moscow, 1980.

[24] V. I. Klyatskin, Statistical description of the diffusion of a passive tracer is a random

velocity field. Physics-Uspekhi 37(5):501–513 (1994).



Refere?ncias Bibliogra?ficas 93

[25] A. Kolmogorov, Uber die analytischen methoden in der wahrscheinlichkeitsrech-nung.

Math. Annalen. 104:415–458 (1931).

[26] R. J. LeVeque, Numerical Methods for Conservation Laws. Birkha?user, Berlin, 1992.

[27] R. J. LeVeque, Finite Volume Methods for Hyperbolic Problems. Cambridge Univer-

sity Press, Cambridge, 2002.

[28] P. O’Leary, M. B. Allen, F. Furtado, Groundwater transport with stochastic re-

tardation: numerical results. Computer Methods in Water Resources XII 1:255–261

(1998).

[29] S. Miller, D. Childers, Probability and Random processes: with applications to signal

processing an communications. Elsevier Academic Press, San Diego, California, 2004.

[30] B. Oksendal, Stochastic Differential Equations: an introduction with applications.

Springer, New York, 2000.

[31] H. Osnes, H. P. Langtangen, A study of some finite difference schemes for a uni-

diretional stochastic transport equation. SIAM Journal on Scientific Computing

19(3):799–812 (1998).

[32] A. D. Polyanin, Handbook of Linear Partial Differential Equations for Engineers and

Scientists. Chapman &amp;amp; Hall/CRC, New York, 2002.

[33] L. J. W. S. Rayleigh, On James Bernoulli’s theorem in probabilities. Phil. Mag.

47:246–251 (1899).

[34] S. M. Rytov, A. Yu, V. I. Tatarskii, Principles of Statistical Radiophysics - Elements

of Random Fields. Springer-Verlag, Berlin, Heidelberg, 1989.

[35] G. I. Schue?ller, A state-of-the-art report on computational stochastic mechanics.

Prob. Engrg. Mech. 12(4):197–322 (1997).

[36] M. Shvidler, K. Karasaki, Exact averaging of stochastic equations for transport in

random velocity field. Transport in Porous Media 50:223–241 (2003).

[37] M. Shvidler, K. Karasaki, Probability density functions for solute in random field.

Transport in Porous Media 50:243–266 (2003).

[38] K. Sobczyk, Stochastic Wave Propagation. Elsevier-PWN Polish Scientific Pub., New

York, 1985.



94 Refere?ncias Bibliogra?ficas

[39] T. T. Soong, Random Differential Equations in Sciences and Engineering. Academic

Press, New York, 1973.

[40] J. C. Strikwerda, Finite Difference Schemes and Partial Differential Equations.

Wadsworth &amp;amp; Brooks/Cole, California, 1989.

[41] J. W. Thomas, Numerical Partial Differential Equations: finite difference methods.

Springer-Verlag, New York, 1995.

[42] Q. Zhang, The asymptotic scaling behavior of mixing induced by a random velocity

field. Adv. Appl. Math. 16:23–58 (1995).

[43] Q. Zhang, The transient behavior of mixing induced by a random velocity field.

Water Research Resources 31:577–591 (1995).

[44] D. Zhang, Stochastic Methods for Flow in Porous Media - Coping with Uncertainties.

Academic Press, San Diego, 2002.




</field>
	</doc>
</add>