<?xml version="1.0" encoding="utf-8"?>
<add>
	<doc>
		<field name="docid">BR-TU.06306</field>
		<field name="filename">10831_DissertacaoMestrado_EduardoTakafuji_VersaoCorrigida.pdf</field>
		<field name="filetype">PDF</field>
		<field name="text">
UNIVERSIDADE DE SÃO PAULO 

INSTITUTO DE GEOCIÊNCIAS 

 

 

ESTUDO COMPARATIVO ENTRE A SIMULAÇÃO SEQUENCIAL 

GAUSSIANA E A SIMULAÇÃO BASEADA EM WAVELETS APLICADO 

A QUANTIFICAÇÃO DE MINÉRIO DE CU EM UM DEPÓSITO 

SINTÉTICO 

 

Eduardo Henrique de Moraes Takafuji 

 

Orientador: Prof. Dr. Marcelo Monteiro da Rocha 

 

 

 

DISSERTAÇÃO DE MESTRADO 

Programa de Pós-Graduação em Geociências: Recursos Minerais e 

Hidrogeologia 

 

SÃO PAULO 

2015 

Versão Corrigida 

  



 

 

 

 

 

  



UNIVERSIDADE DE SÃO PAULO 

INSTITUTO DE GEOCIÊNCIAS 

 

 

ESTUDO COMPARATIVO ENTRE A SIMULAÇÃO SEQUENCIAL 

GAUSSIANA E A SIMULAÇÃO BASEADA EM WAVELETS APLICADO 

A QUANTIFICAÇÃO DE MINÉRIO DE CU EM UM DEPÓSITO 

SINTÉTICO 

 

Eduardo Henrique de Moraes Takafuji 

 

Orientador: Prof. Dr. Marcelo Monteiro da Rocha 

 

 

 

DISSERTAÇÃO DE MESTRADO 

Programa de Pós-Graduação em Geociências: Recursos Minerais e 

Hidrogeologia 

 

 

SÃO PAULO 

2015 

  



 

 

 

 

 

Ficha catalográfica preparada pelo Serviço de Biblioteca e 

Documentação do Instituto de Geociências da Universidade de São Paulo 

    

 

      

           Takafuji, Eduardo Henrique de Moraes 

        Estudo comparativo entre a simulação sequencial 

Gaussiana e a simulação baseada em Wavelets aplicado 

a quantificação de minério de Cu em um depósito 

sintético. / Eduardo Henrique de Moraes Takafuji. -

- São Paulo, 2015. 

           102 p. : il. + anexos 

 

           Dissertação (Mestrado) : IGc/USP 

           Orient.: Rocha, Marcelo Monteiro da 

 

1. Geoestatística 2. Simulação Baseada em Wavelets 
3. Estatística multiponto 4.Recursos minerais I. 

Título 



Agradecimentos 

Eu sou eternamente grato a todos que fizeram parte da trajetória da minha vida e me 

ajudaram a superar as adversidades e chegar aqui. Primeiramente aos meus familiares que 

sempre estiveram dispostos a me apoiar e incentivar, aos meus pais (Eiji e Abigail) que nunca 

pecaram na minha educação e são meu exemplo de vida. E à Renata Nakatu por toda a paciência 

e apoio. Agradeço também aos meus amigos, de infância e da faculdade.  

Um voto de reconhecimento a todos os meus mestres, em particular ao Prof. Dr. Marcelo 

Rocha, que me orientou no mundo da geoestatística. E ao Prof. Dr. Roussos Dimitrakopoulos 

que me proporcionou a oportunidade de ampliar meus horizontes neste mundo da geoestatística. 

E finalmente, aos colegas da sala 105/LIG (IGc-USP) e do COSMO (McGill-Canadá).  

 

 

 

 

 

 

 

  



 

 

 

 

 

  



 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

"I can live with doubt, and uncertainty, and not knowing. I think it's much more interesting to 

live not knowing than to have answers which might be wrong."  

Richard P. Feynman 

  



 

 

 

 

 

  



Resumo 

TAKAFUJI, E. H. M., Estudo comparativo entre a Simulação Sequencial Gaussiana e a 

Simulação Baseada em Wavelets aplicado a quantificação de minério de Cu em um depósito 

sintético. São Paulo: Instituto de Geociências, Universidade de São Paulo, 2015. 102 p. 

Dissertação de Mestrado em Recursos Minerais. 

 

O julgamento da qualidade de um método de estimativa/simulação é mais adequado se 

os resultados puderem ser comparados a dados reais. Uma vez que na mineração isto é inviável, 

este trabalho é baseado em um modelo de depósito mineral de cobre – representando a geologia 

e a distribuição de dados de modo heterogêneos. O modelo reproduz um depósito com 

preenchimento hidrotermal em uma falha inversa e as rochas encaixantes são meta-arenito e 

folhelho dobrados. 

O objetivo é comparar os resultados obtidos pelo método de Simulação Baseada em 

Wavelets – método o qual utiliza a estatística espacial de alta-ordem para reproduzir as 

estruturas da geologia – com o método clássico de Simulação Sequencial Gaussiana, a fim de 

avaliar um método de geoestatística de multiponto aplicado a variável contínua. 

Para comparar os resultados, foi calculado o valor potencial e para qual pilha (minério 

ou estéril) deveria ir cada bloco. Os resultados mostram que, matematicamente, a Simulação 

Sequencial Gaussiana obteve resultados melhores, uma vez que destinou melhor seus blocos e 

perdeu menos dinheiro com estéril na pilha de minério e minério de pilha de estéril. Porém, é 

notória a influência da imagem de treinamento nos resultados da Simulação Baseada em 

Wavelets, o que mostra que a Simulação Baseada em Wavelets de variáveis contínuas é 

promissora se a imagem de treinamento for adequada. O grande problema é que sua escolha ou 

criação é demasiadamente complexa, pois necessita de precisão local e global. 

 

 

Palavras-chave: Geoestatística; Simulação Sequencial Gaussiana; Simulação Baseada 

em Wavelets (Wavesim).  



 

 

 

 

 

  



Abstract 

TAKAFUJI, E. H. M., Comparison between Sequential Gaussian Simulation and Wavelet-

based Simulation applied to quantify copper ore in a synthetic deposit. São Paulo: Institute of 

Geosciences, University of São Paulo, 2015. 102 p. Dissertation in Mineral Resources. 

 

The judgment of the quality of an estimation/simulation method is more suitable if the 

results can be compared to real data. Once in mining that is not feasible, this work is based on 

a synthetic mineral deposit - represented by a very heterogeneous geology and spatial data 

distribution. The model reproduces a deposit with hydrothermal filling in an inverse fault and 

the bedrocks are folded meta-sandstone and phyllite. 

The objective is to compare the results obtained by Wavelet-based Simulation method 

- which uses the spatial high-order statistic to reproduce the geologic structures - with the classic 

method of Sequential Gaussian Simulation in order to evaluate a multipoint geostatistical 

method applied to a continuous variable. 

To compare the results, the potential value was calculated and to which pile (ore or 

waste) each block should go. The results show that, mathematically, Sequential Gaussian 

Simulation’s results are better, since its blocks allocated better and lost less money on waste in 

ore pile and ore in waste pile. However, it is clear the influence of the training image on the 

results of Wavelet-based Simulation. This shows that the Wavelet-based Simulation of 

continuous variables is promising if the training image is appropriate. The big problem is that 

choosing or creating it is too complex, because it requires local and global precision. 

 

 

Keywords: Geostatistics; Sequential Gaussian Simulation; Wavelet-based Simulation 

(Wavesim). 

 

 

 



 

 

 

 

  



LISTA DE FIGURAS 

 

FIGURA 1- SEMIVARIOGRAMA TÍPICO E SUAS PROPRIEDADES (YAMAMOTO, 2001A).............................................. 21 

FIGURA 2 - POSSÍVEIS COMPORTAMENTOS DO SEMIVARIOGRAMA PRÓXIMO À ORIGEM (YAMAMOTO, 2001A). ...... 22 

FIGURA 3 - ANISOTROPIA GEOMÉTRICA (A), ZONAL (B), MISTA (C) (YAMAMOTO, 2001A). ................................. 23 

FIGURA 4 - OS PRINCIPAIS MODELOS TEÓRICOS DE SEMIVARIOGRAMAS (YAMAMOTO, 2001A).............................. 23 

FIGURA 5 - REPRESENTAÇÃO DE UMA VALIDAÇÃO CRUZADA E SUA RETA DE REGRESSÃO (YAMAMOTO, 2001A). . 24 

FIGURA 6 - COMPARAÇÃO ENTRE OS MÉTODOS DE SELEÇÃO DE AMOSTRAS. A FIGURA 6A MOSTRA A CAPTURA DE 

DADOS COM OS PONTOS MAIS PRÓXIMOS; A FIGURA 6B MOSTRA A CAPTURA DE DOIS DADOS PARA CADA 

QUADRANTE E A FIGURA 6C MOSTRA UM DADO PARA CADA OCTANTE (YAMAMOTO, 2001A). ..................... 25 

FIGURA 7 - FLUXOGRAMA DOS PROCESSOS DA ANÁLISE GEOESTATÍSTICA DE VARIÁVEIS CONTÍNUAS (MODIFICADO 

DE CHILÈS E DELFINER, 1999). ..................................................................................................................... 34 

FIGURA 8 – ILUSTRAÇÃO DE COMO FUNCIONA A ESTATÍSTICA DE DOIS PONTOS E DE TRÊS PONTOS ((I) MACHUCA-

MORY E DIMITRAKOPOULOS, 2013). ............................................................................................................ 44 

FIGURA 9 – DEFINIÇÃO DO MODELO, EXEMPLO DE UM MODELO 2D 3X3 NUMA MALHA 11 X 11 (ARPAT E CAERS, 

2007). ........................................................................................................................................................... 47 

FIGURA 10 – EXEMPLO DE IMAGEM DE TREINAMENTO E SEUS PADRÕES OBTIDOS PELO ESCANEAMENTO DE UM 

MAPA ONDE 0 REPRESENTA UM ARGILITO E 1 ARENITO (ARPAT E CAERS, 2007). ......................................... 47 

FIGURA 11 - COMPARAÇÃO ENTRE A IMAGEM ORIGINAL (A) E A RECONSTRUÍDA (B) DEPOIS DA REDUÇÃO DE 75% 

DOS DADOS ORIGINAIS (CHATTERJEE, DIMITRAKOPOULOS E MUSTAPHA, 2012). ......................................... 52 

FIGURA 12 – ESTRUTURA DA ANÁLISE DISCRETA DE WAVELETS DE MÚLTIPLAS RESOLUÇÕES, MOSTRANDO A 

RELAÇÃO ENTRE COEFICIENTES DE WAVELETS DE DUAS ESCALAS E UM EXEMPLO DE UM VETOR EXTRATOR 

COM UM MODELO 3X3 (CHATTERJEE E DIMITRAKOPOULOS, 2011). .............................................................. 54 

FIGURA 13 - A ESQUERDA UM EXEMPLO DE DECOMPOSIÇÃO DE WAVELETS DA LENNA (OUAHABI, 2012) E A 

DIREITA AS BANDAS DE CADA ESCALA DA DECOMPOSIÇÃO (CHATTERJEE, MUSTAPHA E DIMITRAKOPOULOS, 

2015). ........................................................................................................................................................... 55 

FIGURA 14 – FLUXOGRAMA DA ESCOLHA PARA UMA ESCALA ÓTIMA (CHATTERJEE, DIMITRAKOPOULOS E 

MUSTAPHA, 2012). ....................................................................................................................................... 56 

FIGURA 15 – DESCRIÇÃO DOS PASSOS DA WAVESIM. CHATTERJEE, MUSTAPHA E DIMITRAKOPOULOS (2015). ...... 60 

FIGURA 16 – FLUXOGRAMA DO VALOR POTENCIAL DE CADA BLOCO. .................................................................... 63 

FIGURA 17 – STRINGS DO DATAMINE® QUE FORMARAM O MODELO GEOLÓGICO SEM A TOPOGRAFIA. .................. 64 

FIGURA 18 – PERFIL A-A’ DO MAPA GEOLÓGICO. .................................................................................................. 66 

FIGURA 19 – MAPA GEOLÓGICO DO DEPÓSITO SINTÉTICO. ..................................................................................... 67 

FIGURA 20 – PERFIL REPRESENTANDO AS LITOLOGIAS E O PERÍMETRO DO CORPO DE MINÉRIO INTERPRETADO. .... 68 

FIGURA 21 – MODELO TRIDIMENSIONAL DO CORPO DE MINÉRIO, EM VERMELHO, DEFINIDO A PARTIR DAS 

SONDAGENS, EM AZUL E EM VERDE É MOSTRADO A TOPOGRAFIA. ................................................................ 69 

FIGURA 22 – HISTOGRAMA DOS DADOS REAIS DE COBRE DENTRO DO CORPO MINERALIZADO. ............................... 69 



 

 

 

 

FIGURA 23 – HISTOGRAMA DOS DADOS AMOSTRADOS EM FUROS DE SONDA (DH) DE COBRE DENTRO DO CORPO 

MINERALIZADO. ............................................................................................................................................ 70 

FIGURA 24 – BOXPLOT DOS DADOS REAIS E DOS AMOSTRADOS COM E SEM OUTLIERS. ........................................... 71 

FIGURA 25 – VARIOGRAMAS EXPERIMENTAIS COM A QUANTIDADE DE PARES UTILIZADOS PARA SEU CÁLCULO E 

MODELOS TEÓRICOS DE VARIOGRAMA DO COBRE AJUSTADOS. ..................................................................... 73 

FIGURA 26 – RESULTADO DA KRIGAGEM PARA O VOLUME DO DEPÓSITO INTEIRO (IMAGEM DE TREINAMENTO) E A 

IMAGEM DA DIREITA É A REGIÃO CONSIDERADA COMO CORPO DO MINÉRIO DESTE VOLUME . ....................... 74 

FIGURA 27– HISTOGRAMA DOS TEORES DE COBRE ESTIMADOS POR KRIGAGEM ORDINÁRIA QUE SERÃO UTILIZADOS 

COMO IMAGEM DE TREINAMENTO. ............................................................................................................... 75 

FIGURA 28 – VARIOGRAMA EXPERIMENTAL COM A QUANTIDADE DE PARES UTILIZADOS PARA SEU CÁLCULO E 

MODELO TEÓRICO DE VARIOGRAMA DA VARIÁVEL COBRE TRANSFORMADA EM UMA GAUSSIANA NORMAL 

[0,1]. ............................................................................................................................................................. 76 

FIGURA 29 - REALIZAÇÃO Nº100 DA SIMULAÇÃO SEQUENCIAL GAUSSIANA PARA OS DADOS DE COBRE DO 

DEPÓSITO SINTÉTICO. A IMAGEM DA ESQUERDA MOSTRA O MODELO DE BLOCOS TOTAL E DA DIREITA APENAS 

DO CORPO DE MINÉRIO. ................................................................................................................................. 78 

FIGURA 30 - REALIZAÇÃO Nº1 DA SIMULAÇÃO SEQUENCIAL GAUSSIANA. A IMAGEM DA ESQUERDA MOSTRA O 

MODELO DE BLOCOS TOTAL E DA DIREITA APENAS DO CORPO DE MINÉRIO. .................................................. 78 

FIGURA 31 - REALIZAÇÃO Nº65 DA SIMULAÇÃO SEQUENCIAL GAUSSIANA. A IMAGEM DA ESQUERDA MOSTRA O 

MODELO DE BLOCOS TOTAL E DA DIREITA APENAS DO CORPO DE MINÉRIO. .................................................. 79 

FIGURA 32 – HISTOGRAMA DO E-TYPE DOS TEORES DE COBRE NOS BLOCOS SIMULADOS PELA SIMULAÇÃO 

SEQUENCIAL GAUSSIANA. ............................................................................................................................ 80 

FIGURA 33 - REALIZAÇÃO Nº100 DA WAVESIM PARA OS DADOS DE COBRE DO DEPÓSITO SINTÉTICO. OBSERVA-SE A 

PRESENÇA DE UM CORPO MINERALIZADO QUE NÃO ENCONTRA CORRESPONDÊNCIA AMOSTRAL. .................. 81 

FIGURA 34 - REALIZAÇÃO Nº1 DA WAVESIM PARA OS DADOS DE COBRE DO DEPÓSITO SINTÉTICO. A IMAGEM DA 

ESQUERDA MOSTRA O MODELO DE BLOCOS TOTAL E DA DIREITA APENAS DO CORPO DE MINÉRIO. ................ 82 

FIGURA 35 - REALIZAÇÃO Nº65 DA WAVESIM PARA OS DADOS DE COBRE DO DEPÓSITO SINTÉTICO. A IMAGEM DA 

ESQUERDA MOSTRA O MODELO DE BLOCOS TOTAL E DA DIREITA APENAS DO CORPO DE MINÉRIO. ................ 82 

FIGURA 36 – HISTOGRAMA DA MÉDIA DO TEOR DE COBRE DOS BLOCOS SIMULADOS PELO MÉTODO WAVESIM. ..... 83 

FIGURA 37 – HISTOGRAMA DOS VALORES REAIS DE CADA BLOCO DO DEPÓSITO. ................................................... 85 

FIGURA 38 – HISTOGRAMA DA MEDIA DOS VALORES POTENCIAIS DE CADA BLOCO SIMULADO POR SGS. .............. 85 

FIGURA 39 – HISTOGRAMA DA MEDIA DOS VALORES POTENCIAIS DE CADA BLOCO SIMULADO POR WAVESIM. ...... 86 

FIGURA 40 – BOXPLOT DA QUANTIDADE DE COBRE REAL, KRIGADA (KRIG) E SIMULADA PELOS MÉTODOS DE 

SIMULAÇÃO SEQUENCIAL GAUSSIANA (SGS40) E WAVESIM (WS40). ......................................................... 87 

FIGURA 41 – SOMA CUMULATIVA DOS VALORES POTENCIAIS DE CADA BLOCO PARA TODAS AS REALIZAÇÕES DA 

SIMULAÇÃO SEQUENCIAL GAUSSIANA E DA SIMULAÇÃO BASEADA EM WAVELETS, DOS BLOCOS REAIS E DOS 

BLOCOS DE KRIGADOS. A IMAGEM (A) SÃO OS VALORES DOS LUCROS POTENCIAIS DE CADA REALIZAÇÃO DA 

SGS. A IMAGEM (B) SÃO OS VALORES DOS LUCROS POTENCIAIS DE CADA REALIZAÇÃO DA WAVESIM. 

ENQUANTO A IMAGEM (C) É A JUSTAPOSIÇÃO DE (A) E (B). ........................................................................... 88 

FIGURA 42 – BOXPLOT DA DISTRIBUIÇÃO DA QUANTIDADE DE BLOCOS ALOCADOS NAS PILHAS CORRETAS, BLOCOS 

DE ESTÉRIL CONSIDERADOS COMO MINÉRIO E BLOCOS DE MINÉRIO CONSIDERADOS COMO ESTÉRIL. ............ 89 



FIGURA 43 – GRÁFICO BOXPLOT DE QUANTOS REAIS DE MINÉRIO REAL FORAM CLASSIFICADOS COMO ESTÉRIL E 

QUANTOS REAIS ESPERADOS ERAM ESTÉRIL. ................................................................................................ 91 

  



 

 

 

 

LISTA DE TABELAS 

 

TABELA 1 –SEMIVARIOGRAMA DA REGIÃO MINERALIZADA INTERNA (QUARTZITO) E A DA EXTERNA (FORA DO 

QUARTZITO). ................................................................................................................................................. 65 

TABELA 2 – ESTATÍSTICAS DESCRITIVAS DOS DADOS REAIS DE COBRE EM TODA A ZONA MINERALIZADA. ............. 70 

TABELA 3 - ESTATÍSTICAS DESCRITIVAS DOS DADOS AMOSTRADOS NOS FUROS DE SONDA DENTRO DO CORPO DE 

MINÉRIO. ....................................................................................................................................................... 70 

TABELA 4 – MODELO TEÓRICO DE VARIOGRAMA DA VARIÁVEL COBRE. ................................................................ 72 

TABELA 5 – ESTATÍSTICA DESCRITIVA DOS DADOS KRIGADOS. .............................................................................. 74 

TABELA 6 – PARÂMETROS DO MODELO TEÓRICO DE VARIOGRAMA DO COBRE TRANSFORMADO EM UMA 

GAUSSIANA NORMAL [0,1]. .......................................................................................................................... 77 

TABELA 7 – ESTATÍSTICAS DESCRITIVAS DO E-TYPE DOS BLOCOS SIMULADOS PARA TEORES DE COBRE. ............... 79 

TABELA 8 – ESTATÍSTICAS DESCRITIVAS DO E-TYPE DO WAVESIM PARA OS TEORES DE COBRE. ............................ 83 

TABELA 9 – ESTATÍSTICA DESCRITIVA DA QUANTIDADE DE BLOCOS QUE CADA REALIZAÇÃO ACERTOU, PARA A 

SIMULAÇÃO SEQUENCIAL GAUSSIANA (SGS) E WAVESIM (WS). ................................................................. 89 

TABELA 10 – ESTATÍSTICA DESCRITIVA DA QUANTIDADE DE BLOCOS QUE CADA REALIZAÇÃO ACERTOU, PARA A 

SIMULAÇÃO SEQUENCIAL GAUSSIANA (SGS) E WAVESIM (WS). ................................................................. 90 

TABELA 11 – ESTATÍSTICA DESCRITIVA DA QUANTIDADE DE BLOCOS QUE CADA REALIZAÇÃO ACERTOU PARA A 

SIMULAÇÃO SEQUENCIAL GAUSSIANA (SGS) E WAVESIM (WS). ................................................................. 90 

TABELA 12 – ESTATÍSTICA DESCRITIVA DO TOTAL DE REAIS QUE FORAM CONSIDERADOS COMO LUCRO E ERAM 

PREJUÍZO. ...................................................................................................................................................... 92 

TABELA 13 – ESTATÍSTICA DESCRITIVA DO TOTAL DE REAIS QUE FORAM CONSIDERADOS COMO ESTÉRIL E 

GERARIAM LUCRO. ........................................................................................................................................ 92 

  



Sumário 

 

1. INTRODUÇÃO ...........................................................................................................................17 

2. OBJETIVOS ...............................................................................................................................18 

3. MATERIAIS ...............................................................................................................................19 

4. REVISÃO BIBLIOGRÁFICA..........................................................................................................19 

4.1 GEOESTATÍSTICA ............................................................................................................................ 19 

4.2 KRIGAGEM ................................................................................................................................... 26 

4.2.1 Krigagem Ordinária ......................................................................................................... 26 

4.2.2 Krigagem Simples ............................................................................................................ 27 

4.3 SIMULAÇÃO GEOESTATÍSTICA ........................................................................................................... 28 

4.3.1 Justificativa da utilização dos métodos de simulação ..................................................... 30 

4.3.2 Simulação Condicional..................................................................................................... 32 

4.3.3 Simulação Sequencial Gaussiana .................................................................................... 35 

4.3.4 Algoritmo da Simulação Sequencial Gaussiana .............................................................. 38 

4.3.5 Transformação gaussiana normal ................................................................................... 40 

4.4 MÉTODO DE SIMULAÇÃO DE MULTIPONTOS (MPS) ............................................................................. 41 

4.4.1 Imagem de Treinamento ................................................................................................. 45 

4.4.2 Método de Simulação Baseada em Wavelets (Wavesim) ............................................... 47 

4.4.3 Algoritmo da Simulação Baseada em Wavelets .............................................................. 61 

5. ANÁLISE DO LUCRO POR BLOCO...............................................................................................62 

6. BANCO DE DADOS SINTÉTICO ..................................................................................................63 

7. RESULTADOS ............................................................................................................................68 

7.1 AMOSTRAGEM .............................................................................................................................. 68 

7.2 ANÁLISE GEOESTATÍSTICA ................................................................................................................ 71 

7.2.1 Krigagem ......................................................................................................................... 72 

7.2.2 Simulação Sequencial Gaussiana .................................................................................... 75 

7.2.3 Simulação Baseada em Wavelets (Wavesim) .................................................................. 80 

7.3 ANÁLISE DO LUCRO POR BLOCO ......................................................................................................... 83 

8. DISCUSSÃO DOS RESULTADOS .................................................................................................86 

9. CONSIDERAÇÕES FINAIS ...........................................................................................................92 

REFERÊNCIAS BIBLIOGRÁFICAS ..........................................................................................................94 



 

 

 

 

ANEXO 1 ........................................................................................................................................... 98 

ANEXO 2 ......................................................................................................................................... 100 

 

 

 



17 

 

 

 

1. Introdução 

O entendimento da geologia e da distribuição espacial dos teores de minério são 

fundamentais para um empreendimento mineiro. Para isso, entre outras técnicas de 

investigação, realizam-se sondagens de reconhecimento da geologia de subsuperfície de 

onde se coletam as amostras que são descritas por geólogos e analisadas quimicamente, a 

partir das quais se pode criar um modelo espacial de distribuição de teores, estimando 

pontos/regiões não amostrados.  

Existem diversas técnicas já consolidadas para modelar depósitos minerais e prever 

características de locais não amostrados, entre elas, destacam-se as técnicas de Krigagem 

Ordinária, Simulação de Indicadores e Simulação Sequencial Gaussiana. Existem novas 

técnicas sendo desenvolvidas para aprimorar as medições que, ao contrário das técnicas 

tradicionais, não dependem de semivariogramas. Portanto, a correlação espacial não é 

calculada pela estatística de dois pontos do semivariograma, mas sim pela estatística de 

pontos de alta ordem. Estes novos métodos são chamados de estatísticas de multiponto.  

De modo geral, as simulações estocásticas realizam procedimentos muito 

semelhantes, ou seja, simulam os pontos não conhecidos selecionando aleatoriamente o valor 

da função densidade de probabilidade condicional da região à qual o ponto pertence. A 

diferença entre os métodos é a maneira de calcular a função densidade de probabilidade ou 

simplesmente uma variação da implementação do algoritmo.  

As incertezas geológicas têm grande impacto no modelo geológico e, 

consequentemente, na classificação de recursos/reservas minerais. Considerando as 

implicações econômicas nas tomadas de decisão, a habilidade de mapear, quantificar e 

administrar incertezas é fundamental para a avaliação e classificação de recursos minerais. 

Tradicionalmente, a classificação de recursos minerais é baseada em critérios geométricos, 

tais como o número de amostras ou sondagens ao redor dos blocos estimados ou, 

alternativamente, alguma variância derivada da estimativa/simulação. Estes critérios são 

relativamente fáceis de entender e implementar, entretanto, eles não representam plenamente 

a variabilidade local das classes dentro do depósito, levando à diminuição da confiança das 

classes declaradas. 



18 

 

 

 

 

As simulações estocásticas, geralmente expressas com funções não lineares de 

variáveis espaciais, são utilizadas para quantificar as incertezas geradas nas previsões. 

Embora o desempenho de métodos tradicionais bem conhecidos e frequentemente utilizados 

- como a Simulação Sequencial Gaussiana (SGS) - tenha sido exaustivamente testado no 

passado, o desempenho dos novos métodos de simulações, tal como a simulação de 

multipontos (MPS), ainda estão sendo testados. O aspecto fundamental do MPS é encontrar 

o padrão (imagem de treinamento) que melhor corresponda ao depósito de onde as amostras 

foram coletadas, enquanto a simulação de alta-ordem procura por cumulantes espaciais de 

uma dada ordem que irá gerar funções densidade de probabilidade mais realistas, por utilizar 

mais informações dos depósitos minerais. O método de multipontos a ser utilizado neste 

trabalho é a Simulação Baseada em Wavelets (Wavesim), que será comparado ao método de 

SGS já consolidado. 

 

2. Objetivos  

O objetivo deste trabalho é aplicar, analisar e comparar os métodos de Simulação 

Baseada em Wavelets e Simulação Sequencial Gaussiana. Estas simulações têm a finalidade 

de quantificar a probabilidade dos possíveis cenários juntamente com a avaliação e 

classificação do minério/estéril. Além disso, é apresentado o algoritmo de cálculo do lucro 

potencial e classificação dos blocos em pilhas de processamento.  

O objetivo específico deste estudo é comparar os gráficos de distribuição acumulada 

de possíveis lucros, considerando cada realização dos resultados da Simulação Sequencial 

Gaussiana (SGS) e da Simulação Baseada em Wavelets (Wavesim) com os resultados 

esperados do depósito sintético de Cu. Como as realizações das simulações são 

equiprováveis e geram resultados diferentes, pode-se verificar a gama de possíveis lucros e 

qual a precisão destes resultados. Por fim há o propósito de comparar a quantidade de acertos 

das simulações e do resultado “real” na classificação do minério de cobre. 

 

 

 



19 

 

 

 

3. Materiais 

O desenvolvimento deste trabalho foi realizado no Laboratório de Informática 

Geológica (LIG) utilizando os programas Datamine®, SGeMS e R. A geração do banco de 

dados foi feita no programa Datamine® para a modelagem geométrica e no SGeMS para a 

geoestatística.  

A Simulação Sequencial Gaussiana foi feita com o programa SGeMS e a Simulação 

Baseada em Wavelets foi feita por um programa desenvolvido no laboratório de 

geoestatística COSMO, na McGill University, Canadá. A análise de lucro por bloco criada 

foi escrita na linguagem Python. Por último, as estatísticas descritivas, histogramas e 

gráficos boxplot apresentados foram feitos no programa R.  

 

4. Revisão Bibliográfica  

4.1  Geoestatística 

Rendu (1976) discorre que a análise estatística é a ferramenta lógica para a 

otimização de decisões na exploração mineral. Entretanto, geólogos normalmente 

apresentam dificuldades em quantificar suas opiniões e, deste modo, as decisões devem 

considerar a utilização de funções que representam de forma simplificada suas preferências. 

Landim (1997) afirma que a geoestatística é uma parte especial da estatística aplicada que 

trata de problemas referentes às variáveis regionalizadas, as quais apresentam uma aparente 

continuidade no espaço, sendo representadas por funções numéricas ordinárias que assumem 

um valor definido a cada ponto no espaço e matematicamente descrevem um fenômeno 

natural. 

Matheron (1963) explica que a variável regionalizada é, em sensu stricto, uma função 

real definindo um valor em cada ponto do espaço. Primeiramente, ela é localizada no espaço 

e suas variações ocorrem em uma região mineralizada (corpo do depósito) chamada campo 

geométrico da regionalização. Comumente é definida como suporte geométrico e este 

suporte é o volume da amostra, seu formato, tamanho e orientação. Se o suporte geométrico 

é alterado no depósito, uma nova variável regionalizada é obtida. Em segundo lugar, a 

variável pode mostrar a continuidade e variação espacial. Por último, a variável pode mostrar 



20 

 

 

 

 

diferentes tipos de anisotropia. Estas características das variáveis regionalizadas possibilitam 

obter uma ferramenta matemática simples, o semivariograma, que nada mais é que a curva 

que representa o grau de continuidade do fenômeno espacial em estudo. O semivariograma 

é uma função crescente da distância h e, quanto mais espaçadas as amostras estão uma das 

outras, maior a diferença entre os valores amostrados.  

Yamamoto (2001a) conceitua que a teoria das variáveis regionalizadas estuda e 

representa as propriedades estruturais destas variáveis, com o propósito de resolver os 

problemas de estimativa. A hipótese intrínseca é o conceito básico desta teoria, que implica 

que uma função descreve o comportamento espacial da variável regionalizada dentro do 

espaço e que esta é uma característica intrínseca da regionalização, ou seja, a geoestatística 

assume que a distribuição das diferenças entre dois pontos amostrais é, em média, a mesma 

para todo o domínio e que ela depende apenas da distância e orientação entre os pontos. Esta 

distribuição pode ser representada pela função semivariograma. Segundo Armstrong (1998), 

o semivariograma é utilizado para quantificar a correlação espacial entre as amostras. Uma 

vez ajustada uma função matemática (modelo teórico de semivariograma) ao 

semivariograma experimental, ela pode ser utilizada para prever valores não amostrados por 

krigagem ou simulação estocástica, métodos tradicionais de geoestatística utilizados 

atualmente na indústria da mineração.  

A função semivariograma pode ser escrita como: 

 ?(h) = 
1

2?
 ?  ??=1 [Z(xi+h) - Z(xi)]

2       (1) 

Sendo: ?(h) = variância espacial; Z(xi) = variável regionalizada no ponto xi; Z(xi+h) 

= variável regionalizada no ponto (xi+h) e n = número de pares de pontos separados por 

uma distância h. 

Para calcular o semivariograma em uma malha regular é necessário definir:  

(a) Número de direções: quantidade de direções em que o semivariograma 

experimental será calculado;  

(b) Tamanho do passo: distância de separação entre amostras que será 

considerada para o cálculo do semivariograma;  

(c) Número de passos: número de vezes que o passo será incrementado;  



21 

 

 

 

Além disso, se a malha não for regular, é necessário definir algumas tolerâncias: 

(d) Tolerância angular: ângulo de busca dentro do qual as amostras serão 

consideradas sendo de determinada direção para o cálculo do 

semivariograma;  

(e) Tolerância do passo: porcentagem da distância do passo. Esta tolerância faz 

com que os pares de pontos sejam considerados em classes de distâncias.  

Um exemplo de modelo de semivariograma com patamar típico é mostrado na Figura 

1, e suas principais propriedades são: 

(a) Amplitude: distância a partir da qual as amostras passam a ser espacialmente 

independentes. É a distância máxima onde as amostras apresentam correlação 

espacial – esta região é denominada campo estruturado. A partir desta 

distância alcança-se o campo aleatório onde as amostras são espacialmente 

independentes; 

(b) Patamar: valor máximo da variância espacial próximo ao qual os pontos do 

semivariograma experimental se estabilizam; 

(c) Efeito Pepita (ou variância aleatória): ocorre quando o valor da função 

semivariograma é diferente de zero a distâncias muito próximas à origem. 

(d) Variância espacial: diferença entre o patamar (C0+C) e o efeito pepita. É a 

região onde a variância é função da distância.  

 

Figura 1- Semivariograma típico e suas propriedades (Yamamoto, 2001a). 

 



22 

 

 

 

 

Conforme Armstrong (1998) é possível distinguir quatro padrões de comportamento 

próximo à origem do semivariograma (Figura 2): 

(a) Parabólico: indica que a variável regionalizada é contínua (Figura 2A); 

(b) Linear: indica que a variável regionalizada é contínua, porém menos do que 

aquela com comportamento parabólico (Figura 2B); 

(c) Efeito Pepita: apresenta a descontinuidade próxima à origem, causada por 

erros na amostragem e/ou variabilidade natural dos dados em distâncias 

pequenas (Figura 2C); 

(d) Efeito pepita puro: aleatoriedade total. As variáveis Z(x+h) e Z(x) não são 

correlacionáveis a quaisquer valores de h, não importando o quão perto eles 

estiverem. É a total falta de estruturação do semivariograma (Figura 2D).  

 

Figura 2 - Possíveis comportamentos do semivariograma próximo à origem (Yamamoto, 2001a). 

  

Segundo Yamamoto (2001a), existe anisotropia quando o semivariograma for 

calculado em diferentes direções do domínio e este mostrar variações distintas. Para 

identificá-la, deve-se calcular o semivariograma em quatro direções (0º, 45º, 90º, 135º) se os 

dados estiverem distribuídos em domínio bidimensional ou para cinco direções - 

adicionando a direção vertical - se for tridimensional. Depois de calculado, verifica-se a 

existência de uma direção que apresente maior amplitude ou maior patamar, ou ambos.  

Yamamoto (2001a) mostra que podem ser reconhecidos três tipos de anisotropia, 

conforme exemplificada na Figura 3. 

(a) Anisotropia geométrica: a amplitude varia conforme as direções, porém sob 

um patamar constante (Figura 3A); 

(b) Anisotropia zonal: o patamar varia com amplitude constante (Figura 3B);  

(c) Anisotropia mista: tanto a amplitude quanto patamar variam de acordo com 

a direção observada (Figura 3C). 



23 

 

 

 

 

Figura 3 - Anisotropia Geométrica (A), Zonal (B), Mista (C) (Yamamoto, 2001a). 

 

Deutsch (2002) salienta que os pontos calculados que formam o semivariograma 

experimental não são utilizados diretamente nas etapas de estimativa como krigagem e 

simulação estocástica e, para utilizá-los, deve-se se ajustar uma função matemática adequada 

ao semivariograma experimental, que é o modelo teórico de semivariograma. Este ajuste é 

realizado de forma interativa, a partir dos parâmetros interpretados no semivariograma 

experimental (número de estruturas, modelo, variância espacial, e amplitude). Se 

visualmente o modelo teórico não for satisfatório, novos parâmetros deverão ser fornecidos 

até o ajuste ser considerado satisfatório. Os principais modelos e suas respectivas formas 

estão representados na Figura 4. 

 

Figura 4 - Os principais modelos teóricos de semivariogramas (Yamamoto, 2001a). 

 

Yamamoto (2001a) mostra que os modelos esférico, exponencial e gaussiano são, 

respectivamente, determinados pelas funções:  

{
 ?(?) = ?0 +  ? [

3

2
(
?

?
) ?

1

2
(
?

?
)
3

] ;  para h &amp;lt; ?

?(?) = ?0 +  ?;  para h ?  a                               
    (2) 

?(?) =  ?0 +  ? [1 ? ??? (? (
?

?
))]     (3) 

(?) =  ?0 +  ? [1 ? ??? (? (
?

?
)
2

)]     (4) 



24 

 

 

 

 

Para aferir o ajuste do modelo teórico de semivariograma e definir a melhor 

vizinhança a ser utilizada, deve-se realizar a validação cruzada dos dados. Este procedimento 

compara os valores dos pontos amostrados com os valores estimados nas localizações 

amostradas. Esta comparação pode ser feita por meio de um gráfico de dispersão (Figura 5). 

Quanto menor a dispersão da nuvem de pontos, melhor terá sido a validação cruzada, ou 

seja, a reta de regressão ajustada à nuvem deve estar próxima à reta de 45º (Yamamoto, 

2001a). 

 

Figura 5 - Representação de uma validação cruzada e sua reta de regressão (Yamamoto, 2001a). 

 

Segundo Yamamoto (2001a), a escolha de amostras para estimar pontos não 

amostrados deve ser feita de tal modo que garanta uma boa disposição espacial, evitando a 

escolha de amostras concentradas preferencialmente em uma região. Para isso, deve-se 

estabelecer critérios de seleção, dividindo o espaço ao redor do ponto a ser estimado em 

quadrantes ou octantes (Figura 6). Na Figura 6A o critério é apenas a distância, portanto o 

será utilizado apenas os pontos mais próximos para a estimativa, não importando sua 

localização no espaço. Na Figura 6B são utilizados quadrantes para particionar o espaço 

onde as amostras serão procuradas e na Figura 6C o espaço de busca é dividido em octantes. 



25 

 

 

 

 

Figura 6 - Comparação entre os métodos de seleção de amostras. A Figura 6A mostra a captura de 

dados com os pontos mais próximos; A Figura 6B mostra a captura de dois dados para cada quadrante e a 

Figura 6C mostra um dado para cada octante (Yamamoto, 2001a). 

 

De acordo com Armstrong (1998), o procedimento para aplicação de técnicas 

geoestatísticas tradicionais pode ser descrito simplificadamente como: 

1. Coleta e validação dos dados. Caso o profissional responsável pela 

geoestatística não esteja envolvido desde o início do projeto, deve-se descobrir o tipo de 

amostra e análises utilizadas, assim como qualquer mudança de procedimento, a geologia da 

área - incluindo falhas, dobras, entre outras - a presença de amostragem preferencial e 

agrupamentos. Além disso, devem ser definidos os limites da área de estudo, o suporte das 

variáveis e se os dados são ou não estacionários. De acordo com Miesch (1975), um plano 

de amostragem eficiente depende do conhecimento da variabilidade do corpo rochoso. 

Normalmente esta variabilidade é descrita pelos geólogos, embora raramente sejam 

mensuradas de forma rigorosa; 

2. Estatística básica: cálculo da média, variância, correlações, histogramas e 

diagramas de dispersão. Devem-se procurar valores anômalos e/ou discrepantes e verificar 

a presença de dados não homogêneos - o que indicaria uma mistura de populações; 

3. Cálculo do semivariograma experimental; 

4. Ajuste do modelo teórico de semivariograma; 

5. Realização da krigagem ou simulação. 

Royle (1980) descreve os argumentos a favor da utilização da geoestatística como:  

(a) Reconhecer que a variância total é parcialmente aleatória e parcialmente 

espacial, além disso, ela guia à estimativa não enviesada e apresenta menores 

erros de estimativa; 



26 

 

 

 

 

(b) A geoestatística prova que alguns estimadores tradicionais - como os métodos 

dos polígonos - produzem estimativas enviesadas; 

(c) Operações de mina e validações podem ser mantidas sob o controle estatístico 

e seus desvios são possíveis de cálculo, previsíveis e controláveis devido à 

natureza não enviesada das estimativas geoestatísticas. 

 

4.2  Krigagem  

4.2.1 Krigagem Ordinária 

A krigagem ordinária, segundo Isaaks e Srivastava (1989) é um B.L.U.E. (Best 

Linear Unbiased Estimator), ou seja, melhor estimador linear não enviesado. Segundo 

Yamamoto (2001b), a krigagem é um método que permite estimar pontos não amostrados 

Z* (xo) associados a pontos, áreas ou volumes a partir de um conjunto de n dados {Z(xi), i = 

1, n} disponíveis. O estimador da krigagem ordinária é calculado por: 

Z*(x0) = ?  ??=1 ?i . z(xi)        (5) 

Sendo que os ponderadores (?i, i = 1,..., n ) são obtidos pela resolução do sistema de 

equações de krigagem, que nada mais é do que um sistema de equações lineares. A 

minimização da variância do erro sujeita à condição de não enviesamento resulta nas 

equações de krigagem ou sistema de krigagem ordinária (Yamamoto 2001b): 

{
 ?  ? ?? ?(?? – ??) ?  µ =  ?(?0 – ??), para i =  1, … , n

 ?  ?  ??  =  1                                                                             
   (6) 

As equações podem ser representadas como: 

[
 
 
 
 
?(?1 ? ?1) ?(?1 ? ?2) … ?(?1 ? ??) 1
?(?2 ? ?1) ?(?2 ? ?2) … ?(?2 ? ??) 1

? ? … ? ?
?(?? ? ?1) ?(?? ? ?2) … ?(?? ? ??) 1

1 1 … 1 0]
 
 
 
 

 .

[
 
 
 
 
?1
?2
?

??
µ ]

 
 
 
 

 =  

[
 
 
 
 
?(?0 ? ?1)
?(?0 ? ?2)

?
?(?0 ? ??)

1 ]
 
 
 
 

 (7) 

Onde ? (xn - xn) é a variância espacial entre amostras; ?(x0 - xn) é a variância espacial 

entre as amostras e o ponto a ser estimado e µ é o multiplicador de Lagrange, este 



27 

 

 

 

multiplicador é utilizado para minimizar a variância do estimador na restrição da soma dos 

ponderadores ser igual a 1 (Armstrong, 1998).  

 

4.2.2 Krigagem Simples 

Journel (1989) mostra a teoria básica da regressão linear para estimar valores 

desconhecidos de Z(x) pela combinação linear de n valores conhecidos Z(xi), i=1,…, n. Se 

estes valores desconhecidos corresponderem aos mesmos atributos de Z(x) esta operação é 

chamada de krigagem.  

Chilès e Delfiner (1999) mostram que para estimar z(xi) de N observações 

(z(x1),...,z(xN)) usando o estimador  

Z*(x) = ? ???=1 i . z(xi) + ?0        (8) 

a constante ?0 e os pesos ?i são selecionados para minimizar no modelo a esperança 

do erro quadrático E[Z*(x)-Z(x0)]
2. Primeiramente, este erro pode ser descrito como: 

E[Z*(x)-Z(x0)]
2 = Var[Z*(x)-Z(x0)] + [E[Z*(x)-Z(x0)]]

2
   (9) 

Uma vez que a variância é invariante, apenas o termo do lado direito envolve ?0 e a 

fim de minimizar o erro é necessário escolher o ?0 que cancele com E[Z*(x)-Z(x0)], ou seja:

  

?0 = m(x0) - ? ?
?
?=1 i . m(xi)       (10) 

 e o estimador da krigagem simples é: 

Z*(x) = m(x0) + ? ?
?
?=0 i . (Z(xi) – m(xi))      (11) 

Onde m(x0) é o valor esperado, m(xi) é a média dos pontos utilizados e ?i é obtido 

por um sistema linear de N equações. E a notação da matriz do sistema de krigagem simples 

é K ? = k e pode ser observada em forma de matriz em Yamamoto e Landim (2013): 

[

?(?1 ? ?1) ?(?1 ? ?2) … ?(?1 ? ??)
?(?2 ? ?1) ?(?2 ? ?2) … ?(?2 ? ??)

? ? … ?
?(?? ? ?1) ?(?? ? ?2) … ?(?? ? ??)

] . [

?1
?2
?

??

]  =  [

?(?0 ? ?1)
?(?0 ? ?2)

?
?(?0 ? ??)

]  (12) 



28 

 

 

 

 

Onde ?(?? ? ??) é a covariância dos n pontos amostrados, ?i é o peso da krigagem 

a ser calculado e ?(?0 ? ??) é a covariância do ponto a ser estimado com o n-ésimo ponto 

amostrado, com i=1,…, n. 

A variância do estimador (???
2 ) é: 

???
2 = E[Z*(x)-Z(x0)]

2 = ?(?0 ? ?0) - ? ?
?
?=1 i . ?(?0 ? ??)   (13) 

 

4.3  Simulação Geoestatística 

Deutsch e Journel (1998) explicam que a simulação corrige o efeito de suavização 

da krigagem e o mapa suavizado gerado pela krigagem é mais apropriado para mostrar 

tendências gerais, pois preserva as distribuições locais. Por outro lado, mapas de simulação 

condicional são mais apropriados para estudos que são sensíveis aos padrões de 

variabilidade, como ocorre nas simulações de fluxo. A simulação estocástica encontra suas 

principais aplicações em mineração, hidrogeologia e petróleo. O conjunto de mapas de 

simulações condicionais também fornece subsídio para a elaboração do modelo de incerteza 

sobre a distribuição espacial das variáveis. Em resumo, a simulação estocástica é um 

processo que constrói alternativas equiprováveis e modelos de distribuição espacial com alta 

resolução e é dita condicional se o resultado das realizações honrarem os valores dos dados 

e sua localização. 

Goovaerts (1997) mostra que mapas interpolados suavizados não podem ser 

utilizados para aplicações sensíveis à presença de valores extremos e seus padrões de 

continuidade. Ao invés de mapas de estimativas, a simulação estocástica gera mapas ou 

realizações de valores {z(l)(x), x ? A }, com A sendo a área e l sendo a l-ésima realização, 

que reproduzem a estatística da maioria dos problemas. O requisito típico para a simulação 

condicional é garantir que valores amostrados sejam reproduzidos em locais simulados 

(z(l)(x) = z(xi) ? x = xi, i=1,...,N). Os histogramas dos valores simulados devem reproduzir 

rigorosamente o histograma dos dados desagrupados.  

Sinclair e Blackwell (2004) explicam que a simulação no contexto da mineração 

significa imitação de condições. No que se refere especificamente à estimativa, geralmente 

envolve a tentativa de criar uma matriz de valores que apresentam as mesmas características 



29 

 

 

 

estatísticas e espaciais. Entretanto, os valores são gerados em uma escala muito mais local 

do que aquela para a qual a informação verdadeira está disponível. A simulação não é uma 

estimativa, é um conjunto bi ou tridimensional de valores com a mesma natureza estatística 

que os dados originais.  

Os mesmos autores discorrem que é importante perceber por que as simulações são 

necessárias quando existem dados e estimativas. As flutuações locais verdadeiras na 

distribuição espacial são suavizadas pela maioria dos métodos de estimativa, de modo que 

estas estimativas não refletem as variações locais. Matrizes de valores simulados são 

construídas para variar na mesma escala que as variáveis de interesse na amostra. Na 

estimativa, a preocupação é minimizar a variância do erro, enquanto em simulações, a 

preocupação é reproduzir a dispersão da variância dos dados reais. 

Deutsch e Journel (1998) conceituam a simulação estocástica como o processo para 

obter alternativas equiprováveis através de realizações conjuntas de variáveis aleatórias dos 

modelos de funções aleatórias. Estas realizações, normalmente calculadas em malhas 

regulares, representam imagens possíveis da distribuição espacial. Cada realização, também 

chamada de imagem estocástica, reflete as propriedades que foram impostas no modelo de 

função aleatória, portanto, quanto mais propriedades dos dados amostrados forem inferidas 

e incorporadas às propriedades do modelo de função aleatória ou aos algoritmos de 

simulação, melhor será o modelo da função aleatória e melhores serão as realizações 

simuladas.  

Segundo Caers (2011), é necessário assumir que a distribuição condicional dos locais 

não amostrados pode ser representada por uma função de distribuição gaussiana. Isto 

significa que a distribuição marginal (histograma) da variável a ser simulada é gaussiana, 

embora raramente aconteça, uma vez que na natureza a maioria dos dados obtidos na 

amostragem não são gaussianos. Deste modo, deve-se transformá-los para uma distribuição 

gaussiana. 

De acordo com Deutsch e Journel (1998), os erros humanos e de medidas podem ser 

considerados como eventos ou processos independentes e nas ciências da terra os processos 

geológicos/biológicos que geraram os fenômenos observados raramente são independentes 

um do outro. Além disso, se o fenômeno espacial for gerado pela soma de fontes 

independentes com distribuição espacial similar, então sua distribuição espacial pode ser 



30 

 

 

 

 

modelada como um modelo de função aleatória multigaussiana. Esta consideração heurística 

é o suficiente para fazer o modelo gaussiano ser a melhor escolha para modelagem de 

variáveis contínuas.  

 O custo da simplicidade matemática da técnica gaussiana é de caráter de máxima 

entropia, o que implica em desordem espacial máxima, além daquele imposto pela 

correlação do semivariograma (Journel e Deutsch, 1993). Uma consequência é que não há o 

potencial para simular caminhos conectados de valores extremos, o que afeta os resultados 

de simulações de fluxo, por exemplo, outra é que a Simulação Sequencial Gaussiana 

apresentará menos estruturas espaciais em seus resultados do que um reservatório real 

(Deutsch, 2002). 

 

4.3.1 Justificativa da utilização dos métodos de simulação 

A geoestatística tenta solucionar a suavização da krigagem por meio de simulação 

estocástica. Infelizmente, a simulação é apenas um paliativo, uma vez que se ganha em 

precisão global, porém se perde em precisão local. Realizações estocásticas não estão livres 

de erros e interpretações da realidade. Na falta de uma solução perfeita, a escolha entre 

krigagem e simulação deve ser decidida com base no que é mais relevante para cada 

aplicação específica: o mínimo erro de estimativa local (precisão local) obtida pela krigagem 

ou a continuidade espacial correta (precisão global) obtida pela simulação estocástica, que, 

por sua vez, assume que cada realização é equiprovável e o histograma da amostragem é 

bom o suficiente para representar o histograma real, ou seja, que a amostragem fornece boa 

aproximação do real (Olea, 2003). 

De acordo com Journel (1980), a produção de minérios em depósitos marginais, ou 

seja, teores médios baixos e próximos ao teor de corte, exigem melhores taxas de produção, 

técnicas de processamento mais sofisticadas e investimentos maiores. Por sua vez, os 

investidores exigem maior precisão no cálculo das massas totais e dos teores e, 

simultaneamente, a redução dos erros de estimativa e, consequentemente, a melhoria do 

processamento de minério. As técnicas de simulações condicionais criam modelos 

numéricos de qualquer depósito. Assim como se pode usar um modelo de avião em um túnel 



31 

 

 

 

de vento para simular o desempenho do voo, modelos de depósitos podem ser utilizados para 

testar a eficiência das pesquisas ou de processos de mineração. 

Em um empreendimento mineiro, os fatores de risco são diversos, entre eles podem 

ser citadas as condições políticas e sociais, instabilidade do mercado e a quantidade e a 

qualidade do minério. O conhecimento da análise de sensibilidade dos parâmetros de 

incerteza é de extrema importância antes de decidir a continuação de um projeto. A análise 

de sensibilidade considerando apenas o fator econômico normalmente é feita, porém não é 

o suficiente, pois mesmo que as estimativas de produção sejam boas, elas podem se desviar 

da realidade. Este desvio pode ser caracterizado através do conceito de variância da 

estimativa fornecido pela geoestatística. Entretanto, a variância da estimativa é meramente 

um número e não expressa bem a consequência do possível desvio no projeto. Por isso, a 

simulação fornece um resultado melhor (Journel, 1980), uma vez que considera todas as 

possibilidades equiprováveis para estudar os possíveis cenários e calcular a incerteza dos 

resultados esperados. 

Deutsch e Journel (1998) explicam que na maioria dos algoritmos de interpolação, 

incluindo a krigagem, o objetivo é fornecer a melhor estimativa local de uma variável ou seu 

padrão de comportamento sem considerar especificamente a estatística espacial das 

estimativas. Na simulação, a reprodução de características globais e estatísticas (histograma, 

covariância) tem precedência sobre a precisão local – fornecendo representações globais 

alternativas cuja reprodução de padrões de continuidade espacial prevalece. Por outro lado, 

a krigagem fornece um conjunto de representações locais, onde prevalece a precisão local.  

A técnica probabilística de simulação condicional fornece diversas realizações 

possíveis de um depósito, cada uma representa o depósito simulado de maneira similar ao 

depósito real e apresenta as mesmas características geológicas e geoestatísticas. Algumas 

das aplicações das simulações condicionais são: estudo da quantidade de amostras para um 

determinado objetivo e avaliação preliminar de recursos e reservas minerais potenciais em 

corpos de minério com poucas amostras; estudo de sensibilidade de estimativa de recurso-

reserva; estudo detalhado da sensibilidade de recuperação de recursos geológicos in-situ, 

como a unidade mínima de lavra, quantidade de informações futuras, agrupamento dos 

pacotes ricos e pobres do minério e diluição do minério; seleção do melhor método de lavra 

e equipamento – devido ao melhor conhecimento da variância espacial do depósito, assim 



32 

 

 

 

 

como os períodos de maior e menor produção; determinação dos teores de corte e taxas de 

produção e suas variações no tempo (Journel, 1980). Além disso, pode-se fazer o estudo da 

continuidade de teores, planejamento de lavra, otimização de processamento e análise de 

risco financeiro (Sinclair e Blackwell, 2004). 

 

4.3.2 Simulação Condicional 

Sinclair e Blackwell (2004) explicam que as matrizes de dados simulados que 

mantêm a mesma distribuição de densidade (histograma) e caráter de autocorrelação que 

estão ligados espacialmente para reproduzir os dados existentes são chamadas de simulações 

condicionais. Uma simulação condicional e a realidade podem ser consideradas realizações 

da mesma função aleatória. Estas simulações são normalmente utilizadas para aplicações 

que envolvem teores de minério/estéril. 

 Sinclair e Blackwell (2004) discorrem que as simulações condicionais não podem 

ser vistas como um processo de estimativa confiável ou aceitável, uma vez que simulação e 

estimativa têm finalidades diferentes. A simulação permite variações locais de valores da 

variável a ser examinada, particularmente em relação ao impacto que estas variações locais 

têm sobre os planos de amostragem, procedimentos de estimativa, planejamento de lavra e 

questões financeiras. A estimativa, no entanto, é feita para identificar o minério e o estéril 

numa escala em que a separação física pode ser alcançada. 

Segundo Chilès e Delfiner (1999), uma simulação não condicional da função 

aleatória {Z(x): x ? Rn} é simplesmente uma realização de Z(x) selecionada aleatoriamente 

de um conjunto de todas as possíveis realizações. Sua construção exige conhecimento das 

distribuições espaciais da função aleatória representada por Z(x). A função aleatória S(x) 

tem número finito de realizações, quando estas realizações assumem os mesmos valores que 

os observados nos pontos amostrados, estes podem ser considerados para representar melhor 

a variável regionalizada z(x), esta é chamada simulação condicional. Equivalentemente, é 

uma concretização da função aleatória com distribuição espacial condicional. O fluxograma 

de toda análise geoestatística, incluindo da simulação condicional, é apresentado na Figura 

7. 



33 

 

 

 

Simulações condicionais são qualitativamente úteis para obter figuras reais da 

variabilidade espacial. Quantitativamente são a ferramenta de escolha para avaliar o impacto 

da incerteza espacial sobre os resultados de procedimentos complexos, tais como a 

modelagem de um sistema dinâmico ou a otimização econômica do desenvolvimento de uma 

fonte natural. O objetivo da simulação condicional, no entanto, não é reproduzir os 

mecanismos genéticos que geraram o fenômeno observado, mas apenas imitar suas variações 

espaciais da forma mais realista possível. Os resultados da simulação devem sempre ser 

vistos com olhar crítico e verificados contra o fundo da aplicação - o que é muito importante 

quando se estudam problemas como a conectividade, fluxo de fluido ou transporte - que 

obriga a considerar, de forma explícita ou não, um modelo especificado além de seus 

momentos de segunda ordem (Chilès e Delfiner, 1999). 

Chilès e Delfiner (1999) explicam que a resposta para o número ideal de realizações 

a serem feitas depende do objetivo e da estrutura do fenômeno. Quando um depósito 

estacionário for modelado em área muito maior que a busca, apenas uma simulação pode ser 

o suficiente para representar a variedade de possíveis situações locais. Entretanto, se a área 

de estudo for não estacionária - como um reservatório de petróleo, por exemplo - uma única 

realização fornece apenas uma resposta em termos de fluxo e produção. Desta forma, é 

necessária a simulação de diversas realizações para capturar a gama de possíveis resultados. 

Se forem feitas 100 realizações de cada simulação da função aleatória, ao invés de uma 

única, para estimar um momento de segunda ordem numérica, cada simulação da função 

aleatória levaria ao mesmo resultado, porém com flutuações. É compreensível que as 

conclusões com relação à conectividade ou ao fluxo de fluido sejam diferentes, 

especialmente se houver interesse em situações extremas. Para avaliar o alcance dos 

resultados possíveis é necessário construir várias simulações. Geralmente 100 simulações 

são consideradas um bom número para esta avaliação, no entanto, isto depende muito dos 

interesses nos parâmetros de distribuição. Se os dados estiverem enviesados e/ou se houver 

fundamental atenção aos valores extremos, um maior número de simulações se fará 

necessário, ressaltando que as previsões de valores extremos geralmente não possuem 

robustez na estimativa dos valores e que as simulações condicionais não são igualmente 

prováveis, algumas são extremas e outros estão próximas à média. O número de realizações 

necessárias depende de quantas realizações são necessárias para modelar a incerteza 

(Deutsch e Journel, 1998).





34 

 

 

 

 

Figura 7 - Fluxograma dos processos da análise geoestatística de variáveis contínuas (Modificado de Chilès e Delfiner, 1999). 





35 

 

 

 

4.3.3  Simulação Sequencial Gaussiana 

Segundo Deutsch (2002), as propriedades matemáticas que fazem a Simulação 

Sequencial Gaussiana funcionar não estão limitadas à sua distribuição gaussiana. A 

reprodução da covariância da krigagem se mantém independente da distribuição dos dados. 

A correção da variância é por adição de um resíduo aleatório que funciona 

independentemente da forma da distribuição residual. Há um bom motivo para que esta 

distribuição seja utilizada: o uso de qualquer outra distribuição não leva a uma distribuição 

global correta dos valores simulados. A média, a variância e o semivariograma dos dados 

podem estar corretos, porém, a forma da distribuição não, pois, no espaço gaussiano, todas 

as distribuições são gaussianas. Além disso, o teorema do limite central diz que a adição 

sequencial de resíduos aleatórios para obter valores simulados conduz à distribuição 

gaussiana. Para tanto, utiliza-se a média zero e a variância igual à variância da krigagem. 

De acordo com Chilès e Delfiner (1999), deve-se considerar um vetor-valor da 

variável aleatória Z= (Z1,Z2,...,Zn)’ para cada realização de um subvetor (Z1,Z2,...,ZM)’, que é 

conhecido e igual a (z1,z2,...,zM). A distribuição do vetor condicionado a Z em Zi = zi, i=1, 

2,..., M, pode ser fatorizado na forma: 

P{zM+1 ? ZM+1 &amp;lt;zM+1 + dzM+1,...,zN ? ZN &amp;lt;zN + dzN | z1,...,zM} 

= P {zM+1 ? ZM+1&amp;lt;zM+1 + dzM+1 | z1,...,zM} 

. P {zM+2 ? ZM+2&amp;lt;zM+2 + dzM+2 | z1,...,zM, zM+1} 

... 

P {zN ? ZN&amp;lt;zN + dzN | z1,...,zM, zM+1,..., zN-1}    (14) 

 

Segundo Chilès e Delfiner (1999), pode-se simular o vetor Z sequencialmente por 

uma seleção aleatória de Zi a partir de uma distribuição condicional P {Zi &amp;lt;zi,...,zi-1} para i 

= M + 1,...,N e incluir o resultado zi em um banco de dados condicional para a próxima 

etapa. Este processo é absolutamente geral e pode ser usado em particular com Zi de forma 

que Zi = Z(xi), onde Z(x) é uma função aleatória e xi são pontos amostrados (i=1,2,...,M) e 

os pontos onde se deseja simular a função aleatória (i = M + 1,...,N), o que possibilita a 



36 

 

 

 

 

construção de uma simulação não condicional (M = 0) e uma simulação condicional (M &gt; 

0), produzindo simulações que correspondem não apenas à covariância, mas também à 

distribuição espacial. 

A forma de calcular a probabilidade condicional apresentada em (14) é desconhecida, 

exceto no caso ideal de um vetor gaussiano aleatório, onde o método é normalmente 

empregado. Para uma função aleatória gaussiana com média conhecida, a distribuição 

condicional de Zi é Gaussiana com média Z*i e variância ?2ki, onde Z*i é o estimador krigagem 

simples de Zi de {Zj : j &amp;lt;i}, e ?2ki associado à variância de krigagem (Chilès e Delfiner, 1999). 

Chilès e Delfiner (1999) explicam que para realizar uma simulação estocástica 

condicional, deve-se considerar uma função aleatória Z(x) conhecida com N amostras xi, i 

= 1, 2,..., N. A simulação não condicional S(x) é independente de Z(x), porém tem a mesma 

covariância de Z(x). Para condicionar esta simulação é necessário fixar os pontos 

amostrados, esta operação passa o S(x) para T(x).  

Considere Z*(x) como o estimador da krigagem de Z(x) no ponto x, baseado nos 

dados Z(x0): 

         Z(x)        =          Z*(x)          +    [Z(x) - Z*(x)]   (15) 

Valor verdadeiro = estimador da krigagem + erro da krigagem 

O erro da krigagem não é conhecido uma vez que Z(x) é desconhecido. Então, 

considere a igualdade para S(x), onde S*(x) é o estimador da krigagem obtido como se a 

simulação fosse conhecida apenas no ponto x0 (Chilès e Delfiner, 1999). 

         S(x) = S*(x) + [S(x) - S*(x)]      (16) 

O valor verdadeiro de S(x) é conhecido e, consequentemente, seu erro S(x) – S*(x). 

Com a decomposição de Z(x), o erro desconhecido - pela simulação deste erro - gera a 

simulação condicional T(x) definida por Chilès e Delfiner (1999): 

 T(x)   = Z*(x) + [S(x) - S*(x)]      (17) 

Simulação condicional = estimador da krigagem + erro da krigagem simulado 



37 

 

 

 

De acordo com Deutsch e Journel (1998), a função densidade acumulada condicional 

da variável regionalizada Z(x) que modela a incerteza de valores não amostrados é 

caracterizada pela esperança condicional (média) e sua variância, considerando que a 

distribuição dos N dados z(x) é gaussiana normal: 

A esperança condicional (média) é o estimador da krigagem simples: 

E{Z(x0)|z(xi) = zi, i = 1,...,N} ~ [z*(x0)] 

[z*(x0)] = m(x0) + ?  ??=1  ?i[zi – m(xi)]     (18) 

Onde m(x) é E[Z(x)] o valor esperado da variável aleatória Z(x) e os n pesos são 

obtidos pela resolução do sistema de krigagem simples. 

A variância condicional é a variância da krigagem simples: 

Var{Z(x0)| z(xi) = zi, i=1,...,N)} = C(x0,xi) - ?  ??=1  ?i C(x0,xi)  (19) 

Chilès e Delfiner (1999) conceituam que como a krigagem é um interpolador exato, 

no ponto onde se tem Z*(x0) = Z(x0) e S*(x0) = S(x0), então T*(x0) = T(x0). A prova que 

T(x) preserva a covariância regionalizada de Z(x) e S(x), no caso da krigagem simples, é 

obvia, uma vez que Z(x) – Z*(x) não é correlacionado com Z*(x’) para todo x’, como:  

Cov(Z(x),Z(x’)) = Cov(Z*(x), Z*(x’)) + Cov(Z(x) - Z*(x), Z*(x’) - Z*(x’))     (20) 

Desde que S e Z sejam independentes, a covariância de T é a soma da covariância de 

Z* e de S - S*, que se iguala da Z - Z*. Consequentemente: 

Cov(T(x),T(x’)) = Cov(Z(x),Z(x’))         (21) 

Chilès e Delfiner (1999) mostram que a média de uma grande quantidade de 

simulações condicionais independentes em um determinado ponto tende ao valor estimado 

pela krigagem e sua variância tende a variância de krigagem. Em termos figurativos, a 

simulação condicional vibra entre os dados em um envoltório definido pelo desvio padrão 

da krigagem. É esperado que a simulação condicional se comportasse como o campo real, 

mas sem estimá-lo. Ainda em termos figurativos, a simulação T(x), como um estimador de 

Z(x), irá se comportar muito mal, com sua variância sendo o dobro da variância de krigagem: 



38 

 

 

 

 

T(x) – Z(x) =  [ Z*(x) – Z(x)] + [S(x) – S*(x)]    (22) 

E seu momento de segunda ordem: 

E[T(x) – Z(x)]2 = 2 ?2k(x)       (23) 

Segundo Chilès e Delfiner (1999) a prova que a substituição dos erros preserva a 

covariância é válida para a krigagem (ou cokrigagem) com vizinhança global. Se o número 

de dados é muito alto, é necessário utilizar a vizinhança local. Um projeto cuidadoso do 

algoritmo de vizinhança de busca é necessário para evitar a introdução de descontinuidades 

falsas devido a mudanças de vizinhança, uma vez que estas descontinuidades não são 

facilmente diferenciadas pela instabilidade normal das simulações. 

 

4.3.4 Algoritmo da Simulação Sequencial Gaussiana 

Remy, Boucher e Wu (2011) descrevem que o algoritmo de Simulação Sequencial 

Gaussiana utiliza o formalismo da simulação sequencial para simular uma função gaussiana 

e, para isto, é requisito fornecer o modelo do semivariograma dos dados transformados em 

uma Gaussiana Normal (N[0,1]), e não dos dados originais, isto garante a reprodução do 

modelo com flutuações ergóticas nos resultados da simulação. Como os resultados estão no 

espaço gaussiano, é necessário transformá-los de volta ao espaço original após a simulação. 

Ainda segundo Remy, Boucher e Wu (2011), para determinar a função densidade acumulada 

condicional em cada nó da malha, o algoritmo pode ser aplicado com a utilização da 

krigagem simples, krigagem originária, krigagem com média local ou krigagem de 

tendência. Porém, a teoria garante a reprodução do semivariograma apenas quando a 

krigagem simples é utilizada. 

De acordo com Deutsch e Journel (1998), cada variável é simulada sequencialmente 

de acordo com sua função densidade acumulada condicional, caracterizada através de um 

sistema de krigagem simples. Os dados condicionantes são todos os dados originais e os 

valores previamente simulados encontrados dentro da vizinhança do local a ser simulado. A 

simulação condicional de uma variável contínua Z(x) apresenta as seguintes etapas: 

 



39 

 

 

 

1. Determinar a função densidade acumulada da variável Z(x) que represente toda a 

área de estudo e não apenas a amostragem “x” disponível; 

2. Usar a função densidade acumulada da variável Z(x) para realizar a transformação 

normal dos dados “x” para “y” com a função densidade acumulada normal; 

3. Checar a normalidade bivariada dos dados “y” normalizados. Se o modelo 

multigaussino não puder ser retido, então considerar modelos alternativos como a 

mistura gaussiana de populações ou um algoritmo baseado em indicadores para a 

simulação estocástica; 

4. Se o modelo da função aleatória multigaussiana puder ser adotado para variável “y”: 

4.1 - Definir um caminho aleatório que visite cada nó da malha apenas uma 

vez. Em cada nó x, reter um número específico de dados condicionantes na 

vizinhança, incluindo os dados e os valores previamente simulados; 

4.2 – Usar a krigagem simples com o modelo de semivariograma normalizado 

para determinar os parâmetros (média e variância) da função densidade 

acumulada da variável aleatória “Y(x)” no local “x”; 

4.3 – Obter um valor simulado y(l)(x) da função densidade acumulada 

condicional; 

4.4 – Adicionar o valor simulado y(l)(x) ao banco de dados; 

4.5 – Ir ao próximo nó e repetir o procedimento, até que todos os nós tenham 

sido simulados. 

5. Transformação inversa dos valores gaussianos simulados {y(l)(x), x ? A} em valores 

simulados da variável original { y(l)(x) = ?-1 (y(l)(x)), x ? A }. 

Se múltiplas realizações forem desejadas {y(l)(x), x ? A}, l=1,...,L, as etapas descritas 

devem ser repetidas L vezes com um caminho aleatório diferente para cada realização. A 

sequência da configuração dos dados é diferente, assim, diferentes sistemas de krigagem 

simples devem ser criados e resolvidos (Deutsch e Journel, 1998). 

Em teoria, os nós a serem simulados podem ser visitados em qualquer sequência, 

contanto que todos os dados originais e os dados previamente simulados sejam utilizados 



40 

 

 

 

 

para criar as funções densidade acumuladas condicionais. Entretanto, como apenas os dados 

dentro da vizinhança de busca são utilizados, continuidades artificiais podem ser geradas ao 

longo do caminho aleatório de visita dos nós se a vizinhança não for escolhida 

adequadamente (Goovaerts, 1997). 

 

4.3.5 Transformação gaussiana normal 

Deutsch e Journel (1998) explicam que a primeira condição necessária para a 

estacionariedade da função aleatória Y(x) ser multigaussiana é que sua função densidade 

acumulada seja normal. 

Prob {Y(x) ? y} = G(y), ?y       (24) 

Onde G(?) é a função densidade acumulada gaussiana normal e Y(x) é a variável a 

ser normalizada. A variável é normal se a média for zero e a variância um. Infelizmente, a 

maioria dos dados nas ciências da terra não apresenta simetria gaussiana nos histogramas. 

Porém, isto não é um problema, uma vez que a transformação não linear pode transformar 

qualquer função densidade acumulada contínua em outra função densidade acumulada, 

desde que Z(x) seja multigaussiana. 

Sejam Z e Y duas variáveis aleatórias com as funções densidades acumuladas Fz(z) 

e Fy(y) respectivamente. A transformação Y = ?(Z) identifica a probabilidade acumulada 

correspondente aos p quantis de Z e Y. 

FY (yp) = FZ (zp) = p, ?p ? [0,1]      (25) 

consequentemente: 

y = FY-1 (FZ(z))        (26) 

com FY-1(?) sendo o inverso da função densidade acumulada ou função quantil da 

variável aleatória Y: 

yp = FY-1(p), ?p ? [0,1]  (27) 



41 

 

 

 

Se Y apresenta uma distribuição normal padrão com a função densidade acumulada 

FY(y) = G (y), a transformação G-1 (FZ(?)) é a transformação normal score.  

Na prática, os n dados amostrais z são ordenados de forma crescente. A frequência 

acumulada correspondente ao k-ésimo maior valor de z é FZ(z(k)) = k/n. Então a 

transformação normal de z(k) é o k/n quantil de uma função densidade acumulada normal 

padrão y(k) = G-1 (
?

?
). 

 

4.4  Método de Simulação de Multipontos (MPS) 

Caers e Zhang (2002) explicam que a geoestatística tradicional baseada em 

semivariogramas é inadequada, uma vez que o semivariograma é muito limitado para 

capturar a heterogeneidade geológica dos dados, já a técnica intitulada de multipontos não 

utiliza o semivariograma, mas sim estruturas denominadas imagens de treinamento. As 

imagens de treinamento podem representar estruturas geológicas complexas e curvilíneas - 

como regiões com muitas dobras e falhas ou simplesmente canais meandrantes - melhor que 

os métodos geoestatísticos baseados em semivariogramas, que não capturam 

matematicamente esta complexidade. A estatística de multipontos empresta os padrões das 

imagens de treinamento e os coloca nos resultados estocásticos simulados. No entanto, este 

método utiliza dos mesmos princípios da geoestatística tradicional baseada em 

semivariogramas, portanto baseia-se na estacionariedade e ergodicidade que muitas vezes 

são esquecidos. Estes princípios estabelecem que a imagem de treinamento utilizada nas 

técnicas de multipontos não pode ser escolhida arbitrariamente e, ainda, que nem todas as 

informações geram imagens adequadas.  

Ainda de acordo com Caers e Zhang (2002), os algoritmos para qualquer tipo de 

estimativa ou simulação geoestatística exigem que a estacionariedade de segunda ordem 

(momento estatístico de segunda ordem ou variância) seja assumida, uma vez que se baseiam 

no fato de que a mesma função pode ser aplicada e repetida em cada célula da malha. A 

população amostrada é muitas vezes referida como a zona de estacionaridade, ou seja, uma 

região que permite a partilha de informação em conjunto. Tal decisão não é relevante apenas 

para as estatísticas, como histogramas, mas também para as estatísticas de mais alta-ordem, 



42 

 

 

 

 

como assimetria (momento de terceira ordem), curtose (momento de quarta-ordem) e assim 

por diante. De acordo com Magalhães (2011), o momento de variáveis aleatórias é definido 

por E[Xk], para k = 1, 2, …, K, sendo a ordem do momento da variável X, desde que essa 

quantidade exista. Se E[X] = ? &amp;lt;?, definimos o momento central de ordem k por E[(X – 

?)k], sempre que essa quantidade exista. De modo similar, o momento absoluto de ordem k 

da variável aleatória X é definido por E[|X|k].  

Caers e Zhang (2002) explicam que na geoestatística tradicional, a prática recomenda 

não utilizar as informações geradas por passos além do campo geométrico, uma vez que não 

são confiáveis. Neste caso, considerações semelhantes terão que ser feitas para o uso da 

estatística de multipontos. Algoritmos de simulação geoestatística reproduzem as estatísticas 

de entrada, como o semivariograma e o histograma - sob certas "oscilações de ergodicidade". 

A simulação de um domínio densamente amostrado resultará em estatísticas de uma 

realização que correspondem exatamente às estatísticas do modelo. Entretanto, quando a 

simulação é feita em um domínio finito, algumas estatísticas têm variações menores do que 

outras. Sendo assim, a ergodicidade desempenha um papel importante tanto na estimativa 

dos parâmetros do modelo quanto em sua simulação (Caers e Zhang, 2002).  

Segundo Journel e Zhang (2006), a estatística de multipontos vai muito além da 

estatística convencional (histograma) e da estatística de dois pontos (covariograma). Ela 

pode simular realizações com alta entropia – regiões de geologia complexa e alta 

heterogeneidade geológica e de teores – enquanto oferece flexibilidade em considerar 

diversas imagens de treinamento alternativas com vários níveis de estruturas geológicas mais 

simples, ou seja, de baixa entropia.  

Entre os principais algoritmos de multipontos estão: ENESIM (Extended Normal 

Equation Simulation), SNESIM (Single Normal Equation Simulation), FILTERSIM (Filter-

based Simulation), SIMPAT (Simulation with Patterns), DISTPAT (Distance-based Pattern 

Simulator), WAVESIM (Wavelet-based Simulation) e HOSIM (High-Order Simulation). 

Atualmente os mais utilizados são os SNESIM e o FILTERSIM, disponíveis no programa 

SGeMS.  

Para Chatterjee, Dimitrakopoulos e Mustapha (2012), a simulação de variáveis 

contínuas ou categóricas espacialmente correlacionadas - como unidades geológicas e teores 

de elementos químicos de depósitos minerais, ou fácies sedimentares e atributos pertinentes 



43 

 

 

 

de reservatórios de petróleo e aquíferos - é uma tarefa desafiadora. As bem conhecidas 

técnicas baseadas em semivariogramas (estatística de dois pontos) são limitadas em modelar 

adequadamente espaços complexos e, para resolver esta limitação, uma técnica de estatística 

de alta-ordem tem sido desenvolvida na forma de modelos de múltiplos pontos.  

O principal objetivo dos métodos de simulação de multipontos é achar o melhor 

padrão de um banco de dados de padrões considerando as amostras condicionantes. O 

primeiro algoritmo de multipontos foi apresentado por Guardiano e Srivastava (1993) e a 

técnica se chama ENESIM. Durante a simulação, os dados condicionantes são comparados 

com os padrões da imagem de treinamento. O algoritmo SNESIM apresentado por Strebelle 

(2002) gera uma árvore de busca dos padrões da imagem de treinamento e modela a função 

densidade acumulada condicional dos padrões. Este algoritmo procura por uma réplica exata 

dos dados condicionantes e, como nem sempre a réplica exata pode ser encontrada no banco 

de padrões, alguns dados condicionantes não são utilizados.  

O algoritmo SIMPAT - proposto por Arpat e Caers (2005) – considera a imagem de 

treinamento como uma coleção de padrões, onde um padrão pode ser selecionado para 

corresponder localmente o mais próximo possível dos dados condicionantes. A principal 

vantagem do algoritmo é que nenhum dado condicionante é desconsiderado. Por outro lado, 

a maior limitação deste algoritmo é que o banco de padrões inteiro é analisado para encontrar 

o melhor padrão correspondente a um determinado nó, por isso o tempo computacional é 

extremamente alto.  

O algoritmo FILTERSIM apresentado por Zhang, Switzer e Journel (2006) tem como 

maior vantagem o fato de não excluir nenhum ponto condicional para corresponder a um 

padrão do banco de padrões. Diferentes filtros são aplicados para obter valores de filtros de 

contagem. Os padrões, por sua vez, são agrupados baseados em seus filtros de contagem e 

colocados em classes diferentes. Cada classe é representada pelo seu respectivo protótipo, 

que é o valor médio de todos os padrões nesta classe. Durante a simulação, os dados 

condicionantes são comparados com as classes e é encontrado o protótipo mais próximo. 

Diferentemente do SIMPAT, o FILTERSIM não procura no banco de padrões inteiro, pois 

ele não está procurando o correspondente exato e sim o melhor correspondente.  

Em 2010, Honarkhah e Caers apresentaram o algoritmo de simulação baseado na 

distância (DISTPAT) para melhorar a eficiência das classificações de padrões, os resultados 



44 

 

 

 

 

mostraram que esta simulação tem atuação melhor que a do FILTERSIM. Como em todas 

estas simulações baseadas em padrões, o padrão é desenhado aleatoriamente na classe. Não 

há a geração da função densidade acumulada condicional para cada classe como visto no 

SNESIM. O sucesso da técnica depende do quão bem os padrões são classificados, uma vez 

que o padrão obtido é aleatório na classe que melhor a corresponde e nenhuma estatística 

está envolvida nisto. 

Outra abordagem da simulação de multipontos foi proposta por Dimitrakopoulos, 

Mustapha e Gloaguen (2010) e consiste em uma técnica baseada em cumulantes espaciais – 

que são combinações de momentos de alta ordem de multipontos. A Figura 8 (a) mostra a 

coleta da estatística espacial de dois pontos (variograma), portanto, é baseada apenas em 

uma direção para o cálculo da variância espacial. Enquanto a estatística de alta ordem utiliza 

diversas direções e distâncias para capturar variância espacial da distribuição espacial. No 

caso da Figura 8 (b) a variância espacial utilizou duas direções e duas distâncias distintas e 

esta é uma estatística de três pontos.  

 

Figura 8 – Ilustração de como funciona a estatística de dois pontos e de três pontos ((I) Machuca-

Mory e Dimitrakopoulos, 2013)I. 

 

Neste método, a função densidade acumulada condicional é gerada por polinômios 

de Legendre, que, por sua vez, são calculados utilizando mapas de cumulantes gerados a 

                                                 

 

I  Machuca-Mory D. F. e Dimitrakopoulos R., 2013, Notas das aulas da disciplina Stochastic Orebody Modelling 

(MIME 525) da McGill University 

 



45 

 

 

 

partir das imagens de treinamento. Esta técnica é voltada para os dados e não para a imagem 

de treinamento e reproduz a estatística espacial de alta ordem dos dados. O número de 

coeficientes na série varia com respeito ao número de amostras utilizadas para calcular o 

valor em um determinado ponto e a ordem da série (Mustapha et al., 2013). 

Uma técnica diferente de simulação condicional apresentada por Gloaguem e 

Dimitrakopoulos (2009) utiliza a dependência inter-escalar no domínio das wavelets. A 

vantagem deste método é que a abordagem do condicionamento direto é fácil, embora seja 

complicado ajustar os dados condicionantes no domínio das wavelets (Chatterjee, 

Dimitrakopoulos e Mustapha, 2012). 

 

4.4.1 Imagem de Treinamento 

De acordo com Caers e Zhang (2002) e Journel e Zhang (2006), as imagens de 

treinamento são, essencialmente, um banco de dados de padrões geológicos a partir do qual 

as estatísticas de vários pontos - incluindo o semivariograma - podem ser emprestadas. A 

repetição de padrões deve ser suficiente para estimar ou extrair um conjunto de estatísticas 

de vários pontos a partir do tamanho da imagem. Este tamanho é exigido sob o princípio de 

ergodicidade e, juntamente com seu tamanho relativo ao maior padrão a ser reproduzido no 

modelo, são fatores importantes. Ao estimar os parâmetros de um domínio finito ou ao 

simular modelos em uma região finita, todas as estatísticas - incluindo estatísticas de 

multipontos - tendem a oscilar e a se tornar maiores à medida que as distâncias em que estas 

estatísticas calculadas são maiores. Deve ficar claro que meros modelos de afloramento, 

fotografias de depósitos ou sistemas de deposição análogos ou até mesmo desenhos simples 

não podem, necessariamente, ser utilizados como imagens de treinamento se não seguirem 

esses princípios subjacentes de ergodicidade e estacionaridade. 

Lyster, Deutsch e Ortiz (2004) relatam que a imagem de treinamento deve ser 

considerada inteiramente representativa da área a ser simulada e deve conter todas as suas 

características geológicas. Para tanto, Caers e Zhang (2002) explicam que a imagem de 

treinamento substitui o semivariograma como medida de heterogeneidade geológica e é mais 

intuitiva, uma vez que é possível observar - antes de qualquer simulação ou estimativa 

geoestatística - quais serão os padrões a serem reproduzidos. É mais fácil recusar uma 



46 

 

 

 

 

imagem de treinamento baseada em uma inspeção ou interpretação visual da realidade 

geológica a recusar um modelo semivariograma ou suposição multigaussiana. Depois de 

encontrada a imagem de treinamento ideal, uma combinação, rotação e modelagem são 

necessárias e estes procedimentos são similares ao ajuste do modelo teórico de 

semivariograma.  

Ortiz (2003) explica que a reprodução da imagem de treinamento não é o objetivo, 

mas sim a captura da sua essência e, assim, poder simular um modelo que compartilhe as 

características multivariadas da distribuição real. A função de distribuição condicional 

considera a configuração de multipontos na vizinhança e isto é feito pela discretização da 

função de distribuição condicional em quadros de indicadores. A probabilidade de um local 

em particular estar abaixo de um limiar é dada pela frequência de sua ocorrência na 

configuração multipontos na imagem de treinamento. A distribuição multivariada é 

aproximada pelas frequências extraídas a partir da imagem de treinamento. Se uma 

configuração multiponto não é encontrada vezes o suficiente para estimar com 

confiabilidade sua frequência, a busca é reduzida e uma configuração diferente é 

considerada. Em geral, o objetivo de usar uma imagem de treinamento é a obtenção de 

modelos que conduzem à melhor tomada de decisão, o que não pode ser expresso 

diretamente como um conjunto de restrições matemáticas durante o processo de inferir 

estatísticas de multipontos a partir da imagem de treinamento, por isso são utilizados 

métodos heurísticos (Ortiz, 2003).  

O escaneamento da imagem de treinamento é feito por um modelo previamente 

definido, como observado no item (a) da Figura 9. Este modelo é o formato de como a busca 

é realizada ao redor de um ponto central u. Os valores dos pontos ao redor são guardados no 

banco de dados dos padrões. Então o modelo é deslocado ao nó central vizinho, como pode 

ser visto na Figura 10 e todo procedimento é repetido. O caminho total do modelo 

apresentado na Figura 9 é mostrado na parte esquerda da Figura 10 e o resultado guardado 

no banco de dados de padrões do escaneamento de um modelo bidimensional pode ser 

observado na parte a direita da mesma figura. 



47 

 

 

 

 

Figura 9 – Definição do modelo, exemplo de um modelo 2D 3x3 numa malha 11 x 11 (Arpat e Caers, 

2007).  

 

 

Figura 10 – Exemplo de imagem de treinamento e seus padrões obtidos pelo escaneamento de um 

mapa onde 0 representa um argilito e 1 arenito (Arpat e Caers, 2007). 

 

4.4.2 Método de Simulação Baseada em Wavelets (Wavesim) 

Grapes (1995) explica que wavelets são funções matemáticas que cortam os dados 

em componentes de frequência diferentes, então, estuda-se cada componente com uma 

resolução associada à sua escala. Eles têm vantagens sobre os métodos tradicionais de 

Fourier na análise de situações físicas, onde o sinal contém descontinuidades e mudanças 

bruscas. As wavelets foram desenvolvidas de forma independente no campo da matemática, 

física quântica, engenharia elétrica e geologia sísmica. Intercâmbios entre estes campos nos 

últimos dez anos levaram a muitas novas aplicações, como compressão de imagem, 



48 

 

 

 

 

turbulência, visão humana, radar e previsão de terremotos. A ideia fundamental atrás das 

wavelets é a análise de acordo com a escala.  

Wavelets são funções que satisfazem certos requisitos matemáticos e são usados na 

representação de dados ou outras funções. Esta ideia não é nova, a aproximação usando 

superposição de funções existe desde o início de 1800, quando Joseph Fourier descobriu que 

ele poderia sobrepor senos e cossenos para representar outras funções. No entanto, na análise 

de wavelets, a escala usada para estudar os dados desempenha papel especial. Os algoritmos 

de wavelets processam dados em diferentes escalas ou resoluções. Se um sinal for observado 

com uma “grande janela”, serão observáveis recursos brutos. Da mesma forma, em um sinal 

com a “janela pequena", as pequenas características (detalhes) serão destacadas. 

Metaforicamente, o resultado na análise de wavelets é como ver a floresta e as árvores. Por 

muitas décadas os cientistas desejaram funções mais apropriadas que os senos e cossenos, 

componentes da base da análise de Fourier, para aproximações de sinais com mudanças 

bruscas. Com a análise de wavelets, podem-se aproximar funções que estão contidas 

ordenadamente em domínios finitos. As wavelets são muito bem adequadas para aproximar 

dados com descontinuidades abruptas (Grapes, 1995). 

O procedimento de análise wavelet é a adoção de uma função protótipo de wavelet, 

chamado de função mãe do wavelet. A análise temporal é realizada com uma versão do 

protótipo de wavelet contraído, de alta frequência, enquanto a análise de frequência é 

realizada com uma versão dilatada, de baixa frequência, da mesma wavelet, pois, o sinal 

original ou função pode ser representado em termos de expansão de wavelet (usando 

coeficientes numa combinação linear das funções de wavelet). As operações de dados podem 

ser realizadas utilizando apenas os coeficientes de wavelets correspondentes. Se for 

escolhida a função wavelet que melhor se adapta aos seus dados ou truncar os coeficientes 

abaixo de um limiar, os dados são representados de forma esparsa, o que torna este algoritmo 

uma ótima ferramenta para a compressão de dados (Grapes, 1995). 

Chatterjee, Dimitrakopoulos e Mustapha (2012) mostram que os algoritmos de 

simulação de padrões consistem de duas etapas: a primeira é a geração do banco de padrões 

a partir do escaneamento das imagens de treinamento utilizando um modelo previamente 

definido e, na segunda, o padrão que melhor corresponde aos dados condicionantes é 

procurado no banco de padrões. 



49 

 

 

 

Mustapha et al. (2013) discorrem que o método de Simulação Baseada em Wavelets 

é um método de simulação de multipontos baseado em padrões, que utiliza transformações 

discretas para representar a heterogeneidade geológica. A classificação de padrões é feita 

depois de reduzir a dimensão dos padrões pela decomposição de wavelets. A simulação é 

calculada sequencialmente identificando a classe que melhor corresponde aos dados 

condicionantes e pela amostragem aleatória do padrão nesta classe. A classe é representada 

pelo protótipo de classe. Para a simulação, a similaridade do protótipo de classe com os 

dados condicionantes é calculada e um padrão aleatório é gerado a partir da classe de melhor 

correspondência. 

Chatterjee, Dimitrakopoulos e Mustapha (2012) apresentam o Wavesim como um 

algoritmo de simulação baseado em padrões utilizando análises de wavelets, alternativo aos 

outros métodos de simulação de multipontos. O banco de dados de padrões é feito de maneira 

semelhante ao de outras técnicas multipontos. Este banco é classificado por coeficientes de 

sub-bandas aproximadas de wavelets para cada padrão. Esta sub-banda pode capturar a 

maioria das variabilidades dos padrões e ao mesmo tempo diminuir a dimensão do banco de 

padrões. A classificação do banco de dados de padrões utiliza a técnica de agrupamento 

denominada K-Means. Para a simulação de dados categóricos, no nó central do modelo, a 

função densidade acumulada condicional de cada classe é desenvolvida utilizando a 

probabilidade de cada categoria desta classe. Entretanto, para dados contínuos, seleciona-se 

uma amostra aleatória da classe que melhor correspondê-la, ou seja, a função densidade 

acumulada condicional não é calculada. 

Segundo Mustapha et al. (2013), uma representação baseada em wavelets é 

introduzida onde a dimensão das classificações de padrões podem ser reduzidas pela seleção 

da escala da decomposição por wavelets. O termo escala se refere à resolução ou suporte 

para qual um dado comprimento de onda da imagem de treinamento é definida. 

As principais limitações da técnica de redução dimensional baseada em filtros, 

segundo Mustapha et al. (2013), são: 

(a) É sempre complicado representar um padrão complexo apenas utilizando alguns 

filtros de valores e não há garantia que estes filtros irão representar 

adequadamente o padrão. 



50 

 

 

 

 

(b) O método baseado em filtros não mostra nenhuma prova teórica ou numérica de 

que a variabilidade dos dados é representada.  

De acordo com Chatterjee e Dimitrakopoulos (2011), a análise de wavelets de 

múltiplas resoluções é feita de acordo com a escala, então a decomposição dos dados em 

diferentes bancos de dados também é realizada de acordo com sua escala e fornece 

informações espaciais e de frequência. Um banco de dados bidimensional pode ser 

considerado uma função quadrática integrável no espaço de Hilbert – que é o espaço 

Euclidiano formado por vetores complexos de n dimensões (Duarte, 2003). Devido à sua 

propriedade de múltiplas resoluções, a função wavelet fornece uma série de funções básicas 

ortogonais pela escala e deslocada da função base original, que é conhecida como função 

mãe de wavelets. 

Segundo Chatterjee, Dimitrakopoulos e Mustapha (2012), os padrões são extraídos 

pelo escaneamento das imagens de treinamento com um modelo e então são guardados em 

um banco de dados de padrões. A redução das dimensões dos padrões no banco de dados de 

padrões é realizada pela decomposição de wavelets em certa escala e a sub-banda 

aproximada é utilizada para a classificação destes padrões no banco de dados. Esta 

classificação é feita pelo algoritmo de agrupamento K-Means e suas classes são 

representadas por um protótipo. Para a simulação de variáveis categóricas, a função 

densidade acumulada condicional de cada classe é gerada baseada na frequência das 

categorias individuais no nó central do modelo. Durante o processo de simulação, a 

similaridade dos eventos dos dados condicionantes com os protótipos de classes são medidas 

utilizando o L2 –norm – que é o ajuste lp (least power), designação genérica dos ajustes nos 

quais se minimiza a soma dos valores absolutos dos erros elevados ao quadrado, ou seja y = 

?  ??=1 |ri – di|
2, onde ri representa o resultado obtido e di o resultado correto (Duarte, 2003). 

Quando as variáveis categóricas são simuladas, a função densidade acumulada condicional 

da classe mais semelhante é utilizada para sortear os padrões desta classe. Por sua vez, 

quando variáveis contínuas são simuladas, um padrão aleatório é desenhado a partir da classe 

selecionada.  

De acordo Chatterjee, Dimitrakopoulos e Mustapha (2012), considere ti(u) como o 

valor da imagem de treinamento ti onde u ? Gti e Gti é uma malha regular cartesiana 



51 

 

 

 

discretizando a imagem de treinamento, tiT (u) indica um vetor ti(u) com o modelo T 

centrado no nó u, sendo: 

tiT (u) = {ti(u+h1), ti(u+h2),…, ti(u+h?), ..., ti(u+hnT)}     (28) 

onde os vetores h? são os vetores que definem a geometria dos nT nós do modelo T 

e ? = {1, 2, …, nT}. O vetor h1 = 0 representa o local central u do modelo T.  

Chatterjee, Dimitrakopoulos e Mustapha (2012) explicam que o banco de padrões é 

então obtido pelo escaneamento da ti usando o modelo T e guardado no vetor tiT(u) do 

banco. Para uma imagem de treinamento categórica com M categorias, a imagem de 

treinamento primeiramente é transformada em M conjuntos de valores binários Im(u), m 

=1,…,M, u ? T, 

Im (u) = {
1, ?? ? ????????? ? ?é???? ?????????        
0, ???? ?????á???                                               

   (29) 

Assim, um padrão de M categorias pode ser representado como M conjuntos de 

padrões binários, onde o m-ésimo padrão binário com valor 1 representa a presença da 

categoria m e o valor 0 a ausência desta no modelo. Ressalta-se que para imagens de 

treinamento de variáveis contínuas, nenhuma transformação é feita e o padrão tiT (u) é salvo 

exatamente como extraído da imagem de treinamento. O banco de padrões gerado por 

imagens de treinamento contínuas ou categóricas com M categorias é definido como patdbT 

(Chatterjee, Dimitrakopoulos e Mustapha, 2012). 

Após gerar o banco de padrões (patdbT), inicia-se a simulação com a classificação 

do banco de padrões, que é realizada de modo que apenas alguns membros representativos 

sejam comparados com os dados condicionantes, então não se pesquisa em todo patdbT. 

Entretanto, quando a dimensão do modelo for grande, o patdbT também será grande. 

Qualquer padrão no patbdT é representado por 6 x M filtros de valores - para o caso 

bidimensional – no caso da imagem ser contínua M = 1. A representação baseada em padrões 

de wavelets é introduzida onde as dimensões da classificação de padrões pode ser reduzida 

selecionando a escala da decomposição de wavelets. Esta fornece uma imagem de sub-banda 

aproximada e três imagens de sub-bandas de alta frequência depois de uma escala de 

decomposição de uma imagem de treinamento bidimensional (Chatterjee, Dimitrakopoulos 

e Mustapha, 2012).  



52 

 

 

 

 

A imagem aproximada é decomposta para obter a próxima escala de imagens de sub-

bandas. A sub-banda aproximada fornece as informações médias sobre a imagem de 

treinamento e preserva a maioria das informações de variabilidade dos dados da imagem de 

treinamento. Se as sub-bandas de alta frequência forem adicionadas à sub-banda 

aproximada, então a imagem de treinamento é perfeitamente reconstruída. A quantidade de 

dados em uma sub-banda é 2j.d vezes menor que a quantidade de dados na imagem de 

treinamento, onde j é o número da escala na decomposição de wavelets e d é a dimensão 

original da imagem. A Figura 11 mostra um exemplo de uma imagem original e sua 

reconstrução mantendo apenas a imagem da sub-banda aproximada depois da primeira 

escala de decomposição. Esta figura mostra também que a imagem é bem reconstruída após 

reduzir 75% dos dados da imagem original (Chatterjee, Dimitrakopoulos e Mustapha, 2012). 

 

Figura 11 - Comparação entre a imagem original (a) e a reconstruída (b) depois da redução de 75% 

dos dados originais (Chatterjee, Dimitrakopoulos e Mustapha, 2012). 

 

Considerando tiT um padrão do patdbT com tamanho p x p, então se a decomposição 

de wavelets de um dado padrão é feita, ela pode ser apresentada, segundo Chatterjee, 

Dimitrakopoulos e Mustapha (2012), como: 

TiT = ?  
???1
?,?=0 ?J,i,l ?

LLJ,i,l +?   ??? ?  
?
?=1

?  
???1
?,?=0 ?

Bj, i, l ?Bj, i, l   (30) 

Onde D={LH, HL, HH}, Nj = N / 2j;  J é o número de escalas; L e H são os filtros 

passa baixa e passa alta, respectivamente obtidos da função básica da wavelet, N = p quando 

? é par, ? =  (? + 1) quando ? é ímpar; ?J é a função escalar e ?
B

J é a função wavelet. Os 



53 

 

 

 

coeficientes da escala e da wavelet ?j-1e ?j-1 na escala ? ? 1 podem ser calculados 

experimentalmente pelos produtos: 

?? ? 1 = &amp;lt;???, ?? &gt;       (31) 

??? ? 1 = &amp;lt;??, ??? &gt;       (32) 

Cada uma das funções base (?j e ?Bj) é utilizada para o banco de dados dos 

coeficientes de imagem de treinamento binária M. Em cada local de pixel, o modelo de 

valores dos dados vizinhos é torcido por estas funções base para obter os dados da sub-banda 

aproximada e wavelet para a categoria m. O comprimento do vetor generalizado da sub-

banda aproximada para as M categorias da imagem, de acordo com Chatterjee, 

Dimitrakopoulos e Mustapha (2012) é: 

LN = ((N/2j)d . M)        (33) 

Onde d é o número de dimensões desta imagem. É observado que o comprimento 

original do vetor de padrões é ((N)d x M). Entretanto, dependendo do número de escalas j, 

a dimensão do vetor do padrão original pode ser significantemente reduzida. 

Segundo Chatterjee e Dimitrakopoulos (2011), o coeficiente de wavelets exposto no 

espaço e na frequência é hierárquico em diferentes sub-bandas. A sub-banda é obtida 

aplicando filtros em imagens bidimensionais na direção x e y. O coeficiente de wavelets é 

enraizado na estrutura, o que significa que um ponto na escala maior é representado por 

quatro pontos vizinhos na próxima escala menor, conforme representado na Figura 12. 

Então, qualquer ponto na escala j é diretamente relacionado aos quatro pontos vizinhos na 

escala j-1. A área total coberta por uma imagem de sub-banda em qualquer escala j é a 

mesma que da imagem original. Entretanto, a área de influência de qualquer ponto ou 

qualquer sub-banda na escala j é quatro vezes maior que qualquer ponto na sub-banda na 

escala j-1. Esta propriedade dos coeficientes de wavelets facilita a modelagem conjunta de 

dois bancos de dados com diferentes suportes. 

Para gerar o banco de dados dos coeficientes de wavelets correspondente ao modelo 

dos coeficientes da sub-banda aproximada, o tamanho de modelo deve ser definido. 

Posteriormente, o modelo é utilizado para escanear a sub-banda aproximada da imagem de 

treinamento. O coeficiente sub-banda aproximada é extraído da imagem de treinamento 



54 

 

 

 

 

decomposta em wavelets de múltiplas escalas com a janela do modelo e os correspondentes 

coeficientes do valor central do modelo, como pode ser visto na Figura 12 (Chatterjee e 

Dimitrakopoulos, 2011). 

Uma série de vetores de dados pode ser gerado pelo escaneamento das sub-bandas 

aproximadas das imagens de treinamento utilizando o modelo, então eles formam um banco 

de dados de vetores. Para simular os coeficientes de wavelets correspondentes a um ponto 

particular nas sub-bandas aproximadas simuladas, utiliza-se o mesmo modelo que foi usado 

para extrair os dados condicionantes da imagem de treinamento, posicionando o centro do 

modelo sobre o ponto da sub-banda aproximada. Os dados condicionantes serão comparados 

com o modelo de valores do banco de dados de coeficientes de wavelets (Figura 12) para 

obter o melhor vetor correspondente. O coeficiente de wavelet correspondente será extraído 

como os valores do ponto simulado (Chatterjee e Dimitrakopoulos, 2011). 

 

 

Figura 12 – Estrutura da análise discreta de wavelets de múltiplas resoluções, mostrando a relação 

entre coeficientes de wavelets de duas escalas e um exemplo de um vetor extrator com um modelo 3x3 

(Chatterjee e Dimitrakopoulos, 2011). 

 



55 

 

 

 

Um exemplo prático de como são os resultados da decomposição de wavelets é 

ilustrado na Figura 13, onde a icônica imagem da Lenna é decomposta em 3 escalas e as 

bandas de cada escala são mostradas ao lado para facilitar a visualização do método. 

 

Figura 13 - A esquerda um exemplo de decomposição de wavelets da Lenna (Ouahabi, 2012) e a 

direita as bandas de cada escala da decomposição (Chatterjee, Mustapha e Dimitrakopoulos, 2015). 

 

Ainda em Chatterjee, Dimitrakopoulos e Mustapha (2012), para a simulação não 

condicional de uma imagem categórica, a decomposição de wavelets é realizada depois de 

gerar o banco de padrões a fim de reduzir a dimensão deste banco. O passo crítico da 

decomposição de wavelets para a redução da dimensão é manter a variabilidade máxima dos 

dados em poucas dimensões. Na análise de wavelets, a energia é distribuída entre as escalas, 

no entanto, uma escala ótima deve ser selecionada. Esta escala ocorre quando as 

características de frequência forem dominantes, quando comparadas com as outras. Para 

isso, deve-se calcular o valor de entropia da wavelet de cada padrão tiT na escala j utilizando 

a equação: 

Ejti = ? ?   ??? p
Bj log(pBj)       (34) 

Onde  

pBj =( ?  
???1
?,?=0 |?

Bj, i, l|2 ) /( ?   ??? ?  
???1
?,?=0 |?

Bj, i, l|2)    (35) 

e 



56 

 

 

 

 

D = {LH, HL, HH}; Nj = N / 2j      (36) 

Em que j é a escala de decomposição, ?Bj, i, l é o coeficiente da wavelet na escala j no 

local (i, l). 

A média da entropia de todos os padrões no banco de padrões é calculada como: 

Ej = 
1

?
 ?  ???=1 E

tij        (37) 

Onde s é o número de padrões no banco, o valor da média é comparado pela mudança 

da escala j e a escala ótima é selecionada onde o valor seja o máximo. O fluxograma do 

algoritmo de seleção da escala ótima é apresentado na Figura 14. O valor de entropia é 

calculado inicialmente para escala j = 1 e é interrompida quando a escala máxima é atingida, 

ou seja, a escala máxima ocorre quando não é possível fazer mais a decomposição. 

 

Figura 14 – Fluxograma da escolha para uma escala ótima (Chatterjee, Dimitrakopoulos e Mustapha, 2012). 

 



57 

 

 

 

Chatterjee, Dimitrakopoulos e Mustapha (2012) explicam que para a classificação 

do banco de padrões patdbT, a sub-banda aproximada dos padrões é reduzida dependendo 

do valor de j. Basicamente o patdbT é dividido em um número de classes em que a soma 

das distâncias entre classes é maximizada. Neste algoritmo, o banco de padrões é classificado 

baseando-se na seleção de prioridade do número do agrupamento (k). Primeiramente, k 

padrões do patdbT são aleatoriamente selecionados, estes k padrões representam os 

centroides iniciais das classes. Uma vez que a classificação do patdbT foi feita utilizando 

sub-banda aproximada dos padrões, uma sub-banda selecionada aleatoriamente de k padrões 

do patdbT agirá como centroide inicial. Cada padrão do patdbT é atribuído a uma classe 

com a qual tem a distância mais próxima dos centroides, depois de atribuídos todos os 

padrões nas classes, a posição dos centroides são recalculadas. Este é um processo iterativo 

e o algoritmo para quando a posição do centroide não muda mais. O objetivo do algoritmo 

de agrupamento K-Means é minimizar a seguinte função objetivo: 

J = ?  ??=0  ?  
?
?=0  ||ti

(j) – cj||2       (38) 

Onde ||ti(j) – cj||2 é a distância quadrática Euclidiana entre o padrão ti(j) e o centroide 

da classe cj e a medida da distância de n padrões dos seus respectivos centros de 

agrupamento. 

Depois de classificar o patdbT por minimizar a função objetivo (38), as classes são 

recalculadas. Então são utilizados protótipos durante os processos de simulação, quando a 

similaridade entre os dados condicionais e a classe é calculada. O valor do protótipo é obtido 

calculando a média de todos os padrões que estão em uma determinada classe (Chatterjee, 

Dimitrakopoulos e Mustapha, 2012). 

Chatterjee, Dimitrakopoulos e Mustapha (2012) mostram que o algoritmo de 

simulação sequencial é utilizado para simulação baseada em padrões. Em cada nó visitado, 

um evento de dado condicionante é obtido colocando o mesmo modelo da imagem de 

treinamento, centrado no nó a ser simulado. A similaridade entre o evento dos dados 

condicionantes e o protótipo de classes é calculada pela função da distância L2-norm (quanto 

menor o valor, melhor é a correspondência): 

d(x, y) = ?  3?=1 ?i  . ((1/ntype) ?  
?????
?=0 (x(j) – y(j))

2)     (39) 



58 

 

 

 

 

?  3?=1 ?i = 1         (40) 

Onde x são os dados condicionantes; y o protótipo de classe; ntype o número de dados 

de um tipo particular e ?i o peso associado com os tipos de dados. Três diferentes tipos de 

dados são considerados para a distância calculada: dados amostrados, dados previamente 

simulados e dados de padrões de nós anteriores. Geralmente os dados amostrados são os 

dados com maior peso que os outros tipos (Chatterjee, Dimitrakopoulos e Mustapha, 2012).  

Ainda em Chatterjee, Dimitrakopoulos e Mustapha (2012) é comentado que se todos 

os nós dentro de um modelo são conhecidos quando simulando um nó, a distância calculada 

com um modelo grande pode ser computacionalmente demorada. Para reduzir o tempo 

computacional do cálculo da distância, coeficientes de sub-banda aproximadas são utilizados 

depois da decomposição de wavelets dos dados condicionantes. A função da distância pode 

ser representada como: 

d(x, y) = ((1/naprox) ?  
??????
?=? (x

aprox(j) – yaprox(j))2),   (41) 

onde naprox é um número do coeficiente de sub-bandas aproximadas depois da 

decomposição de wavelets, xaprox é o coeficiente de sub-banda aproximada dos dados 

condicionantes e yaprox é o coeficiente de sub-banda aproximada do protótipo de classe. Se 

dentro dos dados condicionantes houver qualquer dado amostrado a equação (39) será 

utilizada para o cálculo de distância, mesmo que todos os nós dentro do modelo sejam 

completamente conhecidos (Chatterjee, Dimitrakopoulos e Mustapha, 2012). 

Depois de medir a similaridade dos dados condicionantes com o protótipo de classes, 

a classe que melhor a corresponde é selecionada. A função distribuição acumulada 

condicional é gerada para cada classe, esta função é desenvolvida pelo cálculo das 

probabilidades de ocorrência de categorias particulares no nó central do modelo, dividido 

pelo número total de padrões nesta classe (Chatterjee, Dimitrakopoulos e Mustapha, 2012). 

Durante o processo de simulação, após achar a melhor classe que corresponde aos 

padrões, um número aleatório é gerado. A partir de função densidade acumulada 

condicional, a categoria no nó central é obtida pela correspondência daquele número 

aleatório. Então um padrão aleatório é desenhado a partir da classe de padrões 

correspondente, pelo qual deve ter o mesmo nó central que a categoria obtida pela função 



59 

 

 

 

densidade acumulada condicional. O próximo nó visitado é escolhido a partir de um caminho 

aleatório. A mesma função de distância e o algoritmo de desenho do padrão são repetidos 

até que todos os nós sejam simulados (Chatterjee, Dimitrakopoulos e Mustapha, 2012). 

Chatterjee e Dimitrakopoulos (2011) explicam que quando a sub-banda simulada em 

mais de uma variável primária e todos os coeficientes de sub-bandas de wavelets são 

simulados, a transformação inversa da equação (30) é aplicada aos dados de maior escala 

para gerar o mapa simulado no domínio do espaço. A transformação inversa é iniciada a 

partir da escala maior e combina os dados simulados da variável primária e as sub-bandas 

de wavelets simuladas pelo modelo correspondente à aproximação na escala menor seguinte, 

e assim por diante. A amostragem é feita pela adição de zeros entre cada coeficiente. As sub-

bandas são, então, envolvidas com a reconstrução dos filtros de escalas para os coeficientes 

de aproximação e também com a reconstrução dos filtros de wavelets para coeficientes de 

detalhes. Estes resultados são somados para obter os mapas simulados no domínio espacial. 

As etapas relacionadas ao Wavesim são apresentadas na Figura 15. Considerando 

que a imagem de treinamento já foi escaneada e seus padrões foram decompostos, agrupados 

e guardados, a simulação inicia com a escolha de uma região aleatoriamente. As amostras 

desta região são comparadas por L2-norm com os padrões do banco de padrões e, dentro do 

grupo mais parecido, é sorteado aleatoriamente um padrão o qual será colocado no local 

sendo simulado e a simulação segue para a próxima região.  



60 

 

 

 

 

 

Figura 15 – Descrição dos passos da wavesim. Chatterjee, Mustapha e Dimitrakopoulos (2015). 

 

Chatterjee, Dimitrakopoulos e Mustapha (2012) mostram que as maiores vantagens 

do Wavesim são: 

(a) A classificação de padrões de dados de várias dimensões pode ser feita com baixo 

esforço computacional, devido à natureza da sub-banda aproximada da 

decomposição de wavelets, que reduz dimensionalmente os padrões e captura sua 

variabilidade, 

(b) O padrão desenhado da classe pode ser feito a partir de sua probabilidade, que 

faz uma reprodução melhor, pois a função densidade acumulada condicional foi 

desenvolvida para cada classe da simulação categórica. 

 

 

 



61 

 

 

 

4.4.3 Algoritmo da Simulação Baseada em Wavelets 

Segundo Chatterjee, Dimitrakopoulos e Mustapha (2012), o algoritmo do wavesim 

pode ser descrito como: 

1. Escanear a imagem de treinamento ti usando o modelo T. Realizar a 

decomposição de wavelets dos padrões gerados utilizando a função básica das 

wavelets e escala. Salvar os coeficientes de wavelets aproximados do banco de 

padrões. Se a imagem de treinamento é categórica, gerar uma imagem binária M 

das categorias da imagem de treinamento antes da decomposição de wavelets; 

2. Classificar os padrões baseado apenas nos coeficientes de sub-bandas 

aproximadas nos números agrupados previamente definidos e calcular o 

protótipo da classe usando a média de todos os padrões na determinada classe; 

3. Definir um caminho aleatório para visitar apenas uma vez cada nó não 

amostrado; 

4. Usar o mesmo modelo T em cada local u não amostrado. A distância do protótipo 

da classe é calculada através dos dados condicionantes disponíveis com o modelo 

utilizando as equações (39) ou (41). Selecionar a classe com a distância mínima 

dos dados condicionantes. Se nenhum dado condicionante estiver disponível 

utilizar uma classe aleatória; 

5. Sortear um padrão aleatório a partir do protótipo de classe e passar o padrão pelo 

centro do local simulado u. Se algum dado amostrado ou o valor do nó central de 

qualquer local já simulado está presente em qualquer nó do modelo T na 

localização u, eles são mantidos antes que a simulação de padrões os apague. 

Para dados categóricos, uma amostra aleatória é desenhada baseada na função 

densidade acumulada condicional gerada para cada classe; 

6. Adicionar o valor simulado no ponto u a um arquivo diferente para ser utilizado 

no cálculo da distância; 

7. Repetir os passos 4 a 6 para todos os próximos pontos do caminho aleatório 

definido no passo 3; 

8. Repetir os passos 3 a 7 para gerar diferentes realizações utilizando diferentes 

caminhos aleatórios. 

 



62 

 

 

 

 

5.  Análise do lucro por bloco 

A análise de sensibilidade na mineração pode ser executada de diversas maneiras 

conforme a necessidade, por exemplo para o cálculo dos teores de corte; probabilidade de 

haver certa quantidade de minério acima de um teor desejado; probabilidade de certo teor de 

minério em cada bloco entre outras. Neste trabalho, esta análise significa a aplicação de uma 

equação não linear para calcular o valor potencial (lucro) de cada bloco simulado. Este valor 

(Vp), por sua vez, é o lucro potencial que cada bloco possui na unidade monetária desejada, 

se considerados os custos de manutenção, transporte, armazenamento e tratamento do 

minério para tipos de pilhas i. Estes tipos de pilhas significam custos diferentes que podem 

ser causados por tratamentos diferentes (diferentes equipamento), equipamentos de 

transporte diferentes ou quaisquer outros custos que possam variar. Estes custos diferentes 

podem ser minimizados se forem escolhidos quais blocos devem ir para cada pilha, desta 

forma o lucro da mineração será maximizado.  

O valor potencial Vp de cada bloco (blk) é a multiplicação do teor simulado (Z(x)) 

pela recuperação da pilha i pelo o preço do minério (MP), pelo volume V do bloco, pela 

densidade D do bloco e por 1000 para a correção dimensional da densidade (1000kg/m3 = 

1g/cm3). O resultado é subtraído da soma de todos os custos (Ci) de cada pilha i.  

O cálculo do valor de cada bloco (Vp) é feito de acordo com a equação: 

Vp     =   Z(x)  .  Ri  .  MP  .   V    .  (D . 1000) -  Ci    (42) 

 [$/blk]  = [g/kg] . [%] . [$/g] . [m3/blk] . [kg/m3] - [$/blk] 

O arquivo de entrada deve ter a coluna de identificador IJK, coluna densidade - que 

será calculada pelo método de krigagem ordinária - e as das colunas de diferentes realizações 

(Z(x)1, Z(x)2,..., Z(x)l, l sendo a l-ésima realização). Além disso, alguns parâmetros devem 

ser definidos como respostas às seguintes questões: 

(a) Qual é o preço do minério? 

(b) Qual é o volume dos blocos? 

(c) Quantas pilhas de processamento serão utilizadas? 

(d) Qual é o custo total de cada bloco? 

(e) Qual é a recuperação de cada pilha? 



63 

 

 

 

(f) Quantas realizações há no arquivo? 

O fluxograma do funcionamento do programa de análise de lucro por bloco é 

apresentado na Figura 16. 

 

 

Figura 16 – Fluxograma do valor potencial de cada bloco. 

 

 

6.  Banco de Dados Sintético 

O depósito sintético foi criado com o intuito de servir para averiguação da qualidade 

dos métodos geoestatísticos, além disso, é adequado para testar a qualidade de amostragens, 

assim como otimizar a localização de adensamentos de sondagens futuras, uma vez que se 

podem fazer furos em qualquer local sem custo e assim verificar a confiabilidade dos 

resultados com a variação da quantidade de furos de sonda. Estes dados serão liberados para 

pesquisas futuras. 

Um depósito real não é satisfatório para o estudo adequado da confiabilidade dos 

métodos geoestatísticos, pois os contornos geológicos em subsuperfície não são conhecidos 

com precisão, bem como a posição e os teores exatos dos corpos de minério, afinal, nem 

todas as camadas são necessariamente aflorantes e a malha de sondagem pode não 

interceptá-las. Para garantir todo o conhecimento do depósito mineral, ele deve ser estudado 

e explotado por inteiro.  



64 

 

 

 

 

A geologia local é de um depósito polimetálico formado por fluidos hidrotermais 

quartzosos que penetraram por uma falha de direção norte-sul e inclinação aproximada de 

45º para leste. A rocha encaixante é formada pela intercalação de camadas de meta-arenito 

e camadas de filitos dobradas, causando alta variabilidade de padrões geológicos, além da 

alta variabilidade intrínseca aos depósitos de metais nobres como cobre. 

Para garantir a verossimilhança, o modelo geológico desenhado utilizando o 

programa Datamine®, foi definido utilizando a simplificação da geologia de um sítio real de 

mineração. O modelo é a interpolação de 2 perfis geológicos semelhantes distantes 600 

metros conforme ilustrado na Figura 17. 

 

 

Figura 17 – Strings do Datamine® que formaram o modelo geológico sem a topografia. 

 

A partir dos wireframes (união das strings – contornos geológicos e de influência 

hidrotermal), foi criado um modelo de blocos com dimensões de um metro na direção X por 

um metro na direção Y e meio metro de profundidade para ter a melhor resolução espacial 

possível sem tornar o desenvolvimento demasiadamente demorado. O depósito tem tamanho 

aproximado de 300 metros para leste, 600 metros para norte e 300 metros de profundidade. 

Em cada bloco a geologia do wireframe foi “carimbada”. Por fim, foi criado um wireframe 

da topografia da região e foram selecionados os blocos abaixo desta topografia. 



65 

 

 

 

Os blocos dentro da zona mineralizada foram separados em blocos internos - aqueles 

que fazem parte do quartzito (zona enriquecida) - e em blocos externos (região mais pobre), 

os quais são compostos da geologia da rocha encaixante (filito e meta-arenito). Os valores 

destes blocos foram gerados para a variável cobre por Simulação Sequencial Gaussiana, da 

qual uma realização foi tomada ao acaso. A simulação não condicional buscou reproduzir 

um semivariograma real obtido em um depósito equivalente em geologia e variabilidade. Os 

teores foram gerados dentro da zona mineralizada e fora da mesma utilizando diferentes 

modelos de semivariogramas, conforme apresentado na Tabela 1.  

 

Tabela 1 –Semivariograma da região mineralizada interna (quartzito) e a da externa (fora do 

quartzito). 

 Zona 

Mineralizada  

Estéril 

Tipo Esférico Esférico 

Efeito pepita 0,2 0,2 

Patamar 0,8 0,8 

Eixo Maior 155 255 

Eixo Médio 90 170 

Eixo Mínimo 55 130 

Rotação em Z 0 0 

Rotação em X 0 0 

Rotação em Y 45 45 

 

Foram feitas 100 realizações para cada região e os teores foram associados apenas 

aos blocos dentro de cada um destes volumes. Os resultados foram, então transformados do 

domínio gaussiano para o domínio real de acordo com a distribuição original dos dados que 

foram utilizados para gerar o semivariograma. 

Os valores simulados de cobre foram atribuídos aos blocos que já apresentavam a 

geologia local. Por último, os valores de densidade foram gerados a partir da média 

ponderada da quantidade de cada minério no bloco e de uma variação da densidade 

aproximada de cada litotipo.  



66 

 

 

 

 

O perfil com a topografia, é apresentado na Figura 18, possibilita observar a 

distribuição das camadas litológicas e ver que em determinadas regiões os minérios são 

aflorantes – o corpo rico é o quartzito, mas há uma nuvem de minério ao seu redor. Além 

disso, foi feito o mapa geológico da região que é apresentado na Figura 19. Também é 

possível ver as dobras da rocha encaixante que foram formadas antes do cisalhamento e 

mineralização. O perfil (Figura 18) mostra a componente inversa da falha, além do quartzito 

rico em cobre e as dobras intervaladas de filitos e meta-arenitos que formam a rocha 

encaixante. 

 

  

Figura 18 – Perfil A-A’ do mapa geológico. 

 



67 

 

 

 

 

Figura 19 – Mapa geológico do depósito sintético. 



68 

 

 

 

 

7. Resultados  

7.1  Amostragem 

A amostragem reproduz furos de sonda coletados a cada 40 metros, 

aproximadamente. As amostras, ao longo dos furos possuem tamanhos irregulares com 

comprimento médio de 60 cm. As amostras foram regularizadas por bancadas, com bancadas 

de 5 metros e, a partir destas informações, o corpo de minério foi delineado considerando o 

teor de cobre como fator de definição do minério. 

A Figura 20 apresenta uma seção com a legenda de litologia e o corpo de minério 

enquanto a Figura 21 mostra uma imagem tridimensional a topografia, distribuição das 

sondagens e o do corpo de minério definido a partir da análise dos furos.  

 

 

Figura 20 – Perfil representando as litologias e o perímetro do corpo de minério interpretado. 

 

 



69 

 

 

 

 

Figura 21 – Modelo tridimensional do corpo de minério, em vermelho, definido a partir das 

sondagens, em azul e em verde é mostrado a topografia.  

 

Na Tabela 2 são apresentadas as estatísticas descritivas de cobre do modelo real de 

todos os blocos dentro do volume do corpo de minério e a Figura 22 o histograma desta 

distribuição. 

 

Figura 22 – Histograma dos dados reais de cobre dentro do corpo mineralizado. 

 



70 

 

 

 

 

Tabela 2 – Estatísticas descritivas dos dados reais de cobre em toda a zona mineralizada. 

REAL 

Mínimo 
Primeiro 

Quartil 
Mediana Média 

Desvio 

padrão 

Terceiro 

Quartil 
Máximo 

Coeficiente 

de Variação 

0 0,5 0,5 0,7 0,4 0,7 9,8 58,7 

 

As estatísticas descritivas do modelo real podem ser comparadas às dos dados dos 

furos regularizados (Tabela 3), a partir da comparação, conclui-se que a amostragem com 

40 metros de distância entre furos foi adequada, uma vez que ambas estatísticas descritivas 

são iguais ou similares. O histograma dos furos de sonda é apresentado na Figura 23, onde 

pode ser observada a assimetria positiva.  

 

 Tabela 3 - Estatísticas descritivas dos dados amostrados nos furos de sonda dentro do corpo de 

minério. 

FUROS 

Mínimo 
Primeiro 

Quartil 
Mediana Média 

Desvio 

padrão 

Terceiro 

Quartil 
Máximo 

Coeficiente 

de Variação 

0,2 0,5 0,5 0,7 0,4 0,7 5,8 55,6 

 

 

Figura 23 – Histograma dos dados amostrados em furos de sonda (DH – drill hole) de cobre dentro 

do corpo mineralizado. 

 



71 

 

 

 

Outra comparação para aferir a qualidade da sondagem foi realizada por meio de 

gráficos do tipo boxplots dos valores reais e dos valores amostrados, ambos (com e sem 

outliers) dentro do corpo de minério (Figura 24), observando-se a figura pode-se concluir 

que a amostragem é representativa. Ou seja, a distribuição dos dados amostrados é adequada 

para a representação do corpo de minério do depósito sintético. 

 

 

Figura 24 – Boxplot dos dados reais e dos amostrados com e sem outliers. 

 

7.2  Análise geoestatística 

A geoestatística realizada foi dividida em três etapas. A primeira, para a realização 

da krigagem ordinária das variáveis cobre e densidade aparente. A segunda etapa foi a 

Simulação Sequencial Gaussiana dos dados de cobre e, finalmente na terceira etapa, foi 

realizada a Simulação Baseada em Wavelets (Wavesim) dos dados de cobre, para tal, o 

resultado da krigagem ordinária da variável cobre foi utilizado como imagem de 

treinamento. Os resultados de todas estas análises foram utilizados na análise de lucro por 

bloco, que gera o valor potencial esperado de cada bloco. 



72 

 

 

 

 

 

7.2.1 Krigagem 

A krigagem ordinária da densidade aparente, apresentada em detalhe no Anexo 1, foi 

realizada para que o valor potencial de cada bloco pudesse ser calculado. A variável cobre 

foi analisada de dois modos diferentes e o modelo de blocos foi estimado por krigagem 

ordinária para cada um dos modos. A primeira estimativa foi realizada para criar uma 

imagem de treinamento da Simulação Baseada em Wavelets e a segunda estimativa, 

apresentada no Anexo 2, foi calculada a partir das amostras contidas no corpo de minério e 

utilizada para comparação com os métodos de simulação. 

A análise geoestatística iniciou-se pela análise exploratória da variável cobre e 

interpretou-se a presença de anisotropia mista representada por um elipsoide com eixo maior 

no azimute 0º e mergulho também igual a 0º, o eixo médio posicionado em 90º de azimute 

e 45º de mergulho, enquanto o eixo menor tem posição 270º/45º. 

Na Figura 25 são apresentados o variograma experimental e seu respectivo modelo 

teórico da variável cobre. Os valores ajustados para o modelo teórico de variograma podem 

ser observados na Tabela 4. 

 

 Tabela 4 – Modelo teórico de variograma da variável cobre. 

Modelo do Variograma do Cobre 

Efeito pepita 0,05 

Tipo Esférico 

Estrutura 1 2 

Patamar 0,1 0,02 

Eixo Maior 208 ? 

Eixo Médio 160 ? 

Eixo Mínimo 95 95 

Rotação em Z 0 0 

Rotação em X 0 0 

Rotação em Y 45 45 

 



73 

 

 

 

 

Figura 25 – Variogramas experimentais com a quantidade de pares utilizados para seu cálculo e 

modelos teóricos de variograma do cobre ajustados. 

 

Com o modelo teórico de variograma definido, a krigagem ordinária foi calculada 

utilizando a vizinhança caracterizada pela busca por octantes, com no mínimo 3 octantes 

preenchidos com amostras, 1 amostra por octante e 2 no máximo. O elipsoide de busca é 

rotacionado em 45º no eixo Y, deixando seus eixos nas mesmas direções definidas no 

elipsoide que representa a anisotropia, os raios de busca têm 208 metros no eixo maior, 160 



74 

 

 

 

 

metros no eixo médio e 95 metros no eixo menor. O modelo de blocos utilizado possui blocos 

com 10 metros nas direções X e Y e 5 metros na direção Z e 30 blocos ao longo de X e 60 

blocos ao longo Y e Z. O resultado da krigagem ordinária do Cu é apresentado na Figura 26, 

onde à esquerda pode-se observar toda a estimativa e à direita apenas a região compreendida 

pelo corpo de minério. 

 

Figura 26 – Resultado da krigagem para o volume do depósito inteiro (Imagem de Treinamento) e a 

imagem da direita é a região considerada como corpo do minério deste volume . 

 

Para verificar o resultado da krigagem foram comparadas as estatísticas descritivas 

dos teores estimados no modelo de blocos (Tabela 5) àquelas dos furos de sonda (Tabela 3). 

Os quartis avaliados (primeiro e terceiro) são iguais em ambas as distribuições e as medidas 

de tendência central são equivalentes, assim como o desvio padrão. Devido à suavização 

inerente à krigagem, o coeficiente de variação é menor, bem como o valor máximo obtido 

pela krigagem. O histograma dos teores de cobre estimados (Figura 27) também possui 

assimetria positiva, porém menos pronunciada do que a observada no histograma da Figura 

23. 

 

Tabela 5 – Estatística descritiva dos dados krigados. 

Krigagem Ordinária (Imagem de Treinamento) 

Mínimo 
Primeiro 

Quartil 
Mediana Média 

Desvio 

padrão 

Terceiro 

Quartil 
Máximo 

Coeficiente 

de Variação 

0 0.5 0.6 0.6 0.3 0.7 2.2 42.3 



75 

 

 

 

 

 

 Figura 27– Histograma dos teores de cobre estimados por krigagem ordinária que serão utilizados 

como Imagem de Treinamento. 

 

7.2.2 Simulação Sequencial Gaussiana 

Para a simulação, os dados foram transformados em uma gaussiana normal com 

média zero e variância igual a um (N[0,1]), para isto foi utilizado o método de transformação 

por Normal Score, que gera a distribuição a partir da comparação das funções densidades 

acumuladas dos dados e de uma distribuição gaussiana conhecida.  

Com os dados transformados, a análise exploratória foi realizada e, a partir dos 

resultados, interpretou-se como os eixos principais do elipsoide que representa a anisotropia 

as direções norte com mergulho 0º, oeste com mergulho 45º e leste com mergulho 45º como, 

respectivamente, os eixos maior, intermediário e menor. Apresenta-se, na Figura 28, o 

variograma experimental calculado e seu respectivo modelo teórico de variograma. 

 



76 

 

 

 

 

 

 

 

Figura 28 – Variograma experimental com a quantidade de pares utilizados para seu cálculo e 

modelo teórico de variograma da variável cobre transformada em uma Gaussiana Normal [0,1]. 

 

Os parâmetros do modelo teórico de variograma ajustado são apresentados na Tabela 

6. 



77 

 

 

 

 Tabela 6 – Parâmetros do modelo teórico de Variograma do cobre transformado em uma Gaussiana 

Normal [0,1]. 

Modelo do Variograma do Cobre 

Transformado em Gaussiana [0,1] 

Tipo Esférico 

Efeito pepita 0,0 

Patamar 1 

Eixo Maior 103 

Eixo Médio 69 

Eixo Mínimo 30 

Rotação em Z 0 

Rotação em X 0 

Rotação em Y 45 

 

A vizinhança para a simulação foi definida por um elipsoide de busca com os eixos 

rotacionados conforme as direções interpretadas na análise exploratória e dimensões de 103 

m, 69 m, 30 m como eixos maior, intermediário e menor respectivamente. A busca foi 

dividida em octantes com ao menos 3 octantes preenchidos, e com no mínimo 1 e máximo 

2 amostras por octante. Foram utilizadas, no máximo, 5 pontos amostrais e 4 nós 

previamente simulados, para a simulação de novos nós.  

Após a simulação os resultados foram transformados para sua distribuição original. 

Para ilustrar os resultados obtidos, escolhidas as realizações 100, 1 e 65 conforme pode-se 

observar na Figura 29, Figura 30 e Figura 31 respectivamente. 

 



78 

 

 

 

 

 

 Figura 29 - Realização nº100 da Simulação Sequencial Gaussiana para os dados de cobre do 

depósito sintético. A imagem da esquerda mostra o modelo de blocos total e da direita apenas do corpo de 

minério. 

 

Figura 30 - Realização nº1 da Simulação Sequencial Gaussiana. A imagem da esquerda mostra o 

modelo de blocos total e da direita apenas do corpo de minério. 

 



79 

 

 

 

 

Figura 31 - Realização nº65 da Simulação Sequencial Gaussiana. A imagem da esquerda mostra o 

modelo de blocos total e da direita apenas do corpo de minério. 

 

Com os resultados destas 100 realizações, foi feita a média de valores para cada bloco 

(e-type) e suas estatísticas descritivas são apresentadas na Tabela 7. Comparando as 

estatísticas descritivas da média das simulações (e-type) àquelas dos furos de sonda (Tabela 

3) e aos dados reais (Tabela 2) observam-se algumas diferenças, mas, de modo geral, todas 

as distribuições são semelhantes, o que pode ser conferido pela observação do histograma 

apresentado na Figura 32. 

 

 Tabela 7 – Estatísticas descritivas do e-type dos blocos simulados para teores de cobre. 

SGS 

Mínimo 
Primeiro 

Quartil 
Mediana Média 

Desvio 

padrão 

Terceiro 

Quartil 
Máximo 

Coeficiente 

de Variação 

0,3 0,5 0,6 0,7 0,2 0,7 3,7 35,9 

 



80 

 

 

 

 

 

 Figura 32 – Histograma do e-type dos teores de cobre nos blocos simulados pela Simulação 

Sequencial Gaussiana. 

 

7.2.3 Simulação Baseada em Wavelets (Wavesim) 

A Wavesim utiliza como base a imagem de treinamento, de onde são coletados todos 

os possíveis padrões a serem considerados na simulação. A imagem de treinamento utilizada 

foi criada por krigagem ordinária conforme apresentado em 7.2.1 e este resultado pode ser 

observado na Figura 26. 

A vizinhança de busca é em unidades matriciais – cada unidade matricial é 

representada por um bloco, ou seja 10 m para X e Y e 5 m para Z - e foi utilizada uma busca 

isométrica com 11 unidades para as três direções. Este número foi escolhido para que a busca 

encontre ao menos 2 furos de sonda distintos em todos os blocos a serem simulados, uma 

vez que os furos estão distanciados em 4 unidades. Além disso, o tamanho dos modelos (ou 

retalhos) que serão alocados no centro do bloco que será simulado é de 5 unidades.  

Foram feitas 100 realizações e o número de grupos de classes escolhido foi de 350 – 

este número é idealmente o número de possíveis padrões que existem. Uma vez que a 

quantidade de padrões é muito alta foi escolhido um número elevado que não comprometesse 



81 

 

 

 

demasiadamente o tempo de processamento. Além disso, a verificação visual dos resultados 

foi considerada satisfatória. 

Outro ponto a ser destacado é que a simulação ocorre apenas quando o número de 

blocos em cada direção é constante. Neste caso a topografia e o corpo de minério, como 

estão apresentados na Figura 33, Figura 34 e Figura 35, foram recortadas posteriormente. A 

Figura 33 mostra a realização número 100, nela pode-se observar que nos locais não 

amostrados a simulação criou um corpo de minério artificial. Isto ocorre, pois não há o 

controle dos dados naquele local e, sem amostragem, qualquer padrão pode aparecer. Além 

disso, tratando-se de uma simulação sequencial, ou seja, que utiliza os dados previamente 

simulados, esta feição tende a crescer por propagação. A Figura 34 e a Figura 35 mostram, 

respectivamente, as realizações nº1 e nº65 onde o corpo de minério está definido da maneira 

esperada. 

 

 

 Figura 33 - Realização nº100 da Wavesim para os dados de cobre do depósito sintético. Observa-se 

a presença de um corpo mineralizado que não encontra correspondência amostral. 

 

 



82 

 

 

 

 

 

Figura 34 - Realização nº1 da Wavesim para os dados de cobre do depósito sintético. A imagem da 

esquerda mostra o modelo de blocos total e da direita apenas do corpo de minério. 

 

 

Figura 35 - Realização nº65 da Wavesim para os dados de cobre do depósito sintético. A imagem da 

esquerda mostra o modelo de blocos total e da direita apenas do corpo de minério. 

 

Com os resultados da simulação a média de cada bloco simulado (e-type) foi 

calculada a fim de compará-las aos resultados obtidos por krigagem, SGS e aos furos de 

sonda. As estatísticas descritivas estão apresentadas na Tabela 8 e é possível observar que 

os resultados são semelhantes aos resultados da krigagem, com quartis (primeiro, mediana e 



83 

 

 

 

terceiro) e média idênticos, enquanto o valor máximo é igual ao dos furos e da simulação, 

ou seja, os resultados não foram suavizados como na krigagem. 

 Tabela 8 – Estatísticas descritivas do e-type do Wavesim para os teores de cobre. 

WS 

Mínimo 
Primeiro 

Quartil 
Mediana Média 

Desvio 

padrão 

Terceiro 

Quartil 
Máximo 

Coeficiente 

de Variação 

0 0,5 0,6 0,6 0,2 0,7 5,8 31,3 

 

O histograma da média das realizações da Simulação Baseada em Wavelets (Figura 

36) é assimétrico positivo e apresenta concentração de dados próximos do teor 0,5, assim 

como os histogramas da Simulação Sequencial Gaussiana, da krigagem, da amostragem e 

dos dados originais.  

 

 

Figura 36 – Histograma da média do teor de cobre dos blocos simulados pelo método Wavesim. 

 

7.3  Análise do lucro por bloco 

Para a realização da análise de lucro por bloco foi escrito um programa de 

computador na linguagem Python utilizando o algoritmo apresentado no item 5. Para isto, o 



84 

 

 

 

 

preço do minério (MP) de cobre foi considerado como R$0,012 por grama. A recuperação 

da pilha de minério rico (pilha i=0) é 92% e o custo total de lavra para cada bloco é de 

R$7727,00. A pilha de minério pobre (pilha i=1) tem com recuperação de 75% e o custo por 

bloco é igual a R$6075,00. A terceira pilha é de estéril (pilha i=2), na qual todo material é 

descartado e o custo é de R$1000,00 por bloco.  

O volume do bloco (V) apresenta 500 m3. A densidade do minério é considerada 

variável e específica para cada bloco, seu valor foi calculado por krigagem ordinária a partir 

da densidade das amostras contidas na sondagem como apresentado no Anexo 1. Outra 

variável é o valor simulado em cada bloco. 

O programa calcula o valor potencial (VP) de cada bloco e a saída são dois arquivos, 

o primeiro com valor potencial de cada bloco e o segundo em qual pilha cada bloco deve ir. 

Para os valores reais dos blocos do depósito, a média, valor mínimo e valor máximo 

do valor potencial resultaram, respectivamente, em R$2.748,60, R$-1.000,00, 

R$318.160,60. Enquanto a média das realizações da Simulação Sequencial Gaussiana 

apresentou valores R$2.692,40, R$-1.000,00, R$59.994,40 e a média das realizações da 

Wavesim R$1.776,50, R$-1.000,00, R$98.379,40, respectivamente como valor médio, 

mínimo e máximo. Isto mostra que a Wavesim conseguiu reproduzir melhor os valores altos 

apresentados no depósito. Os histogramas destes valores potenciais calculados para a média 

de cada simulação é apresentado na Figura 37 e os histogramas dos valores médios da SGS 

e da Wavesim estão na Figura 38 e na Figura 39. A comparação destes resultados mostra 

que o formato da distribuição é semelhante e que a quantidade de blocos do Wavesim 

representou melhor os blocos com valor mais baixo.  

Por outro lado, ao comparar a quantidade de blocos que foram para cada pilha a SGS 

alocou em média 6947 na primeira pilha (minério rico), 8135 na segunda pilha (minério 

pobre) e 1685 na terceira (estéril). E o Wavesim colocou em média 6684 blocos na primeira, 

5437 na segunda e 4645 na terceira, enquanto os valores corretos são 6423, 7641, 2703 , 

respectivamente.  



85 

 

 

 

 

Figura 37 – Histograma dos valores reais de cada bloco do depósito. 

 

Figura 38 – Histograma da media dos valores potenciais de cada bloco simulado por SGS.  

 



86 

 

 

 

 

 

Figura 39 – Histograma da media dos valores potenciais de cada bloco simulado por Wavesim. 

 

8.  Discussão dos Resultados 

Apresentam-se na Figura 40 os boxplots dos teores de cobre dos dados reais e dos 

teores de cobre calculados por krigagem ordinária, Simulação Sequencial Gaussiana e 

Simulação Baseada em Wavelets. Os boxplots à esquerda, na Figura 40, ilustram toda a 

distribuição de frequências e à direita ilustram as distribuições dos teores de cobre sem os 

outliers, porém todos os valores da distribuição são utilizados para os cálculos. Comparando-

se estes gráficos pode-se inferir que a distribuição dos resultados e dos dados reais são 

similares e nenhuma técnica de predição reproduziu as caudas da distribuição. 



87 

 

 

 

 

Figura 40 – Boxplot da quantidade de cobre real, krigada (OK) e simulada pelos métodos de 

Simulação Sequencial Gaussiana (SGS) e Wavesim (WS). 

 

Outra maneira de comparar os métodos foi calcular a soma cumulativa dos valores 

potenciais dos blocos para todas as realizações, estes gráficos são apresentados na Figura 41. 

A linha vermelha mostra a evolução do lucro potencial, atingindo o pico ao redor de R$46 

milhões. A linha verde mostra o crescimento do lucro potencial dos blocos resultantes da 

krigagem ordinária do corpo de minério e seu valor máximo é de R$43,7 milhões. As linhas 

pretas (Figura 41a) representam a evolução dos lucros de cada realização da Simulação 

Sequencial Gaussiana, com lucro máximo de R$50,9 milhões, mínimo de R$39,6 milhões e 

média de R$45,1 milhões. Enquanto as linhas azuis (Figura 41b) são referentes às realizações 

da Simulação Baseada em Wavelets. Sua gama de possibilidades é muito maior e a realização 

com maior soma do valor potencial apresenta R$43 milhões de lucro, a mínima é de R$13,8 

milhões de lucro e média de R$29,8 milhões de lucro. A linha amarela representa a evolução 

dos dados de cobre krigados no depósito inteiro (imagem de treinamento) e teve máximo de 

R$34,7 milhões de lucro. Na Figura 41c pode-se comparar todos os resultados justapostos. 



88 

 

 

 

 

 

Figura 41 – Soma cumulativa dos valores potenciais de cada bloco para todas as realizações da 

Simulação Sequencial Gaussiana e da Simulação Baseada em Wavelets, dos blocos reais e dos blocos de 

krigados. A imagem (a) são os valores dos lucros potenciais de cada realização da SGS. A imagem (b) são os 

valores dos lucros potenciais de cada realização da Wavesim. Enquanto a imagem (c) é a justaposição de (a) 

e (b). 

 

Com os resultados referentes à locação de cada bloco em pilhas, é possível fazer a 

comparação de ambos métodos de simulação, uma vez que é conhecido o valor real de cada 

bloco. Assim, foram verificados quantos blocos foram para as pilhas corretas, quantos blocos 

de estéril foram considerados como minério e quantos blocos de minério foram considerados 

como estéril. Estes resultados estão apresentados na Figura 42. 



89 

 

 

 

 

Figura 42 – Boxplot da distribuição da quantidade de blocos alocados nas pilhas corretas, blocos de 

estéril considerados como minério e blocos de minério considerados como estéril. 

 

Para melhor comparação entre as simulações, as estatísticas descritivas da quantidade 

de blocos alocados corretamente em cada realização são apresentadas na Tabela 9. É notável 

que o número de blocos corretos, em média, é superior no método Simulação Sequencial 

Gaussiana (SGS). Entretanto, o coeficiente de variação é muito maior para a Simulação 

Baseada em Wavelets. 

 

 Tabela 9 – Estatística descritiva da quantidade de blocos que cada realização acertou, para a 

Simulação Sequencial Gaussiana (SGS) e Wavesim (WS). 

Blocos Certos 

Método Mínimo 
Primeiro 

Quartil 
Mediana Média 

Desvio 

padrão 

Terceiro 

Quartil 
Máximo 

Coeficiente 

de Variação 

SGS 9838 10052,8 10130 10123 103 10190,2 10324 1 

WS 6189 7360,8 7543 7532,9 349,5 7728,2 8245 4,6 

 

 



90 

 

 

 

 

As estatísticas descritivas dos dados de blocos de estéril considerados como minério 

por ambos os métodos são apresentados Tabela 10 e como já observado no boxplot o método 

SGS em média superestimou menos blocos. Além disso, novamente, o coeficiente de 

variação é considerado alto. 

 

 Tabela 10 – Estatística descritiva da quantidade de blocos que cada realização acertou, para a 

Simulação Sequencial Gaussiana (SGS) e Wavesim (WS). 

Blocos de estéril considerados como minério 

Método Mínimo 
Primeiro 

Quartil 
Mediana Média 

Desvio 

padrão 

Terceiro 

Quartil 
Máximo 

Coeficiente 

de Variação 

SGS 998 1355,8 1441 1426,8 123,3 1509,5 1755 8,6 

WS 1791 1874 1895 1898 41,5 1922,5 2027 2,2 

 

As estatísticas descritivas da quantidade de blocos de minério considerados como 

estéril podem ser observadas na Tabela 11. A quantidade de bloco de minério que foram 

considerados como estéril é maior na Wavesim, e o coeficiente de variação é notavelmente 

maior no Wavesim. Isto ocorre nas três comparações da distribuição dos blocos em cada 

realização, uma vez que esta simulação produz um leque de possibilidades maior. 

Consequentemente, a gama de possíveis rendimentos futuros é maior, como foi ilustrado na 

Figura 41 (leque de possibilidades). 

 

Tabela 11 – Estatística descritiva da quantidade de blocos que cada realização acertou para a 

Simulação Sequencial Gaussiana (SGS) e Wavesim (WS). 

Blocos de minério considerados como estéril 

Método Mínimo 
Primeiro 

Quartil 
Mediana Média 

Desvio 

padrão 

Terceiro 

Quartil 
Máximo 

Coeficiente 

de Variação 

SGS 758 842,2 883,5 880,4 58,3 917,2 1065 6,6 

WS 2315 3053,5 3323 3369,7 507,5 3652,5 5650 15,1 

 

Outra abordagem de comparação dos resultados destes métodos foi verificar quanto 

dinheiro, em Reais, foi deixado de ganhar, ou seja, minério sendo considerado como estéril 

(perda real). E quanto prejuízo se teve com blocos simulados como minério que eram blocos 

de estéril (diluição). Esta análise foi feita com a soma dos valores potencias dos blocos 

alocados indevidamente, tanto superestimados, quanto subestimados. 



91 

 

 

 

Pelo gráfico boxplot (Figura 43) nota-se que os resultados de ambos os métodos 

considerou aproximadamente a mesma quantidade de estéril como minério. Além disso, 

ressalta-se a quantidade de minério considerado como estéril pelo método Wavesim. A 

Simulação Sequencial Gaussiana mostrou menor subestimação dos blocos e superestimação 

equivalente. 

 

 Figura 43 – Gráfico boxplot de quantos Reais de minério real foram classificados como estéril e 

quantos Reais esperados eram estéril. 

 

Os resultados da estatística descritiva dos valores monetários que foram 

equivocadamente considerados minério pelas simulações são apresentados na Tabela 12. Os 

valores de mínimo e primeiro quartil são consideravelmente menores, enquanto, o terceiro 

quartil e máximo são consideravelmente maiores na Wavesim. E as médias diferem em 

apenas R$17.329,00. 

 

 



92 

 

 

 

 

 Tabela 12 – Estatística descritiva do total de Reais que foram considerados como lucro e eram 

prejuízo. 

Total de estéril considerado como minério (R$) 

Método Mínimo 
Primeiro 

Quartil 
Mediana Média 

Desvio 

padrão 

Terceiro 

Quartil 
Máximo 

Coeficiente 

de Variação 

SGS 3,4x106 3,9x106 4,0x106 4,1x106 4,1x105 4,3x106 5,4x106 9,9 

WS 2,2x106 3,7x106 4,1x106 4,1x106 6,7x105 4,6x106 5,7x106 16 

 

As estatísticas descritivas do total de Reais dos blocos considerados como estéril e 

na verdade gerariam lucro são apresentadas na Tabela 13. Desta vez a diferença da 

quantidade de dinheiro sendo considerado como estéril é enorme e todos os resultados da 

estatística descritiva apontam a Simulação Sequencial Gaussiana como o método que menos 

errou. Como já mencionado, os valores de coeficiente de variação são elevados no método 

Wavesim pois a série de possíveis valores é maior. 

 

 Tabela 13 – Estatística descritiva do total de Reais que foram considerados como estéril e gerariam 

lucro. 

Total de minério considerado como estéril (R$) 

Método Mínimo 
Primeiro 

Quartil 
Mediana Média 

Desvio 

padrão 

Terceiro 

Quartil 
Máximo 

Coeficiente 

de Variação 

SGS 1,2x106 1,3x106 1,4x106 1,4x106 1,3x105 1,5x106 1,9x106 9,4 

WS 6,4x106 8,9x106 1,0x107 1,0x107 2,2x105 1,2x107 1,9x107 20,8 

 

 

9. Considerações Finais 

A Simulação Sequencial Gaussiana é amplamente conhecida, utilizada e consagrada 

nas análises geoestatísticas. Por outro lado, as simulações de multiponto estão ganhando 

espaço principalmente na análise de dados categóricos. Para dados contínuos discute-se 

muito sobre a qualidade da imagem de treinamento e como esta deve ser construída. Neste 

trabalho foi utilizada a krigagem ordinária dos dados, uma vez que este é o melhor estimador 

linear não enviesado, ou seja, é o melhor estimador para precisão local dos dados. 

A comparação matemática indica que a Simulação Sequencial Gaussiana é favorável 

para estimar a quantidade de minério. Esta simulação mostrou maior exatidão e precisão nas 



93 

 

 

 

realizações, uma vez que todos os seus resultados estão mais próximos do real que os 

resultados das realizações da Simulação Baseada em Wavelets. Além disso, suas realizações 

resultam na destinação dos blocos significativamente melhor dos blocos nas pilhas de 

processamento, consequentemente, considerou menos blocos de minério como estéril e 

menos estéril como minério.  

No entanto, é possível observar que os resultados das realizações da Simulação 

Baseada em Wavelets é um leque ao redor da imagem de treinamento. Isto mostra que o 

método é promissor, sempre que o usuário seja capaz de criar uma imagem de treinamento 

adequada, porém a criação do banco de padrões com valores e variabilidade espacial reais 

para cada depósito é a maior dificuldade deste método. 

Outro grande problema é que ainda há uma certa complexidade em parametrizar 

adequadamente o Wavesim, uma vez que, obrigatoriamente, deve-se conhecer cada conceito 

da técnica - que possivelmente são conceitos novos para o geoestatístico tradicional. Além 

disso, a preparação do banco de dados e da imagem de treinamento é demasiadamente 

demorada comparada ao tempo de preparo dos dados para a Simulação Sequencial 

Gaussiana. Outro ponto negativo é a escolha da quantidade de agrupamentos necessária para 

a realização da simulação. Teoricamente, deve-se utilizar a quantidade de possíveis padrões, 

entretanto, na prática, este número é muito grande, portanto, deve-se escolher um número 

que seja grande o suficiente para representar os padrões desejados e, ao mesmo tempo, 

pequeno o suficiente para a simulação não consumir muito tempo de processamento. Devido 

a estes fatores, este método é usualmente mais demorado que a Simulação Sequencial 

Gaussiana. 

Por último, a Simulação Baseada em Wavelets demonstrou que a evolução das 

estatísticas multiponto para dados contínuos é viável e os resultados obtidos apresentam uma 

gama de possibilidades maiores que os resultados apresentados na Simulação Sequencial 

Gaussiana. Isto pode ocorrer devido à utilização de estatística de alta ordem, capturadas na 

decomposição das imagens de treinamento.  



94 

 

 

 

 

Referências Bibliográficas 

Armstrong M., 1998, Basic linear geostatistics, Springer, Berlin, p. 1-12; 59-63. 

Arpat B. e Caers J., 2005, A multiple scale, pattern-based approach to sequential 

simulation, in Leuangthong, O. e Deutsch, C. V., eds., Geostatistics Banff 2004, Springer, 

Dordrecht, p. 255–264. 

Arpat B. e Caers J., 2007, Conditional simulation with patterns. Mathematical 

Geology 39, p. 181-182 

Caers J., 2011, Modeling uncertinty in the Earth Sciences, Wiley-Blackwell, p.104-

106 

Caers J. e Zhang, T., 2002, Multiple-point geostatistics: a quantitative vehicle for 

integrating geologic analogs into multiple reservoir models, Stanford University, Stanford 

Center for Reservoir Forecasting, Stanford, CA, p.1-9 

Chatterjee S., Dimitrakopoulos R., 2011, Multi-scale Stochastic Simulation with a 

wavelet-based approach, Computers &amp;amp; Geosciences 45, p.177-189 

Chatterjee S., Dimitrakopoulos R. e Mustapha H., 2012, Dimensional Reduction of 

Pattern-Based Simulation Using Wavelet Analysis, Mathematical Geosciences 44, p. 343-

374 

Chatterjee S., Mustapha H. e Dimitrakopoulos R., 2015, Fast wavelet-based 

stochastic simulation using training images, Computers and Geoscience 

Chilès J.P. e Delfiner P., 1999, Geostatistics: Modeling Spatial Uncertainty, Wiley-

Interscience Publication, p.449-592 

Deutsch C. V., 2002, Geostatistical Reservoir Modeling, Oxford University Press, p. 

131-138; p. 165  

Deutsch C. V. e Journel A.G., 1998, GSLIB: Geostatistical Software Library and 

User’s Guide, Oxford University Press, p.18-19; 119-147.  



95 

 

 

 

Dimitrakopoulos R., Mustapha H. e Gloaguen E., 2010, High-order statistics of 

spatial random fields: exploring spatial cumulants for modelling complex, non-Gaussian and 

non-linear phenomena, Mathematical Geosciences v.42, p. 65–99 

Duarte O. O., 2003, Dicionário enciclopédico Inglês-Português de Geofísica e 

Geologia, 2ª Edição, Sociedade Brasileira de Geofísica, Rio de Janeiro, p. 115; 144; 264-

265 

Gloaguen E. e Dimitrakopoulos R., 2009, Two-dimensional conditional simulations 

based on the wavelet decomposition of training images, Mathematical Geosciences, v. 41, 

no. 6, pp, 679-701 

Goovaerts P., 1997, Geostatistics for National Resources Evaluation, Oxford 

University Press, New York, p.369-404  

Grapes A., 1995, An introduction to wavelets, IEEE Computational Science and 

Engineering, vol. 2, num. 2, p. 50–61. 

Guardiano F. e Srivastava M., 1993, Multivariate Geostatistics: Beyond Bivariate 

Moments. In: Soares A., 1993, Geostatistics-Tróia '92, Kluwer Academic Publishers, 

Dordrecht, p. 133–144 

Honarkhah M. e Caers J., 2010, Stochastic Simulation of Patterns Using Distance-

Based Pattern Modeling, Mathematical Geosciences 42, p. 487 - 517 

Isaaks E. H. e Srivastava R. M., 1989, An Introduction to Applied Geostatistics. 

Oxford University Press, p. 278 – 279. 

Journel A. G., 1980, Geostatistical Simulation: method for exploration and Mine 

Planning. In: Geostatistics, McGraw Hill, New York, p. 93-106 

Journel A. G., 1989, Fundamentals of Geostatistics in Five Lessons, American 

Geophysical Union, Washington D.C., p. 10-13 

Journel A.G. e Deutsch C.V., 1993, Entropy and Spatial Disorder, Mathematical 

Geology 25, p. 336-354  



96 

 

 

 

 

Journel A. e Zhang T., 2006, The necessity of a Multiple-Point Prior Model, 

Mathematical Geology 38, p. 591-592; p. 597-600 

Landim, P. M. B., 1997, Análise estatística de dados geológicos, Editora UNESP, p. 

156–158 

Lyster S., Deutsch C. V. e Ortiz J. M., 2004, Shot Note: Some Implementation 

Aspects of Multiple-Point Simulation, CCG Annual Report Papers, p. 1-2 

Magalhães N. M., 2011, Probabilidade e Variáveis Aleatórias, 3ª Edição, EDUSP, 

São Paulo, p. 241 

Matheron G., 1963, Principles of Geostatistics, Economic Geology 58. In 

VanLandinghan S. L., 1983, Economic Evaluation of Mineral Property (Benchmark papers 

in geology), Hutchinson Ross Publishing Company, p. 157-177  

Miesch A. T., 1975, Variograms and Variance Components in Geochemistry and Ore 

Evaluation, Quantitative Studies in the Geological Sciences, E.H. Whitten, ed., Geol. Soc. 

America Mem. 142, 1975, p. 333-340. In VanLandinghan S. L., 1983, Economic Evaluation 

of Mineral Property (Benchmark papers in geology), Hutchinson Ross Publishing Company, 

p. 124-131 

Mustapha H., Chatterjee S., Dimitrakopoulos R. e Graf T., 2013, Geologic 

heterogeneity recognition using discrete wavelet transformation for surface flow solute 

transport simulations, Advances in Water Resources, vol. 54, p 22-37 

Olea R. A., 2003, Geostatistics for Engineers and Earth Scientists, Kluwer Academic 

Publishers, p. 141-162 

Ortiz J. M., 2003, Selected Aspects of Multiple-Point Statistics, CCG Annual Report 

Papers, p. 2-6  

Ouahabi A., 2012, Introduction to Multiresolution Analysis. In: Signal and Image 

Multiresolution Analysis, ISTE, London, p. 16- 32 

Remy N., Boucher A. e Wu J., 2011, Applied Geostatistics with SGeMS – A User’s 

Guide, Cambridge University Press, p. 168–214 



97 

 

 

 

Rendu J.M., 1976, Bayesian Decision Theory Applied to Mineral Exploration and 

Mine Validation, Advanced Geostatistics in the Mining Industry, M. Guarascio, M. David e 

C. Huibregts, eds., D. Reidel, Dordrecht, Holland, 1976, p.435-445. In VanLandinghan S. 

L., 1983, Economic Evaluation of Mineral Property (Benchmark papers in geology), 

Hutchinson Ross Publishing Company, p. 132-142 

Royle A. G., 1980, Why Geostatistics?. In: Geoestatistics, McGraw Hill, New York, 

p. 1-16 

Sinclair A.J. e Blackwell G.H., 2004, Applied Mineral Inventory Estimation, 

Combridge University Press, p. 284-292 

Strebelle S., 2002, Conditional simulation of complex geological structures using 

multiple point statistics, Mathematical Geosciences 34 p. 1-22 

Yamamoto J. K., 2001a, Análise Geoestatística. In: Yamamoto, J. K. , 2001, 

Avaliação e Classificação de Reservas Minerais, Edusp, p. 72-78  

Yamamoto J. K., 2001b, Métodos Computacionais. In: Yamamoto, J. K. , 2001, 

Avaliação e Classificação de Reservas Minerais, Edusp, p. 131 

Yamamoto J. K. e Landim P., M., B., 2013, Geoestatística: Conceitos e Aplicações, 

Oficina de Textos, São Paulo, p. 64-65 

Zhang T., Switzer P. e Journel A., 2006, Filter-Based Classification of Training 

Image Patterns for Spatial Simulation, Mathematical Geosciences 38, p. 63–80 

 

  



98 

 

 

 

 

ANEXO 1 – Cálculo da Densidade Aparente 

A variável densidade aparente foi calculada para o depósito inteiro através do método 

de krigagem ordinária. A importância desta variável é para o cálculo do valor potencial. 

Nesta análise, foram utilizados os dados de densidade nos furos de sonda. Primeiramente foi 

feita a análise exploratória dos dados o ajuste do modelo teórico de variograma (Tabela A1 

e Figura A1). 

Tabela A1 – Modelo teórico de Variograma da densidade aparente. 

Modelo do Variograma da 

Densidade Aparente 

Tipo Esférico 

Efeito pepita 0,005 

Patamar 0,012 

Eixo Maior 148 

Eixo Médio 78 

Eixo Mínimo 62 

Rotação em Z 90 

Rotação em X 45 

Rotação em Y 0 

. 

 

A krigagem ordinária para a densidade aparente foi realizada a partir da vizinhança 

com uma busca realizada por octantes, com no mínimo 3 octantes preenchidos com amostras, 

no mínimo 1 amostra por octante e 2 no máximo. O elipsoide de busca da densidade aparente 

é rotacionado em 90º no eixo Z e 45º no eixo X, deixando o elipsoide de busca na mesma 

direção que seu elipsoide de anisotropia. Estes eixos tem comprimentos de 148, 78 e 62 

metros no eixo maior, intermediário e menor, respectivamente.  

 

 

 

 

 



99 

 

 

 

 

 

 

Figura A1 – Variogramas experimentais com a quantidade de pares utilizados para seu cálculo e 

modelo do variograma teóricos de variograma do cobre ajustado. 

 

 

  



100 

 

 

 

 

ANEXO 2 – Krigagem Ordinária dos Dados Dentro do Corpo de Minério 

A krigagem ordinária dos dados de cobre restritos ao corpo do minério foi feita 

apenas para que os resultados possam ser comparados com os resultados da Simulação 

Sequencial Gaussiana e da Simulação Baseada em Wavelets. Neste estudo foi feita a análise 

exploratória dos dados, definido o modelo teórico de variograma (Tabela A2 e Figura A2), 

estatística descritiva dos resultados (Tabela A3), histograma dos resultados (Figura A3) e a 

representação tridimensional dos resultados pode ser vista na Figura A4. 

A vizinhança utilizada nesta estimativa dividiu o domínio em octantes, com no 

mínimo 3 octantes preenchidos com amostras, no mínimo 1 amostra por octante e 2 no 

máximo. O elipsoide de busca da variável cobre é rotacionado em 45º no eixo Y apenas. Os 

eixos da busca tem comprimento de 80, 68 e 25 metros no eixo maior, intermediário e menor, 

respectivamente.  

Tabela A2 – Modelo teórico de Variograma do cobre dentro do corpo de minério. 

Modelo do Variograma do Cobre 

Dentro do Corpo de Minério 

Tipo Esférico 

Efeito pepita 0,05 

Patamar 0,07 

Eixo Maior 80 

Eixo Médio 68 

Eixo Mínimo 25 

Rotação em Z 0 

Rotação em X 0 

Rotação em Y 45 

 

 

 

 



101 

 

 

 

 

 

 

Figura A2 – Variogramas experimentais com a quantidade de pares utilizados para seu cálculo e 

modelo do variograma teóricos de variograma do cobre ajustado. 

 



102 

 

 

 

 

 

Figura A3 – Histograma do resultado da krigagem ordinária do cobre dentro do corpo de minério. 

 

Figura A4 – Resultado da krigagem para o corpo de minério. 

 

Tabela A3 – Estatística descritiva do resultado da krigagem ordinária do cobre dentro do corpo de 

minério. 

OK 

Mínimo 
Primeiro 

Quartil 
Mediana Média 

Desvio 

padrão 

Terceiro 

Quartil 
Máximo 

Coeficiente 

de Variação 

0 0,5 0,6 0,7 0,3 0,7 2,7 37,3 

 


</field>
	</doc>
</add>